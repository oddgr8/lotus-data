{
    "papers": {
        "2409.05925": {
            "authors": [
                "Lars-Peter Meyer",
                "Johannes Frey",
                "Felix Brei",
                "Natanael Arndt"
            ],
            "title": "Assessing SPARQL capabilities of Large Language Models",
            "abstract": "The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs) offers significant synergistic potential for knowledge-driven applications. One possible integration is the interpretation and generation of formal languages, such as those used in the Semantic Web, with SPARQL being a core technology for accessing KGs. In this paper, we focus on measuring out-of-the box capabilities of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries applying a quantitative approach.   We implemented various benchmarking tasks in the LLM-KG-Bench framework for automated execution and evaluation with several LLMs. The tasks assess capabilities along the dimensions of syntax, semantic read, semantic create, and the role of knowledge graph prompt inclusion.   With this new benchmarking tasks, we evaluated a selection of GPT, Gemini, and Claude models. Our findings indicate that working with SPARQL SELECT queries is still challenging for LLMs and heavily depends on the specific LLM as well as the complexity of the task. While fixing basic syntax errors seems to pose no problems for the best of the current LLMs evaluated, creating semantically correct SPARQL SELECT queries is difficult in several cases.",
            "id": "2409.05925",
            "link": "http://arxiv.org/abs/2409.05925v1",
            "published": "2024-09-09T08:29:39+00:00",
            "updated": "2024-09-09T08:29:39+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.AI",
                "cs.CL",
                "cs.IR"
            ],
            "max_author_hindex": 15
        },
        "2409.07497": {
            "authors": [
                "Ningyu Zhang",
                "Zekun Xi",
                "Yujie Luo",
                "Peng Wang",
                "Bozhong Tian",
                "Yunzhi Yao",
                "Jintian Zhang",
                "Shumin Deng",
                "Mengshu Sun",
                "Lei Liang",
                "Zhiqiang Zhang",
                "Xiaowei Zhu",
                "Jun Zhou",
                "Huajun Chen"
            ],
            "title": "OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System",
            "abstract": "Knowledge representation has been a central aim of AI since its inception. Symbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can both represent knowledge. KGs provide highly accurate and explicit knowledge representation, but face scalability issue; while LLMs offer expansive coverage of knowledge, but incur significant training costs and struggle with precise and reliable knowledge manipulation. To this end, we introduce OneEdit, a neural-symbolic prototype system for collaborative knowledge editing using natural language, which facilitates easy-to-use knowledge management with KG and LLM. OneEdit consists of three modules: 1) The Interpreter serves for user interaction with natural language; 2) The Controller manages editing requests from various users, leveraging the KG with rollbacks to handle knowledge conflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the knowledge from the Controller to edit KG and LLM. We conduct experiments on two new datasets with KGs which demonstrate that OneEdit can achieve superior performance.",
            "id": "2409.07497",
            "link": "http://arxiv.org/abs/2409.07497v1",
            "published": "2024-09-09T16:46:47+00:00",
            "updated": "2024-09-09T16:46:47+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.DB",
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 54
        },
        "2409.05512": {
            "authors": [
                "Christian Himpe"
            ],
            "title": "DatAasee -- A Metadata-Lake as Metadata Catalog for a Virtual Data-Lake",
            "abstract": "Metadata management for distributed data sources is a long-standing but ever-growing problem. To counter this challenge in a research-data and library-oriented setting, this work constructs a data architecture, derived from the data-lake: the metadata-lake. A proof-of-concept implementation of this proposed metadata system is presented and evaluated as well.",
            "id": "2409.05512",
            "link": "http://arxiv.org/abs/2409.05512v1",
            "published": "2024-09-09T11:10:45+00:00",
            "updated": "2024-09-09T11:10:45+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.DL",
                "cs.IR",
                "H.2.8; H.3.3"
            ],
            "max_author_hindex": 14
        },
        "2409.05677": {
            "authors": [
                "Tuba Gokhan",
                "Kexin Wang",
                "Iryna Gurevych",
                "Ted Briscoe"
            ],
            "title": "RegNLP in Action: Facilitating Compliance Through Automated Information Retrieval and Answer Generation",
            "abstract": "Regulatory documents, issued by governmental regulatory bodies, establish rules, guidelines, and standards that organizations must adhere to for legal compliance. These documents, characterized by their length, complexity and frequent updates, are challenging to interpret, requiring significant allocation of time and expertise on the part of organizations to ensure ongoing compliance.Regulatory Natural Language Processing (RegNLP) is a multidisciplinary subfield aimed at simplifying access to and interpretation of regulatory rules and obligations. We define an Automated Question-Passage Generation task for RegNLP, create the ObliQA dataset containing 27,869 questions derived from the Abu Dhabi Global Markets (ADGM) financial regulation document collection, design a baseline Regulatory Information Retrieval and Answer Generation system, and evaluate it with RePASs, a novel evaluation metric that tests whether generated answers accurately capture all relevant obligations and avoid contradictions.",
            "id": "2409.05677",
            "link": "http://arxiv.org/abs/2409.05677v1",
            "published": "2024-09-09T14:44:19+00:00",
            "updated": "2024-09-09T14:44:19+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CE",
                "cs.ET",
                "cs.IR"
            ],
            "max_author_hindex": 76
        },
        "2409.05806": {
            "authors": [
                "Tianhe Lu",
                "Jizhan Fang",
                "Yunzhi Yao",
                "Xin Xu",
                "Ningyu Zhang",
                "Huajun Chen"
            ],
            "title": "Benchmarking Chinese Knowledge Rectification in Large Language Models",
            "abstract": "While Large Language Models (LLMs) exhibit remarkable generative capabilities, they are not without flaws, particularly in the form of hallucinations. This issue is even more pronounced when LLMs are applied to specific languages and domains. For example, LLMs may generate nonsense information when handling Chinese ancient poetry, proverbs, or idioms, owing to the lack of specific knowledge. To this end, this paper introduces a benchmark for rectifying Chinese knowledge in LLMs via knowledge editing. Specifically, we introduce a new Chinese dataset, CKnowEdit, by collecting seven type of knowledge from various sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony, antithesis, and logical constructs inherent in the Chinese language. Through the analysis of this dataset, we uncover the challenges faced by current LLMs in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques on this dataset unveil the substantial scope for advancement in the rectification of Chinese knowledge. Code and dataset are available at https://github.com/zjunlp/EasyEdit.",
            "id": "2409.05806",
            "link": "http://arxiv.org/abs/2409.05806v1",
            "published": "2024-09-09T17:11:51+00:00",
            "updated": "2024-09-09T17:11:51+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 25
        },
        "2409.05401": {
            "authors": [
                "Arkadeep Acharya",
                "Rudra Murthy",
                "Vishwajeet Kumar",
                "Jaydeep Sen"
            ],
            "title": "NLLB-E5: A Scalable Multilingual Retrieval Model",
            "abstract": "Despite significant progress in multilingual information retrieval, the lack of models capable of effectively supporting multiple languages, particularly low-resource like Indic languages, remains a critical challenge. This paper presents NLLB-E5: A Scalable Multilingual Retrieval Model. NLLB-E5 leverages the in-built multilingual capabilities in the NLLB encoder for translation tasks. It proposes a distillation approach from multilingual retriever E5 to provide a zero-shot retrieval approach handling multiple languages, including all major Indic languages, without requiring multilingual training data. We evaluate the model on a comprehensive suite of existing benchmarks, including Hindi-BEIR, highlighting its robust performance across diverse languages and tasks. Our findings uncover task and domain-specific challenges, providing valuable insights into the retrieval performance, especially for low-resource languages. NLLB-E5 addresses the urgent need for an inclusive, scalable, and language-agnostic text retrieval model, advancing the field of multilingual information access and promoting digital inclusivity for millions of users globally.",
            "id": "2409.05401",
            "link": "http://arxiv.org/abs/2409.05401v1",
            "published": "2024-09-09T07:57:43+00:00",
            "updated": "2024-09-09T07:57:43+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.CL"
            ],
            "max_author_hindex": 11
        },
        "2409.06226": {
            "authors": [
                "Linfeng Zhang",
                "Changyue Hu",
                "Zhiyu Quan"
            ],
            "title": "NLP-Powered Repository and Search Engine for Academic Papers: A Case Study on Cyber Risk Literature with CyLit",
            "abstract": "As the body of academic literature continues to grow, researchers face increasing difficulties in effectively searching for relevant resources. Existing databases and search engines often fall short of providing a comprehensive and contextually relevant collection of academic literature. To address this issue, we propose a novel framework that leverages Natural Language Processing (NLP) techniques. This framework automates the retrieval, summarization, and clustering of academic literature within a specific research domain. To demonstrate the effectiveness of our approach, we introduce CyLit, an NLP-powered repository specifically designed for the cyber risk literature. CyLit empowers researchers by providing access to context-specific resources and enabling the tracking of trends in the dynamic and rapidly evolving field of cyber risk. Through the automatic processing of large volumes of data, our NLP-powered solution significantly enhances the efficiency and specificity of academic literature searches. We compare the literature categorization results of CyLit to those presented in survey papers or generated by ChatGPT, highlighting the distinctive insights this tool provides into cyber risk research literature. Using NLP techniques, we aim to revolutionize the way researchers discover, analyze, and utilize academic resources, ultimately fostering advancements in various domains of knowledge.",
            "id": "2409.06226",
            "link": "http://arxiv.org/abs/2409.06226v1",
            "published": "2024-09-10T05:41:40+00:00",
            "updated": "2024-09-10T05:41:40+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 31
        },
        "2409.05735": {
            "authors": [
                "Achille Fokoue",
                "Srideepika Jayaraman",
                "Elham Khabiri",
                "Jeffrey O. Kephart",
                "Yingjie Li",
                "Dhruv Shah",
                "Youssef Drissi",
                "Fenno F. Heath III",
                "Anu Bhamidipaty",
                "Fateh A. Tipu",
                "Robert J. Baseman"
            ],
            "title": "A System and Benchmark for LLM-based Q&A on Heterogeneous Data",
            "abstract": "In many industrial settings, users wish to ask questions whose answers may be found in structured data sources such as a spreadsheets, databases, APIs, or combinations thereof. Often, the user doesn't know how to identify or access the right data source. This problem is compounded even further if multiple (and potentially siloed) data sources must be assembled to derive the answer. Recently, various Text-to-SQL applications that leverage Large Language Models (LLMs) have addressed some of these problems by enabling users to ask questions in natural language. However, these applications remain impractical in realistic industrial settings because they fail to cope with the data source heterogeneity that typifies such environments. In this paper, we address heterogeneity by introducing the siwarex platform, which enables seamless natural language access to both databases and APIs. To demonstrate the effectiveness of siwarex, we extend the popular Spider dataset and benchmark by replacing some of its tables by data retrieval APIs. We find that siwarex does a good job of coping with data source heterogeneity. Our modified Spider benchmark will soon be available to the research community",
            "id": "2409.05735",
            "link": "http://arxiv.org/abs/2409.05735v2",
            "published": "2024-09-09T15:44:39+00:00",
            "updated": "2024-09-10T21:46:32+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.AI"
            ],
            "max_author_hindex": 46
        },
        "2409.06916": {
            "authors": [
                "Yongsu Ahn",
                "Quinn K Wolter",
                "Jonilyn Dick",
                "Janet Dick",
                "Yu-Ru Lin"
            ],
            "title": "Interactive Counterfactual Exploration of Algorithmic Harms in Recommender Systems",
            "abstract": "Recommender systems have become integral to digital experiences, shaping user interactions and preferences across various platforms. Despite their widespread use, these systems often suffer from algorithmic biases that can lead to unfair and unsatisfactory user experiences. This study introduces an interactive tool designed to help users comprehend and explore the impacts of algorithmic harms in recommender systems. By leveraging visualizations, counterfactual explanations, and interactive modules, the tool allows users to investigate how biases such as miscalibration, stereotypes, and filter bubbles affect their recommendations. Informed by in-depth user interviews, this tool benefits both general users and researchers by increasing transparency and offering personalized impact assessments, ultimately fostering a better understanding of algorithmic biases and contributing to more equitable recommendation outcomes. This work provides valuable insights for future research and practical applications in mitigating bias and enhancing fairness in machine learning algorithms.",
            "id": "2409.06916",
            "link": "http://arxiv.org/abs/2409.06916v1",
            "published": "2024-09-10T23:58:27+00:00",
            "updated": "2024-09-10T23:58:27+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.AI",
                "cs.HC"
            ],
            "max_author_hindex": 35
        },
        "2409.07033": {
            "authors": [
                "Wenchao Zhao",
                "Xiaoyi Liu",
                "Ruilin Xu",
                "Lingxi Xiao",
                "Muqing Li"
            ],
            "title": "E-commerce Webpage Recommendation Scheme Base on Semantic Mining and Neural Networks",
            "abstract": "In e-commerce websites, web mining web page recommendation technology has been widely used. However, recommendation solutions often cannot meet the actual application needs of online shopping users. To address this problem, this paper proposes an e-commerce web page recommendation solution that combines semantic web mining and BP neural networks. First, the web logs of user searches are processed, and 5 features are extracted: content priority, time consumption priority, online shopping users' explicit/implicit feedback on the website, recommendation semantics and input deviation amount. Then, these features are used as input features of the BP neural network to classify and identify the priority of the final output web page. Finally, the web pages are sorted according to priority and recommended to users. This project uses book sales webpages as samples for experiments. The results show that this solution can quickly and accurately identify the webpages required by users.",
            "id": "2409.07033",
            "link": "http://arxiv.org/abs/2409.07033v1",
            "published": "2024-09-11T06:03:02+00:00",
            "updated": "2024-09-11T06:03:02+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "max_author_hindex": 20
        },
        "2409.07416": {
            "authors": [
                "Luo Ji",
                "Gao Liu",
                "Mingyang Yin",
                "Hongxia Yang",
                "Jingren Zhou"
            ],
            "title": "Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise Recommendation",
            "abstract": "Modern listwise recommendation systems need to consider both long-term user perceptions and short-term interest shifts. Reinforcement learning can be applied on recommendation to study such a problem but is also subject to large search space, sparse user feedback and long interactive latency. Motivated by recent progress in hierarchical reinforcement learning, we propose a novel framework called mccHRL to provide different levels of temporal abstraction on listwise recommendation. Within the hierarchical framework, the high-level agent studies the evolution of user perception, while the low-level agent produces the item selection policy by modeling the process as a sequential decision-making problem. We argue that such framework has a well-defined decomposition of the outra-session context and the intra-session context, which are encoded by the high-level and low-level agents, respectively. To verify this argument, we implement both a simulator-based environment and an industrial dataset-based experiment. Results observe significant performance improvement by our method, compared with several well-known baselines. Data and codes have been made public.",
            "id": "2409.07416",
            "link": "http://arxiv.org/abs/2409.07416v1",
            "published": "2024-09-11T17:01:06+00:00",
            "updated": "2024-09-11T17:01:06+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 54
        },
        "2409.07850": {
            "authors": [
                "S\u00fcmeyye \u00d6zt\u00fcrk",
                "Ahmed Burak Ercan",
                "Resul Tugay",
                "\u015eule G\u00fcnd\u00fcz \u00d6\u011f\u00fcd\u00fcc\u00fc"
            ],
            "title": "Enhancing Cross-Market Recommendation System with Graph Isomorphism Networks: A Novel Approach to Personalized User Experience",
            "abstract": "In today's world of globalized commerce, cross-market recommendation systems (CMRs) are crucial for providing personalized user experiences across diverse market segments. However, traditional recommendation algorithms have difficulties dealing with market specificity and data sparsity, especially in new or emerging markets. In this paper, we propose the CrossGR model, which utilizes Graph Isomorphism Networks (GINs) to improve CMR systems. It outperforms existing benchmarks in NDCG@10 and HR@10 metrics, demonstrating its adaptability and accuracy in handling diverse market segments. The CrossGR model is adaptable and accurate, making it well-suited for handling the complexities of cross-market recommendation tasks. Its robustness is demonstrated by consistent performance across different evaluation timeframes, indicating its potential to cater to evolving market trends and user preferences. Our findings suggest that GINs represent a promising direction for CMRs, paving the way for more sophisticated, personalized, and context-aware recommendation systems in the dynamic landscape of global e-commerce.",
            "id": "2409.07850",
            "link": "http://arxiv.org/abs/2409.07850v1",
            "published": "2024-09-12T08:53:11+00:00",
            "updated": "2024-09-12T08:53:11+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 19
        },
        "2409.06377": {
            "authors": [
                "Weicong Qin",
                "Yi Xu",
                "Weijie Yu",
                "Chenglei Shen",
                "Xiao Zhang",
                "Ming He",
                "Jianping Fan",
                "Jun Xu"
            ],
            "title": "Enhancing Sequential Recommendations through Multi-Perspective Reflections and Iteration",
            "abstract": "Sequence recommendation (SeqRec) aims to predict the next item a user will interact with by understanding user intentions and leveraging collaborative filtering information. Large language models (LLMs) have shown great promise in recommendation tasks through prompt-based, fixed reflection libraries, and fine-tuning techniques. However, these methods face challenges, including lack of supervision, inability to optimize reflection sources, inflexibility to diverse user needs, and high computational costs. Despite promising results, current studies primarily focus on reflections of users' explicit preferences (e.g., item titles) while neglecting implicit preferences (e.g., brands) and collaborative filtering information. This oversight hinders the capture of preference shifts and dynamic user behaviors. Additionally, existing approaches lack mechanisms for reflection evaluation and iteration, often leading to suboptimal recommendations. To address these issues, we propose the Mixture of REflectors (MoRE) framework, designed to model and learn dynamic user preferences in SeqRec. Specifically, MoRE introduces three reflectors for generating LLM-based reflections on explicit preferences, implicit preferences, and collaborative signals. Each reflector incorporates a self-improving strategy, termed refining-and-iteration, to evaluate and iteratively update reflections. Furthermore, a meta-reflector employs a contextual bandit algorithm to select the most suitable expert and corresponding reflections for each user's recommendation, effectively capturing dynamic preferences. Extensive experiments on three real-world datasets demonstrate that MoRE consistently outperforms state-of-the-art methods, requiring less training time and GPU memory compared to other LLM-based approaches in SeqRec.",
            "id": "2409.06377",
            "link": "http://arxiv.org/abs/2409.06377v1",
            "published": "2024-09-10T09:58:55+00:00",
            "updated": "2024-09-10T09:58:55+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.CL"
            ],
            "max_author_hindex": 34
        },
        "2409.07627": {
            "authors": [
                "Shanu Vashishtha",
                "Abhay Kumar",
                "Lalitesh Morishetti",
                "Kaushiki Nag",
                "Kannan Achan"
            ],
            "title": "Leveraging User-Generated Reviews for Recommender Systems with Dynamic Headers",
            "abstract": "E-commerce platforms have a vast catalog of items to cater to their customers' shopping interests. Most of these platforms assist their customers in the shopping process by offering optimized recommendation carousels, designed to help customers quickly locate their desired items. Many models have been proposed in academic literature to generate and enhance the ranking and recall set of items in these carousels. Conventionally, the accompanying carousel title text (header) of these carousels remains static. In most instances, a generic text such as \"Items similar to your current viewing\" is utilized. Fixed variations such as the inclusion of specific attributes \"Other items from a similar seller\" or \"Items from a similar brand\" in addition to \"frequently bought together\" or \"considered together\" are observed as well. This work proposes a novel approach to customize the header generation process of these carousels. Our work leverages user-generated reviews that lay focus on specific attributes (aspects) of an item that were favorably perceived by users during their interaction with the given item. We extract these aspects from reviews and train a graph neural network-based model under the framework of a conditional ranking task. We refer to our innovative methodology as Dynamic Text Snippets (DTS) which generates multiple header texts for an anchor item and its recall set. Our approach demonstrates the potential of utilizing user-generated reviews and presents a unique paradigm for exploring increasingly context-aware recommendation systems.",
            "id": "2409.07627",
            "link": "http://arxiv.org/abs/2409.07627v1",
            "published": "2024-09-11T21:18:21+00:00",
            "updated": "2024-09-11T21:18:21+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 16
        },
        "2409.07691": {
            "authors": [
                "Gabriel de Souza P. Moreira",
                "Ronay Ak",
                "Benedikt Schifferer",
                "Mengyao Xu",
                "Radek Osmulski",
                "Even Oldridge"
            ],
            "title": "Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG",
            "abstract": "Ranking models play a crucial role in enhancing overall accuracy of text retrieval systems. These multi-stage systems typically utilize either dense embedding models or sparse lexical indices to retrieve relevant passages based on a given query, followed by ranking models that refine the ordering of the candidate passages by its relevance to the query.   This paper benchmarks various publicly available ranking models and examines their impact on ranking accuracy. We focus on text retrieval for question-answering tasks, a common use case for Retrieval-Augmented Generation systems. Our evaluation benchmarks include models some of which are commercially viable for industrial applications.   We introduce a state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3, which achieves a significant accuracy increase of ~14% compared to pipelines with other rerankers. We also provide an ablation study comparing the fine-tuning of ranking models with different sizes, losses and self-attention mechanisms.   Finally, we discuss challenges of text retrieval pipelines with ranking models in real-world industry applications, in particular the trade-offs among model size, ranking accuracy and system requirements like indexing and serving latency / throughput.",
            "id": "2409.07691",
            "link": "http://arxiv.org/abs/2409.07691v1",
            "published": "2024-09-12T01:51:06+00:00",
            "updated": "2024-09-12T01:51:06+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 17
        },
        "2409.06692": {
            "authors": [
                "Umair Qudus",
                "Michael Roeder",
                "Muhammad Saleem",
                "Axel-Cyrille Ngonga Ngomo"
            ],
            "title": "HybridFC: A Hybrid Fact-Checking Approach for Knowledge Graphs",
            "abstract": "We consider fact-checking approaches that aim to predict the veracity of assertions in knowledge graphs. Five main categories of fact-checking approaches for knowledge graphs have been proposed in the recent literature, of which each is subject to partially overlapping limitations. In particular, current text-based approaches are limited by manual feature engineering. Path-based and rule-based approaches are limited by their exclusive use of knowledge graphs as background knowledge, and embedding-based approaches suffer from low accuracy scores on current fact-checking tasks. We propose a hybrid approach -- dubbed HybridFC -- that exploits the diversity of existing categories of fact-checking approaches within an ensemble learning setting to achieve a significantly better prediction performance. In particular, our approach outperforms the state of the art by 0.14 to 0.27 in terms of Area Under the Receiver Operating Characteristic curve on the FactBench dataset. Our code is open-source and can be found at https://github.com/dice-group/HybridFC.",
            "id": "2409.06692",
            "link": "http://arxiv.org/abs/2409.06692v1",
            "published": "2024-09-10T17:55:00+00:00",
            "updated": "2024-09-10T17:55:00+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.DB"
            ],
            "max_author_hindex": 49
        },
        "2409.06107": {
            "authors": [
                "Shervin Ghasemlou",
                "Ashish Katiyar",
                "Aparajita Saraf",
                "Seungwhan Moon",
                "Mangesh Pujari",
                "Pinar Donmez",
                "Babak Damavandi",
                "Anuj Kumar"
            ],
            "title": "Doppelg\u00e4nger's Watch: A Split Objective Approach to Large Language Models",
            "abstract": "In this paper, we investigate the problem of \"generation supervision\" in large language models, and present a novel bicameral architecture to separate supervision signals from their core capability, helpfulness. Doppelg\\\"anger, a new module parallel to the underlying language model, supervises the generation of each token, and learns to concurrently predict the supervision score(s) of the sequences up to and including each token. In this work, we present the theoretical findings, and leave the report on experimental results to a forthcoming publication.",
            "id": "2409.06107",
            "link": "http://arxiv.org/abs/2409.06107v1",
            "published": "2024-09-09T23:22:27+00:00",
            "updated": "2024-09-09T23:22:27+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 21
        },
        "2409.06263": {
            "authors": [
                "Jihyun Lee",
                "Solee Im",
                "Wonjun Lee",
                "Gary Geunbae Lee"
            ],
            "title": "Keyword-Aware ASR Error Augmentation for Robust Dialogue State Tracking",
            "abstract": "Dialogue State Tracking (DST) is a key part of task-oriented dialogue systems, identifying important information in conversations. However, its accuracy drops significantly in spoken dialogue environments due to named entity errors from Automatic Speech Recognition (ASR) systems. We introduce a simple yet effective data augmentation method that targets those entities to improve the robustness of DST model. Our novel method can control the placement of errors using keyword-highlighted prompts while introducing phonetically similar errors. As a result, our method generated sufficient error patterns on keywords, leading to improved accuracy in noised and low-accuracy ASR environments.",
            "id": "2409.06263",
            "link": "http://arxiv.org/abs/2409.06263v1",
            "published": "2024-09-10T07:06:40+00:00",
            "updated": "2024-09-10T07:06:40+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 28
        },
        "2409.07286": {
            "authors": [
                "Joris Veerbeek",
                "Nicholas Diakopoulos"
            ],
            "title": "Using Generative Agents to Create Tip Sheets for Investigative Data Reporting",
            "abstract": "This paper introduces a system using generative AI agents to create tip sheets for investigative data reporting. Our system employs three specialized agents--an analyst, a reporter, and an editor--to collaboratively generate and refine tips from datasets. We validate this approach using real-world investigative stories, demonstrating that our agent-based system generally generates more newsworthy and accurate insights compared to a baseline model without agents, although some variability was noted between different stories. Our findings highlight the potential of generative AI to provide leads for investigative data reporting.",
            "id": "2409.07286",
            "link": "http://arxiv.org/abs/2409.07286v1",
            "published": "2024-09-11T14:14:15+00:00",
            "updated": "2024-09-11T14:14:15+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 35
        },
        "2409.05405": {
            "authors": [
                "Suyan Li",
                "Fuxiang Huang",
                "Lei Zhang"
            ],
            "title": "A Survey of Multimodal Composite Editing and Retrieval",
            "abstract": "In the real world, where information is abundant and diverse across different modalities, understanding and utilizing various data types to improve retrieval systems is a key focus of research. Multimodal composite retrieval integrates diverse modalities such as text, image and audio, etc. to provide more accurate, personalized, and contextually relevant results. To facilitate a deeper understanding of this promising direction, this survey explores multimodal composite editing and retrieval in depth, covering image-text composite editing, image-text composite retrieval, and other multimodal composite retrieval. In this survey, we systematically organize the application scenarios, methods, benchmarks, experiments, and future directions. Multimodal learning is a hot topic in large model era, and have also witnessed some surveys in multimodal learning and vision-language models with transformers published in the PAMI journal. To the best of our knowledge, this survey is the first comprehensive review of the literature on multimodal composite retrieval, which is a timely complement of multimodal fusion to existing reviews. To help readers' quickly track this field, we build the project page for this survey, which can be found at https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.",
            "id": "2409.05405",
            "link": "http://arxiv.org/abs/2409.05405v2",
            "published": "2024-09-09T08:06:50+00:00",
            "updated": "2024-09-11T02:44:52+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.IR",
                "cs.MM"
            ],
            "max_author_hindex": 30
        },
        "2409.05283": {
            "authors": [
                "Suyash Fulay",
                "William Brannon",
                "Shrestha Mohanty",
                "Cassandra Overney",
                "Elinor Poole-Dayan",
                "Deb Roy",
                "Jad Kabbara"
            ],
            "title": "On the Relationship between Truth and Political Bias in Language Models",
            "abstract": "Language model alignment research often attempts to ensure that models are not only helpful and harmless, but also truthful and unbiased. However, optimizing these objectives simultaneously can obscure how improving one aspect might impact the others. In this work, we focus on analyzing the relationship between two concepts essential in both language model alignment and political science: \\textit{truthfulness} and \\textit{political bias}. We train reward models on various popular truthfulness datasets and subsequently evaluate their political bias. Our findings reveal that optimizing reward models for truthfulness on these datasets tends to result in a left-leaning political bias. We also find that existing open-source reward models (i.e. those trained on standard human preference datasets) already show a similar bias and that the bias is larger for larger models. These results raise important questions about both the datasets used to represent truthfulness and what language models capture about the relationship between truth and politics.",
            "id": "2409.05283",
            "link": "http://arxiv.org/abs/2409.05283v1",
            "published": "2024-09-09T02:28:53+00:00",
            "updated": "2024-09-09T02:28:53+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 8
        },
        "2409.05286": {
            "authors": [
                "Ruya Jiang",
                "Chun Wang",
                "Weihong Deng"
            ],
            "title": "Seek and Solve Reasoning for Table Question Answering",
            "abstract": "Table-based Question Answering (TQA) involves answering questions based on tabular data. The complexity of table structures and question logic makes this task difficult even for Large Language Models (LLMs). This paper improves TQA performance by leveraging LLMs' reasoning capabilities. Inspired by how humans solve TQA tasks, we propose a Seek-and-Solve pipeline that instructs the LLM to first seek relevant information and then answer questions. The two stages are integrated at the reasoning level, and their Chain of Thought (CoT) paths are integrated into a coherent Seek-and-Solve CoT (SS-CoT). Furthermore, we present a compact single-stage TQA-solving prompt distilled from the pipeline. Experiments demonstrate that under In-Context Learning settings, using samples with SS-CoT paths as demonstrations, the TQA-solving prompt can effectively guide the LLM to solve complex TQA tasks, resulting in improved performance and reliability. Our results highlight the importance of properly eliciting LLMs' reasoning capabilities in solving complex TQA tasks.",
            "id": "2409.05286",
            "link": "http://arxiv.org/abs/2409.05286v1",
            "published": "2024-09-09T02:41:00+00:00",
            "updated": "2024-09-09T02:41:00+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 45
        },
        "2409.05385": {
            "authors": [
                "Hong Xingyun Hong",
                "Shao Yan Shao",
                "Wang Zhilin Wang",
                "Duan Manni Duan",
                "Jin Xiongnan"
            ],
            "title": "Towards Building a Robust Knowledge Intensive Question Answering Model with Large Language Models",
            "abstract": "The development of LLMs has greatly enhanced the intelligence and fluency of question answering, while the emergence of retrieval enhancement has enabled models to better utilize external information. However, the presence of noise and errors in retrieved information poses challenges to the robustness of LLMs. In this work, to evaluate the model's performance under multiple interferences, we first construct a dataset based on machine reading comprehension datasets simulating various scenarios, including critical information absence, noise, and conflicts. To address the issue of model accuracy decline caused by noisy external information, we propose a data augmentation-based fine-tuning method to enhance LLM's robustness against noise. Additionally, contrastive learning approach is utilized to preserve the model's discrimination capability of external information. We have conducted experiments on both existing LLMs and our approach, the results are evaluated by GPT-4, which indicates that our proposed methods improve model robustness while strengthening the model's discrimination capability.",
            "id": "2409.05385",
            "link": "http://arxiv.org/abs/2409.05385v2",
            "published": "2024-09-09T07:32:30+00:00",
            "updated": "2024-09-10T06:11:28+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.05521": {
            "authors": [
                "Anna Kruspe"
            ],
            "title": "Harmonic Reasoning in Large Language Models",
            "abstract": "Large Language Models (LLMs) are becoming very popular and are used for many different purposes, including creative tasks in the arts. However, these models sometimes have trouble with specific reasoning tasks, especially those that involve logical thinking and counting. This paper looks at how well LLMs understand and reason when dealing with musical tasks like figuring out notes from intervals and identifying chords and scales. We tested GPT-3.5 and GPT-4o to see how they handle these tasks. Our results show that while LLMs do well with note intervals, they struggle with more complicated tasks like recognizing chords and scales. This points out clear limits in current LLM abilities and shows where we need to make them better, which could help improve how they think and work in both artistic and other complex areas. We also provide an automatically generated benchmark data set for the described tasks.",
            "id": "2409.05521",
            "link": "http://arxiv.org/abs/2409.05521v1",
            "published": "2024-09-09T11:28:02+00:00",
            "updated": "2024-09-09T11:28:02+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.SD"
            ],
            "max_author_hindex": 11
        },
        "2409.05592": {
            "authors": [
                "Zhaoyue Sun",
                "Jiazheng Li",
                "Gabriele Pergola",
                "Yulan He"
            ],
            "title": "ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language",
            "abstract": "Predicting unknown drug-drug interactions (DDIs) is crucial for improving medication safety. Previous efforts in DDI prediction have typically focused on binary classification or predicting DDI categories, with the absence of explanatory insights that could enhance trust in these predictions. In this work, we propose to generate natural language explanations for DDI predictions, enabling the model to reveal the underlying pharmacodynamics and pharmacokinetics mechanisms simultaneously as making the prediction. To do this, we have collected DDI explanations from DDInter and DrugBank and developed various models for extensive experiments and analysis. Our models can provide accurate explanations for unknown DDIs between known drugs. This paper contributes new tools to the field of DDI prediction and lays a solid foundation for further research on generating explanations for DDI predictions.",
            "id": "2409.05592",
            "link": "http://arxiv.org/abs/2409.05592v1",
            "published": "2024-09-09T13:23:14+00:00",
            "updated": "2024-09-09T13:23:14+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 19
        },
        "2409.05721": {
            "authors": [
                "Bram Willemsen",
                "Gabriel Skantze"
            ],
            "title": "Referring Expression Generation in Visually Grounded Dialogue with Discourse-aware Comprehension Guiding",
            "abstract": "We propose an approach to referring expression generation (REG) in visually grounded dialogue that is meant to produce referring expressions (REs) that are both discriminative and discourse-appropriate. Our method constitutes a two-stage process. First, we model REG as a text- and image-conditioned next-token prediction task. REs are autoregressively generated based on their preceding linguistic context and a visual representation of the referent. Second, we propose the use of discourse-aware comprehension guiding as part of a generate-and-rerank strategy through which candidate REs generated with our REG model are reranked based on their discourse-dependent discriminatory power. Results from our human evaluation indicate that our proposed two-stage approach is effective in producing discriminative REs, with higher performance in terms of text-image retrieval accuracy for reranked REs compared to those generated using greedy decoding.",
            "id": "2409.05721",
            "link": "http://arxiv.org/abs/2409.05721v1",
            "published": "2024-09-09T15:33:07+00:00",
            "updated": "2024-09-09T15:33:07+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 28
        },
        "2409.05771": {
            "authors": [
                "Emily Cheng",
                "Richard J. Antonello"
            ],
            "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models",
            "abstract": "Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first \"composition\" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.",
            "id": "2409.05771",
            "link": "http://arxiv.org/abs/2409.05771v1",
            "published": "2024-09-09T16:33:16+00:00",
            "updated": "2024-09-09T16:33:16+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 55
        },
        "2409.05994": {
            "authors": [
                "Francisco Valentini",
                "Viviana Cotik",
                "Dami\u00e1n Furman",
                "Ivan Bercovich",
                "Edgar Altszyler",
                "Juan Manuel P\u00e9rez"
            ],
            "title": "MessIRve: A Large-Scale Spanish Information Retrieval Dataset",
            "abstract": "Information retrieval (IR) is the task of finding relevant documents in response to a user query. Although Spanish is the second most spoken native language, current IR benchmarks lack Spanish data, hindering the development of information access tools for Spanish speakers. We introduce MessIRve, a large-scale Spanish IR dataset with around 730 thousand queries from Google's autocomplete API and relevant documents sourced from Wikipedia. MessIRve's queries reflect diverse Spanish-speaking regions, unlike other datasets that are translated from English or do not consider dialectal variations. The large size of the dataset allows it to cover a wide variety of topics, unlike smaller datasets. We provide a comprehensive description of the dataset, comparisons with existing datasets, and baseline evaluations of prominent IR models. Our contributions aim to advance Spanish IR research and improve information access for Spanish speakers.",
            "id": "2409.05994",
            "link": "http://arxiv.org/abs/2409.05994v1",
            "published": "2024-09-09T18:45:04+00:00",
            "updated": "2024-09-09T18:45:04+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 9
        },
        "2409.06131": {
            "authors": [
                "Neha Prakriya",
                "Jui-Nan Yen",
                "Cho-Jui Hsieh",
                "Jason Cong"
            ],
            "title": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review",
            "abstract": "Large Language Model (LLM) pretraining traditionally relies on autoregressive language modeling on randomly sampled data blocks from web-scale datasets. We take inspiration from human learning techniques like spaced repetition to hypothesize that random data sampling for LLMs leads to high training cost and low quality models which tend to forget data. In order to effectively commit web-scale information to long-term memory, we propose the LFR (Learn, Focus, and Review) pedagogy, a new dynamic training paradigm which focuses and repeatedly reviews complex data blocks at systematic intervals based on the model's learning pace and progress. LFR records the model perplexities for different data blocks and frequently revisits blocks with higher perplexity which are more likely to be forgotten. We pretrain the GPT-2 models (124M - 1.5B) from scratch on the OpenWebText dataset using LFR. We test on downstream tasks from the language modeling, question answering, translation, and problem solving domains to achieve consistently lower perplexity and higher accuracy than the baseline OpenAI models, while obtaining a 20x pretraining speed-up.",
            "id": "2409.06131",
            "link": "http://arxiv.org/abs/2409.06131v1",
            "published": "2024-09-10T00:59:18+00:00",
            "updated": "2024-09-10T00:59:18+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 80
        },
        "2409.06173": {
            "authors": [
                "Georgios Chochlakis",
                "Niyantha Maruthu Pandiyan",
                "Kristina Lerman",
                "Shrikanth Narayanan"
            ],
            "title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks",
            "abstract": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to \"adapt\" the LLM to perform the present task at a competitive or state-of-the-art level at a fraction of the computational cost. ICL can be augmented by incorporating the reasoning process to arrive at the final label explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting. However, recent work has found that ICL relies mostly on the retrieval of task priors and less so on \"learning\" to perform tasks, especially for complex subjective domains like emotion and morality, where priors ossify posterior predictions. In this work, we examine whether \"enabling\" reasoning also creates the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors that remain relatively unchanged despite the evidence in the prompt. We find that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL for larger language models. Code is avalaible at https://github.com/gchochla/cot-priors.",
            "id": "2409.06173",
            "link": "http://arxiv.org/abs/2409.06173v1",
            "published": "2024-09-10T03:06:17+00:00",
            "updated": "2024-09-10T03:06:17+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 60
        },
        "2409.06185": {
            "authors": [
                "Sandeep Kumar",
                "Tirthankar Ghosal",
                "Vinayak Goyal",
                "Asif Ekbal"
            ],
            "title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
            "abstract": "\"An idea is nothing more nor less than a new combination of old elements\" (Young, J.W.). The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives. This study explores the capability of LLMs in generating novel research ideas based on information from research papers. We conduct a thorough examination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics). We found that the future research ideas generated by Claude-2 and GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini. We also found that Claude-2 generates more diverse future research ideas than GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the novelty, relevancy, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. We make our datasets and codes publicly available.",
            "id": "2409.06185",
            "link": "http://arxiv.org/abs/2409.06185v1",
            "published": "2024-09-10T03:26:42+00:00",
            "updated": "2024-09-10T03:26:42+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.HC",
                "cs.LG"
            ],
            "max_author_hindex": 47
        },
        "2409.06192": {
            "authors": [
                "Yoonji Nam",
                "TaeWoong Seo",
                "Gyeongcheol Shin",
                "Sangji Lee",
                "JaeEun Im"
            ],
            "title": "NOVI : Chatbot System for University Novice with BERT and LLMs",
            "abstract": "To mitigate the difficulties of university freshmen in adapting to university life, we developed NOVI, a chatbot system based on GPT-4o. This system utilizes post and comment data from SKKU 'Everytime', a university community site. Developed using LangChain, NOVI's performance has been evaluated with a BLEU score, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR score. This approach is not only limited to help university freshmen but is also expected to help various people adapting to new environments with different data. This research explores the development and potential application of new educational technology tools, contributing to easier social adaptation for beginners and settling a foundation for future advancement in LLM studies.",
            "id": "2409.06192",
            "link": "http://arxiv.org/abs/2409.06192v1",
            "published": "2024-09-10T03:43:26+00:00",
            "updated": "2024-09-10T03:43:26+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 4
        },
        "2409.06518": {
            "authors": [
                "Juhwan Choi",
                "YoungBin Kim"
            ],
            "title": "Questioning Internal Knowledge Structure of Large Language Models Through the Lens of the Olympic Games",
            "abstract": "Large language models (LLMs) have become a dominant approach in natural language processing, yet their internal knowledge structures remain largely unexplored. In this paper, we analyze the internal knowledge structures of LLMs using historical medal tallies from the Olympic Games. We task the models with providing the medal counts for each team and identifying which teams achieved specific rankings. Our results reveal that while state-of-the-art LLMs perform remarkably well in reporting medal counts for individual teams, they struggle significantly with questions about specific rankings. This suggests that the internal knowledge structures of LLMs are fundamentally different from those of humans, who can easily infer rankings from known medal counts. To support further research, we publicly release our code, dataset, and model outputs.",
            "id": "2409.06518",
            "link": "http://arxiv.org/abs/2409.06518v1",
            "published": "2024-09-10T13:54:04+00:00",
            "updated": "2024-09-10T13:54:04+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 7
        },
        "2409.06883": {
            "authors": [
                "Yuya Fujisaki",
                "Shiro Takagi",
                "Hideki Asoh",
                "Wataru Kumagai"
            ],
            "title": "A Dataset for Evaluating LLM-based Evaluation Functions for Research Question Extraction Task",
            "abstract": "The progress in text summarization techniques has been remarkable. However the task of accurately extracting and summarizing necessary information from highly specialized documents such as research papers has not been sufficiently investigated. We are focusing on the task of extracting research questions (RQ) from research papers and construct a new dataset consisting of machine learning papers, RQ extracted from these papers by GPT-4, and human evaluations of the extracted RQ from multiple perspectives. Using this dataset, we systematically compared recently proposed LLM-based evaluation functions for summarizations, and found that none of the functions showed sufficiently high correlations with human evaluations. We expect our dataset provides a foundation for further research on developing better evaluation functions tailored to the RQ extraction task, and contribute to enhance the performance of the task. The dataset is available at https://github.com/auto-res/PaperRQ-HumanAnno-Dataset.",
            "id": "2409.06883",
            "link": "http://arxiv.org/abs/2409.06883v1",
            "published": "2024-09-10T21:54:46+00:00",
            "updated": "2024-09-10T21:54:46+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 27
        },
        "2409.06949": {
            "authors": [
                "Jaewoo Song",
                "Andrew Zhu",
                "Chris Callison-Burch"
            ],
            "title": "You Have Thirteen Hours in Which to Solve the Labyrinth: Enhancing AI Game Masters with Function Calling",
            "abstract": "Developing a consistent and reliable AI game master for text-based games is a challenging task due to the limitations of large language models (LLMs) and the complexity of the game master's role. This paper presents a novel approach to enhance AI game masters by leveraging function calling in the context of the table-top role-playing game \"Jim Henson's Labyrinth: The Adventure Game.\" Our methodology involves integrating game-specific controls through functions, which we show improves the narrative quality and state update consistency of the AI game master. The experimental results, based on human evaluations and unit tests, demonstrate the effectiveness of our approach in enhancing gameplay experience and maintaining coherence with the game state. This work contributes to the advancement of game AI and interactive storytelling, offering insights into the design of more engaging and consistent AI-driven game masters.",
            "id": "2409.06949",
            "link": "http://arxiv.org/abs/2409.06949v1",
            "published": "2024-09-11T02:03:51+00:00",
            "updated": "2024-09-11T02:03:51+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 103
        },
        "2409.07045": {
            "authors": [
                "Hanyu Zhao",
                "Li Du",
                "Yiming Ju",
                "Chengwei Wu",
                "Tengfei Pan"
            ],
            "title": "Beyond IID: Optimizing Instruction Learning from the Perspective of Instruction Interaction and Dependency",
            "abstract": "With the availability of various instruction datasets, a pivotal challenge is how to effectively select and integrate these instructions to fine-tune large language models (LLMs). Previous research mainly focuses on selecting individual high-quality instructions. However, these works overlooked the joint interactions and dependencies between different categories of instructions, leading to suboptimal selection strategies. Moreover, the nature of these interaction patterns remains largely unexplored, let alone optimize the instruction set with regard to them. To fill these gaps, in this paper, we: (1) systemically investigate interaction and dependency patterns between different categories of instructions, (2) manage to optimize the instruction set concerning the interaction patterns using a linear programming-based method, and optimize the learning schema of SFT using an instruction dependency taxonomy guided curriculum learning. Experimental results across different LLMs demonstrate improved performance over strong baselines on widely adopted benchmarks.",
            "id": "2409.07045",
            "link": "http://arxiv.org/abs/2409.07045v1",
            "published": "2024-09-11T06:27:50+00:00",
            "updated": "2024-09-11T06:27:50+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 23
        },
        "2409.07054": {
            "authors": [
                "Mohamed Bayan Kmainasi",
                "Rakif Khan",
                "Ali Ezzat Shahroor",
                "Boushra Bendou",
                "Maram Hasanain",
                "Firoj Alam"
            ],
            "title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
            "abstract": "Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks. To elicit knowledge from LLMs, prompts play a key role, consisting of natural language instructions. Most open and closed source LLMs are trained on available labeled and unlabeled resources--digital content such as text, images, audio, and videos. Hence, these models have better knowledge for high-resourced languages but struggle with low-resourced languages. Since prompts play a crucial role in understanding their capabilities, the language used for prompts remains an important research question. Although there has been significant research in this area, it is still limited, and less has been explored for medium to low-resourced languages. In this study, we investigate different prompting strategies (native vs. non-native) on 11 different NLP tasks associated with 12 different Arabic datasets (9.7K data points). In total, we conducted 197 experiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our findings suggest that, on average, the non-native prompt performs the best, followed by mixed and native prompts.",
            "id": "2409.07054",
            "link": "http://arxiv.org/abs/2409.07054v1",
            "published": "2024-09-11T06:59:37+00:00",
            "updated": "2024-09-11T06:59:37+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "68T50",
                "F.2.2; I.2.7"
            ],
            "max_author_hindex": 33
        },
        "2409.07055": {
            "authors": [
                "Junkai Liu",
                "Yujie Tong",
                "Hui Huang",
                "Shuyuan Zheng",
                "Muyun Yang",
                "Peicheng Wu",
                "Makoto Onizuka",
                "Chuan Xiao"
            ],
            "title": "Legal Fact Prediction: Task Definition and Dataset Construction",
            "abstract": "Legal facts refer to the facts that can be proven by acknowledged evidence in a trial. They form the basis for the determination of court judgments. This paper introduces a novel NLP task: legal fact prediction, which aims to predict the legal fact based on a list of evidence. The predicted facts can instruct the parties and their lawyers involved in a trial to strengthen their submissions and optimize their strategies during the trial. Moreover, since real legal facts are difficult to obtain before the final judgment, the predicted facts also serve as an important basis for legal judgment prediction. We construct a benchmark dataset consisting of evidence lists and ground-truth legal facts for real civil loan cases, LFPLoan. Our experiments on this dataset show that this task is non-trivial and requires further considerable research efforts.",
            "id": "2409.07055",
            "link": "http://arxiv.org/abs/2409.07055v1",
            "published": "2024-09-11T07:01:08+00:00",
            "updated": "2024-09-11T07:01:08+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CY"
            ],
            "max_author_hindex": 71
        },
        "2409.07088": {
            "authors": [
                "Daehee Kim",
                "Deokhyung Kang",
                "Sangwon Ryu",
                "Gary Geunbae Lee"
            ],
            "title": "Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model",
            "abstract": "Knowledge Graph-to-Text (G2T) generation involves verbalizing structured knowledge graphs into natural language text. Recent advancements in Pretrained Language Models (PLMs) have improved G2T performance, but their effectiveness depends on datasets with precise graph-text alignment. However, the scarcity of high-quality, general-domain G2T generation datasets restricts progress in the general-domain G2T generation research. To address this issue, we introduce Wikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T dataset generated using a novel method that leverages Large Language Model (LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain graph-text pairs, offers high graph-text consistency without relying on external ontologies. Experimental results demonstrate that PLM fine-tuned on WikiOFGraph outperforms those trained on other datasets across various evaluation metrics. Our method proves to be a scalable and effective solution for generating high-quality G2T data, significantly advancing the field of G2T generation.",
            "id": "2409.07088",
            "link": "http://arxiv.org/abs/2409.07088v1",
            "published": "2024-09-11T08:16:20+00:00",
            "updated": "2024-09-11T08:16:20+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 28
        },
        "2409.07246": {
            "authors": [
                "Firoj Alam",
                "Md. Rafiul Biswas",
                "Uzair Shah",
                "Wajdi Zaghouani",
                "Georgios Mikros"
            ],
            "title": "Propaganda to Hate: A Multimodal Analysis of Arabic Memes with Multi-Agent LLMs",
            "abstract": "In the past decade, social media platforms have been used for information dissemination and consumption. While a major portion of the content is posted to promote citizen journalism and public awareness, some content is posted to mislead users. Among different content types such as text, images, and videos, memes (text overlaid on images) are particularly prevalent and can serve as powerful vehicles for propaganda, hate, and humor. In the current literature, there have been efforts to individually detect such content in memes. However, the study of their intersection is very limited. In this study, we explore the intersection between propaganda and hate in memes using a multi-agent LLM-based approach. We extend the propagandistic meme dataset with coarse and fine-grained hate labels. Our finding suggests that there is an association between propaganda and hate in memes. We provide detailed experimental results that can serve as a baseline for future studies. We will make the experimental resources publicly available to the community.",
            "id": "2409.07246",
            "link": "http://arxiv.org/abs/2409.07246v1",
            "published": "2024-09-11T13:04:34+00:00",
            "updated": "2024-09-11T13:04:34+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "68T50",
                "F.2.2; I.2.7"
            ],
            "max_author_hindex": 33
        },
        "2409.08098": {
            "authors": [
                "Huiyuan Xie",
                "Felix Steffek",
                "Joana Ribeiro de Faria",
                "Christine Carter",
                "Jonathan Rutherford"
            ],
            "title": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal",
            "abstract": "This paper explores the intersection of technological innovation and access to justice by developing a benchmark for predicting case outcomes in the UK Employment Tribunal (UKET). To address the challenge of extensive manual annotation, the study employs a large language model (LLM) for automatic annotation, resulting in the creation of the CLC-UKET dataset. The dataset consists of approximately 19,000 UKET cases and their metadata. Comprehensive legal annotations cover facts, claims, precedent references, statutory references, case outcomes, reasons and jurisdiction codes. Facilitated by the CLC-UKET data, we examine a multi-class case outcome prediction task in the UKET. Human predictions are collected to establish a performance reference for model comparison. Empirical results from baseline models indicate that finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET prediction task. The performance of zero-shot LLMs can be enhanced by integrating task-related information into few-shot examples. We hope that the CLC-UKET dataset, along with human annotations and empirical findings, can serve as a valuable benchmark for employment-related dispute resolution.",
            "id": "2409.08098",
            "link": "http://arxiv.org/abs/2409.08098v1",
            "published": "2024-09-12T14:51:43+00:00",
            "updated": "2024-09-12T14:51:43+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.08199": {
            "authors": [
                "Hyunjong Ok",
                "Suho Yoo",
                "Jaeho Lee"
            ],
            "title": "AudioBERT: Audio Knowledge Augmented Language Model",
            "abstract": "Recent studies have identified that language models, pretrained on text-only datasets, often lack elementary visual knowledge, \\textit{e.g.,} colors of everyday objects. Motivated by this observation, we ask whether a similar shortcoming exists in terms of the \\textit{auditory} knowledge. To answer this question, we construct a new dataset called AuditoryBench, which consists of two novel tasks for evaluating auditory knowledge. Based on our analysis using the benchmark, we find that language models also suffer from a severe lack of auditory knowledge. To address this limitation, we propose AudioBERT, a novel method to augment the auditory knowledge of BERT through a retrieval-based approach. First, we detect auditory knowledge spans in prompts to query our retrieval model efficiently. Then, we inject audio knowledge into BERT and switch on low-rank adaptation for effective adaptation when audio knowledge is required. Our experiments demonstrate that AudioBERT is quite effective, achieving superior performance on the AuditoryBench. The dataset and code are available at \\bulurl{https://github.com/HJ-Ok/AudioBERT}.",
            "id": "2409.08199",
            "link": "http://arxiv.org/abs/2409.08199v1",
            "published": "2024-09-12T16:36:39+00:00",
            "updated": "2024-09-12T16:36:39+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 11
        },
        "2409.08239": {
            "authors": [
                "Alisia Lupidi",
                "Carlos Gemmell",
                "Nicola Cancedda",
                "Jane Dwivedi-Yu",
                "Jason Weston",
                "Jakob Foerster",
                "Roberta Raileanu",
                "Maria Lomeli"
            ],
            "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
            "abstract": "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.",
            "id": "2409.08239",
            "link": "http://arxiv.org/abs/2409.08239v1",
            "published": "2024-09-12T17:39:08+00:00",
            "updated": "2024-09-12T17:39:08+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 105
        },
        "2409.06096": {
            "authors": [
                "Michele Mancusi",
                "Yurii Halychansky",
                "Kin Wai Cheuk",
                "Chieh-Hsin Lai",
                "Stefan Uhlich",
                "Junghyun Koo",
                "Marco A. Mart\u00ednez-Ram\u00edrez",
                "Wei-Hsiang Liao",
                "Giorgio Fabbro",
                "Yuhki Mitsufuji"
            ],
            "title": "Latent Diffusion Bridges for Unsupervised Musical Audio Timbre Transfer",
            "abstract": "Music timbre transfer is a challenging task that involves modifying the timbral characteristics of an audio signal while preserving its melodic structure. In this paper, we propose a novel method based on dual diffusion bridges, trained using the CocoChorales Dataset, which consists of unpaired monophonic single-instrument audio data. Each diffusion model is trained on a specific instrument with a Gaussian prior. During inference, a model is designated as the source model to map the input audio to its corresponding Gaussian prior, and another model is designated as the target model to reconstruct the target audio from this Gaussian prior, thereby facilitating timbre transfer. We compare our approach against existing unsupervised timbre transfer models such as VAEGAN and Gaussian Flow Bridges (GFB). Experimental results demonstrate that our method achieves both better Fr\\'echet Audio Distance (FAD) and melody preservation, as reflected by lower pitch distances (DPD) compared to VAEGAN and GFB. Additionally, we discover that the noise level from the Gaussian prior, $\\sigma$, can be adjusted to control the degree of melody preservation and amount of timbre transferred.",
            "id": "2409.06096",
            "link": "http://arxiv.org/abs/2409.06096v1",
            "published": "2024-09-09T22:16:48+00:00",
            "updated": "2024-09-09T22:16:48+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.IR",
                "eess.AS"
            ],
            "max_author_hindex": 13
        },
        "2409.06859": {
            "authors": [
                "William English",
                "Dominic Simon",
                "Rickard Ewetz",
                "Sumit Jha"
            ],
            "title": "NSP: A Neuro-Symbolic Natural Language Navigational Planner",
            "abstract": "Path planners that can interpret free-form natural language instructions hold promise to automate a wide range of robotics applications. These planners simplify user interactions and enable intuitive control over complex semi-autonomous systems. While existing symbolic approaches offer guarantees on the correctness and efficiency, they struggle to parse free-form natural language inputs. Conversely, neural approaches based on pre-trained Large Language Models (LLMs) can manage natural language inputs but lack performance guarantees. In this paper, we propose a neuro-symbolic framework for path planning from natural language inputs called NSP. The framework leverages the neural reasoning abilities of LLMs to i) craft symbolic representations of the environment and ii) a symbolic path planning algorithm. Next, a solution to the path planning problem is obtained by executing the algorithm on the environment representation. The framework uses a feedback loop from the symbolic execution environment to the neural generation process to self-correct syntax errors and satisfy execution time constraints. We evaluate our neuro-symbolic approach using a benchmark suite with 1500 path-planning problems. The experimental evaluation shows that our neuro-symbolic approach produces 90.1% valid paths that are on average 19-77% shorter than state-of-the-art neural approaches.",
            "id": "2409.06859",
            "link": "http://arxiv.org/abs/2409.06859v1",
            "published": "2024-09-10T20:49:05+00:00",
            "updated": "2024-09-10T20:49:05+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.HC"
            ],
            "max_author_hindex": 27
        },
        "2409.07335": {
            "authors": [
                "Mehrdad Zakershahrak",
                "Samira Ghodratnama"
            ],
            "title": "Explanation, Debate, Align: A Weak-to-Strong Framework for Language Model Generalization",
            "abstract": "The rapid advancement of artificial intelligence systems has brought the challenge of AI alignment to the forefront of research, particularly in complex decision-making and task execution. As these systems surpass human-level performance in sophisticated problems, ensuring their alignment with human values, intentions, and ethical guidelines becomes crucial. Building on previous work in explanation generation for human-agent alignment, we address the more complex dynamics of multi-agent systems and human-AI teams. This paper introduces a novel approach to model alignment through weak-to-strong generalization in the context of language models. We present a framework where a strong model facilitates the improvement of a weaker model, bridging the gap between explanation generation and model alignment. Our method, formalized as a facilitation function, allows for the transfer of capabilities from advanced models to less capable ones without direct access to extensive training data. Our results suggest that this facilitation-based approach not only enhances model performance but also provides insights into the nature of model alignment and the potential for scalable oversight of AI systems.",
            "id": "2409.07335",
            "link": "http://arxiv.org/abs/2409.07335v1",
            "published": "2024-09-11T15:16:25+00:00",
            "updated": "2024-09-11T15:16:25+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 8
        },
        "2409.08069": {
            "authors": [
                "Aili Chen",
                "Xuyang Ge",
                "Ziquan Fu",
                "Yanghua Xiao",
                "Jiangjie Chen"
            ],
            "title": "TravelAgent: An AI Assistant for Personalized Travel Planning",
            "abstract": "As global tourism expands and artificial intelligence technology advances, intelligent travel planning services have emerged as a significant research focus. Within dynamic real-world travel scenarios with multi-dimensional constraints, services that support users in automatically creating practical and customized travel itineraries must address three key objectives: Rationality, Comprehensiveness, and Personalization. However, existing systems with rule-based combinations or LLM-based planning methods struggle to fully satisfy these criteria. To overcome the challenges, we introduce TravelAgent, a travel planning system powered by large language models (LLMs) designed to provide reasonable, comprehensive, and personalized travel itineraries grounded in dynamic scenarios. TravelAgent comprises four modules: Tool-usage, Recommendation, Planning, and Memory Module. We evaluate TravelAgent's performance with human and simulated users, demonstrating its overall effectiveness in three criteria and confirming the accuracy of personalized recommendations.",
            "id": "2409.08069",
            "link": "http://arxiv.org/abs/2409.08069v1",
            "published": "2024-09-12T14:24:45+00:00",
            "updated": "2024-09-12T14:24:45+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 28
        },
        "2409.05486": {
            "authors": [
                "Camilo Thorne",
                "Christian Druckenbrodt",
                "Kinga Szarkowska",
                "Deepika Goyal",
                "Pranita Marajan",
                "Vijay Somanath",
                "Corey Harper",
                "Mao Yan",
                "Tony Scerri"
            ],
            "title": "Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models",
            "abstract": "The quality and capabilities of large language models cannot be currently fully assessed with automated, benchmark evaluations. Instead, human evaluations that expand on traditional qualitative techniques from natural language generation literature are required. One recent best-practice consists in using A/B-testing frameworks, which capture preferences of human evaluators for specific models. In this paper we describe a human evaluation experiment focused on the biomedical domain (health, biology, chemistry/pharmacology) carried out at Elsevier. In it a large but not massive (8.8B parameter) decoder-only foundational transformer trained on a relatively small (135B tokens) but highly curated collection of Elsevier datasets is compared to OpenAI's GPT-3.5-turbo and Meta's foundational 7B parameter Llama 2 model against multiple criteria. Results indicate -- even if IRR scores were generally low -- a preference towards GPT-3.5-turbo, and hence towards models that possess conversational abilities, are very large and were trained on very large datasets. But at the same time, indicate that for less massive models training on smaller but well-curated training sets can potentially give rise to viable alternatives in the biomedical domain.",
            "id": "2409.05486",
            "link": "http://arxiv.org/abs/2409.05486v1",
            "published": "2024-09-09T10:30:00+00:00",
            "updated": "2024-09-09T10:30:00+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 19
        },
        "2409.05591": {
            "authors": [
                "Hongjin Qian",
                "Peitian Zhang",
                "Zheng Liu",
                "Kelong Mao",
                "Zhicheng Dou"
            ],
            "title": "MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery",
            "abstract": "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access external databases, thereby enhancing the generation quality of large language models (LLMs) through optimized context. However, the existing retrieval methods are constrained inherently, as they can only perform relevance matching between explicitly stated queries and well-formed knowledge, but unable to handle tasks involving ambiguous information needs or unstructured knowledge. Consequently, existing RAG systems are primarily effective for straightforward question-answering tasks. In this work, we propose MemoRAG, a novel retrieval-augmented generation paradigm empowered by long-term memory. MemoRAG adopts a dual-system architecture. On the one hand, it employs a light but long-range LLM to form the global memory of database. Once a task is presented, it generates draft answers, cluing the retrieval tools to locate useful information within the database. On the other hand, it leverages an expensive but expressive LLM, which generates the ultimate answer based on the retrieved information. Building on this general framework, we further optimize MemoRAG's performance by enhancing its cluing mechanism and memorization capacity. In our experiment, MemoRAG achieves superior performance across a variety of evaluation tasks, including both complex ones where conventional RAG fails and straightforward ones where RAG is commonly applied.",
            "id": "2409.05591",
            "link": "http://arxiv.org/abs/2409.05591v2",
            "published": "2024-09-09T13:20:31+00:00",
            "updated": "2024-09-10T02:01:43+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 34
        },
        "2409.06624": {
            "authors": [
                "Ningyuan Xi",
                "Yetao Wu",
                "Kun Fan",
                "Teng Chen",
                "Qingqing Gu",
                "Peng Yu",
                "Jinxian Qu",
                "Chenxi Liu",
                "Zhonglin Jiang",
                "Yong Chen",
                "Luo Ji"
            ],
            "title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio",
            "abstract": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to obtain the unfamiliar language skill or adapt into new domains. The huge training cost of CPT often asks for cautious choice of key hyper-parameters such as the mixture ratio of extra language or domain corpus. However, there is no systematic study which bridge the gap between the optimal mixture ratio and the actual model performance, and the gap between experimental scaling law and the actual deployment in the full model size. In this paper, we perform CPT on Llama-3 8B and 70B to enhance its Chinese ability. We study the optimal correlation between the Additional Language Mixture Ratio (ALMR) and the Learning Rate (LR) on the 8B size which directly indicate the optimal experimental set up. By thorough choice of hyper-parameter, and subsequent fine-tuning, the model capability is improved not only on the Chinese-related benchmark, but also some specific domains including math, coding and emotional intelligence. We deploy the final 70B version of LLM on an real-life chat system which obtain satisfying performance.",
            "id": "2409.06624",
            "link": "http://arxiv.org/abs/2409.06624v1",
            "published": "2024-09-10T16:26:43+00:00",
            "updated": "2024-09-10T16:26:43+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 29
        },
        "2409.06666": {
            "authors": [
                "Qingkai Fang",
                "Shoutao Guo",
                "Yan Zhou",
                "Zhengrui Ma",
                "Shaolei Zhang",
                "Yang Feng"
            ],
            "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
            "abstract": "Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.",
            "id": "2409.06666",
            "link": "http://arxiv.org/abs/2409.06666v1",
            "published": "2024-09-10T17:34:34+00:00",
            "updated": "2024-09-10T17:34:34+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.SD",
                "eess.AS",
                "I.2.7"
            ],
            "max_author_hindex": 42
        },
        "2409.07136": {
            "authors": [
                "Rui Ye",
                "Rui Ge",
                "Yuchi Fengting",
                "Jingyi Chai",
                "Yanfeng Wang",
                "Siheng Chen"
            ],
            "title": "Leveraging Unstructured Text Data for Federated Instruction Tuning of Large Language Models",
            "abstract": "Federated instruction tuning enables multiple clients to collaboratively fine-tune a shared large language model (LLM) that can follow humans' instructions without directly sharing raw data. However, existing literature impractically requires that all the clients readily hold instruction-tuning data (i.e., structured instruction-response pairs), which necessitates massive human annotations since clients' data is usually unstructured text instead. Addressing this, we propose a novel and flexible framework FedIT-U2S, which can automatically transform unstructured corpus into structured data for federated instruction tuning. FedIT-U2S consists two key steps: (1) few-shot instruction-tuning data generation, where each unstructured data piece together with several examples is combined to prompt an LLM in generating an instruction-response pair. To further enhance the flexibility, a retrieval-based example selection technique is proposed, where the examples are automatically selected based on the relatedness between the client's data piece and example pool, bypassing the need of determining examples in advance. (2) A typical federated instruction tuning process based on the generated data. Overall, FedIT-U2S can be applied to diverse scenarios as long as the client holds valuable text corpus, broadening the application scope of federated instruction tuning. We conduct a series of experiments on three domains (medicine, knowledge, and math), showing that our proposed FedIT-U2S can consistently and significantly brings improvement over the base LLM.",
            "id": "2409.07136",
            "link": "http://arxiv.org/abs/2409.07136v1",
            "published": "2024-09-11T09:31:44+00:00",
            "updated": "2024-09-11T09:31:44+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.MA"
            ],
            "max_author_hindex": 40
        },
        "2409.07314": {
            "authors": [
                "Praveen K Kanithi",
                "Cl\u00e9ment Christophe",
                "Marco AF Pimentel",
                "Tathagata Raha",
                "Nada Saadi",
                "Hamza Javed",
                "Svetlana Maslenkova",
                "Nasir Hayat",
                "Ronnie Rajan",
                "Shadab Khan"
            ],
            "title": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications",
            "abstract": "The rapid development of Large Language Models (LLMs) for healthcare applications has spurred calls for holistic evaluation beyond frequently-cited benchmarks like USMLE, to better reflect real-world performance. While real-world assessments are valuable indicators of utility, they often lag behind the pace of LLM evolution, likely rendering findings obsolete upon deployment. This temporal disconnect necessitates a comprehensive upfront evaluation that can guide model selection for specific clinical applications. We introduce MEDIC, a framework assessing LLMs across five critical dimensions of clinical competence: medical reasoning, ethics and bias, data and language understanding, in-context learning, and clinical safety. MEDIC features a novel cross-examination framework quantifying LLM performance across areas like coverage and hallucination detection, without requiring reference outputs. We apply MEDIC to evaluate LLMs on medical question-answering, safety, summarization, note generation, and other tasks. Our results show performance disparities across model sizes, baseline vs medically finetuned models, and have implications on model selection for applications requiring specific model strengths, such as low hallucination or lower cost of inference. MEDIC's multifaceted evaluation reveals these performance trade-offs, bridging the gap between theoretical capabilities and practical implementation in healthcare settings, ensuring that the most promising models are identified and adapted for diverse healthcare applications.",
            "id": "2409.07314",
            "link": "http://arxiv.org/abs/2409.07314v1",
            "published": "2024-09-11T14:44:51+00:00",
            "updated": "2024-09-11T14:44:51+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 88
        },
        "2409.07372": {
            "authors": [
                "Daniel Zhang-Li",
                "Zheyuan Zhang",
                "Jifan Yu",
                "Joy Lim Jia Yin",
                "Shangqing Tu",
                "Linlu Gong",
                "Haohua Wang",
                "Zhiyuan Liu",
                "Huiqin Liu",
                "Lei Hou",
                "Juanzi Li"
            ],
            "title": "Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring System via Language Model Coordination",
            "abstract": "The vast pre-existing slides serve as rich and important materials to carry lecture knowledge. However, effectively leveraging lecture slides to serve students is difficult due to the multi-modal nature of slide content and the heterogeneous teaching actions. We study the problem of discovering effective designs that convert a slide into an interactive lecture. We develop Slide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring system that can (1) effectively convert an input lecture slide into a structured teaching agenda consisting of a set of heterogeneous teaching actions; (2) create and manage an interactive lecture that generates responsive interactions catering to student learning demands while regulating the interactions to follow teaching actions. Slide2Lecture contains a complete pipeline for learners to obtain an interactive classroom experience to learn the slide. For teachers and developers, Slide2Lecture enables customization to cater to personalized demands. The evaluation rated by annotators and students shows that Slide2Lecture is effective in outperforming the remaining implementation. Slide2Lecture's online deployment has made more than 200K interaction with students in the 3K lecture sessions. We open source Slide2Lecture's implementation in https://anonymous.4open.science/r/slide2lecture-4210/.",
            "id": "2409.07372",
            "link": "http://arxiv.org/abs/2409.07372v1",
            "published": "2024-09-11T16:03:09+00:00",
            "updated": "2024-09-11T16:03:09+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.HC"
            ],
            "max_author_hindex": 51
        },
        "2409.07672": {
            "authors": [
                "Xia Hou",
                "Qifeng Li",
                "Tongliang Li"
            ],
            "title": "An Unsupervised Dialogue Topic Segmentation Model Based on Utterance Rewriting",
            "abstract": "Dialogue topic segmentation plays a crucial role in various types of dialogue modeling tasks. The state-of-the-art unsupervised DTS methods learn topic-aware discourse representations from conversation data through adjacent discourse matching and pseudo segmentation to further mine useful clues in unlabeled conversational relations. However, in multi-round dialogs, discourses often have co-references or omissions, leading to the fact that direct use of these discourses for representation learning may negatively affect the semantic similarity computation in the neighboring discourse matching task. In order to fully utilize the useful cues in conversational relations, this study proposes a novel unsupervised dialog topic segmentation method that combines the Utterance Rewriting (UR) technique with an unsupervised learning algorithm to efficiently utilize the useful cues in unlabeled dialogs by rewriting the dialogs in order to recover the co-referents and omitted words. Compared with existing unsupervised models, the proposed Discourse Rewriting Topic Segmentation Model (UR-DTS) significantly improves the accuracy of topic segmentation. The main finding is that the performance on DialSeg711 improves by about 6% in terms of absolute error score and WD, achieving 11.42% in terms of absolute error score and 12.97% in terms of WD. on Doc2Dial the absolute error score and WD improves by about 3% and 2%, respectively, resulting in SOTA reaching 35.17% in terms of absolute error score and 38.49% in terms of WD. This shows that the model is very effective in capturing the nuances of conversational topics, as well as the usefulness and challenges of utilizing unlabeled conversations.",
            "id": "2409.07672",
            "link": "http://arxiv.org/abs/2409.07672v1",
            "published": "2024-09-12T00:27:31+00:00",
            "updated": "2024-09-12T00:27:31+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 16
        },
        "2409.08185": {
            "authors": [
                "Aaron Steiner",
                "Ralph Peeters",
                "Christian Bizer"
            ],
            "title": "Fine-tuning Large Language Models for Entity Matching",
            "abstract": "Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model's ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini.",
            "id": "2409.08185",
            "link": "http://arxiv.org/abs/2409.08185v1",
            "published": "2024-09-12T16:20:57+00:00",
            "updated": "2024-09-12T16:20:57+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "68T50",
                "I.2.7"
            ],
            "max_author_hindex": 61
        },
        "2409.05556": {
            "authors": [
                "Alireza Ghafarollahi",
                "Markus J. Buehler"
            ],
            "title": "SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning",
            "abstract": "A key challenge in artificial intelligence is the creation of systems capable of autonomously advancing scientific understanding by exploring novel domains, identifying complex patterns, and uncovering previously unseen connections in vast scientific data. In this work, we present SciAgents, an approach that leverages three core concepts: (1) the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts, (2) a suite of large language models (LLMs) and data retrieval tools, and (3) multi-agent systems with in-situ learning capabilities. Applied to biologically inspired materials, SciAgents reveals hidden interdisciplinary relationships that were previously considered unrelated, achieving a scale, precision, and exploratory power that surpasses traditional human-driven research methods. The framework autonomously generates and refines research hypotheses, elucidating underlying mechanisms, design principles, and unexpected material properties. By integrating these capabilities in a modular fashion, the intelligent system yields material discoveries, critique and improve existing hypotheses, retrieve up-to-date data about existing research, and highlights their strengths and limitations. Our case studies demonstrate scalable capabilities to combine generative AI, ontological representations, and multi-agent modeling, harnessing a `swarm of intelligence' similar to biological systems. This provides new avenues for materials discovery and accelerates the development of advanced materials by unlocking Nature's design principles.",
            "id": "2409.05556",
            "link": "http://arxiv.org/abs/2409.05556v1",
            "published": "2024-09-09T12:25:10+00:00",
            "updated": "2024-09-09T12:25:10+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cond-mat.dis-nn",
                "cond-mat.mtrl-sci",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 107
        },
        "2409.07440": {
            "authors": [
                "Ben Bogin",
                "Kejuan Yang",
                "Shashank Gupta",
                "Kyle Richardson",
                "Erin Bransom",
                "Peter Clark",
                "Ashish Sabharwal",
                "Tushar Khot"
            ],
            "title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories",
            "abstract": "Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPERaims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.",
            "id": "2409.07440",
            "link": "http://arxiv.org/abs/2409.07440v1",
            "published": "2024-09-11T17:37:48+00:00",
            "updated": "2024-09-11T17:37:48+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.SE"
            ],
            "max_author_hindex": 50
        },
        "2409.07638": {
            "authors": [
                "Thomas Ball",
                "Shuo Chen",
                "Cormac Herley"
            ],
            "title": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4 Capabilities",
            "abstract": "In this paper we explore evaluation of LLM capabilities. We present measurements of GPT-4 performance on several deterministic tasks; each task involves a basic calculation and takes as input parameter some element drawn from a large well-defined population (e.g., count elements in a list, multiply two k-digit numbers, etc). We examine several conditions per-task and perform enough trials so that statistically significant differences can be detected. This allows us to investigate the sensitivity of task-accuracy both to query phrasing and input parameter population. We find that seemingly trivial modifications in the task-prompt or input population can yield differences far larger than can be explained by sampling effects. For example, performance on a simple list-counting task varies with query-phrasing and list-length, but also with list composition (i.e., the thing-to-be-counted) and object frequency (e.g., success when an element accounts for $\\approx$ 50\\% of a list is different from when it accounts for $\\approx$ 70\\% etc).   We conclude that efforts to quantify LLM capabilities easily succumb to the language-as-fixed-effect fallacy, where experimental observations are improperly generalized beyond what the data supports. A consequence appears to be that intuitions that have been formed based on interactions with humans form a very unreliable guide as to which input modifications should ``make no difference'' to LLM performance.",
            "id": "2409.07638",
            "link": "http://arxiv.org/abs/2409.07638v1",
            "published": "2024-09-11T21:48:33+00:00",
            "updated": "2024-09-11T21:48:33+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 54
        },
        "2409.07703": {
            "authors": [
                "Liqiang Jing",
                "Zhehui Huang",
                "Xiaoyang Wang",
                "Wenlin Yao",
                "Wenhao Yu",
                "Kaixin Ma",
                "Hongming Zhang",
                "Xinya Du",
                "Dong Yu"
            ],
            "title": "DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?",
            "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.",
            "id": "2409.07703",
            "link": "http://arxiv.org/abs/2409.07703v1",
            "published": "2024-09-12T02:08:00+00:00",
            "updated": "2024-09-12T02:08:00+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 64
        },
        "2409.06635": {
            "authors": [
                "Wenyu Zhang",
                "Shuo Sun",
                "Bin Wang",
                "Xunlong Zou",
                "Zhuohan Liu",
                "Yingxu He",
                "Geyu Lin",
                "Nancy F. Chen",
                "Ai Ti Aw"
            ],
            "title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders",
            "abstract": "The rapid advancements in large language models (LLMs) have significantly enhanced natural language processing capabilities, facilitating the development of AudioLLMs that process and understand speech and audio inputs alongside text. Existing AudioLLMs typically combine a pre-trained audio encoder with a pre-trained LLM, which are subsequently finetuned on specific audio tasks. However, the pre-trained audio encoder has constrained capacity to capture features for new tasks and datasets. To address this, we propose to incorporate mixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE supplements a base encoder with a pool of relatively light weight encoders, selectively activated based on the audio input to enhance feature extraction without significantly increasing model size. Our empirical results demonstrate that MoWE effectively improves multi-task performance, broadening the applicability of AudioLLMs to more diverse audio tasks.",
            "id": "2409.06635",
            "link": "http://arxiv.org/abs/2409.06635v1",
            "published": "2024-09-10T16:46:18+00:00",
            "updated": "2024-09-10T16:46:18+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 24
        },
        "2409.06691": {
            "authors": [
                "Hiroki Furuta",
                "Kuang-Huei Lee",
                "Shixiang Shane Gu",
                "Yutaka Matsuo",
                "Aleksandra Faust",
                "Heiga Zen",
                "Izzeddin Gur"
            ],
            "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels",
            "abstract": "Many algorithms for aligning LLMs with human preferences assume that human preferences are binary and deterministic. However, it is reasonable to think that they can vary with different individuals, and thus should be distributional to reflect the fine-grained relationship between the responses. In this work, we introduce the distributional soft preference labels and improve Direct Preference Optimization (DPO) with a weighted geometric average of the LLM output likelihood in the loss function. In doing so, the scale of learning loss is adjusted based on the soft labels, and the loss with equally preferred responses would be close to zero. This simple modification can be easily applied to any DPO family and helps the models escape from the over-optimization and objective mismatch prior works suffer from. In our experiments, we simulate the soft preference labels with AI feedback from LLMs and demonstrate that geometric averaging consistently improves performance on standard benchmarks for alignment research. In particular, we observe more preferable responses than binary labels and significant improvements with data where modestly-confident labels are in the majority.",
            "id": "2409.06691",
            "link": "http://arxiv.org/abs/2409.06691v1",
            "published": "2024-09-10T17:54:28+00:00",
            "updated": "2024-09-10T17:54:28+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 52
        },
        "2409.07503": {
            "authors": [
                "Lijia Lv",
                "Weigang Zhang",
                "Xuehai Tang",
                "Jie Wen",
                "Feng Liu",
                "Jizhong Han",
                "Songlin Hu"
            ],
            "title": "AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs",
            "abstract": "Jailbreak vulnerabilities in Large Language Models (LLMs) refer to methods that extract malicious content from the model by carefully crafting prompts or suffixes, which has garnered significant attention from the research community. However, traditional attack methods, which primarily focus on the semantic level, are easily detected by the model. These methods overlook the difference in the model's alignment protection capabilities at different output stages. To address this issue, we propose an adaptive position pre-fill jailbreak attack approach for executing jailbreak attacks on LLMs. Our method leverages the model's instruction-following capabilities to first output pre-filled safe content, then exploits its narrative-shifting abilities to generate harmful content. Extensive black-box experiments demonstrate our method can improve the attack success rate by 47% on the widely recognized secure model (Llama2) compared to existing approaches. Our code can be found at: https://github.com/Yummy416/AdaPPA.",
            "id": "2409.07503",
            "link": "http://arxiv.org/abs/2409.07503v1",
            "published": "2024-09-11T00:00:58+00:00",
            "updated": "2024-09-11T00:00:58+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 43
        },
        "2409.07747": {
            "authors": [
                "Yanan Wang",
                "Shuichiro Haruta",
                "Donghuo Zeng",
                "Julio Vizcarra",
                "Mori Kurokawa"
            ],
            "title": "Multi-object event graph representation learning for Video Question Answering",
            "abstract": "Video question answering (VideoQA) is a task to predict the correct answer to questions posed about a given video. The system must comprehend spatial and temporal relationships among objects extracted from videos to perform causal and temporal reasoning. While prior works have focused on modeling individual object movements using transformer-based methods, they falter when capturing complex scenarios involving multiple objects (e.g., \"a boy is throwing a ball in a hoop\"). We propose a contrastive language event graph representation learning method called CLanG to address this limitation. Aiming to capture event representations associated with multiple objects, our method employs a multi-layer GNN-cluster module for adversarial graph representation learning, enabling contrastive learning between the question text and its relevant multi-object event graph. Our method outperforms a strong baseline, achieving up to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal and temporal questions, highlighting its strength in reasoning multiple object-based events.",
            "id": "2409.07747",
            "link": "http://arxiv.org/abs/2409.07747v1",
            "published": "2024-09-12T04:42:51+00:00",
            "updated": "2024-09-12T04:42:51+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 7
        },
        "2409.07748": {
            "authors": [
                "Yanan Wang",
                "Shuichiro Haruta",
                "Donghuo Zeng",
                "Julio Vizcarra",
                "Mori Kurokawa"
            ],
            "title": "Top-down Activity Representation Learning for Video Question Answering",
            "abstract": "Capturing complex hierarchical human activities, from atomic actions (e.g., picking up one present, moving to the sofa, unwrapping the present) to contextual events (e.g., celebrating Christmas) is crucial for achieving high-performance video question answering (VideoQA). Recent works have expanded multimodal models (e.g., CLIP, LLaVA) to process continuous video sequences, enhancing the model's temporal reasoning capabilities. However, these approaches often fail to capture contextual events that can be decomposed into multiple atomic actions non-continuously distributed over relatively long-term sequences. In this paper, to leverage the spatial visual context representation capability of the CLIP model for obtaining non-continuous visual representations in terms of contextual events in videos, we convert long-term video sequences into a spatial image domain and finetune the multimodal model LLaVA for the VideoQA task. Our approach achieves competitive performance on the STAR task, in particular, with a 78.4% accuracy score, exceeding the current state-of-the-art score by 2.8 points on the NExTQA task.",
            "id": "2409.07748",
            "link": "http://arxiv.org/abs/2409.07748v1",
            "published": "2024-09-12T04:43:27+00:00",
            "updated": "2024-09-12T04:43:27+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 7
        },
        "2409.08234": {
            "authors": [
                "Hakan T. Otal",
                "M. Abdullah Canbaz"
            ],
            "title": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems",
            "abstract": "The rapid evolution of cyber threats necessitates innovative solutions for detecting and analyzing malicious activity. Honeypots, which are decoy systems designed to lure and interact with attackers, have emerged as a critical component in cybersecurity. In this paper, we present a novel approach to creating realistic and interactive honeypot systems using Large Language Models (LLMs). By fine-tuning a pre-trained open-source language model on a diverse dataset of attacker-generated commands and responses, we developed a honeypot capable of sophisticated engagement with attackers. Our methodology involved several key steps: data collection and processing, prompt engineering, model selection, and supervised fine-tuning to optimize the model's performance. Evaluation through similarity metrics and live deployment demonstrated that our approach effectively generates accurate and informative responses. The results highlight the potential of LLMs to revolutionize honeypot technology, providing cybersecurity professionals with a powerful tool to detect and analyze malicious activity, thereby enhancing overall security infrastructure.",
            "id": "2409.08234",
            "link": "http://arxiv.org/abs/2409.08234v1",
            "published": "2024-09-12T17:33:06+00:00",
            "updated": "2024-09-12T17:33:06+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "cs.NI",
                "68T50, 68M10",
                "I.2.7; D.4.6; K.6.5"
            ],
            "max_author_hindex": 6
        },
        "2409.08013": {
            "authors": [
                "Mihail Stoian",
                "Andreas Kipf"
            ],
            "title": "DPconv: Super-Polynomially Faster Join Ordering",
            "abstract": "We revisit the join ordering problem in query optimization. The standard exact algorithm, DPccp, has a worst-case running time of $O(3^n)$. This is prohibitively expensive for large queries, which are not that uncommon anymore. We develop a new algorithmic framework based on subset convolution. DPconv achieves a super-polynomial speedup over DPccp, breaking the $O(3^n)$ time-barrier for the first time. We show that the instantiation of our framework for the $C_\\max$ cost function is up to 30x faster than DPccp for large clique queries.",
            "id": "2409.08013",
            "link": "http://arxiv.org/abs/2409.08013v1",
            "published": "2024-09-12T12:56:43+00:00",
            "updated": "2024-09-12T12:56:43+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 18
        },
        "2409.06468": {
            "authors": [
                "Yi-Cheng Wang",
                "Li-Ting Pai",
                "Bi-Cheng Yan",
                "Hsin-Wei Wang",
                "Chi-Han Lin",
                "Berlin Chen"
            ],
            "title": "An Effective Context-Balanced Adaptation Approach for Long-Tailed Speech Recognition",
            "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models have become standard practice for various commercial applications. However, in real-world scenarios, the long-tailed nature of word distribution often leads E2E ASR models to perform well on common words but fall short in recognizing uncommon ones. Recently, the notion of a contextual adapter (CA) was proposed to infuse external knowledge represented by a context word list into E2E ASR models. Although CA can improve recognition performance on rare words, two crucial data imbalance problems remain. First, when using low-frequency words as context words during training, since these words rarely occur in the utterance, CA becomes prone to overfit on attending to the <no-context> token due to higher-frequency words not being present in the context list. Second, the long-tailed distribution within the context list itself still causes the model to perform poorly on low-frequency context words. In light of this, we explore in-depth the impact of altering the context list to have words with different frequency distributions on model performance, and meanwhile extend CA with a simple yet effective context-balanced learning objective. A series of experiments conducted on the AISHELL-1 benchmark dataset suggests that using all vocabulary words from the training corpus as the context list and pairing them with our balanced objective yields the best performance, demonstrating a significant reduction in character error rate (CER) by up to 1.21% and a more pronounced 9.44% reduction in the error rate of zero-shot words.",
            "id": "2409.06468",
            "link": "http://arxiv.org/abs/2409.06468v1",
            "published": "2024-09-10T12:52:36+00:00",
            "updated": "2024-09-10T12:52:36+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 96
        },
        "2409.06067": {
            "authors": [
                "Jianyi Zhang",
                "Hao Frank Yang",
                "Ang Li",
                "Xin Guo",
                "Pu Wang",
                "Haiming Wang",
                "Yiran Chen",
                "Hai Li"
            ],
            "title": "MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data",
            "abstract": "Previous studies on federated learning (FL) often encounter performance degradation due to data heterogeneity among different clients. In light of the recent advances in multimodal large language models (MLLMs), such as GPT-4v and LLaVA, which demonstrate their exceptional proficiency in multimodal tasks, such as image captioning and multimodal question answering. We introduce a novel federated learning framework, named Multimodal Large Language Model Assisted Federated Learning (MLLM-FL), which which employs powerful MLLMs at the server end to address the heterogeneous and long-tailed challenges. Owing to the advanced cross-modality representation capabilities and the extensive open-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing the extensive, yet previously underexploited, open-source data accessible from websites and powerful server-side computational resources. Hence, the MLLM-FL not only enhances the performance but also avoids increasing the risk of privacy leakage and the computational burden on local devices, distinguishing it from prior methodologies. Our framework has three key stages. Initially, prior to local training on local datasets of clients, we conduct global visual-text pretraining of the model. This pretraining is facilitated by utilizing the extensive open-source data available online, with the assistance of multimodal large language models. Subsequently, the pretrained model is distributed among various clients for local training. Finally, once the locally trained models are transmitted back to the server, a global alignment is carried out under the supervision of MLLMs to further enhance the performance. Experimental evaluations on established benchmarks, show that our framework delivers promising performance in the typical scenarios with data heterogeneity and long-tail distribution across different clients in FL.",
            "id": "2409.06067",
            "link": "http://arxiv.org/abs/2409.06067v1",
            "published": "2024-09-09T21:04:16+00:00",
            "updated": "2024-09-09T21:04:16+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 55
        },
        "2409.07402": {
            "authors": [
                "Benoit Dufumier",
                "Javiera Castillo-Navarro",
                "Devis Tuia",
                "Jean-Philippe Thiran"
            ],
            "title": "What to align in multimodal contrastive learning?",
            "abstract": "Humans perceive the world through multisensory integration, blending the information of different modalities to adapt their behavior. Contrastive learning offers an appealing solution for multimodal self-supervised learning. Indeed, by considering each modality as a different view of the same entity, it learns to align features of different modalities in a shared representation space. However, this approach is intrinsically limited as it only learns shared or redundant information between modalities, while multimodal interactions can arise in other ways. In this work, we introduce CoMM, a Contrastive MultiModal learning strategy that enables the communication between modalities in a single multimodal space. Instead of imposing cross- or intra- modality constraints, we propose to align multimodal representations by maximizing the mutual information between augmented versions of these multimodal features. Our theoretical analysis shows that shared, synergistic and unique terms of information naturally emerge from this formulation, allowing us to estimate multimodal interactions beyond redundancy. We test CoMM both in a controlled and in a series of real-world settings: in the former, we demonstrate that CoMM effectively captures redundant, unique and synergistic information between modalities. In the latter, CoMM learns complex multimodal interactions and achieves state-of-the-art results on the six multimodal benchmarks.",
            "id": "2409.07402",
            "link": "http://arxiv.org/abs/2409.07402v1",
            "published": "2024-09-11T16:42:22+00:00",
            "updated": "2024-09-11T16:42:22+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CV"
            ],
            "max_author_hindex": 68
        },
        "2409.07431": {
            "authors": [
                "Zitong Yang",
                "Neil Band",
                "Shuangping Li",
                "Emmanuel Cand\u00e8s",
                "Tatsunori Hashimoto"
            ],
            "title": "Synthetic continued pretraining",
            "abstract": "Pretraining on large-scale, unstructured internet text has enabled language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient -- to learn a given fact, models must be trained on hundreds to thousands of diverse representations of it. This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once. We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus. We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source documents and then generates diverse text by drawing connections between the sampled entities. Synthetic continued pretraining using EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them. If instead, the source documents are available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation. To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can \"rearrange\" knowledge to enable more data-efficient learning.",
            "id": "2409.07431",
            "link": "http://arxiv.org/abs/2409.07431v1",
            "published": "2024-09-11T17:21:59+00:00",
            "updated": "2024-09-11T17:21:59+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "stat.ML"
            ],
            "max_author_hindex": 99
        },
        "2409.08202": {
            "authors": [
                "Joy Hsu",
                "Jiayuan Mao",
                "Joshua B. Tenenbaum",
                "Noah D. Goodman",
                "Jiajun Wu"
            ],
            "title": "What Makes a Maze Look Like a Maze?",
            "abstract": "A unique aspect of human visual understanding is the ability to flexibly interpret abstract concepts: acquiring lifted rules explaining what they symbolize, grounding them across familiar and unfamiliar contexts, and making predictions or reasoning about them. While off-the-shelf vision-language models excel at making literal interpretations of images (e.g., recognizing object categories such as tree branches), they still struggle to make sense of such visual abstractions (e.g., how an arrangement of tree branches may form the walls of a maze). To address this challenge, we introduce Deep Schema Grounding (DSG), a framework that leverages explicit structured representations of visual abstractions for grounding and reasoning. At the core of DSG are schemas--dependency graph descriptions of abstract concepts that decompose them into more primitive-level symbols. DSG uses large language models to extract schemas, then hierarchically grounds concrete to abstract components of the schema onto images with vision-language models. The grounded schema is used to augment visual abstraction understanding. We systematically evaluate DSG and different methods in reasoning on our new Visual Abstractions Dataset, which consists of diverse, real-world images of abstract concepts and corresponding question-answer pairs labeled by humans. We show that DSG significantly improves the abstract visual reasoning performance of vision-language models, and is a step toward human-aligned understanding of visual abstractions.",
            "id": "2409.08202",
            "link": "http://arxiv.org/abs/2409.08202v1",
            "published": "2024-09-12T16:41:47+00:00",
            "updated": "2024-09-12T16:41:47+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 126
        },
        "2409.06148": {
            "authors": [
                "Xinjie Zhou",
                "Mengxuan Zhang",
                "Lei Li",
                "Xiaofang Zhou"
            ],
            "title": "High Throughput Shortest Distance Query Processing on Large Dynamic Road Networks",
            "abstract": "Shortest path (SP) computation is the building block for many location-based services, and achieving high throughput SP query processing is an essential goal for the real-time response of those services. However, the large number of queries submitted in large-scale dynamic road networks still poses challenges to this goal. Therefore, in this work, we propose a novel framework aiming to process SP queries with high throughput in large and dynamic road networks, by leveraging the Partitioned Shortest Path (PSP) index. Specifically, we first put forward a cross-boundary strategy to accelerate the query processing of PSP index and analyze its efficiency upper-bound by discovering the curse of PSP index query efficiency. After that, we propose a non-trivial Partitioned Multi-stage Hub Labeling (PMHL) that utilizes multiple PSP strategies and thread parallelization to achieve consecutive query efficiency improvement and fast index maintenance. Finally, to further increase query throughput, we design tree decomposition-based graph partitioning and propose Post-partitioned Multi-stage Hub Labeling (PostMHL) with faster query processing and index update than PMHL. Experiments on real-world road networks show that our methods outperform state-of-the-art baselines in query throughput, yielding up to 1-4 orders of magnitude improvement.",
            "id": "2409.06148",
            "link": "http://arxiv.org/abs/2409.06148v1",
            "published": "2024-09-10T01:51:14+00:00",
            "updated": "2024-09-10T01:51:14+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 64
        },
        "2409.06936": {
            "authors": [
                "Xinran Wu",
                "Hui Zou",
                "Yidan Xing",
                "Jingjing Qu",
                "Qiongxiu Li",
                "Renxia Xue",
                "Xiaoming Fu"
            ],
            "title": "Intelligent Innovation Dataset on Scientific Research Outcomes and Patents",
            "abstract": "Various stakeholders, such as researchers, government agencies, businesses, and laboratories require reliable scientific research outcomes and patent data to support their work. These data are crucial for advancing scientific research, conducting business evaluations, and policy analysis. However, collecting such data is often a time-consuming and laborious task. Consequently, many users turn to using openly accessible data for their research. However, these open data releases may suffer from lack of relationship between different data sources or limited temporal coverage. In this context, we present a new Intelligent Innovation Dataset (IIDS dataset), which comprises six inter-related datasets spanning nearly 120 years, encompassing paper information, paper citation relationships, patent details, patent legal statuses, funding information and funding relationship. The extensive contextual and extensive temporal coverage of the IIDS dataset will provide researchers with comprehensive data support, enabling them to delve into in-depth scientific research and conduct thorough data analysis.",
            "id": "2409.06936",
            "link": "http://arxiv.org/abs/2409.06936v1",
            "published": "2024-09-11T01:29:45+00:00",
            "updated": "2024-09-11T01:29:45+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.DL"
            ],
            "max_author_hindex": 13
        },
        "2409.07083": {
            "authors": [
                "Albert K. Engstfeld",
                "Johannes M. Hermann",
                "Nicolas G. H\u00f6rmann",
                "Julian R\u00fcth"
            ],
            "title": "echemdb Toolkit -- a Lightweight Approach to Getting Data Ready for Data Management Solutions",
            "abstract": "According to the FAIR (findability, accessibility, interoperability, and reusability) principles, scientific data should always be stored with machine-readable descriptive metadata. Existing solutions to store data with metadata, such as electronic lab notebooks (ELN), are often very domain-specific and not sufficiently generic for arbitrary experimental or computational results.   In this work, we present open-source echemdb toolkit for creating and handling data and metadata. The toolkit is running entirely on the file system level using a file-based approach, which facilitates integration with other tools in a FAIR data life cycle and means that no complicated server setup is required. This also makes the toolkit more accessible to the average researcher since no understanding of more sophisticated database technologies is required.   We showcase several aspects and applications of the toolkit: automatic annotation of raw research data with human- and machine-readable metadata, data conversion into standardised frictionless Data Packages, and an API for exploring the data. We also illustrate the web frameworks to illustrate the data using example data from research into energy conversion and storage.",
            "id": "2409.07083",
            "link": "http://arxiv.org/abs/2409.07083v1",
            "published": "2024-09-11T08:10:45+00:00",
            "updated": "2024-09-11T08:10:45+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 18
        },
        "2409.05292": {
            "authors": [
                "Nirmalya Thakur"
            ],
            "title": "Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis",
            "abstract": "The world is currently experiencing an outbreak of mpox, which has been declared a Public Health Emergency of International Concern by WHO. No prior work related to social media mining has focused on the development of a dataset of Instagram posts about the mpox outbreak. The work presented in this paper aims to address this research gap and makes two scientific contributions to this field. First, it presents a multilingual dataset of 60,127 Instagram posts about mpox, published between July 23, 2022, and September 5, 2024. The dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram posts about mpox in 52 languages. For each of these posts, the Post ID, Post Description, Date of publication, language, and translated version of the post (translation to English was performed using the Google Translate API) are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis, hate speech detection, and anxiety or stress detection were performed. This process included classifying each post into (i) one of the sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no anxiety/stress detected. These results are presented as separate attributes in the dataset. Second, this paper presents the results of performing sentiment analysis, hate speech analysis, and anxiety or stress analysis. The variation of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and 50.64%, respectively. In terms of hate speech detection, 95.75% of the posts did not contain hate and the remaining 4.25% of the posts contained hate. Finally, 72.05% of the posts did not indicate any anxiety/stress, and the remaining 27.95% of the posts represented some form of anxiety/stress.",
            "id": "2409.05292",
            "link": "http://arxiv.org/abs/2409.05292v2",
            "published": "2024-09-09T03:00:53+00:00",
            "updated": "2024-09-10T17:29:21+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CY",
                "cs.SI",
                "I.2.7; I.2.8; I.5.4; K.4.2; H.2.8; I.2.6"
            ],
            "max_author_hindex": 14
        },
        "2409.05674": {
            "authors": [
                "Carlos Arriaga",
                "Alejandro Pozo",
                "Javier Conde",
                "Alvaro Alonso"
            ],
            "title": "Evaluation of real-time transcriptions using end-to-end ASR models",
            "abstract": "Automatic Speech Recognition (ASR) or Speech-to-text (STT) has greatly evolved in the last few years. Traditional architectures based on pipelines have been replaced by joint end-to-end (E2E) architectures that simplify and streamline the model training process. In addition, new AI training methods, such as weak-supervised learning have reduced the need for high-quality audio datasets for model training. However, despite all these advancements, little to no research has been done on real-time transcription. In real-time scenarios, the audio is not pre-recorded, and the input audio must be fragmented to be processed by the ASR systems. To achieve real-time requirements, these fragments must be as short as possible to reduce latency. However, audio cannot be split at any point as dividing an utterance into two separate fragments will generate an incorrect transcription. Also, shorter fragments provide less context for the ASR model. For this reason, it is necessary to design and test different splitting algorithms to optimize the quality and delay of the resulting transcription. In this paper, three audio splitting algorithms are evaluated with different ASR models to determine their impact on both the quality of the transcription and the end-to-end delay. The algorithms are fragmentation at fixed intervals, voice activity detection (VAD), and fragmentation with feedback. The results are compared to the performance of the same model, without audio fragmentation, to determine the effects of this division. The results show that VAD fragmentation provides the best quality with the highest delay, whereas fragmentation at fixed intervals provides the lowest quality and the lowest delay. The newly proposed feedback algorithm exchanges a 2-4% increase in WER for a reduction of 1.5-2s delay, respectively, to the VAD splitting.",
            "id": "2409.05674",
            "link": "http://arxiv.org/abs/2409.05674v2",
            "published": "2024-09-09T14:41:57+00:00",
            "updated": "2024-09-11T10:00:53+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.CL",
                "I.2.7"
            ],
            "max_author_hindex": 39
        },
        "2409.06446": {
            "authors": [
                "Hossein Hajipour",
                "Lea Sch\u00f6nherr",
                "Thorsten Holz",
                "Mario Fritz"
            ],
            "title": "HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training Data",
            "abstract": "Large language models (LLMs) have shown great potential for automatic code generation and form the basis for various tools such as GitHub Copilot. However, recent studies highlight that many LLM-generated code contains serious security vulnerabilities. While previous work tries to address this by training models that generate secure code, these attempts remain constrained by limited access to training data and labor-intensive data preparation.   In this paper, we introduce HexaCoder, a novel approach to enhance the ability of LLMs to generate secure codes by automatically synthesizing secure codes, which reduces the effort of finding suitable training data. HexaCoder comprises two key components: an oracle-guided data synthesis pipeline and a two-step process for secure code generation. The data synthesis pipeline generates pairs of vulnerable and fixed codes for specific Common Weakness Enumeration (CWE) types by utilizing a state-of-the-art LLM for repairing vulnerable code. A security oracle identifies vulnerabilities, and a state-of-the-art LLM repairs them by extending and/or editing the codes, creating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA) method. Each example of our fine-tuning dataset includes the necessary security-related libraries and code that form the basis of our novel two-step generation approach. This allows the model to integrate security-relevant libraries before generating the main code, significantly reducing the number of generated vulnerable codes by up to 85% compared to the baseline methods. We perform extensive evaluations on three different benchmarks for four LLMs, demonstrating that HexaCoder not only improves the security of the generated code but also maintains a high level of functional correctness.",
            "id": "2409.06446",
            "link": "http://arxiv.org/abs/2409.06446v1",
            "published": "2024-09-10T12:01:43+00:00",
            "updated": "2024-09-10T12:01:43+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "cs.SE"
            ],
            "max_author_hindex": 67
        },
        "2409.05351": {
            "authors": [
                "Ronie Salgado"
            ],
            "title": "$\u03bc\u03bb\u03b5\u03b4$-Calculus: A Self Optimizing Language that Seems to Exhibit Paradoxical Transfinite Cognitive Capabilities",
            "abstract": "Formal mathematics and computer science proofs are formalized using Hilbert-Russell-style logical systems which are designed to not admit paradoxes and self-refencing reasoning. These logical systems are natural way to describe and reason syntactic about tree-like data structures. We found that Wittgenstein-style logic is an alternate system whose propositional elements are directed graphs (points and arrows) capable of performing paraconsistent self-referencing reasoning without exploding. Imperative programming language are typically compiled and optimized with SSA-based graphs whose most general representation is the Sea of Node. By restricting the Sea of Nodes to only the data dependencies nodes, we attempted to stablish syntactic-semantic correspondences with the Lambda-calculus optimization. Surprisingly, when we tested our optimizer of the lambda calculus we performed a natural extension onto the $\\mu\\lambda$ which is always terminating. This always terminating algorithm is an actual paradox whose resulting graphs are geometrical fractals, which seem to be isomorphic to original source program. These fractal structures looks like a perfect compressor of a program, which seem to resemble an actual physical black-hole with a naked singularity. In addition to these surprising results, we propose two additional extensions to the calculus to model the cognitive process of self-aware beings: 1) $\\epsilon$-expressions to model syntactic to semantic expansion as a general model of macros; 2) $\\delta$-functional expressions as a minimal model of input and output. We provide detailed step-by-step construction of our language interpreter, compiler and optimizer.",
            "id": "2409.05351",
            "link": "http://arxiv.org/abs/2409.05351v1",
            "published": "2024-09-09T06:14:42+00:00",
            "updated": "2024-09-09T06:14:42+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL"
            ],
            "max_author_hindex": 2
        },
        "2409.07950": {
            "authors": [
                "Viktor Palmkvist",
                "Anders \u00c5gren Thun\u00e9",
                "Elias Castegren",
                "David Broman"
            ],
            "title": "Repr Types: One Abstraction to Rule Them All",
            "abstract": "The choice of how to represent an abstract type can have a major impact on the performance of a program, yet mainstream compilers cannot perform optimizations at such a high level. When dealing with optimizations of data type representations, an important feature is having extensible representation-flexible data types; the ability for a programmer to add new abstract types and operations, as well as concrete implementations of these, without modifying the compiler or a previously defined library. Many research projects support high-level optimizations through static analysis, instrumentation, or benchmarking, but they are all restricted in at least one aspect of extensibility.   This paper presents a new approach to representation-flexible data types without such restrictions and which still finds efficient optimizations. Our approach centers around a single built-in type $\\texttt{repr}$ and function overloading with cost annotations for operation implementations. We evaluate our approach (i) by defining a universal collection type as a library, a single type for all conventional collections, and (ii) by designing and implementing a representation-flexible graph library. Programs using $\\texttt{repr}$ types are typically faster than programs with idiomatic representation choices -- sometimes dramatically so -- as long as the compiler finds good implementations for all operations. Our compiler performs the analysis efficiently by finding optimized solutions quickly and by reusing previous results to avoid recomputations.",
            "id": "2409.07950",
            "link": "http://arxiv.org/abs/2409.07950v1",
            "published": "2024-09-12T11:22:04+00:00",
            "updated": "2024-09-12T11:22:04+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL",
                "cs.PF",
                "D.3.3; D.4.8"
            ],
            "max_author_hindex": 27
        },
        "2409.08142": {
            "authors": [
                "Nikolaos Tziavelis",
                "Wolfgang Gatterbauer",
                "Mirek Riedewald"
            ],
            "title": "Ranked Enumeration for Database Queries",
            "abstract": "Ranked enumeration is a query-answering paradigm where the query answers are returned incrementally in order of importance (instead of returning all answers at once). Importance is defined by a ranking function that can be specific to the application, but typically involves either a lexicographic order (e.g., \"ORDER BY R.A, S.B\" in SQL) or a weighted sum of attributes (e.g., \"ORDER BY 3*R.A + 2*S.B\"). We recently introduced any-k algorithms for (multi-way) join queries, which push ranking into joins and avoid materializing intermediate results until necessary. The top-ranked answers are returned asymptotically faster than the common join-then-rank approach of database systems, resulting in orders-of-magnitude speedup in practice.   In addition to their practical usefulness, our techniques complement a long line of theoretical research on unranked enumeration, where answers are also returned incrementally, but with no explicit ordering requirement. For a broad class of ranking functions with certain monotonicity properties, including lexicographic orders and sum-based rankings, the ordering requirement surprisingly does not increase the asymptotic time or space complexity, apart from logarithmic factors.   A key insight of our work is the connection between ranked enumeration for database queries and the fundamental task of computing the kth-shortest path in a graph. Uncovering these connections allowed us to ground our approach in the rich literature of that problem and connect ideas that had been explored in isolation before. In this article, we adopt a pragmatic approach and present a slightly simplified version of the algorithm without the shortest-path interpretation. We believe that this will benefit practitioners looking to implement and optimize any-k approaches.",
            "id": "2409.08142",
            "link": "http://arxiv.org/abs/2409.08142v1",
            "published": "2024-09-12T15:34:23+00:00",
            "updated": "2024-09-12T15:34:23+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 30
        },
        "2409.05417": {
            "authors": [
                "J\u00fcri Keller",
                "Timo Breuer",
                "Philipp Schaer"
            ],
            "title": "Replicability Measures for Longitudinal Information Retrieval Evaluation",
            "abstract": "Information Retrieval (IR) systems are exposed to constant changes in most components. Documents are created, updated, or deleted, the information needs are changing, and even relevance might not be static. While it is generally expected that the IR systems retain a consistent utility for the users, test collection evaluations rely on a fixed experimental setup. Based on the LongEval shared task and test collection, this work explores how the effectiveness measured in evolving experiments can be assessed. Specifically, the persistency of effectiveness is investigated as a replicability task. It is observed how the effectiveness progressively deteriorates over time compared to the initial measurement. Employing adapted replicability measures provides further insight into the persistence of effectiveness. The ranking of systems varies across retrieval measures and time. In conclusion, it was found that the most effective systems are not necessarily the ones with the most persistent performance.",
            "id": "2409.05417",
            "link": "http://arxiv.org/abs/2409.05417v1",
            "published": "2024-09-09T08:19:43+00:00",
            "updated": "2024-09-09T08:19:43+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 14
        },
        "2409.05462": {
            "authors": [
                "Jibin Jia",
                "Peihao Dong",
                "Fuhui Zhou",
                "Qihui Wu"
            ],
            "title": "Federated Transfer Learning Based Cooperative Wideband Spectrum Sensing with Model Pruning",
            "abstract": "For ultra-wideband and high-rate wireless communication systems, wideband spectrum sensing (WSS) is critical, since it empowers secondary users (SUs) to capture the spectrum holes for opportunistic transmission. However, WSS encounters challenges such as excessive costs of hardware and computation due to the high sampling rate, as well as robustness issues arising from scenario mismatch. In this paper, a WSS neural network (WSSNet) is proposed by exploiting multicoset preprocessing to enable the sub-Nyquist sampling, with the two dimensional convolution design specifically tailored to work with the preprocessed samples. A federated transfer learning (FTL) based framework mobilizing multiple SUs is further developed to achieve a robust model adaptable to various scenarios, which is paved by the selective weight pruning for the fast model adaptation and inference. Simulation results demonstrate that the proposed FTL-WSSNet achieves the fairly good performance in different target scenarios even without local adaptation samples.",
            "id": "2409.05462",
            "link": "http://arxiv.org/abs/2409.05462v1",
            "published": "2024-09-09T09:42:46+00:00",
            "updated": "2024-09-09T09:42:46+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 47
        },
        "2409.05526": {
            "authors": [
                "Xinyang Shao",
                "Edoardo D'Amico",
                "Gabor Fodor",
                "Tri Kurniawan Wijaya"
            ],
            "title": "RBoard: A Unified Platform for Reproducible and Reusable Recommender System Benchmarks",
            "abstract": "Recommender systems research lacks standardized benchmarks for reproducibility and algorithm comparisons. We introduce RBoard, a novel framework addressing these challenges by providing a comprehensive platform for benchmarking diverse recommendation tasks, including CTR prediction, Top-N recommendation, and others. RBoard's primary objective is to enable fully reproducible and reusable experiments across these scenarios. The framework evaluates algorithms across multiple datasets within each task, aggregating results for a holistic performance assessment. It implements standardized evaluation protocols, ensuring consistency and comparability. To facilitate reproducibility, all user-provided code can be easily downloaded and executed, allowing researchers to reliably replicate studies and build upon previous work. By offering a unified platform for rigorous, reproducible evaluation across various recommendation scenarios, RBoard aims to accelerate progress in the field and establish a new standard for recommender systems benchmarking in both academia and industry. The platform is available at https://rboard.org and the demo video can be found at https://bit.ly/rboard-demo.",
            "id": "2409.05526",
            "link": "http://arxiv.org/abs/2409.05526v2",
            "published": "2024-09-09T11:35:35+00:00",
            "updated": "2024-09-10T16:46:10+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 16
        },
        "2409.05546": {
            "authors": [
                "Enze Liu",
                "Bowen Zheng",
                "Cheng Ling",
                "Lantao Hu",
                "Han Li",
                "Wayne Xin Zhao"
            ],
            "title": "End-to-End Learnable Item Tokenization for Generative Recommendation",
            "abstract": "Recently, generative recommendation has emerged as a promising new paradigm that directly generates item identifiers for recommendation. However, a key challenge lies in how to effectively construct item identifiers that are suitable for recommender systems. Existing methods typically decouple item tokenization from subsequent generative recommendation training, likely resulting in suboptimal performance. To address this limitation, we propose ETEGRec, a novel End-To-End Generative Recommender by seamlessly integrating item tokenization and generative recommendation. Our framework is developed based on the dual encoder-decoder architecture, which consists of an item tokenizer and a generative recommender. In order to achieve mutual enhancement between the two components, we propose a recommendation-oriented alignment approach by devising two specific optimization objectives: sequence-item alignment and preference-semantic alignment. These two alignment objectives can effectively couple the learning of item tokenizer and generative recommender, thereby fostering the mutual enhancement between the two components. Finally, we further devise an alternating optimization method, to facilitate stable and effective end-to-end learning of the entire framework. Extensive experiments demonstrate the effectiveness of our proposed framework compared to a series of traditional sequential recommendation models and generative recommendation baselines.",
            "id": "2409.05546",
            "link": "http://arxiv.org/abs/2409.05546v1",
            "published": "2024-09-09T12:11:53+00:00",
            "updated": "2024-09-09T12:11:53+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 57
        },
        "2409.05570": {
            "authors": [
                "Tri Kurniawan Wijaya",
                "Edoardo D'Amico",
                "Gabor Fodor",
                "Manuel V. Loureiro"
            ],
            "title": "Rs4rs: Semantically Find Recent Publications from Top Recommendation System-Related Venues",
            "abstract": "Rs4rs is a web application designed to perform semantic search on recent papers from top conferences and journals related to Recommender Systems. Current scholarly search engine tools like Google Scholar, Semantic Scholar, and ResearchGate often yield broad results that fail to target the most relevant high-quality publications. Moreover, manually visiting individual conference and journal websites is a time-consuming process that primarily supports only syntactic searches. Rs4rs addresses these issues by providing a user-friendly platform where researchers can input their topic of interest and receive a list of recent, relevant papers from top Recommender Systems venues. Utilizing semantic search techniques, Rs4rs ensures that the search results are not only precise and relevant but also comprehensive, capturing papers regardless of variations in wording. This tool significantly enhances research efficiency and accuracy, thereby benefitting the research community and public by facilitating access to high-quality, pertinent academic resources in the field of Recommender Systems. Rs4rs is available at https://rs4rs.com.",
            "id": "2409.05570",
            "link": "http://arxiv.org/abs/2409.05570v2",
            "published": "2024-09-09T12:53:06+00:00",
            "updated": "2024-09-11T07:51:10+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 16
        },
        "2409.06150": {
            "authors": [
                "Naren Khatwani",
                "James Geller"
            ],
            "title": "What makes a good concept anyway ?",
            "abstract": "A good medical ontology is expected to cover its domain completely and correctly. On the other hand, large ontologies are hard to build, hard to understand, and hard to maintain. Thus, adding new concepts (often multi-word concepts) to an existing ontology must be done judiciously. Only \"good\" concepts should be added; however, it is difficult to define what makes a concept good. In this research, we propose a metric to measure the goodness of a concept. We identified factors that appear to influence goodness judgments of medical experts and combined them into a single metric. These factors include concept name length (in words), concept occurrence frequency in the medical literature, and syntactic categories of component words. As an added factor, we used the simplicity of a term after mapping it into a specific foreign language. We performed Bayesian optimization of factor weights to achieve maximum agreement between the metric and three medical experts. The results showed that our metric had a 50.67% overall agreement with the experts, as measured by Krippendorff's alpha.",
            "id": "2409.06150",
            "link": "http://arxiv.org/abs/2409.06150v1",
            "published": "2024-09-10T01:52:29+00:00",
            "updated": "2024-09-10T01:52:29+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 36
        },
        "2409.06464": {
            "authors": [
                "Jimmy Lin"
            ],
            "title": "Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or Inverted Indexes?",
            "abstract": "Practitioners working on dense retrieval today face a bewildering number of choices. Beyond selecting the embedding model, another consequential choice is the actual implementation of nearest-neighbor vector search. While best practices recommend HNSW indexes, flat vector indexes with brute-force search represent another viable option, particularly for smaller corpora and for rapid prototyping. In this paper, we provide experimental results on the BEIR dataset using the open-source Lucene search library that explicate the tradeoffs between HNSW and flat indexes (including quantized variants) from the perspectives of indexing time, query evaluation performance, and retrieval quality. With additional comparisons between dense and sparse retrievers, our results provide guidance for today's search practitioner in understanding the design space of dense and sparse retrievers. To our knowledge, we are the first to provide operational advice supported by empirical experiments in this regard.",
            "id": "2409.06464",
            "link": "http://arxiv.org/abs/2409.06464v1",
            "published": "2024-09-10T12:46:23+00:00",
            "updated": "2024-09-10T12:46:23+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 80
        },
        "2409.07237": {
            "authors": [
                "Haokai Ma",
                "Ruobing Xie",
                "Lei Meng",
                "Fuli Feng",
                "Xiaoyu Du",
                "Xingwu Sun",
                "Zhanhui Kang",
                "Xiangxu Meng"
            ],
            "title": "Negative Sampling in Recommendation: A Survey and Future Directions",
            "abstract": "Recommender systems aim to capture users' personalized preferences from the cast amount of user behaviors, making them pivotal in the era of information explosion. However, the presence of the dynamic preference, the \"information cocoons\", and the inherent feedback loops in recommendation make users interact with a limited number of items. Conventional recommendation algorithms typically focus on the positive historical behaviors, while neglecting the essential role of negative feedback in user interest understanding. As a promising but easy-to-ignored area, negative sampling is proficients in revealing the genuine negative aspect inherent in user behaviors, emerging as an inescapable procedure in recommendation. In this survey, we first discuss the role of negative sampling in recommendation and thoroughly analyze challenges that consistently impede its progress. Then, we conduct an extensive literature review on the existing negative sampling strategies in recommendation and classify them into five categories with their discrepant techniques. Finally, we detail the insights of the tailored negative sampling strategies in diverse recommendation scenarios and outline an overview of the prospective research directions toward which the community may engage and benefit.",
            "id": "2409.07237",
            "link": "http://arxiv.org/abs/2409.07237v1",
            "published": "2024-09-11T12:48:52+00:00",
            "updated": "2024-09-11T12:48:52+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 52
        },
        "2409.07272": {
            "authors": [
                "Alexey Vasilev",
                "Anna Volodkevich",
                "Denis Kulandin",
                "Tatiana Bysheva",
                "Anton Klenitskiy"
            ],
            "title": "RePlay: a Recommendation Framework for Experimentation and Production Use",
            "abstract": "Using a single tool to build and compare recommender systems significantly reduces the time to market for new models. In addition, the comparison results when using such tools look more consistent. This is why many different tools and libraries for researchers in the field of recommendations have recently appeared. Unfortunately, most of these frameworks are aimed primarily at researchers and require modification for use in production due to the inability to work on large datasets or an inappropriate architecture. In this demo, we present our open-source toolkit RePlay - a framework containing an end-to-end pipeline for building recommender systems, which is ready for production use. RePlay also allows you to use a suitable stack for the pipeline on each stage: Pandas, Polars, or Spark. This allows the library to scale computations and deploy to a cluster. Thus, RePlay allows data scientists to easily move from research mode to production mode using the same interfaces.",
            "id": "2409.07272",
            "link": "http://arxiv.org/abs/2409.07272v1",
            "published": "2024-09-11T13:46:52+00:00",
            "updated": "2024-09-11T13:46:52+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.LG",
                "cs.SE"
            ],
            "max_author_hindex": 4
        },
        "2409.07367": {
            "authors": [
                "Pavan Seshadri",
                "Shahrzad Shashaani",
                "Peter Knees"
            ],
            "title": "Enhancing Sequential Music Recommendation with Negative Feedback-informed Contrastive Learning",
            "abstract": "Modern music streaming services are heavily based on recommendation engines to serve content to users. Sequential recommendation -- continuously providing new items within a single session in a contextually coherent manner -- has been an emerging topic in current literature. User feedback -- a positive or negative response to the item presented -- is used to drive content recommendations by learning user preferences. We extend this idea to session-based recommendation to provide context-coherent music recommendations by modelling negative user feedback, i.e., skips, in the loss function. We propose a sequence-aware contrastive sub-task to structure item embeddings in session-based music recommendation, such that true next-positive items (ignoring skipped items) are structured closer in the session embedding space, while skipped tracks are structured farther away from all items in the session. This directly affects item rankings using a K-nearest-neighbors search for next-item recommendations, while also promoting the rank of the true next item. Experiments incorporating this task into SoTA methods for sequential item recommendation show consistent performance gains in terms of next-item hit rate, item ranking, and skip down-ranking on three music recommendation datasets, strongly benefiting from the increasing presence of user feedback.",
            "id": "2409.07367",
            "link": "http://arxiv.org/abs/2409.07367v1",
            "published": "2024-09-11T15:56:05+00:00",
            "updated": "2024-09-11T15:56:05+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 28
        },
        "2409.07433": {
            "authors": [
                "Daniele Malitesta",
                "Alberto Carlo Maria Mancino",
                "Pasquale Minervini",
                "Tommaso Di Noia"
            ],
            "title": "Dot Product is All You Need: Bridging the Gap Between Item Recommendation and Link Prediction",
            "abstract": "Item recommendation (the task of predicting if a user may interact with new items from the catalogue in a recommendation system) and link prediction (the task of identifying missing links in a knowledge graph) have long been regarded as distinct problems. In this work, we show that the item recommendation problem can be seen as an instance of the link prediction problem, where entities in the graph represent users and items, and the task consists of predicting missing instances of the relation type <<interactsWith>>. In a preliminary attempt to demonstrate the assumption, we decide to test three popular factorisation-based link prediction models on the item recommendation task, showing that their predictive accuracy is competitive with ten state-of-the-art recommendation models. The purpose is to show how the former may be seamlessly and effectively applied to the recommendation task without any specific modification to their architectures. Finally, while beginning to unveil the key reasons behind the recommendation performance of the selected link prediction models, we explore different settings for their hyper-parameter values, paving the way for future directions.",
            "id": "2409.07433",
            "link": "http://arxiv.org/abs/2409.07433v1",
            "published": "2024-09-11T17:27:04+00:00",
            "updated": "2024-09-11T17:27:04+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 39
        },
        "2409.07604": {
            "authors": [
                "Makbule Gulcin Ozsoy"
            ],
            "title": "Multilingual Prompts in LLM-Based Recommenders: Performance Across Languages",
            "abstract": "Large language models (LLMs) are increasingly used in natural language processing tasks. Recommender systems traditionally use methods such as collaborative filtering and matrix factorization, as well as advanced techniques like deep learning and reinforcement learning. Although language models have been applied in recommendation, the recent trend have focused on leveraging the generative capabilities of LLMs for more personalized suggestions. While current research focuses on English due to its resource richness, this work explores the impact of non-English prompts on recommendation performance. Using OpenP5, a platform for developing and evaluating LLM-based recommendations, we expanded its English prompt templates to include Spanish and Turkish. Evaluation on three real-world datasets, namely ML1M, LastFM, and Amazon-Beauty, showed that usage of non-English prompts generally reduce performance, especially in less-resourced languages like Turkish. We also retrained an LLM-based recommender model with multilingual prompts to analyze performance variations. Retraining with multilingual prompts resulted in more balanced performance across languages, but slightly reduced English performance. This work highlights the need for diverse language support in LLM-based recommenders and suggests future research on creating evaluation datasets, using newer models and additional languages.",
            "id": "2409.07604",
            "link": "http://arxiv.org/abs/2409.07604v1",
            "published": "2024-09-11T20:31:42+00:00",
            "updated": "2024-09-11T20:31:42+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 10
        },
        "2409.07946": {
            "authors": [
                "Chaowei He",
                "Peihao Dong",
                "Fuhui Zhou",
                "Qihui Wu"
            ],
            "title": "Collaborative Automatic Modulation Classification via Deep Edge Inference for Hierarchical Cognitive Radio Networks",
            "abstract": "In hierarchical cognitive radio networks, edge or cloud servers utilize the data collected by edge devices for modulation classification, which, however, is faced with problems of the transmission overhead, data privacy, and computation load. In this article, an edge learning (EL) based framework jointly mobilizing the edge device and the edge server for intelligent co-inference is proposed to realize the collaborative automatic modulation classification (C-AMC) between them. A spectrum semantic compression neural network (SSCNet) with the lightweight structure is designed for the edge device to compress the collected raw data into a compact semantic message that is then sent to the edge server via the wireless channel. On the edge server side, a modulation classification neural network (MCNet) combining bidirectional long short-term memory (Bi?LSTM) and multi-head attention layers is elaborated to deter?mine the modulation type from the noisy semantic message. By leveraging the computation resources of both the edge device and the edge server, high transmission overhead and risks of data privacy leakage are avoided. The simulation results verify the effectiveness of the proposed C-AMC framework, significantly reducing the model size and computational complexity.",
            "id": "2409.07946",
            "link": "http://arxiv.org/abs/2409.07946v1",
            "published": "2024-09-12T11:14:25+00:00",
            "updated": "2024-09-12T11:14:25+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 47
        },
        "2409.08014": {
            "authors": [
                "Hanane Djeddal",
                "Pierre Erbacher",
                "Raouf Toukal",
                "Laure Soulier",
                "Karen Pinel-Sauvagnat",
                "Sophia Katrenko",
                "Lynda Tamine"
            ],
            "title": "An Evaluation Framework for Attributed Information Retrieval using Large Language Models",
            "abstract": "With the growing success of Large Language models (LLMs) in information-seeking scenarios, search engines are now adopting generative approaches to provide answers along with in-line citations as attribution. While existing work focuses mainly on attributed question answering, in this paper, we target information-seeking scenarios which are often more challenging due to the open-ended nature of the queries and the size of the label space in terms of the diversity of candidate-attributed answers per query. We propose a reproducible framework to evaluate and benchmark attributed information seeking, using any backbone LLM, and different architectural designs: (1) Generate (2) Retrieve then Generate, and (3) Generate then Retrieve. Experiments using HAGRID, an attributed information-seeking dataset, show the impact of different scenarios on both the correctness and attributability of answers.",
            "id": "2409.08014",
            "link": "http://arxiv.org/abs/2409.08014v1",
            "published": "2024-09-12T12:57:08+00:00",
            "updated": "2024-09-12T12:57:08+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 22
        },
        "2409.05257": {
            "authors": [
                "Kuiyun Chen",
                "Yanbin Wei"
            ],
            "title": "UPCS: Unbiased Persona Construction for Dialogue Generation",
            "abstract": "Narrative systems, such as dialogue and storytelling systems, often utilize persona profiles to enhance personalized interactions. Existing persona profiles frequently exhibit biases, posing risks to system integrity and fairness. To address this, we introduce the UPCS framework, which categorizes character descriptions into eight dimensions, including bias mitigation strategies. Experimental results demonstrate UPCS's superiority in accuracy, diversity, bias elimination, and user satisfaction, marking a significant advancement in persona construction for reliable narrative systems.",
            "id": "2409.05257",
            "link": "http://arxiv.org/abs/2409.05257v1",
            "published": "2024-09-09T00:40:47+00:00",
            "updated": "2024-09-09T00:40:47+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 6
        },
        "2409.07081": {
            "authors": [
                "Satoru Watanabe"
            ],
            "title": "Data Backup System with No Impact on Business Processing Utilizing Storage and Container Technologies",
            "abstract": "Data backup is a core technology for improving system resilience to system failures. Data backup in enterprise systems is required to minimize the impacts on business processing, which can be categorized into two factors: system slowdown and downtime. To eliminate system slowdown, asynchronous data copy (ADC) technology is prevalent, which copies data asynchronously with original data updates. However, the ADC can collapse backup data when applied to enterprise systems with multiple resources. Then, the demonstration system employed consistency group technology, which makes the order of data updates the same between the original and backup data. In addition, we developed a container platform operator to unravel the complicated correspondence between storage volumes and applications. The operator automates the configuration of the ADC with the setting of consistency groups. We integrated the storage and container technologies into the demonstration system, which can eliminate both system slowdown and downtime.",
            "id": "2409.07081",
            "link": "http://arxiv.org/abs/2409.07081v1",
            "published": "2024-09-11T08:09:28+00:00",
            "updated": "2024-09-11T08:09:28+00:00",
            "primary_category": "cs.DC",
            "categories": [
                "cs.DC",
                "cs.DB"
            ],
            "max_author_hindex": 20
        },
        "2409.05461": {
            "authors": [
                "Lukas Wegmeth",
                "Tobias Vente",
                "Joeran Beel"
            ],
            "title": "Recommender Systems Algorithm Selection for Ranking Prediction on Implicit Feedback Datasets",
            "abstract": "The recommender systems algorithm selection problem for ranking prediction on implicit feedback datasets is under-explored. Traditional approaches in recommender systems algorithm selection focus predominantly on rating prediction on explicit feedback datasets, leaving a research gap for ranking prediction on implicit feedback datasets. Algorithm selection is a critical challenge for nearly every practitioner in recommender systems. In this work, we take the first steps toward addressing this research gap. We evaluate the NDCG@10 of 24 recommender systems algorithms, each with two hyperparameter configurations, on 72 recommender systems datasets. We train four optimized machine-learning meta-models and one automated machine-learning meta-model with three different settings on the resulting meta-dataset. Our results show that the predictions of all tested meta-models exhibit a median Spearman correlation ranging from 0.857 to 0.918 with the ground truth. We show that the median Spearman correlation between meta-model predictions and the ground truth increases by an average of 0.124 when the meta-model is optimized to predict the ranking of algorithms instead of their performance. Furthermore, in terms of predicting the best algorithm for an unknown dataset, we demonstrate that the best optimized traditional meta-model, e.g., XGBoost, achieves a recall of 48.6%, outperforming the best tested automated machine learning meta-model, e.g., AutoGluon, which achieves a recall of 47.2%.",
            "id": "2409.05461",
            "link": "http://arxiv.org/abs/2409.05461v1",
            "published": "2024-09-09T09:42:31+00:00",
            "updated": "2024-09-09T09:42:31+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 9
        },
        "2409.05633": {
            "authors": [
                "Bowen Zheng",
                "Junjie Zhang",
                "Hongyu Lu",
                "Yu Chen",
                "Ming Chen",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "title": "Enhancing Graph Contrastive Learning with Reliable and Informative Augmentation for Recommendation",
            "abstract": "Graph neural network (GNN) has been a powerful approach in collaborative filtering (CF) due to its ability to model high-order user-item relationships. Recently, to alleviate the data sparsity and enhance representation learning, many efforts have been conducted to integrate contrastive learning (CL) with GNNs. Despite the promising improvements, the contrastive view generation based on structure and representation perturbations in existing methods potentially disrupts the collaborative information in contrastive views, resulting in limited effectiveness of positive alignment. To overcome this issue, we propose CoGCL, a novel framework that aims to enhance graph contrastive learning by constructing contrastive views with stronger collaborative information via discrete codes. The core idea is to map users and items into discrete codes rich in collaborative information for reliable and informative contrastive view generation. To this end, we initially introduce a multi-level vector quantizer in an end-to-end manner to quantize user and item representations into discrete codes. Based on these discrete codes, we enhance the collaborative information of contrastive views by considering neighborhood structure and semantic relevance respectively. For neighborhood structure, we propose virtual neighbor augmentation by treating discrete codes as virtual neighbors, which expands an observed user-item interaction into multiple edges involving discrete codes. Regarding semantic relevance, we identify similar users/items based on shared discrete codes and interaction targets to generate the semantically relevant view. Through these strategies, we construct contrastive views with stronger collaborative information and develop a triple-view graph contrastive learning approach. Extensive experiments on four public datasets demonstrate the effectiveness of our proposed approach.",
            "id": "2409.05633",
            "link": "http://arxiv.org/abs/2409.05633v1",
            "published": "2024-09-09T14:04:17+00:00",
            "updated": "2024-09-09T14:04:17+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 57
        },
        "2409.06177": {
            "authors": [
                "Yuxuan Liu",
                "Haipeng Liu",
                "Ting Long"
            ],
            "title": "HierLLM: Hierarchical Large Language Model for Question Recommendation",
            "abstract": "Question recommendation is a task that sequentially recommends questions for students to enhance their learning efficiency. That is, given the learning history and learning target of a student, a question recommender is supposed to select the question that will bring the most improvement for students. Previous methods typically model the question recommendation as a sequential decision-making problem, estimating students' learning state with the learning history, and feeding the learning state with the learning target to a neural network to select the recommended question from a question set. However, previous methods are faced with two challenges: (1) learning history is unavailable in the cold start scenario, which makes the recommender generate inappropriate recommendations; (2) the size of the question set is much large, which makes it difficult for the recommender to select the best question precisely. To address the challenges, we propose a method called hierarchical large language model for question recommendation (HierLLM), which is a LLM-based hierarchical structure. The LLM-based structure enables HierLLM to tackle the cold start issue with the strong reasoning abilities of LLM. The hierarchical structure takes advantage of the fact that the number of concepts is significantly smaller than the number of questions, narrowing the range of selectable questions by first identifying the relevant concept for the to-recommend question, and then selecting the recommended question based on that concept. This hierarchical structure reduces the difficulty of the recommendation.To investigate the performance of HierLLM, we conduct extensive experiments, and the results demonstrate the outstanding performance of HierLLM.",
            "id": "2409.06177",
            "link": "http://arxiv.org/abs/2409.06177v1",
            "published": "2024-09-10T03:12:39+00:00",
            "updated": "2024-09-10T03:12:39+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 12
        },
        "2409.06297": {
            "authors": [
                "Julien Albert",
                "Martin Balfroid",
                "Miriam Doh",
                "Jeremie Bogaert",
                "Luca La Fisca",
                "Liesbet De Vos",
                "Bryan Renard",
                "Vincent Stragier",
                "Emmanuel Jean"
            ],
            "title": "User Preferences for Large Language Model versus Template-Based Explanations of Movie Recommendations: A Pilot Study",
            "abstract": "Recommender systems have become integral to our digital experiences, from online shopping to streaming platforms. Still, the rationale behind their suggestions often remains opaque to users. While some systems employ a graph-based approach, offering inherent explainability through paths associating recommended items and seed items, non-experts could not easily understand these explanations. A popular alternative is to convert graph-based explanations into textual ones using a template and an algorithm, which we denote here as ''template-based'' explanations. Yet, these can sometimes come across as impersonal or uninspiring. A novel method would be to employ large language models (LLMs) for this purpose, which we denote as ''LLM-based''. To assess the effectiveness of LLMs in generating more resonant explanations, we conducted a pilot study with 25 participants. They were presented with three explanations: (1) traditional template-based, (2) LLM-based rephrasing of the template output, and (3) purely LLM-based explanations derived from the graph-based explanations. Although subject to high variance, preliminary findings suggest that LLM-based explanations may provide a richer and more engaging user experience, further aligning with user expectations. This study sheds light on the potential limitations of current explanation methods and offers promising directions for leveraging large language models to improve user satisfaction and trust in recommender systems.",
            "id": "2409.06297",
            "link": "http://arxiv.org/abs/2409.06297v1",
            "published": "2024-09-10T07:51:53+00:00",
            "updated": "2024-09-10T07:51:53+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.HC",
                "cs.LG"
            ],
            "max_author_hindex": 22
        },
        "2409.06638": {
            "authors": [
                "Haoan Feng",
                "Yunting Song",
                "Leila De Floriani"
            ],
            "title": "Critical Features Tracking on Triangulated Irregular Networks by a Scale-Space Method",
            "abstract": "The scale-space method is a well-established framework that constructs a hierarchical representation of an input signal and facilitates coarse-to-fine visual reasoning. Considering the terrain elevation function as the input signal, the scale-space method can identify and track significant topographic features across different scales. The number of scales a feature persists, called its life span, indicates the importance of that feature. In this way, important topographic features of a landscape can be selected, which are useful for many applications, including cartography, nautical charting, and land-use planning. The scale-space methods developed for terrain data use gridded Digital Elevation Models (DEMs) to represent the terrain. However, gridded DEMs lack the flexibility to adapt to the irregular distribution of input data and the varied topological complexity of different regions. Instead, Triangulated Irregular Networks (TINs) can be directly generated from irregularly distributed point clouds and accurately preserve important features. In this work, we introduce a novel scale-space analysis pipeline for TINs, addressing the multiple challenges in extending grid-based scale-space methods to TINs. Our pipeline can efficiently identify and track topologically important features on TINs. Moreover, it is capable of analyzing terrains with irregular boundaries, which poses challenges for grid-based methods. Comprehensive experiments show that, compared to grid-based methods, our TIN-based pipeline is more efficient, accurate, and has better resolution robustness.",
            "id": "2409.06638",
            "link": "http://arxiv.org/abs/2409.06638v1",
            "published": "2024-09-10T16:48:05+00:00",
            "updated": "2024-09-10T16:48:05+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 41
        },
        "2409.07276": {
            "authors": [
                "Qijiong Liu",
                "Jieming Zhu",
                "Lu Fan",
                "Zhou Zhao",
                "Xiao-Ming Wu"
            ],
            "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM",
            "abstract": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tail or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. In this way, it preserves the item's semantics within these tokens and ensures that semantically similar items are represented by similar tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing generative recommendation methods typically involve multiple sub-models for embedding, quantization, and recommendation, leading to an overly complex system. In this paper, we propose to streamline the semantic tokenization and generative recommendation process with a unified framework, dubbed STORE, which leverages a single large language model (LLM) for both tasks. Specifically, we formulate semantic tokenization as a text-to-token task and generative recommendation as a token-to-token task, supplemented by a token-to-text reconstruction task and a text-to-token auxiliary task. All these tasks are framed in a generative manner and trained using a single LLM backbone. Extensive experiments have been conducted to validate the effectiveness of our STORE framework across various recommendation tasks and datasets. We will release the source code and configurations for reproducible research.",
            "id": "2409.07276",
            "link": "http://arxiv.org/abs/2409.07276v1",
            "published": "2024-09-11T13:49:48+00:00",
            "updated": "2024-09-11T13:49:48+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 30
        },
        "2409.07773": {
            "authors": [
                "Chaoqun Yang",
                "Wei Yuan",
                "Liang Qu",
                "Thanh Tam Nguyen"
            ],
            "title": "PDC-FRS: Privacy-preserving Data Contribution for Federated Recommender System",
            "abstract": "Federated recommender systems (FedRecs) have emerged as a popular research direction for protecting users' privacy in on-device recommendations. In FedRecs, users keep their data locally and only contribute their local collaborative information by uploading model parameters to a central server. While this rigid framework protects users' raw data during training, it severely compromises the recommendation model's performance due to the following reasons: (1) Due to the power law distribution nature of user behavior data, individual users have few data points to train a recommendation model, resulting in uploaded model updates that may be far from optimal; (2) As each user's uploaded parameters are learned from local data, which lacks global collaborative information, relying solely on parameter aggregation methods such as FedAvg to fuse global collaborative information may be suboptimal. To bridge this performance gap, we propose a novel federated recommendation framework, PDC-FRS. Specifically, we design a privacy-preserving data contribution mechanism that allows users to share their data with a differential privacy guarantee. Based on the shared but perturbed data, an auxiliary model is trained in parallel with the original federated recommendation process. This auxiliary model enhances FedRec by augmenting each user's local dataset and integrating global collaborative information. To demonstrate the effectiveness of PDC-FRS, we conduct extensive experiments on two widely used recommendation datasets. The empirical results showcase the superiority of PDC-FRS compared to baseline methods.",
            "id": "2409.07773",
            "link": "http://arxiv.org/abs/2409.07773v1",
            "published": "2024-09-12T06:13:07+00:00",
            "updated": "2024-09-12T06:13:07+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 29
        },
        "2409.08046": {
            "authors": [
                "Savvina Daniil",
                "Manel Slokom",
                "Mirjam Cuper",
                "Cynthia C. S. Liem",
                "Jacco van Ossenbruggen",
                "Laura Hollink"
            ],
            "title": "On the challenges of studying bias in Recommender Systems: A UserKNN case study",
            "abstract": "Statements on the propagation of bias by recommender systems are often hard to verify or falsify. Research on bias tends to draw from a small pool of publicly available datasets and is therefore bound by their specific properties. Additionally, implementation choices are often not explicitly described or motivated in research, while they may have an effect on bias propagation. In this paper, we explore the challenges of measuring and reporting popularity bias. We showcase the impact of data properties and algorithm configurations on popularity bias by combining synthetic data with well known recommender systems frameworks that implement UserKNN. First, we identify data characteristics that might impact popularity bias, based on the functionality of UserKNN. Accordingly, we generate various datasets that combine these characteristics. Second, we locate UserKNN configurations that vary across implementations in literature. We evaluate popularity bias for five synthetic datasets and five UserKNN configurations, and offer insights on their joint effect. We find that, depending on the data characteristics, various UserKNN configurations can lead to different conclusions regarding the propagation of popularity bias. These results motivate the need for explicitly addressing algorithmic configuration and data properties when reporting and interpreting bias in recommender systems.",
            "id": "2409.08046",
            "link": "http://arxiv.org/abs/2409.08046v1",
            "published": "2024-09-12T13:51:06+00:00",
            "updated": "2024-09-12T13:51:06+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 30
        },
        "2409.07360": {
            "authors": [
                "Mohamed Hassan"
            ],
            "title": "Choosing the Right Communication Protocol for your Web Application",
            "abstract": "Selecting the appropriate communication protocol is crucial for optimizing the performance, scalability, and user experience of web applications. In the diverse ecosystem of web technologies, various protocols like RESTful APIs, gRPC, WebSockets, and others serve distinct purposes. RESTful APIs are widely favored for their simplicity and stateless nature, making them ideal for standard CRUD operations. They offer a straightforward approach to interacting with resources over HTTP/1.1, providing broad compatibility and ease of integration across different platforms. However, in scenarios where applications require high efficiency and real-time communication, gRPC and WebSockets emerge as powerful alternatives. Each protocol comes with its strengths and limitations, influencing factors such as ease of implementation, performance under load, and support for complex data structures. RESTful APIs, while easy to use and widely supported, may introduce overhead due to their stateless nature and reliance on multiple HTTP/1.1 requests. In contrast, gRPC advanced features, while powerful, require a steeper learning curve and more sophisticated infrastructure. Similarly, WebSockets, while excellent for real-time applications, require careful management of persistent connections and security considerations. This paper explores the key considerations in choosing the right communication protocol, emphasizing the need to align technical choices with application requirements and user expectations. By understanding the unique attributes of each protocol, developers can make informed decisions that enhance the responsiveness and reliability of their web applications. The choice of protocol can significantly impact the user experience, scalability, and maintainability of the application, making it a critical decision in the web development process.",
            "id": "2409.07360",
            "link": "http://arxiv.org/abs/2409.07360v1",
            "published": "2024-09-11T15:45:14+00:00",
            "updated": "2024-09-11T15:45:14+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.PL"
            ],
            "max_author_hindex": 21
        },
        "2409.07014": {
            "authors": [
                "Peizhi Wu",
                "Haoshu Xu",
                "Ryan Marcus",
                "Zachary G. Ives"
            ],
            "title": "A Practical Theory of Generalization in Selectivity Learning",
            "abstract": "Query-driven machine learning models have emerged as a promising estimation technique for query selectivities. Yet, surprisingly little is known about the efficacy of these techniques from a theoretical perspective, as there exist substantial gaps between practical solutions and state-of-the-art (SOTA) theory based on the Probably Approximately Correct (PAC) learning framework. In this paper, we aim to bridge the gaps between theory and practice. First, we demonstrate that selectivity predictors induced by signed measures are learnable, which relaxes the reliance on probability measures in SOTA theory. More importantly, beyond the PAC learning framework (which only allows us to characterize how the model behaves when both training and test workloads are drawn from the same distribution), we establish, under mild assumptions, that selectivity predictors from this class exhibit favorable out-of-distribution (OOD) generalization error bounds.   These theoretical advances provide us with a better understanding of both the in-distribution and OOD generalization capabilities of query-driven selectivity learning, and facilitate the design of two general strategies to improve OOD generalization for existing query-driven selectivity models. We empirically verify that our techniques help query-driven selectivity models generalize significantly better to OOD queries both in terms of prediction accuracy and query latency performance, while maintaining their superior in-distribution generalization performance.",
            "id": "2409.07014",
            "link": "http://arxiv.org/abs/2409.07014v1",
            "published": "2024-09-11T05:10:32+00:00",
            "updated": "2024-09-11T05:10:32+00:00",
            "primary_category": "stat.ML",
            "categories": [
                "stat.ML",
                "cs.DB",
                "cs.LG"
            ],
            "max_author_hindex": 42
        },
        "2409.06243": {
            "authors": [
                "Jihyun Lee",
                "Gary Geunbae Lee"
            ],
            "title": "Inference is All You Need: Self Example Retriever for Cross-domain Dialogue State Tracking with ChatGPT",
            "abstract": "Traditional dialogue state tracking approaches heavily rely on extensive training data and handcrafted features, limiting their scalability and adaptability to new domains. In this paper, we propose a novel method that leverages inference and in-context learning with ChatGPT for domain transfer in dialogue state tracking, without any parameter updates. By guiding ChatGPT's chain of thought, we enable it to retrieve relevant examples and generalize knowledge to accurately infer dialogue states, solely through inference. Experimental results on the MultiWOZ dataset demonstrate competitive performance and promising generalization across domains. Our parameter-free approach offers a scalable and adaptable solution, opening new research directions in domain transfer learning.",
            "id": "2409.06243",
            "link": "http://arxiv.org/abs/2409.06243v1",
            "published": "2024-09-10T06:24:46+00:00",
            "updated": "2024-09-10T06:24:46+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 28
        },
        "2409.06328": {
            "authors": [
                "Nicholas Pochinkov",
                "Angelo Benoit",
                "Lovkush Agarwal",
                "Zainab Ali Majid",
                "Lucile Ter-Minassian"
            ],
            "title": "Extracting Paragraphs from LLM Token Activations",
            "abstract": "Generative large language models (LLMs) excel in natural language processing tasks, yet their inner workings remain underexplored beyond token-level predictions. This study investigates the degree to which these models decide the content of a paragraph at its onset, shedding light on their contextual understanding. By examining the information encoded in single-token activations, specifically the \"\\textbackslash n\\textbackslash n\" double newline token, we demonstrate that patching these activations can transfer significant information about the context of the following paragraph, providing further insights into the model's capacity to plan ahead.",
            "id": "2409.06328",
            "link": "http://arxiv.org/abs/2409.06328v1",
            "published": "2024-09-10T08:33:31+00:00",
            "updated": "2024-09-10T08:33:31+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 3
        },
        "2409.06790": {
            "authors": [
                "Eleftheria Briakou",
                "Jiaming Luo",
                "Colin Cherry",
                "Markus Freitag"
            ],
            "title": "Translating Step-by-Step: Decomposing the Translation Process for Improved Translation Quality of Long-Form Texts",
            "abstract": "In this paper we present a step-by-step approach to long-form text translation, drawing on established processes in translation studies. Instead of viewing machine translation as a single, monolithic task, we propose a framework that engages language models in a multi-turn interaction, encompassing pre-translation research, drafting, refining, and proofreading, resulting in progressively improved translations. Extensive automatic evaluations using Gemini 1.5 Pro across ten language pairs show that translating step-by-step yields large translation quality improvements over conventional zero-shot prompting approaches and earlier human-like baseline strategies, resulting in state-of-the-art results on WMT2024.",
            "id": "2409.06790",
            "link": "http://arxiv.org/abs/2409.06790v1",
            "published": "2024-09-10T18:02:21+00:00",
            "updated": "2024-09-10T18:02:21+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 39
        },
        "2409.06820": {
            "authors": [
                "Ilya Gusev"
            ],
            "title": "PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation",
            "abstract": "We introduce a novel benchmark for evaluating the role-playing capabilities of language models. Our approach leverages language models themselves to emulate users in dynamic, multi-turn conversations and to assess the resulting dialogues. The framework consists of three main components: a player model assuming a specific character role, an interrogator model simulating user behavior, and a judge model evaluating conversation quality. We conducted experiments comparing automated evaluations with human annotations to validate our approach, demonstrating strong correlations across multiple criteria. This work provides a foundation for a robust and dynamic evaluation of model capabilities in interactive scenarios.",
            "id": "2409.06820",
            "link": "http://arxiv.org/abs/2409.06820v1",
            "published": "2024-09-10T19:00:44+00:00",
            "updated": "2024-09-10T19:00:44+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 6
        },
        "2409.07737": {
            "authors": [
                "Hayato Tsukagoshi",
                "Ryohei Sasano"
            ],
            "title": "Ruri: Japanese General Text Embeddings",
            "abstract": "We report the development of Ruri, a series of Japanese general text embedding models. While the development of general-purpose text embedding models in English and multilingual contexts has been active in recent years, model development in Japanese remains insufficient. The primary reasons for this are the lack of datasets and the absence of necessary expertise. In this report, we provide a detailed account of the development process of Ruri. Specifically, we discuss the training of embedding models using synthesized datasets generated by LLMs, the construction of the reranker for dataset filtering and knowledge distillation, and the performance evaluation of the resulting general-purpose text embedding models.",
            "id": "2409.07737",
            "link": "http://arxiv.org/abs/2409.07737v1",
            "published": "2024-09-12T04:06:31+00:00",
            "updated": "2024-09-12T04:06:31+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 14
        },
        "2409.07780": {
            "authors": [
                "Maike Behrendt",
                "Stefan Sylvius Wagner",
                "Stefan Harmeling"
            ],
            "title": "Supporting Online Discussions: Integrating AI Into the adhocracy+ Participation Platform To Enhance Deliberation",
            "abstract": "Online spaces allow people to discuss important issues and make joint decisions, regardless of their location or time zone. However, without proper support and thoughtful design, these discussions often lack structure and politeness during the exchanges of opinions. Artificial intelligence (AI) represents an opportunity to support both participants and organizers of large-scale online participation processes. In this paper, we present an extension of adhocracy+, a large-scale open source participation platform, that provides two additional debate modules that are supported by AI to enhance the discussion quality and participant interaction.",
            "id": "2409.07780",
            "link": "http://arxiv.org/abs/2409.07780v1",
            "published": "2024-09-12T06:27:35+00:00",
            "updated": "2024-09-12T06:27:35+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 35
        },
        "2409.05692": {
            "authors": [
                "Henrique F. de Arruda",
                "Sandro M. Reia",
                "Shiyang Ruan",
                "Kuldip S. Atwal",
                "Hamdi Kavak",
                "Taylor Anderson",
                "Dieter Pfoser"
            ],
            "title": "Extracting the U.S. building types from OpenStreetMap data",
            "abstract": "Building type information is crucial for population estimation, traffic planning, urban planning, and emergency response applications. Although essential, such data is often not readily available. To alleviate this problem, this work creates a comprehensive dataset by providing residential/non-residential building classification covering the entire United States. We propose and utilize an unsupervised machine learning method to classify building types based on building footprints and available OpenStreetMap information. The classification result is validated using authoritative ground truth data for select counties in the U.S. The validation shows a high precision for non-residential building classification and a high recall for residential buildings. We identified various approaches to improving the quality of the classification, such as removing sheds and garages from the dataset. Furthermore, analyzing the misclassifications revealed that they are mainly due to missing and scarce metadata in OSM. A major result of this work is the resulting dataset of classifying 67,705,475 buildings. We hope that this data is of value to the scientific community, including urban and transportation planners.",
            "id": "2409.05692",
            "link": "http://arxiv.org/abs/2409.05692v1",
            "published": "2024-09-09T15:05:27+00:00",
            "updated": "2024-09-09T15:05:27+00:00",
            "primary_category": "cs.SI",
            "categories": [
                "cs.SI",
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 31
        },
        "2409.07238": {
            "authors": [
                "Yingling Lu",
                "Yijun Yang",
                "Zhaohu Xing",
                "Qiong Wang",
                "Lei Zhu"
            ],
            "title": "Diff-VPS: Video Polyp Segmentation via a Multi-task Diffusion Network with Adversarial Temporal Reasoning",
            "abstract": "Diffusion Probabilistic Models have recently attracted significant attention in the community of computer vision due to their outstanding performance. However, while a substantial amount of diffusion-based research has focused on generative tasks, no work introduces diffusion models to advance the results of polyp segmentation in videos, which is frequently challenged by polyps' high camouflage and redundant temporal cues.In this paper, we present a novel diffusion-based network for video polyp segmentation task, dubbed as Diff-VPS. We incorporate multi-task supervision into diffusion models to promote the discrimination of diffusion models on pixel-by-pixel segmentation. This integrates the contextual high-level information achieved by the joint classification and detection tasks. To explore the temporal dependency, Temporal Reasoning Module (TRM) is devised via reasoning and reconstructing the target frame from the previous frames. We further equip TRM with a generative adversarial self-supervised strategy to produce more realistic frames and thus capture better dynamic cues. Extensive experiments are conducted on SUN-SEG, and the results indicate that our proposed Diff-VPS significantly achieves state-of-the-art performance. Code is available at https://github.com/lydia-yllu/Diff-VPS.",
            "id": "2409.07238",
            "link": "http://arxiv.org/abs/2409.07238v1",
            "published": "2024-09-11T12:51:41+00:00",
            "updated": "2024-09-11T12:51:41+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.IR"
            ],
            "max_author_hindex": 29
        },
        "2409.07500": {
            "authors": [
                "Qitao Qin",
                "Yucong Luo",
                "Mingyue Cheng",
                "Qingyang Mao",
                "Chenyi Lei"
            ],
            "title": "DV-FSR: A Dual-View Target Attack Framework for Federated Sequential Recommendation",
            "abstract": "Federated recommendation (FedRec) preserves user privacy by enabling decentralized training of personalized models, but this architecture is inherently vulnerable to adversarial attacks. Significant research has been conducted on targeted attacks in FedRec systems, motivated by commercial and social influence considerations. However, much of this work has largely overlooked the differential robustness of recommendation models. Moreover, our empirical findings indicate that existing targeted attack methods achieve only limited effectiveness in Federated Sequential Recommendation (FSR) tasks. Driven by these observations, we focus on investigating targeted attacks in FSR and propose a novel dualview attack framework, named DV-FSR. This attack method uniquely combines a sampling-based explicit strategy with a contrastive learning-based implicit gradient strategy to orchestrate a coordinated attack. Additionally, we introduce a specific defense mechanism tailored for targeted attacks in FSR, aiming to evaluate the mitigation effects of the attack method we proposed. Extensive experiments validate the effectiveness of our proposed approach on representative sequential models.",
            "id": "2409.07500",
            "link": "http://arxiv.org/abs/2409.07500v1",
            "published": "2024-09-10T15:24:13+00:00",
            "updated": "2024-09-10T15:24:13+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.IR"
            ],
            "max_author_hindex": 11
        },
        "2409.06122": {
            "authors": [
                "E. Wes Bethel",
                "Vianna Cramer",
                "Alexander del Rio",
                "Lothar Narins",
                "Chris Pestano",
                "Satvik Verma",
                "Erick Arias",
                "Nicola Bertelli",
                "Talita Perciano",
                "Syun'ichi Shiraiwa",
                "\u00c1lvaro S\u00e1nchez Villar",
                "Greg Wallace",
                "John C. Wright"
            ],
            "title": "Case Study: Leveraging GenAI to Build AI-based Surrogates and Regressors for Modeling Radio Frequency Heating in Fusion Energy Science",
            "abstract": "This work presents a detailed case study on using Generative AI (GenAI) to develop AI surrogates for simulation models in fusion energy research. The scope includes the methodology, implementation, and results of using GenAI to assist in model development and optimization, comparing these results with previous manually developed models.",
            "id": "2409.06122",
            "link": "http://arxiv.org/abs/2409.06122v1",
            "published": "2024-09-10T00:22:19+00:00",
            "updated": "2024-09-10T00:22:19+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.SE"
            ],
            "max_author_hindex": 37
        },
        "2409.07119": {
            "authors": [
                "Kai Sauerwald"
            ],
            "title": "Credibility-Limited Revision for Epistemic Spaces",
            "abstract": "We consider credibility-limited revision in the framework of belief change for epistemic spaces, permitting inconsistent belief sets and inconsistent beliefs. In this unrestricted setting, the class of credibility-limited revision operators does not include any AGM revision operators. We extend the class of credibility-limited revision operators in a way that all AGM revision operators are included while keeping the original spirit of credibility-limited revision. Extended credibility-limited revision operators are defined axiomatically. A semantic characterization of extended credibility-limited revision operators that employ total preorders on possible worlds is presented.",
            "id": "2409.07119",
            "link": "http://arxiv.org/abs/2409.07119v1",
            "published": "2024-09-11T09:15:43+00:00",
            "updated": "2024-09-11T09:15:43+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LO"
            ],
            "max_author_hindex": 5
        },
        "2409.06793": {
            "authors": [
                "Zhihao Dou",
                "Xin Hu",
                "Haibo Yang",
                "Zhuqing Liu",
                "Minghong Fang"
            ],
            "title": "Adversarial Attacks to Multi-Modal Models",
            "abstract": "Multi-modal models have gained significant attention due to their powerful capabilities. These models effectively align embeddings across diverse data modalities, showcasing superior performance in downstream tasks compared to their unimodal counterparts. Recent study showed that the attacker can manipulate an image or audio file by altering it in such a way that its embedding matches that of an attacker-chosen targeted input, thereby deceiving downstream models. However, this method often underperforms due to inherent disparities in data from different modalities. In this paper, we introduce CrossFire, an innovative approach to attack multi-modal models. CrossFire begins by transforming the targeted input chosen by the attacker into a format that matches the modality of the original image or audio file. We then formulate our attack as an optimization problem, aiming to minimize the angular deviation between the embeddings of the transformed input and the modified image or audio file. Solving this problem determines the perturbations to be added to the original media. Our extensive experiments on six real-world benchmark datasets reveal that CrossFire can significantly manipulate downstream tasks, surpassing existing attacks. Additionally, we evaluate six defensive strategies against CrossFire, finding that current defenses are insufficient to counteract our CrossFire.",
            "id": "2409.06793",
            "link": "http://arxiv.org/abs/2409.06793v1",
            "published": "2024-09-10T18:02:51+00:00",
            "updated": "2024-09-10T18:02:51+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 41
        },
        "2409.07709": {
            "authors": [
                "Kajal Patel",
                "Zubair Shafiq",
                "Mateus Nogueira",
                "Daniel Sadoc Menasch\u00e9",
                "Enrico Lovat",
                "Taimur Kashif",
                "Ashton Woiwood",
                "Matheus Martins"
            ],
            "title": "Harnessing TI Feeds for Exploitation Detection",
            "abstract": "Many organizations rely on Threat Intelligence (TI) feeds to assess the risk associated with security threats. Due to the volume and heterogeneity of data, it is prohibitive to manually analyze the threat information available in different loosely structured TI feeds. Thus, there is a need to develop automated methods to vet and extract actionable information from TI feeds. To this end, we present a machine learning pipeline to automatically detect vulnerability exploitation from TI feeds. We first model threat vocabulary in loosely structured TI feeds using state-of-the-art embedding techniques (Doc2Vec and BERT) and then use it to train a supervised machine learning classifier to detect exploitation of security vulnerabilities. We use our approach to identify exploitation events in 191 different TI feeds. Our longitudinal evaluation shows that it is able to accurately identify exploitation events from TI feeds only using past data for training and even on TI feeds withheld from training. Our proposed approach is useful for a variety of downstream tasks such as data-driven vulnerability risk assessment.",
            "id": "2409.07709",
            "link": "http://arxiv.org/abs/2409.07709v1",
            "published": "2024-09-12T02:25:41+00:00",
            "updated": "2024-09-12T02:25:41+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.IR"
            ],
            "max_author_hindex": 25
        },
        "2409.07730": {
            "authors": [
                "T. Aleksandra Ma",
                "Alexander Lerch"
            ],
            "title": "Music auto-tagging in the long tail: A few-shot approach",
            "abstract": "In the realm of digital music, using tags to efficiently organize and retrieve music from extensive databases is crucial for music catalog owners. Human tagging by experts is labor-intensive but mostly accurate, whereas automatic tagging through supervised learning has approached satisfying accuracy but is restricted to a predefined set of training tags. Few-shot learning offers a viable solution to expand beyond this small set of predefined tags by enabling models to learn from only a few human-provided examples to understand tag meanings and subsequently apply these tags autonomously. We propose to integrate few-shot learning methodology into multi-label music auto-tagging by using features from pre-trained models as inputs to a lightweight linear classifier, also known as a linear probe. We investigate different popular pre-trained features, as well as different few-shot parametrizations with varying numbers of classes and samples per class. Our experiments demonstrate that a simple model with pre-trained features can achieve performance close to state-of-the-art models while using significantly less training data, such as 20 samples per tag. Additionally, our linear probe performs competitively with leading models when trained on the entire training dataset. The results show that this transfer learning-based few-shot approach could effectively address the issue of automatically assigning long-tail tags with only limited labeled data.",
            "id": "2409.07730",
            "link": "http://arxiv.org/abs/2409.07730v1",
            "published": "2024-09-12T03:33:19+00:00",
            "updated": "2024-09-12T03:33:19+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.IR",
                "cs.LG",
                "cs.SD",
                "H.3.3"
            ],
            "max_author_hindex": 20
        },
        "2409.05367": {
            "authors": [
                "Nils Dycke",
                "Matej Ze\u010devi\u0107",
                "Ilia Kuznetsov",
                "Beatrix Suess",
                "Kristian Kersting",
                "Iryna Gurevych"
            ],
            "title": "Diagnostic Reasoning in Natural Language: Computational Model and Application",
            "abstract": "Diagnostic reasoning is a key component of expert work in many domains. It is a hard, time-consuming activity that requires expertise, and AI research has investigated the ways automated systems can support this process. Yet, due to the complexity of natural language, the applications of AI for diagnostic reasoning to language-related tasks are lacking. To close this gap, we investigate diagnostic abductive reasoning (DAR) in the context of language-grounded tasks (NL-DAR). We propose a novel modeling framework for NL-DAR based on Pearl's structural causal models and instantiate it in a comprehensive study of scientific paper assessment in the biomedical domain. We use the resulting dataset to investigate the human decision-making process in NL-DAR and determine the potential of LLMs to support structured decision-making over text. Our framework, open resources and tools lay the groundwork for the empirical study of collaborative diagnostic reasoning in the age of LLMs, in the scholarly domain and beyond.",
            "id": "2409.05367",
            "link": "http://arxiv.org/abs/2409.05367v1",
            "published": "2024-09-09T06:55:37+00:00",
            "updated": "2024-09-09T06:55:37+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 76
        },
        "2409.05368": {
            "authors": [
                "Rohit Raj Rai",
                "Angana Borah",
                "Amit Awekar"
            ],
            "title": "Application Specific Compression of Deep Learning Models",
            "abstract": "Large Deep Learning models are compressed and deployed for specific applications. However, current Deep Learning model compression methods do not utilize the information about the target application. As a result, the compressed models are application agnostic. Our goal is to customize the model compression process to create a compressed model that will perform better for the target application. Our method, Application Specific Compression (ASC), identifies and prunes components of the large Deep Learning model that are redundant specifically for the given target application. The intuition of our work is to prune the parts of the network that do not contribute significantly to updating the data representation for the given application. We have experimented with the BERT family of models for three applications: Extractive QA, Natural Language Inference, and Paraphrase Identification. We observe that customized compressed models created using ASC method perform better than existing model compression methods and off-the-shelf compressed models.",
            "id": "2409.05368",
            "link": "http://arxiv.org/abs/2409.05368v1",
            "published": "2024-09-09T06:55:38+00:00",
            "updated": "2024-09-09T06:55:38+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 11
        },
        "2409.05423": {
            "authors": [
                "Dylan Hillier",
                "Leon Guertler",
                "Bobby Cheng",
                "Cheston Tan"
            ],
            "title": "STLM Engineering Report: Dropout",
            "abstract": "In this work we explore the relevance of dropout for modern language models, particularly in the context of models on the scale of <100M parameters. We explore it's relevance firstly in the regime of improving the sample efficiency of models given small, high quality datasets, and secondly in the regime of improving the quality of its fit on larger datasets where models may underfit. We find that concordant with conventional wisdom, dropout remains effective in the overfitting scenario, and that furthermore it may have some relevance for improving the fit of models even in the case of excess data, as suggested by previous research. In the process we find that the existing explanation for the mechanism behind this performance gain is not applicable in the case of language modelling.",
            "id": "2409.05423",
            "link": "http://arxiv.org/abs/2409.05423v1",
            "published": "2024-09-09T08:24:29+00:00",
            "updated": "2024-09-09T08:24:29+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "I.2.7"
            ],
            "max_author_hindex": 17
        },
        "2409.05530": {
            "authors": [
                "Bruno D. Ferreira-Saraiva",
                "Zuil Pirola",
                "Jo\u00e3o P. Matos-Carvalho",
                "Manuel Marques-Pita"
            ],
            "title": "QiBERT -- Classifying Online Conversations Messages with BERT as a Feature",
            "abstract": "Recent developments in online communication and their usage in everyday life have caused an explosion in the amount of a new genre of text data, short text. Thus, the need to classify this type of text based on its content has a significant implication in many areas. Online debates are no exception, once these provide access to information about opinions, positions and preferences of its users. This paper aims to use data obtained from online social conversations in Portuguese schools (short text) to observe behavioural trends and to see if students remain engaged in the discussion when stimulated. This project used the state of the art (SoA) Machine Learning (ML) algorithms and methods, through BERT based models to classify if utterances are in or out of the debate subject. Using SBERT embeddings as a feature, with supervised learning, the proposed model achieved results above 0.95 average accuracy for classifying online messages. Such improvements can help social scientists better understand human communication, behaviour, discussion and persuasion.",
            "id": "2409.05530",
            "link": "http://arxiv.org/abs/2409.05530v1",
            "published": "2024-09-09T11:38:06+00:00",
            "updated": "2024-09-09T11:38:06+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 13
        },
        "2409.05583": {
            "authors": [
                "Muraleekrishna Gopinathan",
                "Martin Masek",
                "Jumana Abu-Khalaf",
                "David Suter"
            ],
            "title": "Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation",
            "abstract": "Embodied AI aims to develop robots that can \\textit{understand} and execute human language instructions, as well as communicate in natural languages. On this front, we study the task of generating highly detailed navigational instructions for the embodied robots to follow. Although recent studies have demonstrated significant leaps in the generation of step-by-step instructions from sequences of images, the generated instructions lack variety in terms of their referral to objects and landmarks. Existing speaker models learn strategies to evade the evaluation metrics and obtain higher scores even for low-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker), an instruction generator or \\textit{Speaker} model that utilises both structural and semantic knowledge of the environment to produce richer instructions. For training, we employ a reward learning method in an adversarial setting to avoid systematic bias introduced by language evaluation metrics. Empirically, our method outperforms existing instruction generation models, evaluated using standard metrics. Our code is available at \\url{https://github.com/gmuraleekrishna/SAS}.",
            "id": "2409.05583",
            "link": "http://arxiv.org/abs/2409.05583v1",
            "published": "2024-09-09T13:12:11+00:00",
            "updated": "2024-09-09T13:12:11+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "I.2.7; I.2.10; I.2.9"
            ],
            "max_author_hindex": 47
        },
        "2409.05653": {
            "authors": [
                "Vagrant Gautam",
                "Julius Steuer",
                "Eileen Bingert",
                "Ray Johns",
                "Anne Lauscher",
                "Dietrich Klakow"
            ],
            "title": "Revisiting English Winogender Schemas for Consistency, Coverage, and Grammatical Case",
            "abstract": "While measuring bias and robustness in coreference resolution are important goals, such measurements are only as good as the tools we use to measure them with. Winogender schemas (Rudinger et al., 2018) are an influential dataset proposed to evaluate gender bias in coreference resolution, but a closer look at the data reveals issues with the instances that compromise their use for reliable evaluation, including treating different grammatical cases of pronouns in the same way, violations of template constraints, and typographical errors. We identify these issues and fix them, contributing a new dataset: Winogender 2.0. Our changes affect performance with state-of-the-art supervised coreference resolution systems as well as all model sizes of the language model FLAN-T5, with F1 dropping on average 0.1 points. We also propose a new method to evaluate pronominal bias in coreference resolution that goes beyond the binary. With this method and our new dataset which is balanced for grammatical case, we empirically demonstrate that bias characteristics vary not just across pronoun sets, but also across surface forms of those sets.",
            "id": "2409.05653",
            "link": "http://arxiv.org/abs/2409.05653v1",
            "published": "2024-09-09T14:19:21+00:00",
            "updated": "2024-09-09T14:19:21+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 32
        },
        "2409.05732": {
            "authors": [
                "Meng Zhou",
                "Surajsinh Parmar",
                "Anubhav Bhatti"
            ],
            "title": "Towards Democratizing Multilingual Large Language Models For Medicine Through A Two-Stage Instruction Fine-tuning Approach",
            "abstract": "Open-source, multilingual medical large language models (LLMs) have the potential to serve linguistically diverse populations across different regions. Adapting generic LLMs for healthcare often requires continual pretraining, but this approach is computationally expensive and sometimes impractical. Instruction fine-tuning on a specific task may not always guarantee optimal performance due to the lack of broader domain knowledge that the model needs to understand and reason effectively in diverse scenarios. To address these challenges, we introduce two multilingual instruction fine-tuning datasets, MMed-IFT and MMed-IFT-MC, containing over 200k high-quality medical samples in six languages. We propose a two-stage training paradigm: the first stage injects general medical knowledge using MMed-IFT, while the second stage fine-tunes task-specific multiple-choice questions with MMed-IFT-MC. Our method achieves competitive results on both English and multilingual benchmarks, striking a balance between computational efficiency and performance. We plan to make our dataset and model weights public at \\url{https://github.com/SpassMed/Med-Llama3} in the future.",
            "id": "2409.05732",
            "link": "http://arxiv.org/abs/2409.05732v1",
            "published": "2024-09-09T15:42:19+00:00",
            "updated": "2024-09-09T15:42:19+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 107
        },
        "2409.05816": {
            "authors": [
                "Tristan Thrush",
                "Christopher Potts",
                "Tatsunori Hashimoto"
            ],
            "title": "Improving Pretraining Data Using Perplexity Correlations",
            "abstract": "Quality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier.",
            "id": "2409.05816",
            "link": "http://arxiv.org/abs/2409.05816v1",
            "published": "2024-09-09T17:23:29+00:00",
            "updated": "2024-09-09T17:23:29+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG",
                "stat.ML"
            ],
            "max_author_hindex": 26
        },
        "2409.05977": {
            "authors": [
                "Xichen Tang"
            ],
            "title": "AI for Mathematics Mathematical Formalized Problem Solving and Theorem Proving in Different Fields in Lean4",
            "abstract": "Using computerized verifiable formal languages like Lean 4 to prove mathematical theorems has a significant impact on mathematical formalization. Lean 4 offers prominent potential for advancing mathematical reasoning. However, existing efforts are limited to mathematical formalization languages in substantial online corpora and are dedicated to keeping pace with rapidly evolving languages. To bridge the gap between the traditional and computerized proof, my approach to formalizing theorem proving involves generating formal steps and complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. The method is to introduce the basic structure and tactics in general, determine how AI can assist the mathematical formalization process to improve its performance, and give examples of solving problems in Lean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract algebra.",
            "id": "2409.05977",
            "link": "http://arxiv.org/abs/2409.05977v1",
            "published": "2024-09-09T18:21:28+00:00",
            "updated": "2024-09-09T18:21:28+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 1
        },
        "2409.05997": {
            "authors": [
                "Lukas Garbas",
                "Max Ploner",
                "Alan Akbik"
            ],
            "title": "TransformerRanker: A Tool for Efficiently Finding the Best-Suited Language Models for Downstream Classification Tasks",
            "abstract": "Classification tasks in NLP are typically addressed by selecting a pre-trained language model (PLM) from a model hub, and fine-tuning it for the task at hand. However, given the very large number of PLMs that are currently available, a practical challenge is to determine which of them will perform best for a specific downstream task. With this paper, we introduce TransformerRanker, a lightweight library that efficiently ranks PLMs for classification tasks without the need for computationally costly fine-tuning. Our library implements current approaches for transferability estimation (LogME, H-Score, kNN), in combination with layer aggregation options, which we empirically showed to yield state-of-the-art rankings of PLMs (Garbas et al., 2024). We designed the interface to be lightweight and easy to use, allowing users to directly connect to the HuggingFace Transformers and Dataset libraries. Users need only select a downstream classification task and a list of PLMs to create a ranking of likely best-suited PLMs for their task. We make TransformerRanker available as a pip-installable open-source library https://github.com/flairNLP/transformer-ranker.",
            "id": "2409.05997",
            "link": "http://arxiv.org/abs/2409.05997v1",
            "published": "2024-09-09T18:47:00+00:00",
            "updated": "2024-09-09T18:47:00+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 15
        },
        "2409.06013": {
            "authors": [
                "Leanne Nortje",
                "Dan Oneata",
                "Herman Kamper"
            ],
            "title": "Improved Visually Prompted Keyword Localisation in Real Low-Resource Settings",
            "abstract": "Given an image query, visually prompted keyword localisation (VPKL) aims to find occurrences of the depicted word in a speech collection. This can be useful when transcriptions are not available for a low-resource language (e.g. if it is unwritten). Previous work showed that VPKL can be performed with a visually grounded speech model trained on paired images and unlabelled speech. But all experiments were done on English. Moreover, transcriptions were used to get positive and negative pairs for the contrastive loss. This paper introduces a few-shot learning scheme to mine pairs automatically without transcriptions. On English, this results in only a small drop in performance. We also - for the first time - consider VPKL on a real low-resource language, Yoruba. While scores are reasonable, here we see a bigger drop in performance compared to using ground truth pairs because the mining is less accurate in Yoruba.",
            "id": "2409.06013",
            "link": "http://arxiv.org/abs/2409.06013v1",
            "published": "2024-09-09T19:12:03+00:00",
            "updated": "2024-09-09T19:12:03+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.CV",
                "eess.AS"
            ],
            "max_author_hindex": 27
        },
        "2409.06043": {
            "authors": [
                "Christina Walker",
                "Joan C. Timoneda"
            ],
            "title": "Identifying the sources of ideological bias in GPT models through linguistic variation in output",
            "abstract": "Extant work shows that generative AI models such as GPT-3.5 and 4 perpetuate social stereotypes and biases. One concerning but less explored source of bias is ideology. Do GPT models take ideological stances on politically sensitive topics? In this article, we provide an original approach to identifying ideological bias in generative models, showing that bias can stem from both the training data and the filtering algorithm. We leverage linguistic variation in countries with contrasting political attitudes to evaluate bias in average GPT responses to sensitive political topics in those languages. First, we find that GPT output is more conservative in languages that map well onto conservative societies (i.e., Polish), and more liberal in languages used uniquely in liberal societies (i.e., Swedish). This result provides strong evidence of training data bias in GPT models. Second, differences across languages observed in GPT-3.5 persist in GPT-4, even though GPT-4 is significantly more liberal due to OpenAI's filtering policy. Our main takeaway is that generative model training must focus on high-quality, curated datasets to reduce bias, even if it entails a compromise in training data size. Filtering responses after training only introduces new biases and does not remove the underlying training biases.",
            "id": "2409.06043",
            "link": "http://arxiv.org/abs/2409.06043v1",
            "published": "2024-09-09T20:11:08+00:00",
            "updated": "2024-09-09T20:11:08+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 6
        },
        "2409.06097": {
            "authors": [
                "Yujian Gan",
                "Changling Li",
                "Jinxia Xie",
                "Luou Wen",
                "Matthew Purver",
                "Massimo Poesio"
            ],
            "title": "ClarQ-LLM: A Benchmark for Models Clarifying and Requesting Information in Task-Oriented Dialog",
            "abstract": "We introduce ClarQ-LLM, an evaluation framework consisting of bilingual English-Chinese conversation tasks, conversational agents and evaluation metrics, designed to serve as a strong benchmark for assessing agents' ability to ask clarification questions in task-oriented dialogues. The benchmark includes 31 different task types, each with 10 unique dialogue scenarios between information seeker and provider agents. The scenarios require the seeker to ask questions to resolve uncertainty and gather necessary information to complete tasks. Unlike traditional benchmarks that evaluate agents based on fixed dialogue content, ClarQ-LLM includes a provider conversational agent to replicate the original human provider in the benchmark. This allows both current and future seeker agents to test their ability to complete information gathering tasks through dialogue by directly interacting with our provider agent. In tests, LLAMA3.1 405B seeker agent managed a maximum success rate of only 60.05\\%, showing that ClarQ-LLM presents a strong challenge for future research.",
            "id": "2409.06097",
            "link": "http://arxiv.org/abs/2409.06097v1",
            "published": "2024-09-09T22:29:35+00:00",
            "updated": "2024-09-09T22:29:35+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 50
        },
        "2409.06109": {
            "authors": [
                "Sung-Lin Yeh",
                "Hao Tang"
            ],
            "title": "Estimating the Completeness of Discrete Speech Units",
            "abstract": "Representing speech with discrete units has been widely used in speech codec and speech generation. However, there are several unverified claims about self-supervised discrete units, such as disentangling phonetic and speaker information with k-means, or assuming information loss after k-means. In this work, we take an information-theoretic perspective to answer how much information is present (information completeness) and how much information is accessible (information accessibility), before and after residual vector quantization. We show a lower bound for information completeness and estimate completeness on discretized HuBERT representations after residual vector quantization. We find that speaker information is sufficiently present in HuBERT discrete units, and that phonetic information is sufficiently present in the residual, showing that vector quantization does not achieve disentanglement. Our results offer a comprehensive assessment on the choice of discrete units, and suggest that a lot more information in the residual should be mined rather than discarded.",
            "id": "2409.06109",
            "link": "http://arxiv.org/abs/2409.06109v1",
            "published": "2024-09-09T23:31:56+00:00",
            "updated": "2024-09-09T23:31:56+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 14
        },
        "2409.06216": {
            "authors": [
                "Kohei Tsuji",
                "Tatsuya Hiraoka",
                "Yuchang Cheng",
                "Tomoya Iwakura"
            ],
            "title": "SubRegWeigh: Effective and Efficient Annotation Weighing with Subword Regularization",
            "abstract": "Many datasets of natural language processing (NLP) sometimes include annotation errors. Researchers have attempted to develop methods to reduce the adverse effect of errors in datasets automatically. However, an existing method is time-consuming because it requires many trained models to detect errors. We propose a novel method to reduce the time of error detection. Specifically, we use a tokenization technique called subword regularization to create pseudo-multiple models which are used to detect errors. Our proposed method, SubRegWeigh, can perform annotation weighting four to five times faster than the existing method. Additionally, SubRegWeigh improved performance in both document classification and named entity recognition tasks. In experiments with pseudo-incorrect labels, pseudo-incorrect labels were adequately detected.",
            "id": "2409.06216",
            "link": "http://arxiv.org/abs/2409.06216v1",
            "published": "2024-09-10T04:48:36+00:00",
            "updated": "2024-09-10T04:48:36+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 52
        },
        "2409.06338": {
            "authors": [
                "Zi Yang"
            ],
            "title": "Retrieval Or Holistic Understanding? Dolce: Differentiate Our Long Context Evaluation Tasks",
            "abstract": "We argue that there are two major distinct capabilities in long context understanding: retrieval and holistic understanding. Understanding and further improving LLMs' long context capabilities would not be possible without knowing the tasks' focus categories. We aim to automatically identify retrieval focused and holistic understanding focused problems from suites of benchmarks and quantitatively measure the difficulty within each focus. In this paper, we present the Dolce framework, which parameterizes each problem by $\\lambda$ (complexity) and $k$ (redundancy) and assigns to one of five predefined focus categories. We propose to sample short contexts from the full context and estimate the probability an LLM solves the problem using the sampled spans. To find the $\\lambda$ and $k$ for each problem, we further propose a mixture model of a non-parametric background noise component and a parametric/non-parametric hybrid oracle component, where we derive the probability functions parameterized by $\\lambda$ and $k$ for both the correct-or-wrong (COW) scenario and the partial-point-in-grading (PIG) scenario. Our proposed methods can identify 0% to 67% of the problems are retrieval focused and 0% to 90% of the problems are holistic understanding focused across 44 existing long context evaluation tasks.",
            "id": "2409.06338",
            "link": "http://arxiv.org/abs/2409.06338v1",
            "published": "2024-09-10T08:48:05+00:00",
            "updated": "2024-09-10T08:48:05+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 13
        },
        "2409.06386": {
            "authors": [
                "Masato Kikuchi",
                "Masatsugu Ono",
                "Toshioki Soga",
                "Tetsu Tanabe",
                "Tadachika Ozono"
            ],
            "title": "Coarse-Grained Sense Inventories Based on Semantic Matching between English Dictionaries",
            "abstract": "WordNet is one of the largest handcrafted concept dictionaries visualizing word connections through semantic relationships. It is widely used as a word sense inventory in natural language processing tasks. However, WordNet's fine-grained senses have been criticized for limiting its usability. In this paper, we semantically match sense definitions from Cambridge dictionaries and WordNet and develop new coarse-grained sense inventories. We verify the effectiveness of our inventories by comparing their semantic coherences with that of Coarse Sense Inventory. The advantages of the proposed inventories include their low dependency on large-scale resources, better aggregation of closely related senses, CEFR-level assignments, and ease of expansion and improvement.",
            "id": "2409.06386",
            "link": "http://arxiv.org/abs/2409.06386v1",
            "published": "2024-09-10T10:08:58+00:00",
            "updated": "2024-09-10T10:08:58+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 36
        },
        "2409.06540": {
            "authors": [
                "Jan Elfes"
            ],
            "title": "Mapping News Narratives Using LLMs and Narrative-Structured Text Embeddings",
            "abstract": "Given the profound impact of narratives across various societal levels, from personal identities to international politics, it is crucial to understand their distribution and development over time. This is particularly important in online spaces. On the Web, narratives can spread rapidly and intensify societal divides and conflicts. While many qualitative approaches exist, quantifying narratives remains a significant challenge. Computational narrative analysis lacks frameworks that are both comprehensive and generalizable. To address this gap, we introduce a numerical narrative representation grounded in structuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a narrative through a constellation of six functional character roles. These so-called actants are genre-agnostic, making the model highly generalizable. We extract the actants using an open-source LLM and integrate them into a Narrative-Structured Text Embedding that captures both the semantics and narrative structure of a text. We demonstrate the analytical insights of the method on the example of 5000 full-text news articles from Al Jazeera and The Washington Post on the Israel-Palestine conflict. Our method successfully distinguishes articles that cover the same topics but differ in narrative structure.",
            "id": "2409.06540",
            "link": "http://arxiv.org/abs/2409.06540v1",
            "published": "2024-09-10T14:15:30+00:00",
            "updated": "2024-09-10T14:15:30+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 1
        },
        "2409.06550": {
            "authors": [
                "Victor Bocharov",
                "Romaric Besan\u00e7on",
                "Ga\u00ebl de Chalendar",
                "Olivier Ferret",
                "Nasredine Semmar"
            ],
            "title": "From LIMA to DeepLIMA: following a new path of interoperability",
            "abstract": "In this article, we describe the architecture of the LIMA (Libre Multilingual Analyzer) framework and its recent evolution with the addition of new text analysis modules based on deep neural networks. We extended the functionality of LIMA in terms of the number of supported languages while preserving existing configurable architecture and the availability of previously developed rule-based and statistical analysis components. Models were trained for more than 60 languages on the Universal Dependencies 2.5 corpora, WikiNer corpora, and CoNLL-03 dataset. Universal Dependencies allowed us to increase the number of supported languages and to generate models that could be integrated into other platforms. This integration of ubiquitous Deep Learning Natural Language Processing models and the use of standard annotated collections using Universal Dependencies can be viewed as a new path of interoperability, through the normalization of models and data, that are complementary to a more standard technical interoperability, implemented in LIMA through services available in Docker containers on Docker Hub.",
            "id": "2409.06550",
            "link": "http://arxiv.org/abs/2409.06550v1",
            "published": "2024-09-10T14:26:12+00:00",
            "updated": "2024-09-10T14:26:12+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 22
        },
        "2409.06567": {
            "authors": [
                "Vivi Nastase",
                "Chunyang Jiang",
                "Giuseppe Samo",
                "Paola Merlo"
            ],
            "title": "Exploring syntactic information in sentence embeddings through multilingual subject-verb agreement",
            "abstract": "In this paper, our goal is to investigate to what degree multilingual pretrained language models capture cross-linguistically valid abstract linguistic representations. We take the approach of developing curated synthetic data on a large scale, with specific properties, and using them to study sentence representations built using pretrained language models. We use a new multiple-choice task and datasets, Blackbird Language Matrices (BLMs), to focus on a specific grammatical structural phenomenon -- subject-verb agreement across a variety of sentence structures -- in several languages. Finding a solution to this task requires a system detecting complex linguistic patterns and paradigms in text representations. Using a two-level architecture that solves the problem in two steps -- detect syntactic objects and their properties in individual sentences, and find patterns across an input sequence of sentences -- we show that despite having been trained on multilingual texts in a consistent manner, multilingual pretrained language models have language-specific differences, and syntactic structure is not shared, even across closely related languages.",
            "id": "2409.06567",
            "link": "http://arxiv.org/abs/2409.06567v1",
            "published": "2024-09-10T14:58:55+00:00",
            "updated": "2024-09-10T14:58:55+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "68T50",
                "I.2.7"
            ],
            "max_author_hindex": 19
        },
        "2409.06595": {
            "authors": [
                "Sacha Muller",
                "Ant\u00f3nio Loison",
                "Bilel Omrani",
                "Gautier Viaud"
            ],
            "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering",
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use Large Language Models (LLMs) alongside private and up-to-date knowledge bases. In this work, we address the challenges of using LLM-as-a-Judge when evaluating grounded answers generated by RAG systems. To assess the calibration and discrimination capabilities of judge models, we identify 7 generator failure modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a meta-evaluation benchmark of 144 unit tests. This benchmark reveals that existing automated RAG evaluation frameworks often overlook important failure modes, even when using GPT-4 as a judge.   To improve on the current design of automated RAG evaluation frameworks, we propose a novel pipeline and find that while closed models perform well on GroUSE, state-of-the-art open-source judges do not generalize to our proposed criteria, despite strong correlation with GPT-4's judgement. Our findings suggest that correlation with GPT-4 is an incomplete proxy for the practical performance of judge models and should be supplemented with evaluations on unit tests for precise failure mode detection.   We further show that finetuning Llama-3 on GPT-4's reasoning traces significantly boosts its evaluation capabilities, improving upon both correlation with GPT-4's evaluations and calibration on reference situations.",
            "id": "2409.06595",
            "link": "http://arxiv.org/abs/2409.06595v1",
            "published": "2024-09-10T15:39:32+00:00",
            "updated": "2024-09-10T15:39:32+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "I.2.7"
            ],
            "max_author_hindex": 6
        },
        "2409.06601": {
            "authors": [
                "Yetao Wu",
                "Yihong Wang",
                "Teng Chen",
                "Chenxi Liu",
                "Ningyuan Xi",
                "Qingqing Gu",
                "Hongyang Lei",
                "Zhonglin Jiang",
                "Yong Chen",
                "Luo Ji"
            ],
            "title": "Alleviating Hallucinations in Large Language Models with Scepticism Modeling",
            "abstract": "Hallucinations is a major challenge for large language models (LLMs), prevents adoption in diverse fields. Uncertainty estimation could be used for alleviating the damages of hallucinations. The skeptical emotion of human could be useful for enhancing the ability of self estimation. Inspirited by this observation, we proposed a new approach called Skepticism Modeling (SM). This approach is formalized by combining the information of token and logits for self estimation. We construct the doubt emotion aware data, perform continual pre-training, and then fine-tune the LLMs, improve their ability of self estimation. Experimental results demonstrate this new approach effectively enhances a model's ability to estimate their uncertainty, and validate its generalization ability of other tasks by out-of-domain experiments.",
            "id": "2409.06601",
            "link": "http://arxiv.org/abs/2409.06601v1",
            "published": "2024-09-10T15:51:15+00:00",
            "updated": "2024-09-10T15:51:15+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 60
        },
        "2409.06622": {
            "authors": [
                "Vivi Nastase",
                "Giuseppe Samo",
                "Chunyang Jiang",
                "Paola Merlo"
            ],
            "title": "Exploring Italian sentence embeddings properties through multi-tasking",
            "abstract": "We investigate to what degree existing LLMs encode abstract linguistic information in Italian in a multi-task setting. We exploit curated synthetic data on a large scale -- several Blackbird Language Matrices (BLMs) problems in Italian -- and use them to study how sentence representations built using pre-trained language models encode specific syntactic and semantic information. We use a two-level architecture to model separately a compression of the sentence embeddings into a representation that contains relevant information for a task, and a BLM task. We then investigate whether we can obtain compressed sentence representations that encode syntactic and semantic information relevant to several BLM tasks. While we expected that the sentence structure -- in terms of sequence of phrases/chunks -- and chunk properties could be shared across tasks, performance and error analysis show that the clues for the different tasks are encoded in different manners in the sentence embeddings, suggesting that abstract linguistic notions such as constituents or thematic roles does not seem to be present in the pretrained sentence embeddings.",
            "id": "2409.06622",
            "link": "http://arxiv.org/abs/2409.06622v1",
            "published": "2024-09-10T16:22:18+00:00",
            "updated": "2024-09-10T16:22:18+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "68T50",
                "I.2.7"
            ],
            "max_author_hindex": 19
        },
        "2409.06639": {
            "authors": [
                "Kyudan Jung",
                "Nam-Joon Kim",
                "Hyongon Ryu",
                "Sieun Hyeon",
                "Seung-jun Lee",
                "Hyeok-jae Lee"
            ],
            "title": "TeXBLEU: Automatic Metric for Evaluate LaTeX Format",
            "abstract": "LaTeX is suitable for creating specially formatted documents in science, technology, mathematics, and computer science. Although the use of mathematical expressions in LaTeX format along with language models is increasing, there are no proper evaluation matrices to evaluate them. In this study, we propose TeXBLEU, a metric for evaluating mathematical expressions in the LaTeX format built on the n-gram-based BLEU metric widely used in translation tasks. The proposed TeXBLEU consists of a predefined tokenizer trained on the arXiv paper dataset and a fine-tuned embedding model with positional encoding. The TeXBLEU score was calculated by replacing BLUE's modified precision score with the similarity of n-gram-based tokens. TeXBLEU showed improvements of 86\\%, 121\\%, and 610\\% over traditional evaluation metrics, such as BLEU, sacreBLEU, and Rouge, respectively, on the MathBridge dataset with 1,000 data points. The code is available at https://github.com/KyuDan1/TeXBLEU.",
            "id": "2409.06639",
            "link": "http://arxiv.org/abs/2409.06639v2",
            "published": "2024-09-10T16:54:32+00:00",
            "updated": "2024-09-12T17:35:42+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 27
        },
        "2409.06679": {
            "authors": [
                "Zihan Liao",
                "Jun Wang",
                "Hang Yu",
                "Lingxiao Wei",
                "Jianguo Li",
                "Jun Wang",
                "Wei Zhang"
            ],
            "title": "E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning",
            "abstract": "In the realm of Large Language Models (LLMs), the ability to process long contexts is increasingly crucial for tasks such as multi-round dialogues, code generation, and document summarization. This paper addresses the challenges of enhancing the long-context performance, reducing computational complexity, and leveraging pretrained models collectively termed the \"impossible triangle.\" We introduce E2LLM (Encoder Elongated Large Language Models), a novel approach that effectively navigates this paradox. The method involves splitting long contexts into chunks, compressing each into embedding vectors via a pretrained text encoder, and utilizing an adapter to align these representations with a decoder-only LLM. Two training objectives, focusing on reconstruction of the encoder output and long-context instruction fine-tuning, are employed to facilitate the understanding of soft prompts by the LLM. Experimental results demonstrate that E2LLM achieves superior performance in long-context scenarios while balancing efficiency, performance, and compatibility with pretrained models. Our framework thus represents a significant advancement in the field, contributing to effective long-text modeling.",
            "id": "2409.06679",
            "link": "http://arxiv.org/abs/2409.06679v1",
            "published": "2024-09-10T17:44:35+00:00",
            "updated": "2024-09-10T17:44:35+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 29
        },
        "2409.06803": {
            "authors": [
                "Jiaxuan Li",
                "Richard Futrell"
            ],
            "title": "Decomposition of surprisal: Unified computational model of ERP components in language processing",
            "abstract": "The functional interpretation of language-related ERP components has been a central debate in psycholinguistics for decades. We advance an information-theoretic model of human language processing in the brain in which incoming linguistic input is processed at first shallowly and later with more depth, with these two kinds of information processing corresponding to distinct electroencephalographic signatures. Formally, we show that the information content (surprisal) of a word in context can be decomposed into two quantities: (A) heuristic surprise, which signals shallow processing difficulty for a word, and corresponds with the N400 signal; and (B) discrepancy signal, which reflects the discrepancy between shallow and deep interpretations, and corresponds to the P600 signal. Both of these quantities can be estimated straightforwardly using modern NLP models. We validate our theory by successfully simulating ERP patterns elicited by a variety of linguistic manipulations in previously-reported experimental data from six experiments, with successful novel qualitative and quantitative predictions. Our theory is compatible with traditional cognitive theories assuming a `good-enough' heuristic interpretation stage, but with a precise information-theoretic formulation. The model provides an information-theoretic model of ERP components grounded on cognitive processes, and brings us closer to a fully-specified neuro-computational model of language processing.",
            "id": "2409.06803",
            "link": "http://arxiv.org/abs/2409.06803v1",
            "published": "2024-09-10T18:14:02+00:00",
            "updated": "2024-09-10T18:14:02+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.IT",
                "math.IT"
            ],
            "max_author_hindex": 30
        },
        "2409.06857": {
            "authors": [
                "Lihu Chen",
                "Ga\u00ebl Varoquaux"
            ],
            "title": "What is the Role of Small Models in the LLM Era: A Survey",
            "abstract": "Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available at https://github.com/tigerchen52/role_of_small_models",
            "id": "2409.06857",
            "link": "http://arxiv.org/abs/2409.06857v2",
            "published": "2024-09-10T20:45:43+00:00",
            "updated": "2024-09-12T15:04:57+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 58
        },
        "2409.07064": {
            "authors": [
                "Jiun-Ting Li",
                "Bi-Cheng Yan",
                "Tien-Hong Lo",
                "Yi-Cheng Wang",
                "Yung-Chang Hsu",
                "Berlin Chen"
            ],
            "title": "Automated Speaking Assessment of Conversation Tests with Novel Graph-based Modeling on Spoken Response Coherence",
            "abstract": "Automated speaking assessment in conversation tests (ASAC) aims to evaluate the overall speaking proficiency of an L2 (second-language) speaker in a setting where an interlocutor interacts with one or more candidates. Although prior ASAC approaches have shown promising performance on their respective datasets, there is still a dearth of research specifically focused on incorporating the coherence of the logical flow within a conversation into the grading model. To address this critical challenge, we propose a hierarchical graph model that aptly incorporates both broad inter-response interactions (e.g., discourse relations) and nuanced semantic information (e.g., semantic words and speaker intents), which is subsequently fused with contextual information for the final prediction. Extensive experimental results on the NICT-JLE benchmark dataset suggest that our proposed modeling approach can yield considerable improvements in prediction accuracy with respect to various assessment metrics, as compared to some strong baselines. This also sheds light on the importance of investigating coherence-related facets of spoken responses in ASAC.",
            "id": "2409.07064",
            "link": "http://arxiv.org/abs/2409.07064v1",
            "published": "2024-09-11T07:24:07+00:00",
            "updated": "2024-09-11T07:24:07+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 96
        },
        "2409.07072": {
            "authors": [
                "Milad Alshomary",
                "Narutatsu Ri",
                "Marianna Apidianaki",
                "Ajay Patel",
                "Smaranda Muresan",
                "Kathleen McKeown"
            ],
            "title": "Latent Space Interpretation for Stylistic Analysis and Explainable Authorship Attribution",
            "abstract": "Recent state-of-the-art authorship attribution methods learn authorship representations of texts in a latent, non-interpretable space, hindering their usability in real-world applications. Our work proposes a novel approach to interpreting these learned embeddings by identifying representative points in the latent space and utilizing LLMs to generate informative natural language descriptions of the writing style of each point. We evaluate the alignment of our interpretable space with the latent one and find that it achieves the best prediction agreement compared to other baselines. Additionally, we conduct a human evaluation to assess the quality of these style descriptions, validating their utility as explanations for the latent space. Finally, we investigate whether human performance on the challenging AA task improves when aided by our system's explanations, finding an average improvement of around +20% in accuracy.",
            "id": "2409.07072",
            "link": "http://arxiv.org/abs/2409.07072v1",
            "published": "2024-09-11T07:48:06+00:00",
            "updated": "2024-09-11T07:48:06+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 72
        },
        "2409.07085": {
            "authors": [
                "Alina Fastowski",
                "Gjergji Kasneci"
            ],
            "title": "Understanding Knowledge Drift in LLMs through Misinformation",
            "abstract": "Large Language Models (LLMs) have revolutionized numerous applications, making them an integral part of our digital ecosystem. However, their reliability becomes critical, especially when these models are exposed to misinformation. We primarily analyze the susceptibility of state-of-the-art LLMs to factual inaccuracies when they encounter false information in a QnA scenario, an issue that can lead to a phenomenon we refer to as *knowledge drift*, which significantly undermines the trustworthiness of these models. We evaluate the factuality and the uncertainty of the models' responses relying on Entropy, Perplexity, and Token Probability metrics. Our experiments reveal that an LLM's uncertainty can increase up to 56.6% when the question is answered incorrectly due to the exposure to false information. At the same time, repeated exposure to the same false information can decrease the models uncertainty again (-52.8% w.r.t. the answers on the untainted prompts), potentially manipulating the underlying model's beliefs and introducing a drift from its original knowledge. These findings provide insights into LLMs' robustness and vulnerability to adversarial inputs, paving the way for developing more reliable LLM applications across various domains. The code is available at https://github.com/afastowski/knowledge_drift.",
            "id": "2409.07085",
            "link": "http://arxiv.org/abs/2409.07085v1",
            "published": "2024-09-11T08:11:16+00:00",
            "updated": "2024-09-11T08:11:16+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 34
        },
        "2409.07131": {
            "authors": [
                "Ant\u00f3nio Farinhas",
                "Haau-Sing Li",
                "Andr\u00e9 F. T. Martins"
            ],
            "title": "Reranking Laws for Language Generation: A Communication-Theoretic Perspective",
            "abstract": "To ensure large language models (LLMs) are used safely, one must reduce their propensity to hallucinate or to generate unacceptable answers. A simple and often used strategy is to first let the LLM generate multiple hypotheses and then employ a reranker to choose the best one. In this paper, we draw a parallel between this strategy and the use of redundancy to decrease the error rate in noisy communication channels. We conceptualize the generator as a sender transmitting multiple descriptions of a message through parallel noisy channels. The receiver decodes the message by ranking the (potentially corrupted) descriptions and selecting the one found to be most reliable. We provide conditions under which this protocol is asymptotically error-free (i.e., yields an acceptable answer almost surely) even in scenarios where the reranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the channel distributions are statistically dependent. We use our framework to obtain reranking laws which we validate empirically on two real-world tasks using LLMs: text-to-code generation with DeepSeek-Coder 7B and machine translation of medical data with TowerInstruct 13B.",
            "id": "2409.07131",
            "link": "http://arxiv.org/abs/2409.07131v1",
            "published": "2024-09-11T09:27:50+00:00",
            "updated": "2024-09-11T09:27:50+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG",
                "stat.ML"
            ],
            "max_author_hindex": 37
        },
        "2409.07146": {
            "authors": [
                "Yu Zhang",
                "Songlin Yang",
                "Ruijie Zhu",
                "Yue Zhang",
                "Leyang Cui",
                "Yiqiao Wang",
                "Bolun Wang",
                "Freda Shi",
                "Bailin Wang",
                "Wei Bi",
                "Peng Zhou",
                "Guohong Fu"
            ],
            "title": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling",
            "abstract": "Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch. This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC) by incorporating a gating mechanism inspired by Gated Linear Attention (GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size. This design greatly enhances both training and inference efficiency through GLA's hardware-efficient training algorithm and reduced state size. Additionally, retaining the softmax operation is particularly beneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings, reducing the need for extensive training from scratch. Extensive experiments confirm GSA's superior performance in scenarios requiring in-context recall and in T2R settings.",
            "id": "2409.07146",
            "link": "http://arxiv.org/abs/2409.07146v1",
            "published": "2024-09-11T09:49:50+00:00",
            "updated": "2024-09-11T09:49:50+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 20
        },
        "2409.07162": {
            "authors": [
                "Faiz Ali Shah",
                "Ahmed Sabir",
                "Rajesh Sharma"
            ],
            "title": "A Fine-grained Sentiment Analysis of App Reviews using Large Language Models: An Evaluation Study",
            "abstract": "Analyzing user reviews for sentiment towards app features can provide valuable insights into users' perceptions of app functionality and their evolving needs. Given the volume of user reviews received daily, an automated mechanism to generate feature-level sentiment summaries of user reviews is needed. Recent advances in Large Language Models (LLMs) such as ChatGPT have shown impressive performance on several new tasks without updating the model's parameters i.e. using zero or a few labeled examples. Despite these advancements, LLMs' capabilities to perform feature-specific sentiment analysis of user reviews remain unexplored. This study compares the performance of state-of-the-art LLMs, including GPT-4, ChatGPT, and LLama-2-chat variants, for extracting app features and associated sentiments under 0-shot, 1-shot, and 5-shot scenarios. Results indicate the best-performing GPT-4 model outperforms rule-based approaches by 23.6% in f1-score with zero-shot feature extraction; 5-shot further improving it by 6%. GPT-4 achieves a 74% f1-score for predicting positive sentiment towards correctly predicted app features, with 5-shot enhancing it by 7%. Our study suggests that LLM models are promising for generating feature-specific sentiment summaries of user reviews.",
            "id": "2409.07162",
            "link": "http://arxiv.org/abs/2409.07162v1",
            "published": "2024-09-11T10:21:13+00:00",
            "updated": "2024-09-11T10:21:13+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.SE"
            ],
            "max_author_hindex": 17
        },
        "2409.07170": {
            "authors": [
                "Jonathan D. Thomas",
                "Andrea Silvi",
                "Devdatt Dubhashi",
                "Emil Carlsson",
                "Moa Johansson"
            ],
            "title": "Learning Efficient Recursive Numeral Systems via Reinforcement Learning",
            "abstract": "The emergence of mathematical concepts, such as number systems, is an understudied area in AI for mathematics and reasoning. It has previously been shown Carlsson et al. (2021) that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems. However, it is a major challenge to show how more complex recursive numeral systems, similar to the one utilised in English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of recursive number systems where we consider an RL agent which directly optimizes a lexicon under a given meta-grammar. Utilising a slightly modified version of the seminal meta-grammar of Hurford (1975), we demonstrate that our RL agent can effectively modify the lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems.",
            "id": "2409.07170",
            "link": "http://arxiv.org/abs/2409.07170v1",
            "published": "2024-09-11T10:33:30+00:00",
            "updated": "2024-09-11T10:33:30+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 18
        },
        "2409.07355": {
            "authors": [
                "SeongYeub Chu",
                "JongWoo Kim",
                "MunYong Yi"
            ],
            "title": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation",
            "abstract": "This study introduces \\textbf{InteractEval}, a framework that integrates human expertise and Large Language Models (LLMs) using the Think-Aloud (TA) method to generate attributes for checklist-based text evaluation. By combining human flexibility and reasoning with LLM consistency, InteractEval outperforms traditional non-LLM-based and LLM-based baselines across four distinct dimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The experiment also investigates the effectiveness of the TA method, showing that it promotes divergent thinking in both humans and LLMs, leading to the generation of a wider range of relevant attributes and enhance text evaluation performance. Comparative analysis reveals that humans excel at identifying attributes related to internal quality (Coherence and Fluency), but LLMs perform better at those attributes related to external alignment (Consistency and Relevance). Consequently, leveraging both humans and LLMs together produces the best evaluation outcomes. In other words, this study emphasizes the necessity of effectively combining humans and LLMs in an automated checklist-based text evaluation framework. The code is available at \\textbf{\\url{https://github.com/BBeeChu/InteractEval.git}}.",
            "id": "2409.07355",
            "link": "http://arxiv.org/abs/2409.07355v1",
            "published": "2024-09-11T15:40:07+00:00",
            "updated": "2024-09-11T15:40:07+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 6
        },
        "2409.07423": {
            "authors": [
                "Alexandros Koulakos",
                "Maria Lymperaiou",
                "Giorgos Filandrianos",
                "Giorgos Stamou"
            ],
            "title": "Enhancing adversarial robustness in Natural Language Inference using explanations",
            "abstract": "The surge of state-of-the-art Transformer-based models has undoubtedly pushed the limits of NLP model performance, excelling in a variety of tasks. We cast the spotlight on the underexplored task of Natural Language Inference (NLI), since models trained on popular well-suited datasets are susceptible to adversarial attacks, allowing subtle input interventions to mislead the model. In this work, we validate the usage of natural language explanation as a model-agnostic defence strategy through extensive experimentation: only by fine-tuning a classifier on the explanation rather than premise-hypothesis inputs, robustness under various adversarial attacks is achieved in comparison to explanation-free baselines. Moreover, since there is no standard strategy of testing the semantic validity of the generated explanations, we research the correlation of widely used language generation metrics with human perception, in order for them to serve as a proxy towards robust NLI models. Our approach is resource-efficient and reproducible without significant computational limitations.",
            "id": "2409.07423",
            "link": "http://arxiv.org/abs/2409.07423v1",
            "published": "2024-09-11T17:09:49+00:00",
            "updated": "2024-09-11T17:09:49+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 29
        },
        "2409.07429": {
            "authors": [
                "Zora Zhiruo Wang",
                "Jiayuan Mao",
                "Daniel Fried",
                "Graham Neubig"
            ],
            "title": "Agent Workflow Memory",
            "abstract": "Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories. In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions. To build agents that can similarly benefit from this process, we introduce Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations. AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. We experiment on two major web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.",
            "id": "2409.07429",
            "link": "http://arxiv.org/abs/2409.07429v1",
            "published": "2024-09-11T17:21:00+00:00",
            "updated": "2024-09-11T17:21:00+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 81
        },
        "2409.07615": {
            "authors": [
                "Matthieu Dubois",
                "Fran\u00e7ois Yvon",
                "Pablo Piantanida"
            ],
            "title": "Zero-Shot Machine-Generated Text Detection Using Mixture of Large Language Models",
            "abstract": "The dissemination of Large Language Models (LLMs), trained at scale, and endowed with powerful text-generating abilities has vastly increased the threats posed by generative AI technologies by reducing the cost of producing harmful, toxic, faked or forged content. In response, various proposals have been made to automatically discriminate artificially generated from human-written texts, typically framing the problem as a classification problem. Most approaches evaluate an input document by a well-chosen detector LLM, assuming that low-perplexity scores reliably signal machine-made content. As using one single detector can induce brittleness of performance, we instead consider several and derive a new, theoretically grounded approach to combine their respective strengths. Our experiments, using a variety of generator LLMs, suggest that our method effectively increases the robustness of detection.",
            "id": "2409.07615",
            "link": "http://arxiv.org/abs/2409.07615v1",
            "published": "2024-09-11T20:55:12+00:00",
            "updated": "2024-09-11T20:55:12+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 34
        },
        "2409.07641": {
            "authors": [
                "Qi Jia",
                "Xiang Yue",
                "Tianyu Zheng",
                "Jie Huang",
                "Bill Yuchen Lin"
            ],
            "title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks",
            "abstract": "We introduce SimulBench, a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation scenarios, such as acting as a Linux terminal or playing text games with users. While these simulation tasks serve as effective measures of an LLM's general intelligence, they are seldom incorporated into existing benchmarks. A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi-round interactive nature of simulation tasks between users and AI. To tackle this issue, we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks. Then, challenging dialogue scripts are extracted for evaluating different target LLMs. To facilitate automatic assessment on \\DataName{}, GPT-4 is employed as the evaluator, tasked with reviewing the quality of the final response generated by the target LLMs given multi-turn dialogue scripts. Our comprehensive experiments indicate that these simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs. For example, GPT-4-turbo outperforms LLaMA-3-70b-Chat on 18.55\\% more cases.",
            "id": "2409.07641",
            "link": "http://arxiv.org/abs/2409.07641v1",
            "published": "2024-09-11T21:53:20+00:00",
            "updated": "2024-09-11T21:53:20+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 30
        },
        "2409.07713": {
            "authors": [
                "Jonathan Li",
                "Rohan Bhambhoria",
                "Samuel Dahan",
                "Xiaodan Zhu"
            ],
            "title": "Experimenting with Legal AI Solutions: The Case of Question-Answering for Access to Justice",
            "abstract": "Generative AI models, such as the GPT and Llama series, have significant potential to assist laypeople in answering legal questions. However, little prior work focuses on the data sourcing, inference, and evaluation of these models in the context of laypersons. To this end, we propose a human-centric legal NLP pipeline, covering data sourcing, inference, and evaluation. We introduce and release a dataset, LegalQA, with real and specific legal questions spanning from employment law to criminal law, corresponding answers written by legal experts, and citations for each answer. We develop an automatic evaluation protocol for this dataset, then show that retrieval-augmented generation from only 850 citations in the train set can match or outperform internet-wide retrieval, despite containing 9 orders of magnitude less data. Finally, we propose future directions for open-sourced efforts, which fall behind closed-sourced models.",
            "id": "2409.07713",
            "link": "http://arxiv.org/abs/2409.07713v1",
            "published": "2024-09-12T02:40:28+00:00",
            "updated": "2024-09-12T02:40:28+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 48
        },
        "2409.07787": {
            "authors": [
                "Woojin Chung",
                "Jiwoo Hong",
                "Na Min An",
                "James Thorne",
                "Se-Young Yun"
            ],
            "title": "Stable Language Model Pre-training by Reducing Embedding Variability",
            "abstract": "Stable pre-training is essential for achieving better-performing language models. However, tracking pre-training stability by calculating gradient variance at every step is impractical due to the significant computational costs. We explore Token Embedding Variability (TEV) as a simple and efficient proxy for assessing pre-training stability in language models with pre-layer normalization, given that shallower layers are more prone to gradient explosion (section 2.2). Moreover, we propose Multi-head Low-Rank Attention (MLRA) as an architecture to alleviate such instability by limiting the exponential growth of output embedding variance, thereby preventing the gradient explosion (section 3.2). Empirical results on GPT-2 with MLRA demonstrate increased stability and lower perplexity, particularly in deeper models.",
            "id": "2409.07787",
            "link": "http://arxiv.org/abs/2409.07787v1",
            "published": "2024-09-12T06:37:46+00:00",
            "updated": "2024-09-12T06:37:46+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 35
        },
        "2409.08103": {
            "authors": [
                "Michael Ong",
                "Sean Robertson",
                "Leo Peckham",
                "Alba Jorquera Jimenez de Aberasturi",
                "Paula Arkhangorodsky",
                "Robin Huo",
                "Aman Sakhardande",
                "Mark Hallap",
                "Naomi Nagy",
                "Ewan Dunbar"
            ],
            "title": "The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language",
            "abstract": "We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark corpus designed to push the limits of current approaches to low-resource speech recognition. Faetar, a Franco-Proven\\c{c}al variety spoken primarily in Italy, has no standard orthography, has virtually no existing textual or speech resources other than what is included in the benchmark, and is quite different from other forms of Franco-Proven\\c{c}al. The corpus comes from field recordings, most of which are noisy, for which only 5 hrs have matching transcriptions, and for which forced alignment is of variable quality. The corpus contains an additional 20 hrs of unlabelled speech. We report baseline results from state-of-the-art multilingual speech foundation models with a best phone error rate of 30.4%, using a pipeline that continues pre-training on the foundation model using the unlabelled set.",
            "id": "2409.08103",
            "link": "http://arxiv.org/abs/2409.08103v1",
            "published": "2024-09-12T14:55:33+00:00",
            "updated": "2024-09-12T14:55:33+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 20
        },
        "2409.08107": {
            "authors": [
                "Gil Ayache",
                "Menachem Pirchi",
                "Aviv Navon",
                "Aviv Shamsian",
                "Gill Hetz",
                "Joseph Keshet"
            ],
            "title": "WhisperNER: Unified Open Named Entity and Speech Recognition",
            "abstract": "Integrating named entity recognition (NER) with automatic speech recognition (ASR) can significantly enhance transcription accuracy and informativeness. In this paper, we introduce WhisperNER, a novel model that allows joint speech transcription and entity recognition. WhisperNER supports open-type NER, enabling recognition of diverse and evolving entities at inference. Building on recent advancements in open NER research, we augment a large synthetic dataset with synthetic speech samples. This allows us to train WhisperNER on a large number of examples with diverse NER tags. During training, the model is prompted with NER labels and optimized to output the transcribed utterance along with the corresponding tagged entities. To evaluate WhisperNER, we generate synthetic speech for commonly used NER benchmarks and annotate existing ASR datasets with open NER tags. Our experiments demonstrate that WhisperNER outperforms natural baselines on both out-of-domain open type NER and supervised finetuning.",
            "id": "2409.08107",
            "link": "http://arxiv.org/abs/2409.08107v1",
            "published": "2024-09-12T15:00:56+00:00",
            "updated": "2024-09-12T15:00:56+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 29
        },
        "2409.08160": {
            "authors": [
                "Andreas Opedal",
                "Eleanor Chodroff",
                "Ryan Cotterell",
                "Ethan Gotlieb Wilcox"
            ],
            "title": "On the Role of Context in Reading Time Prediction",
            "abstract": "We present a new perspective on how readers integrate context during real-time language comprehension. Our proposals build on surprisal theory, which posits that the processing effort of a linguistic unit (e.g., a word) is an affine function of its in-context information content. We first observe that surprisal is only one out of many potential ways that a contextual predictor can be derived from a language model. Another one is the pointwise mutual information (PMI) between a unit and its context, which turns out to yield the same predictive power as surprisal when controlling for unigram frequency. Moreover, both PMI and surprisal are correlated with frequency. This means that neither PMI nor surprisal contains information about context alone. In response to this, we propose a technique where we project surprisal onto the orthogonal complement of frequency, yielding a new contextual predictor that is uncorrelated with frequency. Our experiments show that the proportion of variance in reading times explained by context is a lot smaller when context is represented by the orthogonalized predictor. From an interpretability standpoint, this indicates that previous studies may have overstated the role that context has in predicting reading times.",
            "id": "2409.08160",
            "link": "http://arxiv.org/abs/2409.08160v1",
            "published": "2024-09-12T15:52:22+00:00",
            "updated": "2024-09-12T15:52:22+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 41
        },
        "2409.06754": {
            "authors": [
                "Qingyun Sun",
                "Zhen Guo"
            ],
            "title": "Scaling Law Hypothesis for Multimodal Model",
            "abstract": "We propose a scaling law hypothesis for multimodal models processing text, audio, images, and video within a shared token and embedding space. Our framework predicts model performance based on modality-specific compression and tokenization efficiency, extending established scaling laws from text-based decoder models to mixed-modality systems. We explore whether leveraging more training data in multiple modalities can reduce the size of the multimodal model, enabling efficient deployment on resource-constrained devices.",
            "id": "2409.06754",
            "link": "http://arxiv.org/abs/2409.06754v1",
            "published": "2024-09-10T16:05:02+00:00",
            "updated": "2024-09-10T16:05:02+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.07448": {
            "authors": [
                "Mohamed elShehaby",
                "Ashraf Matrawy"
            ],
            "title": "Introducing Perturb-ability Score (PS) to Enhance Robustness Against Evasion Adversarial Attacks on ML-NIDS",
            "abstract": "This paper proposes a novel Perturb-ability Score (PS) that can be used to identify Network Intrusion Detection Systems (NIDS) features that can be easily manipulated by attackers in the problem-space. We demonstrate that using PS to select only non-perturb-able features for ML-based NIDS maintains detection performance while enhancing robustness against adversarial attacks.",
            "id": "2409.07448",
            "link": "http://arxiv.org/abs/2409.07448v1",
            "published": "2024-09-11T17:52:37+00:00",
            "updated": "2024-09-11T17:52:37+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 2
        },
        "2409.05275": {
            "authors": [
                "Chengyuan Liu",
                "Shihang Wang",
                "Fubang Zhao",
                "Kun Kuang",
                "Yangyang Kang",
                "Weiming Lu",
                "Changlong Sun",
                "Fei Wu"
            ],
            "title": "RexUniNLU: Recursive Method with Explicit Schema Instructor for Universal NLU",
            "abstract": "Information Extraction (IE) and Text Classification (CLS) serve as the fundamental pillars of NLU, with both disciplines relying on analyzing input sequences to categorize outputs into pre-established schemas. However, there is no existing encoder-based model that can unify IE and CLS tasks from this perspective. To fully explore the foundation shared within NLU tasks, we have proposed a Recursive Method with Explicit Schema Instructor for Universal NLU. Specifically, we firstly redefine the true universal information extraction (UIE) with a formal formulation that covers almost all extraction schemas, including quadruples and quintuples which remain unsolved for previous UIE models. Then, we expands the formulation to all CLS and multi-modal NLU tasks. Based on that, we introduce RexUniNLU, an universal NLU solution that employs explicit schema constraints for IE and CLS, which encompasses all IE and CLS tasks and prevent incorrect connections between schema and input sequence. To avoid interference between different schemas, we reset the position ids and attention mask matrices. Extensive experiments are conducted on IE, CLS in both English and Chinese, and multi-modality, revealing the effectiveness and superiority. Our codes are publicly released.",
            "id": "2409.05275",
            "link": "http://arxiv.org/abs/2409.05275v1",
            "published": "2024-09-09T01:59:29+00:00",
            "updated": "2024-09-09T01:59:29+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 22
        },
        "2409.05840": {
            "authors": [
                "Run Luo",
                "Haonan Zhang",
                "Longze Chen",
                "Ting-En Lin",
                "Xiong Liu",
                "Yuchuan Wu",
                "Min Yang",
                "Minzheng Wang",
                "Pengpeng Zeng",
                "Lianli Gao",
                "Heng Tao Shen",
                "Yunshui Li",
                "Xiaobo Xia",
                "Fei Huang",
                "Jingkuan Song",
                "Yongbin Li"
            ],
            "title": "MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct",
            "abstract": "The development of Multimodal Large Language Models (MLLMs) has seen significant advancements. However, the quantity and quality of multimodal instruction data have emerged as significant bottlenecks in their progress. Manually creating multimodal instruction data is both time-consuming and inefficient, posing challenges in producing instructions of high complexity. Moreover, distilling instruction data from black-box commercial models (e.g., GPT-4o, GPT-4V) often results in simplistic instruction data, which constrains performance to that of these models. The challenge of curating diverse and complex instruction data remains substantial. We propose MMEvol, a novel multimodal instruction data evolution framework that combines fine-grained perception evolution, cognitive reasoning evolution, and interaction evolution. This iterative approach breaks through data quality bottlenecks to generate a complex and diverse image-text instruction dataset, thereby empowering MLLMs with enhanced capabilities. Beginning with an initial set of instructions, SEED-163K, we utilize MMEvol to systematically broadens the diversity of instruction types, integrates reasoning steps to enhance cognitive capabilities, and extracts detailed information from images to improve visual understanding and robustness. To comprehensively evaluate the effectiveness of our data, we train LLaVA-NeXT using the evolved data and conduct experiments across 13 vision-language tasks. Compared to the baseline trained with seed data, our approach achieves an average accuracy improvement of 3.1 points and reaches state-of-the-art (SOTA) performance on 9 of these tasks.",
            "id": "2409.05840",
            "link": "http://arxiv.org/abs/2409.05840v1",
            "published": "2024-09-09T17:44:00+00:00",
            "updated": "2024-09-09T17:44:00+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 78
        },
        "2409.05972": {
            "authors": [
                "Mariana Yukari Noguti",
                "Edduardo Vellasques",
                "Luiz Eduardo Soares Oliveira"
            ],
            "title": "A Small Claims Court for the NLP: Judging Legal Text Classification Strategies With Small Datasets",
            "abstract": "Recent advances in language modelling has significantly decreased the need of labelled data in text classification tasks. Transformer-based models, pre-trained on unlabeled data, can outmatch the performance of models trained from scratch for each task. However, the amount of labelled data need to fine-tune such type of model is still considerably high for domains requiring expert-level annotators, like the legal domain. This paper investigates the best strategies for optimizing the use of a small labeled dataset and large amounts of unlabeled data and perform a classification task in the legal area with 50 predefined topics. More specifically, we use the records of demands to a Brazilian Public Prosecutor's Office aiming to assign the descriptions in one of the subjects, which currently demands deep legal knowledge for manual filling. The task of optimizing the performance of classifiers in this scenario is especially challenging, given the low amount of resources available regarding the Portuguese language, especially in the legal domain. Our results demonstrate that classic supervised models such as logistic regression and SVM and the ensembles random forest and gradient boosting achieve better performance along with embeddings extracted with word2vec when compared to BERT language model. The latter demonstrates superior performance in association with the architecture of the model itself as a classifier, having surpassed all previous models in that regard. The best result was obtained with Unsupervised Data Augmentation (UDA), which jointly uses BERT, data augmentation, and strategies of semi-supervised learning, with an accuracy of 80.7% in the aforementioned task.",
            "id": "2409.05972",
            "link": "http://arxiv.org/abs/2409.05972v1",
            "published": "2024-09-09T18:10:05+00:00",
            "updated": "2024-09-09T18:10:05+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 42
        },
        "2409.06072": {
            "authors": [
                "Joymallya Chakraborty",
                "Wei Xia",
                "Anirban Majumder",
                "Dan Ma",
                "Walid Chaabene",
                "Naveed Janvekar"
            ],
            "title": "DetoxBench: Benchmarking Large Language Models for Multitask Fraud & Abuse Detection",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks. However, their practical application in high-stake domains, such as fraud and abuse detection, remains an area that requires further exploration. The existing applications often narrowly focus on specific tasks like toxicity or hate speech detection. In this paper, we present a comprehensive benchmark suite designed to assess the performance of LLMs in identifying and mitigating fraudulent and abusive language across various real-world scenarios. Our benchmark encompasses a diverse set of tasks, including detecting spam emails, hate speech, misogynistic language, and more. We evaluated several state-of-the-art LLMs, including models from Anthropic, Mistral AI, and the AI21 family, to provide a comprehensive assessment of their capabilities in this critical domain. The results indicate that while LLMs exhibit proficient baseline performance in individual fraud and abuse detection tasks, their performance varies considerably across tasks, particularly struggling with tasks that demand nuanced pragmatic reasoning, such as identifying diverse forms of misogynistic language. These findings have important implications for the responsible development and deployment of LLMs in high-risk applications. Our benchmark suite can serve as a tool for researchers and practitioners to systematically evaluate LLMs for multi-task fraud detection and drive the creation of more robust, trustworthy, and ethically-aligned systems for fraud and abuse detection.",
            "id": "2409.06072",
            "link": "http://arxiv.org/abs/2409.06072v1",
            "published": "2024-09-09T21:12:03+00:00",
            "updated": "2024-09-09T21:12:03+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 21
        },
        "2409.06164": {
            "authors": [
                "Yining Chen",
                "Jianqiang Li",
                "Changwei Song",
                "Qing Zhao",
                "Yongsheng Tong",
                "Guanghui Fu"
            ],
            "title": "Deep Learning and Large Language Models for Audio and Text Analysis in Predicting Suicidal Acts in Chinese Psychological Support Hotlines",
            "abstract": "Suicide is a pressing global issue, demanding urgent and effective preventive interventions. Among the various strategies in place, psychological support hotlines had proved as a potent intervention method. Approximately two million people in China attempt suicide annually, with many individuals making multiple attempts. Prompt identification and intervention for high-risk individuals are crucial to preventing tragedies. With the rapid advancement of artificial intelligence (AI), especially the development of large-scale language models (LLMs), new technological tools have been introduced to the field of mental health. This study included 1284 subjects, and was designed to validate whether deep learning models and LLMs, using audio and transcribed text from support hotlines, can effectively predict suicide risk. We proposed a simple LLM-based pipeline that first summarizes transcribed text from approximately one hour of speech to extract key features, and then predict suicidial bahaviours in the future. We compared our LLM-based method with the traditional manual scale approach in a clinical setting and with five advanced deep learning models. Surprisingly, the proposed simple LLM pipeline achieved strong performance on a test set of 46 subjects, with an F1 score of 76\\% when combined with manual scale rating. This is 7\\% higher than the best speech-based deep learning models and represents a 27.82\\% point improvement in F1 score compared to using the manual scale apporach alone. Our study explores new applications of LLMs and demonstrates their potential for future use in suicide prevention efforts.",
            "id": "2409.06164",
            "link": "http://arxiv.org/abs/2409.06164v1",
            "published": "2024-09-10T02:22:50+00:00",
            "updated": "2024-09-10T02:22:50+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 38
        },
        "2409.06222": {
            "authors": [
                "Sakshi Deo Shukla",
                "Pavel Denisov",
                "Tugtekin Turan"
            ],
            "title": "Advancing Topic Segmentation of Broadcasted Speech with Multilingual Semantic Embeddings",
            "abstract": "Recent advancements in speech-based topic segmentation have highlighted the potential of pretrained speech encoders to capture semantic representations directly from speech. Traditionally, topic segmentation has relied on a pipeline approach in which transcripts of the automatic speech recognition systems are generated, followed by text-based segmentation algorithms. In this paper, we introduce an end-to-end scheme that bypasses this conventional two-step process by directly employing semantic speech encoders for segmentation. Focused on the broadcasted news domain, which poses unique challenges due to the diversity of speakers and topics within single recordings, we address the challenge of accessing topic change points efficiently in an end-to-end manner. Furthermore, we propose a new benchmark for spoken news topic segmentation by utilizing a dataset featuring approximately 1000 hours of publicly available recordings across six European languages and including an evaluation set in Hindi to test the model's cross-domain performance in a cross-lingual, zero-shot scenario. This setup reflects real-world diversity and the need for models adapting to various linguistic settings. Our results demonstrate that while the traditional pipeline approach achieves a state-of-the-art $P_k$ score of 0.2431 for English, our end-to-end model delivers a competitive $P_k$ score of 0.2564. When trained multilingually, these scores further improve to 0.1988 and 0.2370, respectively. To support further research, we release our model along with data preparation scripts, facilitating open research on multilingual spoken news topic segmentation.",
            "id": "2409.06222",
            "link": "http://arxiv.org/abs/2409.06222v1",
            "published": "2024-09-10T05:24:36+00:00",
            "updated": "2024-09-10T05:24:36+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 11
        },
        "2409.06372": {
            "authors": [
                "Lennart Keller",
                "Goran Glava\u0161"
            ],
            "title": "SpeechTaxi: On Multilingual Semantic Speech Classification",
            "abstract": "Recent advancements in multilingual speech encoding as well as transcription raise the question of the most effective approach to semantic speech classification. Concretely, can (1) end-to-end (E2E) classifiers obtained by fine-tuning state-of-the-art multilingual speech encoders (MSEs) match or surpass the performance of (2) cascading (CA), where speech is first transcribed into text and classification is delegated to a text-based classifier. To answer this, we first construct SpeechTaxi, an 80-hour multilingual dataset for semantic speech classification of Bible verses, covering 28 diverse languages. We then leverage SpeechTaxi to conduct a wide range of experiments comparing E2E and CA in monolingual semantic speech classification as well as in cross-lingual transfer. We find that E2E based on MSEs outperforms CA in monolingual setups, i.e., when trained on in-language data. However, MSEs seem to have poor cross-lingual transfer abilities, with E2E substantially lagging CA both in (1) zero-shot transfer to languages unseen in training and (2) multilingual training, i.e., joint training on multiple languages. Finally, we devise a novel CA approach based on transcription to Romanized text as a language-agnostic intermediate representation and show that it represents a robust solution for languages without native ASR support. Our SpeechTaxi dataset is publicly available at: https://huggingface.co/ datasets/LennartKeller/SpeechTaxi/.",
            "id": "2409.06372",
            "link": "http://arxiv.org/abs/2409.06372v1",
            "published": "2024-09-10T09:56:15+00:00",
            "updated": "2024-09-10T09:56:15+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 30
        },
        "2409.07123": {
            "authors": [
                "Qianli Wang",
                "Tatiana Anikina",
                "Nils Feldhus",
                "Simon Ostermann",
                "Sebastian M\u00f6ller",
                "Vera Schmitt"
            ],
            "title": "Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem",
            "abstract": "Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.",
            "id": "2409.07123",
            "link": "http://arxiv.org/abs/2409.07123v1",
            "published": "2024-09-11T09:21:20+00:00",
            "updated": "2024-09-11T09:21:20+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 38
        },
        "2409.07388": {
            "authors": [
                "Guimin Hu",
                "Yi Xin",
                "Weimin Lyu",
                "Haojian Huang",
                "Chang Sun",
                "Zhihong Zhu",
                "Lin Gui",
                "Ruichu Cai"
            ],
            "title": "Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective",
            "abstract": "Multimodal affective computing (MAC) has garnered increasing attention due to its broad applications in analyzing human behaviors and intentions, especially in text-dominated multimodal affective computing field. This survey presents the recent trends of multimodal affective computing from NLP perspective through four hot tasks: multimodal sentiment analysis, multimodal emotion recognition in conversation, multimodal aspect-based sentiment analysis and multimodal multi-label emotion recognition. The goal of this survey is to explore the current landscape of multimodal affective research, identify development trends, and highlight the similarities and differences across various tasks, offering a comprehensive report on the recent progress in multimodal affective computing from an NLP perspective. This survey covers the formalization of tasks, provides an overview of relevant works, describes benchmark datasets, and details the evaluation metrics for each task. Additionally, it briefly discusses research in multimodal affective computing involving facial expressions, acoustic signals, physiological signals, and emotion causes. Additionally, we discuss the technical approaches, challenges, and future directions in multimodal affective computing. To support further research, we released a repository that compiles related works in multimodal affective computing, providing detailed resources and references for the community.",
            "id": "2409.07388",
            "link": "http://arxiv.org/abs/2409.07388v1",
            "published": "2024-09-11T16:24:06+00:00",
            "updated": "2024-09-11T16:24:06+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 92
        },
        "2409.07394": {
            "authors": [
                "Han Wang",
                "Archiki Prasad",
                "Elias Stengel-Eskin",
                "Mohit Bansal"
            ],
            "title": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge",
            "abstract": "Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM's output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently misjudge the degree of conflict and struggle to handle instances that vary in their amount of conflict, with static methods over-adjusting when conflict is absent. We propose a fine-grained, instance-level approach called AdaCAD, which dynamically infers the weight of adjustment based on the degree of conflict, as measured by the Jensen-Shannon divergence between distributions representing contextual and parametric knowledge. Our experiments across four models on six diverse question-answering (QA) datasets and three summarization tasks demonstrate that our training-free adaptive method consistently outperforms other decoding methods on QA, with average accuracy gains of 14.21% (absolute) over a static contrastive baseline, and improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our analysis shows that while decoding with contrastive baselines hurts performance when conflict is absent, AdaCAD mitigates these losses, making it more applicable to real-world datasets in which some examples have conflict and others do not.",
            "id": "2409.07394",
            "link": "http://arxiv.org/abs/2409.07394v1",
            "published": "2024-09-11T16:35:18+00:00",
            "updated": "2024-09-11T16:35:18+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 72
        },
        "2409.07424": {
            "authors": [
                "Gavin Butts",
                "Pegah Emdad",
                "Jethro Lee",
                "Shannon Song",
                "Chiman Salavati",
                "Willmar Sosa Diaz",
                "Shiri Dori-Hacohen",
                "Fabricio Murai"
            ],
            "title": "Towards Fairer Health Recommendations: finding informative unbiased samples via Word Sense Disambiguation",
            "abstract": "There have been growing concerns around high-stake applications that rely on models trained with biased data, which consequently produce biased predictions, often harming the most vulnerable. In particular, biased medical data could cause health-related applications and recommender systems to create outputs that jeopardize patient care and widen disparities in health outcomes. A recent framework titled Fairness via AI posits that, instead of attempting to correct model biases, researchers must focus on their root causes by using AI to debias data. Inspired by this framework, we tackle bias detection in medical curricula using NLP models, including LLMs, and evaluate them on a gold standard dataset containing 4,105 excerpts annotated by medical experts for bias from a large corpus. We build on previous work by coauthors which augments the set of negative samples with non-annotated text containing social identifier terms. However, some of these terms, especially those related to race and ethnicity, can carry different meanings (e.g., \"white matter of spinal cord\"). To address this issue, we propose the use of Word Sense Disambiguation models to refine dataset quality by removing irrelevant sentences. We then evaluate fine-tuned variations of BERT models as well as GPT models with zero- and few-shot prompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for bias detection, while fine-tuned BERT models generally perform well across all evaluated metrics.",
            "id": "2409.07424",
            "link": "http://arxiv.org/abs/2409.07424v1",
            "published": "2024-09-11T17:10:20+00:00",
            "updated": "2024-09-11T17:10:20+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.CY",
                "cs.LG",
                "I.2.7; J.3; K.4"
            ],
            "max_author_hindex": 12
        },
        "2409.07790": {
            "authors": [
                "Zhiyuan Tang",
                "Dong Wang",
                "Shen Huang",
                "Shidong Shang"
            ],
            "title": "Full-text Error Correction for Chinese Speech Recognition with Large Language Model",
            "abstract": "Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic Speech Recognition (ASR). However, most research focuses on utterances from short-duration speech recordings, which are the predominant form of speech data for supervised ASR training. This paper investigates the effectiveness of LLMs for error correction in full-text generated by ASR systems from longer speech recordings, such as transcripts from podcasts, news broadcasts, and meetings. First, we develop a Chinese dataset for full-text error correction, named ChFT, utilizing a pipeline that involves text-to-speech synthesis, ASR, and error-correction pair extractor. This dataset enables us to correct errors across contexts, including both full-text and segment, and to address a broader range of error types, such as punctuation restoration and inverse text normalization, thus making the correction process comprehensive. Second, we fine-tune a pre-trained LLM on the constructed dataset using a diverse set of prompts and target formats, and evaluate its performance on full-text error correction. Specifically, we design prompts based on full-text and segment, considering various output formats, such as directly corrected text and JSON-based error-correction pairs. Through various test settings, including homogeneous, up-to-date, and hard test sets, we find that the fine-tuned LLMs perform well in the full-text setting with different prompts, each presenting its own strengths and weaknesses. This establishes a promising baseline for further research. The dataset is available on the website.",
            "id": "2409.07790",
            "link": "http://arxiv.org/abs/2409.07790v1",
            "published": "2024-09-12T06:50:45+00:00",
            "updated": "2024-09-12T06:50:45+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 21
        },
        "2409.07809": {
            "authors": [
                "Tal Baumel",
                "Andre Manoel",
                "Daniel Jones",
                "Shize Su",
                "Huseyin Inan",
                "Aaron",
                "Bornstein",
                "Robert Sim"
            ],
            "title": "Controllable Synthetic Clinical Note Generation with Privacy Guarantees",
            "abstract": "In the field of machine learning, domain-specific annotated data is an invaluable resource for training effective models. However, in the medical domain, this data often includes Personal Health Information (PHI), raising significant privacy concerns. The stringent regulations surrounding PHI limit the availability and sharing of medical datasets, which poses a substantial challenge for researchers and practitioners aiming to develop advanced machine learning models. In this paper, we introduce a novel method to \"clone\" datasets containing PHI. Our approach ensures that the cloned datasets retain the essential characteristics and utility of the original data without compromising patient privacy. By leveraging differential-privacy techniques and a novel fine-tuning task, our method produces datasets that are free from identifiable information while preserving the statistical properties necessary for model training. We conduct utility testing to evaluate the performance of machine learning models trained on the cloned datasets. The results demonstrate that our cloned datasets not only uphold privacy standards but also enhance model performance compared to those trained on traditional anonymized datasets. This work offers a viable solution for the ethical and effective utilization of sensitive medical data in machine learning, facilitating progress in medical research and the development of robust predictive models.",
            "id": "2409.07809",
            "link": "http://arxiv.org/abs/2409.07809v1",
            "published": "2024-09-12T07:38:34+00:00",
            "updated": "2024-09-12T07:38:34+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 90
        },
        "2409.07869": {
            "authors": [
                "Zihang Peng",
                "Daria Stepanova",
                "Vinh Thinh Ho",
                "Heike Adel",
                "Alessandra Russo",
                "Simon Ott"
            ],
            "title": "Learning Rules from KGs Guided by Language Models",
            "abstract": "Advances in information extraction have enabled the automatic construction of large knowledge graphs (e.g., Yago, Wikidata or Google KG), which are widely used in many applications like semantic search or data analytics. However, due to their semi-automatic construction, KGs are often incomplete. Rule learning methods, concerned with the extraction of frequent patterns from KGs and casting them into rules, can be applied to predict potentially missing facts. A crucial step in this process is rule ranking. Ranking of rules is especially challenging over highly incomplete or biased KGs (e.g., KGs predominantly storing facts about famous people), as in this case biased rules might fit the data best and be ranked at the top based on standard statistical metrics like rule confidence. To address this issue, prior works proposed to rank rules not only relying on the original KG but also facts predicted by a KG embedding model. At the same time, with the recent rise of Language Models (LMs), several works have claimed that LMs can be used as alternative means for KG completion. In this work, our goal is to verify to which extent the exploitation of LMs is helpful for improving the quality of rule learning systems.",
            "id": "2409.07869",
            "link": "http://arxiv.org/abs/2409.07869v1",
            "published": "2024-09-12T09:27:36+00:00",
            "updated": "2024-09-12T09:27:36+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 37
        },
        "2409.08147": {
            "authors": [
                "Zhengliang Liu",
                "Yiwei Li",
                "Oleksandra Zolotarevych",
                "Rongwei Yang",
                "Tianming Liu"
            ],
            "title": "LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models",
            "abstract": "Large language models have demonstrated remarkable capabilities in natural language processing, yet their application to political discourse analysis remains underexplored. This paper introduces a novel approach to evaluating presidential debate performances using LLMs, addressing the longstanding challenge of objectively assessing debate outcomes. We propose a framework that analyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they resonate with the \"Interests, Ideologies, and Identity\" (3I) of four key audience groups: voters, businesses, donors, and politicians. Our method employs large language models to generate the LLM-POTUS Score, a quantitative measure of debate performance based on the alignment between 3P and 3I. We apply this framework to analyze transcripts from recent U.S. presidential debates, demonstrating its ability to provide nuanced, multi-dimensional assessments of candidate performances. Our results reveal insights into the effectiveness of different debating strategies and their impact on various audience segments. This study not only offers a new tool for political analysis but also explores the potential and limitations of using LLMs as impartial judges in complex social contexts. In addition, this framework provides individual citizens with an independent tool to evaluate presidential debate performances, which enhances democratic engagement and reduces reliance on potentially biased media interpretations and institutional influence, thereby strengthening the foundation of informed civic participation.",
            "id": "2409.08147",
            "link": "http://arxiv.org/abs/2409.08147v1",
            "published": "2024-09-12T15:40:45+00:00",
            "updated": "2024-09-12T15:40:45+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 23
        },
        "2409.05559": {
            "authors": [
                "Jinwei He",
                "Feng Lu"
            ],
            "title": "CauseJudger: Identifying the Cause with LLMs for Abductive Logical Reasoning",
            "abstract": "Large language models (LLMs) have been utilized in solving diverse reasoning tasks, encompassing common sense, arithmetic and deduction tasks. However, with difficulties of reversing thinking patterns and irrelevant premises, how to determine the authenticity of the cause in abductive logical reasoning remains underexplored. Inspired by hypothesis and verification method and identification of irrelevant information in human thinking process, we propose a new framework for LLMs abductive logical reasoning called CauseJudger (CJ), which identifies the authenticity of possible cause by transforming thinking from reverse to forward and removing irrelevant information. In addition, we construct an abductive logical reasoning dataset for decision task called CauseLogics, which contains 200,000 tasks of varying reasoning lengths. Our experiments show the efficiency of CJ with overall experiments and ablation experiments as well as case studies on our dataset and reconstructed public dataset. Notably, CJ's implementation is efficient, requiring only two calls to LLM. Its impact is profound: when using gpt-3.5, CJ achieves a maximum correctness improvement of 41% compared to Zero-Shot-CoT. Moreover, with gpt-4, CJ attains an accuracy exceeding 90% across all datasets.",
            "id": "2409.05559",
            "link": "http://arxiv.org/abs/2409.05559v1",
            "published": "2024-09-09T12:30:43+00:00",
            "updated": "2024-09-09T12:30:43+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 29
        },
        "2409.05831": {
            "authors": [
                "Xiang Yin",
                "Nico Potyka",
                "Francesca Toni"
            ],
            "title": "Applying Attribution Explanations in Truth-Discovery Quantitative Bipolar Argumentation Frameworks",
            "abstract": "Explaining the strength of arguments under gradual semantics is receiving increasing attention. For example, various studies in the literature offer explanations by computing the attribution scores of arguments or edges in Quantitative Bipolar Argumentation Frameworks (QBAFs). These explanations, known as Argument Attribution Explanations (AAEs) and Relation Attribution Explanations (RAEs), commonly employ removal-based and Shapley-based techniques for computing the attribution scores. While AAEs and RAEs have proven useful in several applications with acyclic QBAFs, they remain largely unexplored for cyclic QBAFs. Furthermore, existing applications tend to focus solely on either AAEs or RAEs, but do not compare them directly. In this paper, we apply both AAEs and RAEs, to Truth Discovery QBAFs (TD-QBAFs), which assess the trustworthiness of sources (e.g., websites) and their claims (e.g., the severity of a virus), and feature complex cycles. We find that both AAEs and RAEs can provide interesting explanations and can give non-trivial and surprising insights.",
            "id": "2409.05831",
            "link": "http://arxiv.org/abs/2409.05831v1",
            "published": "2024-09-09T17:36:39+00:00",
            "updated": "2024-09-09T17:36:39+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.06351": {
            "authors": [
                "David Bani-Harouni",
                "Nassir Navab",
                "Matthias Keicher"
            ],
            "title": "MAGDA: Multi-agent guideline-driven diagnostic assistance",
            "abstract": "In emergency departments, rural hospitals, or clinics in less developed regions, clinicians often lack fast image analysis by trained radiologists, which can have a detrimental effect on patients' healthcare. Large Language Models (LLMs) have the potential to alleviate some pressure from these clinicians by providing insights that can help them in their decision-making. While these LLMs achieve high test results on medical exams showcasing their great theoretical medical knowledge, they tend not to follow medical guidelines. In this work, we introduce a new approach for zero-shot guideline-driven decision support. We model a system of multiple LLM agents augmented with a contrastive vision-language model that collaborate to reach a patient diagnosis. After providing the agents with simple diagnostic guidelines, they will synthesize prompts and screen the image for findings following these guidelines. Finally, they provide understandable chain-of-thought reasoning for their diagnosis, which is then self-refined to consider inter-dependencies between diseases. As our method is zero-shot, it is adaptable to settings with rare diseases, where training data is limited, but expert-crafted disease descriptions are available. We evaluate our method on two chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing performance improvement over existing zero-shot methods and generalizability to rare diseases.",
            "id": "2409.06351",
            "link": "http://arxiv.org/abs/2409.06351v1",
            "published": "2024-09-10T09:10:30+00:00",
            "updated": "2024-09-10T09:10:30+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 89
        },
        "2409.07127": {
            "authors": [
                "Dongkun Huo",
                "Huateng Zhang",
                "Yixue Hao",
                "Yuanlin Ye",
                "Long Hu",
                "Rui Wang",
                "Min Chen"
            ],
            "title": "DCMAC: Demand-aware Customized Multi-Agent Communication via Upper Bound Training",
            "abstract": "Efficient communication can enhance the overall performance of collaborative multi-agent reinforcement learning. A common approach is to share observations through full communication, leading to significant communication overhead. Existing work attempts to perceive the global state by conducting teammate model based on local information. However, they ignore that the uncertainty generated by prediction may lead to difficult training. To address this problem, we propose a Demand-aware Customized Multi-Agent Communication (DCMAC) protocol, which use an upper bound training to obtain the ideal policy. By utilizing the demand parsing module, agent can interpret the gain of sending local message on teammate, and generate customized messages via compute the correlation between demands and local observation using cross-attention mechanism. Moreover, our method can adapt to the communication resources of agents and accelerate the training progress by appropriating the ideal policy which is trained with joint observation. Experimental results reveal that DCMAC significantly outperforms the baseline algorithms in both unconstrained and communication constrained scenarios.",
            "id": "2409.07127",
            "link": "http://arxiv.org/abs/2409.07127v1",
            "published": "2024-09-11T09:23:27+00:00",
            "updated": "2024-09-11T09:23:27+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.MA"
            ],
            "max_author_hindex": 46
        },
        "2409.07340": {
            "authors": [
                "Akash Saravanan",
                "Matthew Guzdial"
            ],
            "title": "A Framework for Predicting the Impact of Game Balance Changes through Meta Discovery",
            "abstract": "A metagame is a collection of knowledge that goes beyond the rules of a game. In competitive, team-based games like Pok\\'emon or League of Legends, it refers to the set of current dominant characters and/or strategies within the player base. Developer changes to the balance of the game can have drastic and unforeseen consequences on these sets of meta characters. A framework for predicting the impact of balance changes could aid developers in making more informed balance decisions. In this paper we present such a Meta Discovery framework, leveraging Reinforcement Learning for automated testing of balance changes. Our results demonstrate the ability to predict the outcome of balance changes in Pok\\'emon Showdown, a collection of competitive Pok\\'emon tiers, with high accuracy.",
            "id": "2409.07340",
            "link": "http://arxiv.org/abs/2409.07340v1",
            "published": "2024-09-11T15:20:43+00:00",
            "updated": "2024-09-11T15:20:43+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 14
        },
        "2409.07453": {
            "authors": [
                "Shengxin Hong",
                "Chang Cai",
                "Sixuan Du",
                "Haiyue Feng",
                "Siyuan Liu",
                "Xiuyi Fan"
            ],
            "title": "\"My Grade is Wrong!\": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays",
            "abstract": "Interactive feedback, where feedback flows in both directions between teacher and student, is more effective than traditional one-way feedback. However, it is often too time-consuming for widespread use in educational practice. While Large Language Models (LLMs) have potential for automating feedback, they struggle with reasoning and interaction in an interactive setting. This paper introduces CAELF, a Contestable AI Empowered LLM Framework for automating interactive feedback. CAELF allows students to query, challenge, and clarify their feedback by integrating a multi-agent system with computational argumentation. Essays are first assessed by multiple Teaching-Assistant Agents (TA Agents), and then a Teacher Agent aggregates the evaluations through formal reasoning to generate feedback and grades. Students can further engage with the feedback to refine their understanding. A case study on 500 critical thinking essays with user studies demonstrates that CAELF significantly improves interactive feedback, enhancing the reasoning and interaction capabilities of LLMs. This approach offers a promising solution to overcoming the time and resource barriers that have limited the adoption of interactive feedback in educational settings.",
            "id": "2409.07453",
            "link": "http://arxiv.org/abs/2409.07453v1",
            "published": "2024-09-11T17:59:01+00:00",
            "updated": "2024-09-11T17:59:01+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.HC"
            ],
            "max_author_hindex": 17
        },
        "2409.07507": {
            "authors": [
                "Daniel Adam",
                "Tom\u00e1\u0161 Kliegr"
            ],
            "title": "Traceable LLM-based validation of statements in knowledge graphs",
            "abstract": "This article presents a method for verifying RDF triples using LLMs, with an emphasis on providing traceable arguments. Because the LLMs cannot currently reliably identify the origin of the information used to construct the response to the user query, our approach is to avoid using internal LLM factual knowledge altogether. Instead, verified RDF statements are compared to chunks of external documents retrieved through a web search or Wikipedia. To assess the possible application of this workflow on biosciences content, we evaluated 1,719 positive statements from the BioRED dataset and the same number of newly generated negative statements. The resulting precision is 88%, and recall is 44%. This indicates that the method requires human oversight. We demonstrate the method on Wikidata, where a SPARQL query is used to automatically retrieve statements needing verification. Overall, the results suggest that LLMs could be used for large-scale verification of statements in KGs, a task previously unfeasible due to human annotation costs.",
            "id": "2409.07507",
            "link": "http://arxiv.org/abs/2409.07507v1",
            "published": "2024-09-11T12:27:41+00:00",
            "updated": "2024-09-11T12:27:41+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 15
        },
        "2409.07578": {
            "authors": [
                "B. Sankar",
                "Dibakar Sen"
            ],
            "title": "A Novel Mathematical Framework for Objective Evaluation of Ideas using a Conversational AI (CAI) System",
            "abstract": "The demand for innovation in product design necessitates a prolific ideation phase. Conversational AI (CAI) systems that use Large Language Models (LLMs) such as GPT (Generative Pre-trained Transformer) have been shown to be fruitful in augmenting human creativity, providing numerous novel and diverse ideas. Despite the success in ideation quantity, the qualitative assessment of these ideas remains challenging and traditionally reliant on expert human evaluation. This method suffers from limitations such as human judgment errors, bias, and oversight. Addressing this gap, our study introduces a comprehensive mathematical framework for automated analysis to objectively evaluate the plethora of ideas generated by CAI systems and/or humans. This framework is particularly advantageous for novice designers who lack experience in selecting promising ideas. By converting the ideas into higher dimensional vectors and quantitatively measuring the diversity between them using tools such as UMAP, DBSCAN and PCA, the proposed method provides a reliable and objective way of selecting the most promising ideas, thereby enhancing the efficiency of the ideation phase.",
            "id": "2409.07578",
            "link": "http://arxiv.org/abs/2409.07578v1",
            "published": "2024-09-11T19:10:29+00:00",
            "updated": "2024-09-11T19:10:29+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "53A45",
                "I.2.7; G.3"
            ],
            "max_author_hindex": 39
        },
        "2409.07618": {
            "authors": [
                "Alan F. Smeaton"
            ],
            "title": "Understanding Foundation Models: Are We Back in 1924?",
            "abstract": "This position paper explores the rapid development of Foundation Models (FMs) in AI and their implications for intelligence and reasoning. It examines the characteristics of FMs, including their training on vast datasets and use of embedding spaces to capture semantic relationships. The paper discusses recent advancements in FMs' reasoning abilities which we argue cannot be attributed to increased model size but to novel training techniques which yield learning phenomena like grokking. It also addresses the challenges in benchmarking FMs and compares their structure to the human brain. We argue that while FMs show promising developments in reasoning and knowledge representation, understanding their inner workings remains a significant challenge, similar to ongoing efforts in neuroscience to comprehend human brain function. Despite having some similarities, fundamental differences between FMs and the structure of human brain warn us against making direct comparisons or expecting neuroscience to provide immediate insights into FM function.",
            "id": "2409.07618",
            "link": "http://arxiv.org/abs/2409.07618v1",
            "published": "2024-09-11T20:59:27+00:00",
            "updated": "2024-09-11T20:59:27+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 65
        },
        "2409.07656": {
            "authors": [
                "Bernardo Gon\u00e7alves"
            ],
            "title": "Passed the Turing Test: Living in Turing Futures",
            "abstract": "The world has seen the emergence of machines based on pretrained models, transformers, also known as generative artificial intelligences for their ability to produce various types of content, including text, images, audio, and synthetic data. Without resorting to preprogramming or special tricks, their intelligence grows as they learn from experience, and to ordinary people, they can appear human-like in conversation. This means that they can pass the Turing test, and that we are now living in one of many possible Turing futures where machines can pass for what they are not. However, the learning machines that Turing imagined would pass his imitation tests were machines inspired by the natural development of the low-energy human cortex. They would be raised like human children and naturally learn the ability to deceive an observer. These ``child machines,'' Turing hoped, would be powerful enough to have an impact on society and nature.",
            "id": "2409.07656",
            "link": "http://arxiv.org/abs/2409.07656v1",
            "published": "2024-09-11T22:56:30+00:00",
            "updated": "2024-09-11T22:56:30+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CY"
            ],
            "max_author_hindex": 9
        },
        "2409.07965": {
            "authors": [
                "Asen Nachkov",
                "Danda Pani Paudel",
                "Luc Van Gool"
            ],
            "title": "Autonomous Vehicle Controllers From End-to-End Differentiable Simulation",
            "abstract": "Current methods to learn controllers for autonomous vehicles (AVs) focus on behavioural cloning. Being trained only on exact historic data, the resulting agents often generalize poorly to novel scenarios. Simulators provide the opportunity to go beyond offline datasets, but they are still treated as complicated black boxes, only used to update the global simulation state. As a result, these RL algorithms are slow, sample-inefficient, and prior-agnostic. In this work, we leverage a differentiable simulator and design an analytic policy gradients (APG) approach to training AV controllers on the large-scale Waymo Open Motion Dataset. Our proposed framework brings the differentiable simulator into an end-to-end training loop, where gradients of the environment dynamics serve as a useful prior to help the agent learn a more grounded policy. We combine this setup with a recurrent architecture that can efficiently propagate temporal information across long simulated trajectories. This APG method allows us to learn robust, accurate, and fast policies, while only requiring widely-available expert trajectories, instead of scarce expert actions. We compare to behavioural cloning and find significant improvements in performance and robustness to noise in the dynamics, as well as overall more intuitive human-like handling.",
            "id": "2409.07965",
            "link": "http://arxiv.org/abs/2409.07965v1",
            "published": "2024-09-12T11:50:06+00:00",
            "updated": "2024-09-12T11:50:06+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 171
        },
        "2409.07985": {
            "authors": [
                "Charlie Griffin",
                "Louis Thomson",
                "Buck Shlegeris",
                "Alessandro Abate"
            ],
            "title": "Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols",
            "abstract": "To evaluate the safety and usefulness of deployment protocols for untrusted AIs, AI Control uses a red-teaming exercise played between a protocol designer and an adversary. This paper introduces AI-Control Games, a formal decision-making model of the red-teaming exercise as a multi-objective, partially observable, stochastic game. We also introduce methods for finding optimal protocols in AI-Control Games, by reducing them to a set of zero-sum partially observable stochastic games. We apply our formalism to model, evaluate and synthesise protocols for deploying untrusted language models as programming assistants, focusing on Trusted Monitoring protocols, which use weaker language models and limited human assistance. Finally, we demonstrate the utility of our formalism by showcasing improvements over empirical studies in existing settings, evaluating protocols in new settings, and analysing how modelling assumptions affect the safety and usefulness of protocols.",
            "id": "2409.07985",
            "link": "http://arxiv.org/abs/2409.07985v1",
            "published": "2024-09-12T12:30:07+00:00",
            "updated": "2024-09-12T12:30:07+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 46
        },
        "2409.05356": {
            "authors": [
                "Ashwin Sankar",
                "Srija Anand",
                "Praveen Srinivasa Varadhan",
                "Sherry Thomas",
                "Mehak Singal",
                "Shridhar Kumar",
                "Deovrat Mehendale",
                "Aditi Krishana",
                "Giri Raju",
                "Mitesh Khapra"
            ],
            "title": "IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech Corpus for Scaling Indian TTS",
            "abstract": "Recent advancements in text-to-speech (TTS) synthesis show that large-scale models trained with extensive web data produce highly natural-sounding output. However, such data is scarce for Indian languages due to the lack of high-quality, manually subtitled data on platforms like LibriVox or YouTube. To address this gap, we enhance existing large-scale ASR datasets containing natural conversations collected in low-quality environments to generate high-quality TTS training data. Our pipeline leverages the cross-lingual generalization of denoising and speech enhancement models trained on English and applied to Indian languages. This results in IndicVoices-R (IV-R), the largest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704 hours of high-quality speech from 10,496 speakers across 22 Indian languages. IV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS, and IndicTTS. We also introduce the IV-R Benchmark, the first to assess zero-shot, few-shot, and many-shot speaker generalization capabilities of TTS models on Indian voices, ensuring diversity in age, gender, and style. We demonstrate that fine-tuning an English pre-trained model on a combined dataset of high-quality IndicTTS and our IV-R dataset results in better zero-shot speaker generalization compared to fine-tuning on the IndicTTS dataset alone. Further, our evaluation reveals limited zero-shot generalization for Indian voices in TTS models trained on prior datasets, which we improve by fine-tuning the model on our data containing diverse set of speakers across language families. We open-source all data and code, releasing the first TTS model for all 22 official Indian languages.",
            "id": "2409.05356",
            "link": "http://arxiv.org/abs/2409.05356v1",
            "published": "2024-09-09T06:28:47+00:00",
            "updated": "2024-09-09T06:28:47+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG",
                "cs.SD",
                "eess.SP"
            ],
            "max_author_hindex": 36
        },
        "2409.05448": {
            "authors": [
                "Qin Dai",
                "Benjamin Heinzerling",
                "Kentaro Inui"
            ],
            "title": "Representational Analysis of Binding in Large Language Models",
            "abstract": "Entity tracking is essential for complex reasoning. To perform in-context entity tracking, language models (LMs) must bind an entity to its attribute (e.g., bind a container to its content) to recall attribute for a given entity. For example, given a context mentioning ``The coffee is in Box Z, the stone is in Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later, LMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs, Feng and Steinhardt (2023) introduce a Binding ID mechanism and state that LMs use a abstract concept called Binding ID (BI) to internally mark entity-attribute pairs. However, they have not directly captured the BI determinant information from entity activations. In this work, we provide a novel view of the Binding ID mechanism by localizing the prototype of BI information. Specifically, we discover that there exists a low-rank subspace in the hidden state (or activation) of LMs, that primarily encodes the order of entity and attribute and which is used as the prototype of BI to causally determine the binding. To identify this subspace, we choose principle component analysis as our first attempt and it is empirically proven to be effective. Moreover, we also discover that when editing representations along directions in the subspace, LMs tend to bind a given entity to other attributes accordingly. For example, by patching activations along the BI encoding direction we can make the LM to infer ``Box Z contains the stone'' and ``Box Z contains the map''.",
            "id": "2409.05448",
            "link": "http://arxiv.org/abs/2409.05448v2",
            "published": "2024-09-09T09:04:56+00:00",
            "updated": "2024-09-12T01:32:25+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 40
        },
        "2409.07891": {
            "authors": [
                "Xiaoyun Jin",
                "Mirjam Ernestus",
                "R. Harald Baayen"
            ],
            "title": "A corpus-based investigation of pitch contours of monosyllabic words in conversational Taiwan Mandarin",
            "abstract": "In Mandarin, the tonal contours of monosyllabic words produced in isolation or in careful speech are characterized by four lexical tones: a high-level tone (T1), a rising tone (T2), a dipping tone (T3) and a falling tone (T4). However, in spontaneous speech, the actual tonal realization of monosyllabic words can deviate significantly from these canonical tones due to intra-syllabic co-articulation and inter-syllabic co-articulation with adjacent tones. In addition, Chuang et al. (2024) recently reported that the tonal contours of disyllabic Mandarin words with T2-T4 tone pattern are co-determined by their meanings. Following up on their research, we present a corpus-based investigation of how the pitch contours of monosyllabic words are realized in spontaneous conversational Mandarin, focusing on the effects of contextual predictors on the one hand, and the way in words' meanings co-determine pitch contours on the other hand. We analyze the F0 contours of 3824 tokens of 63 different word types in a spontaneous Taiwan Mandarin corpus, using the generalized additive (mixed) model to decompose a given observed pitch contour into a set of component pitch contours. We show that the tonal context substantially modify a word's canonical tone. Once the effect of tonal context is controlled for, T2 and T3 emerge as low flat tones, contrasting with T1 as a high tone, and with T4 as a high-to-mid falling tone. The neutral tone (T0), which in standard descriptions, is realized based on the preceding tone, emerges as a low tone in its own right, modified by the other predictors in the same way as the standard tones T1, T2, T3, and T4. We also show that word, and even more so, word sense, co-determine words' F0 contours. Analyses of variable importance using random forests further supported the substantial effect of tonal context and an effect of word sense.",
            "id": "2409.07891",
            "link": "http://arxiv.org/abs/2409.07891v1",
            "published": "2024-09-12T09:51:56+00:00",
            "updated": "2024-09-12T09:51:56+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 57
        },
        "2409.05846": {
            "authors": [
                "Samuel Yen-Chi Chen"
            ],
            "title": "An Introduction to Quantum Reinforcement Learning (QRL)",
            "abstract": "Recent advancements in quantum computing (QC) and machine learning (ML) have sparked considerable interest in the integration of these two cutting-edge fields. Among the various ML techniques, reinforcement learning (RL) stands out for its ability to address complex sequential decision-making problems. RL has already demonstrated substantial success in the classical ML community. Now, the emerging field of Quantum Reinforcement Learning (QRL) seeks to enhance RL algorithms by incorporating principles from quantum computing. This paper offers an introduction to this exciting area for the broader AI and ML community.",
            "id": "2409.05846",
            "link": "http://arxiv.org/abs/2409.05846v1",
            "published": "2024-09-09T17:45:37+00:00",
            "updated": "2024-09-09T17:45:37+00:00",
            "primary_category": "quant-ph",
            "categories": [
                "quant-ph",
                "cs.AI",
                "cs.ET",
                "cs.LG",
                "cs.NE"
            ],
            "max_author_hindex": 17
        },
        "2409.06561": {
            "authors": [
                "Ehsan Firouzi",
                "Mohammad Ghafari",
                "Mike Ebrahimi"
            ],
            "title": "ChatGPT's Potential in Cryptography Misuse Detection: A Comparative Analysis with Static Analysis Tools",
            "abstract": "The correct adoption of cryptography APIs is challenging for mainstream developers, often resulting in widespread API misuse. Meanwhile, cryptography misuse detectors have demonstrated inconsistent performance and remain largely inaccessible to most developers. We investigated the extent to which ChatGPT can detect cryptography misuses and compared its performance with that of the state-of-the-art static analysis tools. Our investigation, mainly based on the CryptoAPI-Bench benchmark, demonstrated that ChatGPT is effective in identifying cryptography API misuses, and with the use of prompt engineering, it can even outperform leading static cryptography misuse detectors.",
            "id": "2409.06561",
            "link": "http://arxiv.org/abs/2409.06561v1",
            "published": "2024-09-10T14:50:12+00:00",
            "updated": "2024-09-10T14:50:12+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 23
        },
        "2409.06892": {
            "authors": [
                "Rania Saber",
                "Anna Fariha"
            ],
            "title": "Formative Study for AI-assisted Data Visualization",
            "abstract": "This formative study investigates the impact of data quality on AI-assisted data visualizations, focusing on how uncleaned datasets influence the outcomes of these tools. By generating visualizations from datasets with inherent quality issues, the research aims to identify and categorize the specific visualization problems that arise. The study further explores potential methods and tools to address these visualization challenges efficiently and effectively. Although tool development has not yet been undertaken, the findings emphasize enhancing AI visualization tools to handle flawed data better. This research underscores the critical need for more robust, user-friendly solutions that facilitate quicker and easier correction of data and visualization errors, thereby improving the overall reliability and usability of AI-assisted data visualization processes.",
            "id": "2409.06892",
            "link": "http://arxiv.org/abs/2409.06892v1",
            "published": "2024-09-10T22:20:28+00:00",
            "updated": "2024-09-10T22:20:28+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "max_author_hindex": 10
        },
        "2409.07736": {
            "authors": [
                "Aaryan Panda",
                "Damodar Panigrahi",
                "Shaswata Mitra",
                "Sudip Mittal",
                "Shahram Rahimi"
            ],
            "title": "Transfer Learning Applied to Computer Vision Problems: Survey on Current Progress, Limitations, and Opportunities",
            "abstract": "The field of Computer Vision (CV) has faced challenges. Initially, it relied on handcrafted features and rule-based algorithms, resulting in limited accuracy. The introduction of machine learning (ML) has brought progress, particularly Transfer Learning (TL), which addresses various CV problems by reusing pre-trained models. TL requires less data and computing while delivering nearly equal accuracy, making it a prominent technique in the CV landscape. Our research focuses on TL development and how CV applications use it to solve real-world problems. We discuss recent developments, limitations, and opportunities.",
            "id": "2409.07736",
            "link": "http://arxiv.org/abs/2409.07736v1",
            "published": "2024-09-12T03:59:15+00:00",
            "updated": "2024-09-12T03:59:15+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 24
        },
        "2409.07825": {
            "authors": [
                "Renjie Wu",
                "Hu Wang",
                "Hsiang-Ting Chen"
            ],
            "title": "A Comprehensive Survey on Deep Multimodal Learning with Missing Modality",
            "abstract": "During multimodal model training and reasoning, data samples may miss certain modalities and lead to compromised model performance due to sensor limitations, cost constraints, privacy concerns, data loss, and temporal and spatial factors. This survey provides an overview of recent progress in Multimodal Learning with Missing Modality (MLMM), focusing on deep learning techniques. It is the first comprehensive survey that covers the historical background and the distinction between MLMM and standard multimodal learning setups, followed by a detailed analysis of current MLMM methods, applications, and datasets, concluding with a discussion about challenges and potential future directions in the field.",
            "id": "2409.07825",
            "link": "http://arxiv.org/abs/2409.07825v1",
            "published": "2024-09-12T08:15:39+00:00",
            "updated": "2024-09-12T08:15:39+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 30
        },
        "2409.08023": {
            "authors": [
                "Francesco Della Santa",
                "Antonio Mastropietro",
                "Sandra Pieraccini",
                "Francesco Vaccarino"
            ],
            "title": "Edge-Wise Graph-Instructed Neural Networks",
            "abstract": "The problem of multi-task regression over graph nodes has been recently approached through Graph-Instructed Neural Network (GINN), which is a promising architecture belonging to the subset of message-passing graph neural networks. In this work, we discuss the limitations of the Graph-Instructed (GI) layer, and we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages of the EWGI layer and we provide numerical evidence that EWGINNs perform better than GINNs over graph-structured input data with chaotic connectivity, like the ones inferred from the Erdos-R\\'enyi graph.",
            "id": "2409.08023",
            "link": "http://arxiv.org/abs/2409.08023v1",
            "published": "2024-09-12T13:05:28+00:00",
            "updated": "2024-09-12T13:05:28+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.NA",
                "math.NA",
                "05C21, 65D15, 68T07, 90C35"
            ],
            "max_author_hindex": 19
        },
        "2409.08229": {
            "authors": [
                "M. AbuGhanem"
            ],
            "title": "Photonic Quantum Computers",
            "abstract": "In the pursuit of scalable and fault-tolerant quantum computing architectures, photonic-based quantum computers have emerged as a leading frontier. This article provides a comprehensive overview of advancements in photonic quantum computing, developed by leading industry players, examining current performance, architectural designs, and strategies for developing large-scale, fault-tolerant photonic quantum computers. It also highlights recent groundbreaking experiments that leverage the unique advantages of photonic technologies, underscoring their transformative potential. This review captures a pivotal moment of photonic quantum computing in the noisy intermediate-scale quantum (NISQ) era, offering insights into how photonic quantum computers might reshape the future of quantum computing.",
            "id": "2409.08229",
            "link": "http://arxiv.org/abs/2409.08229v1",
            "published": "2024-09-12T17:16:38+00:00",
            "updated": "2024-09-12T17:16:38+00:00",
            "primary_category": "quant-ph",
            "categories": [
                "quant-ph",
                "cs.AI",
                "cs.AR"
            ],
            "max_author_hindex": 3
        },
        "2409.08231": {
            "authors": [
                "Jinsu Kim",
                "Jaemin Seo"
            ],
            "title": "Design Optimization of Nuclear Fusion Reactor through Deep Reinforcement Learning",
            "abstract": "This research explores the application of Deep Reinforcement Learning (DRL) to optimize the design of a nuclear fusion reactor. DRL can efficiently address the challenging issues attributed to multiple physics and engineering constraints for steady-state operation. The fusion reactor design computation and the optimization code applicable to parallelization with DRL are developed. The proposed framework enables finding the optimal reactor design that satisfies the operational requirements while reducing building costs. Multi-objective design optimization for a fusion reactor is now simplified by DRL, indicating the high potential of the proposed framework for advancing the efficient and sustainable design of future reactors.",
            "id": "2409.08231",
            "link": "http://arxiv.org/abs/2409.08231v1",
            "published": "2024-09-12T17:23:01+00:00",
            "updated": "2024-09-12T17:23:01+00:00",
            "primary_category": "physics.plasm-ph",
            "categories": [
                "physics.plasm-ph",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 14
        },
        "2409.05435": {
            "authors": [
                "Jasmina Gajcin",
                "Jovan Jeromela",
                "Ivana Dusparic"
            ],
            "title": "Semifactual Explanations for Reinforcement Learning",
            "abstract": "Reinforcement Learning (RL) is a learning paradigm in which the agent learns from its environment through trial and error. Deep reinforcement learning (DRL) algorithms represent the agent's policies using neural networks, making their decisions difficult to interpret. Explaining the behaviour of DRL agents is necessary to advance user trust, increase engagement, and facilitate integration with real-life tasks. Semifactual explanations aim to explain an outcome by providing \"even if\" scenarios, such as \"even if the car were moving twice as slowly, it would still have to swerve to avoid crashing\". Semifactuals help users understand the effects of different factors on the outcome and support the optimisation of resources. While extensively studied in psychology and even utilised in supervised learning, semifactuals have not been used to explain the decisions of RL systems. In this work, we develop a first approach to generating semifactual explanations for RL agents. We start by defining five properties of desirable semifactual explanations in RL and then introducing SGRL-Rewind and SGRL-Advance, the first algorithms for generating semifactual explanations in RL. We evaluate the algorithms in two standard RL environments and find that they generate semifactuals that are easier to reach, represent the agent's policy better, and are more diverse compared to baselines. Lastly, we conduct and analyse a user study to assess the participant's perception of semifactual explanations of the agent's actions.",
            "id": "2409.05435",
            "link": "http://arxiv.org/abs/2409.05435v1",
            "published": "2024-09-09T08:37:47+00:00",
            "updated": "2024-09-09T08:37:47+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 19
        },
        "2409.06016": {
            "authors": [
                "Yasaman Etesam",
                "Hyunmin Cheong",
                "Mohammadmehdi Ataei",
                "Pradeep Kumar Jayaraman"
            ],
            "title": "Deep Generative Model for Mechanical System Configuration Design",
            "abstract": "Generative AI has made remarkable progress in addressing various design challenges. One prominent area where generative AI could bring significant value is in engineering design. In particular, selecting an optimal set of components and their interfaces to create a mechanical system that meets design requirements is one of the most challenging and time-consuming tasks for engineers. This configuration design task is inherently challenging due to its categorical nature, multiple design requirements a solution must satisfy, and the reliance on physics simulations for evaluating potential solutions. These characteristics entail solving a combinatorial optimization problem with multiple constraints involving black-box functions. To address this challenge, we propose a deep generative model to predict the optimal combination of components and interfaces for a given design problem. To demonstrate our approach, we solve a gear train synthesis problem by first creating a synthetic dataset using a grammar, a parts catalogue, and a physics simulator. We then train a Transformer using this dataset, named GearFormer, which can not only generate quality solutions on its own, but also augment search methods such as an evolutionary algorithm and Monte Carlo tree search. We show that GearFormer outperforms such search methods on their own in terms of satisfying the specified design requirements with orders of magnitude faster generation time. Additionally, we showcase the benefit of hybrid methods that leverage both GearFormer and search methods, which further improve the quality of the solutions.",
            "id": "2409.06016",
            "link": "http://arxiv.org/abs/2409.06016v1",
            "published": "2024-09-09T19:15:45+00:00",
            "updated": "2024-09-09T19:15:45+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 42
        },
        "2409.07510": {
            "authors": [
                "Falaah Arif Khan",
                "Denys Herasymuk",
                "Nazar Protsiv",
                "Julia Stoyanovich"
            ],
            "title": "Still More Shades of Null: A Benchmark for Responsible Missing Value Imputation",
            "abstract": "We present Shades-of-NULL, a benchmark for responsible missing value imputation. Our benchmark includes state-of-the-art imputation techniques, and embeds them into the machine learning development lifecycle. We model realistic missingness scenarios that go beyond Rubin's classic Missing Completely at Random (MCAR), Missing At Random (MAR) and Missing Not At Random (MNAR), to include multi-mechanism missingness (when different missingness patterns co-exist in the data) and missingness shift (when the missingness mechanism changes between training and test). Another key novelty of our work is that we evaluate imputers holistically, based on the predictive performance, fairness and stability of the models that are trained and tested on the data they produce.   We use Shades-of-NULL to conduct a large-scale empirical study involving 20,952 experimental pipelines, and find that, while there is no single best-performing imputation approach for all missingness types, interesting performance patterns do emerge when comparing imputer performance in simpler vs. more complex missingness scenarios. Further, while predictive performance, fairness and stability can be seen as orthogonal, we identify trade-offs among them that arise due to the combination of missingness scenario, the choice of an imputer, and the architecture of the model trained on the data post-imputation. We make Shades-of-NULL publicly available, and hope to enable researchers to comprehensively and rigorously evaluate new missing value imputation methods on a wide range of evaluation metrics, in plausible and socially meaningful missingness scenarios.",
            "id": "2409.07510",
            "link": "http://arxiv.org/abs/2409.07510v1",
            "published": "2024-09-11T17:58:39+00:00",
            "updated": "2024-09-11T17:58:39+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ],
            "max_author_hindex": 29
        },
        "2409.07547": {
            "authors": [
                "Aymen Ben Said",
                "Malek Mouhoub"
            ],
            "title": "Machine Learning and Constraint Programming for Efficient Healthcare Scheduling",
            "abstract": "Solving combinatorial optimization problems involve satisfying a set of hard constraints while optimizing some objectives. In this context, exact or approximate methods can be used. While exact methods guarantee the optimal solution, they often come with an exponential running time as opposed to approximate methods that trade the solutions quality for a better running time. In this context, we tackle the Nurse Scheduling Problem (NSP). The NSP consist in assigning nurses to daily shifts within a planning horizon such that workload constraints are satisfied while hospitals costs and nurses preferences are optimized. To solve the NSP, we propose implicit and explicit approaches. In the implicit solving approach, we rely on Machine Learning methods using historical data to learn and generate new solutions through the constraints and objectives that may be embedded in the learned patterns. To quantify the quality of using our implicit approach in capturing the embedded constraints and objectives, we rely on the Frobenius Norm, a quality measure used to compute the average error between the generated solutions and historical data. To compensate for the uncertainty related to the implicit approach given that the constraints and objectives may not be concretely visible in the produced solutions, we propose an alternative explicit approach where we first model the NSP using the Constraint Satisfaction Problem (CSP) framework. Then we develop Stochastic Local Search methods and a new Branch and Bound algorithm enhanced with constraint propagation techniques and variables/values ordering heuristics. Since our implicit approach may not guarantee the feasibility or optimality of the generated solution, we propose a data-driven approach to passively learn the NSP as a constraint network. The learned constraint network, formulated as a CSP, will then be solved using the methods we listed earlier.",
            "id": "2409.07547",
            "link": "http://arxiv.org/abs/2409.07547v1",
            "published": "2024-09-11T18:09:25+00:00",
            "updated": "2024-09-11T18:09:25+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.07775": {
            "authors": [
                "Yinbo Yu",
                "Saihao Yan",
                "Jiajia Liu"
            ],
            "title": "A Spatiotemporal Stealthy Backdoor Attack against Cooperative Multi-Agent Deep Reinforcement Learning",
            "abstract": "Recent studies have shown that cooperative multi-agent deep reinforcement learning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor trigger is observed, it will perform abnormal actions leading to failures or malicious goals. However, existing proposed backdoors suffer from several issues, e.g., fixed visual trigger patterns lack stealthiness, the backdoor is trained or activated by an additional network, or all agents are backdoored. To this end, in this paper, we propose a novel backdoor attack against c-MADRL, which attacks the entire multi-agent team by embedding the backdoor only in a single agent. Firstly, we introduce adversary spatiotemporal behavior patterns as the backdoor trigger rather than manual-injected fixed visual patterns or instant status and control the attack duration. This method can guarantee the stealthiness and practicality of injected backdoors. Secondly, we hack the original reward function of the backdoored agent via reward reverse and unilateral guidance during training to ensure its adverse influence on the entire team. We evaluate our backdoor attacks on two classic c-MADRL algorithms VDN and QMIX, in a popular c-MADRL environment SMAC. The experimental results demonstrate that our backdoor attacks are able to reach a high attack success rate (91.6\\%) while maintaining a low clean performance variance rate (3.7\\%).",
            "id": "2409.07775",
            "link": "http://arxiv.org/abs/2409.07775v1",
            "published": "2024-09-12T06:17:37+00:00",
            "updated": "2024-09-12T06:17:37+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CR"
            ],
            "max_author_hindex": 10
        },
        "2409.05601": {
            "authors": [
                "Nithin Rao Koluguri",
                "Travis Bartley",
                "Hainan Xu",
                "Oleksii Hrinchuk",
                "Jagadeesh Balam",
                "Boris Ginsburg",
                "Georg Kucsko"
            ],
            "title": "Longer is (Not Necessarily) Stronger: Punctuated Long-Sequence Training for Enhanced Speech Recognition and Translation",
            "abstract": "This paper presents a new method for training sequence-to-sequence models for speech recognition and translation tasks. Instead of the traditional approach of training models on short segments containing only lowercase or partial punctuation and capitalization (PnC) sentences, we propose training on longer utterances that include complete sentences with proper punctuation and capitalization. We achieve this by using the FastConformer architecture which allows training 1 Billion parameter models with sequences up to 60 seconds long with full attention. However, while training with PnC enhances the overall performance, we observed that accuracy plateaus when training on sequences longer than 40 seconds across various evaluation settings. Our proposed method significantly improves punctuation and capitalization accuracy, showing a 25% relative word error rate (WER) improvement on the Earnings-21 and Earnings-22 benchmarks. Additionally, training on longer audio segments increases the overall model accuracy across speech recognition and translation benchmarks. The model weights and training code are open-sourced though NVIDIA NeMo.",
            "id": "2409.05601",
            "link": "http://arxiv.org/abs/2409.05601v1",
            "published": "2024-09-09T13:35:52+00:00",
            "updated": "2024-09-09T13:35:52+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.CL"
            ],
            "max_author_hindex": 27
        },
        "2409.05799": {
            "authors": [
                "Massa Baali",
                "Abdulhamid Aldoobi",
                "Hira Dhamyal",
                "Rita Singh",
                "Bhiksha Raj"
            ],
            "title": "PDAF: A Phonetic Debiasing Attention Framework For Speaker Verification",
            "abstract": "Speaker verification systems are crucial for authenticating identity through voice. Traditionally, these systems focus on comparing feature vectors, overlooking the speech's content. However, this paper challenges this by highlighting the importance of phonetic dominance, a measure of the frequency or duration of phonemes, as a crucial cue in speaker verification. A novel Phoneme Debiasing Attention Framework (PDAF) is introduced, integrating with existing attention frameworks to mitigate biases caused by phonetic dominance. PDAF adjusts the weighting for each phoneme and influences feature extraction, allowing for a more nuanced analysis of speech. This approach paves the way for more accurate and reliable identity authentication through voice. Furthermore, by employing various weighting strategies, we evaluate the influence of phonetic features on the efficacy of the speaker verification system.",
            "id": "2409.05799",
            "link": "http://arxiv.org/abs/2409.05799v1",
            "published": "2024-09-09T17:03:38+00:00",
            "updated": "2024-09-09T17:03:38+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.CL"
            ],
            "max_author_hindex": 54
        },
        "2409.06178": {
            "authors": [
                "Yuan Tian",
                "Jonathan K. Kummerfeld",
                "Toby Jia-Jun Li",
                "Tianyi Zhang"
            ],
            "title": "SQLucid: Grounding Natural Language Database Queries with Interactive Explanations",
            "abstract": "Though recent advances in machine learning have led to significant improvements in natural language interfaces for databases, the accuracy and reliability of these systems remain limited, especially in high-stakes domains. This paper introduces SQLucid, a novel user interface that bridges the gap between non-expert users and complex database querying processes. SQLucid addresses existing limitations by integrating visual correspondence, intermediate query results, and editable step-by-step SQL explanations in natural language to facilitate user understanding and engagement. This unique blend of features empowers users to understand and refine SQL queries easily and precisely. Two user studies and one quantitative experiment were conducted to validate SQLucid's effectiveness, showing significant improvement in task completion accuracy and user confidence compared to existing interfaces. Our code is available at https://github.com/magic-YuanTian/SQLucid.",
            "id": "2409.06178",
            "link": "http://arxiv.org/abs/2409.06178v1",
            "published": "2024-09-10T03:14:09+00:00",
            "updated": "2024-09-10T03:14:09+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.CL"
            ],
            "max_author_hindex": 23
        },
        "2409.06223": {
            "authors": [
                "Arvind Krishna Sridhar",
                "Yinyi Guo",
                "Erik Visser"
            ],
            "title": "Enhancing Temporal Understanding in Audio Question Answering for Large Audio Language Models",
            "abstract": "The Audio Question Answering task includes audio event classification, audio captioning, and open ended reasoning. Recently, Audio Question Answering has garnered attention due to the advent of Large Audio Language Models. Current literature focuses on constructing LALMs by integrating audio encoders with text only Large Language Models through a projection module. While Large Audio Language Models excel in general audio understanding, they are limited in temporal reasoning which may hinder their commercial applications and on device deployment. This paper addresses these challenges and limitations in audio temporal reasoning. First, we introduce a data augmentation technique for generating reliable audio temporal questions and answers using an LLM. Second, we propose a continued finetuning curriculum learning strategy to specialize in temporal reasoning without compromising performance on finetuned tasks. Finally, we develop a reliable and transparent automated metric, assisted by an LLM, to measure the correlation between Large Audio Language Model responses and ground truth data intelligently. We demonstrate the effectiveness of our proposed techniques using SOTA LALMs on public audio benchmark datasets.",
            "id": "2409.06223",
            "link": "http://arxiv.org/abs/2409.06223v1",
            "published": "2024-09-10T05:26:53+00:00",
            "updated": "2024-09-10T05:26:53+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 9
        },
        "2409.07265": {
            "authors": [
                "Kazuki Yamauchi",
                "Yuki Saito",
                "Hiroshi Saruwatari"
            ],
            "title": "Cross-Dialect Text-To-Speech in Pitch-Accent Language Incorporating Multi-Dialect Phoneme-Level BERT",
            "abstract": "We explore cross-dialect text-to-speech (CD-TTS), a task to synthesize learned speakers' voices in non-native dialects, especially in pitch-accent languages. CD-TTS is important for developing voice agents that naturally communicate with people across regions. We present a novel TTS model comprising three sub-modules to perform competitively at this task. We first train a backbone TTS model to synthesize dialect speech from a text conditioned on phoneme-level accent latent variables (ALVs) extracted from speech by a reference encoder. Then, we train an ALV predictor to predict ALVs tailored to a target dialect from input text leveraging our novel multi-dialect phoneme-level BERT. We conduct multi-dialect TTS experiments and evaluate the effectiveness of our model by comparing it with a baseline derived from conventional dialect TTS methods. The results show that our model improves the dialectal naturalness of synthetic speech in CD-TTS.",
            "id": "2409.07265",
            "link": "http://arxiv.org/abs/2409.07265v1",
            "published": "2024-09-11T13:40:27+00:00",
            "updated": "2024-09-11T13:40:27+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 38
        },
        "2409.07437": {
            "authors": [
                "Gallil Maimon",
                "Amit Roth",
                "Yossi Adi"
            ],
            "title": "A Suite for Acoustic Language Model Evaluation",
            "abstract": "Speech language models have recently demonstrated great potential as universal speech processing systems. Such models have the ability to model the rich acoustic information existing in audio signals, beyond spoken content, such as emotion, background noise, etc. Despite this, evaluation benchmarks which evaluate awareness to a wide range of acoustic aspects, are lacking. To help bridge this gap, we introduce SALMon, a novel evaluation suite encompassing background noise, emotion, speaker identity and room impulse response. The proposed benchmarks both evaluate the consistency of the inspected element and how much it matches the spoken text. We follow a modelling based approach, measuring whether a model gives correct samples higher scores than incorrect ones. This approach makes the benchmark fast to compute even for large models. We evaluated several speech language models on SALMon, thus highlighting the strengths and weaknesses of each evaluated method. Code and data are publicly available at https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .",
            "id": "2409.07437",
            "link": "http://arxiv.org/abs/2409.07437v1",
            "published": "2024-09-11T17:34:52+00:00",
            "updated": "2024-09-11T17:34:52+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 30
        },
        "2409.07823": {
            "authors": [
                "Ekaterina Svikhnushina",
                "Pearl Pu"
            ],
            "title": "Online vs Offline: A Comparative Study of First-Party and Third-Party Evaluations of Social Chatbots",
            "abstract": "This paper explores the efficacy of online versus offline evaluation methods in assessing conversational chatbots, specifically comparing first-party direct interactions with third-party observational assessments. By extending a benchmarking dataset of user dialogs with empathetic chatbots with offline third-party evaluations, we present a systematic comparison between the feedback from online interactions and the more detached offline third-party evaluations. Our results reveal that offline human evaluations fail to capture the subtleties of human-chatbot interactions as effectively as online assessments. In comparison, automated third-party evaluations using a GPT-4 model offer a better approximation of first-party human judgments given detailed instructions. This study highlights the limitations of third-party evaluations in grasping the complexities of user experiences and advocates for the integration of direct interaction feedback in conversational AI evaluation to enhance system development and user satisfaction.",
            "id": "2409.07823",
            "link": "http://arxiv.org/abs/2409.07823v1",
            "published": "2024-09-12T08:11:08+00:00",
            "updated": "2024-09-12T08:11:08+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.CL"
            ],
            "max_author_hindex": 44
        },
        "2409.07839": {
            "authors": [
                "Xinying Lu",
                "Jianli Xiao"
            ],
            "title": "FPMT: Enhanced Semi-Supervised Model for Traffic Incident Detection",
            "abstract": "For traffic incident detection, the acquisition of data and labels is notably resource-intensive, rendering semi-supervised traffic incident detection both a formidable and consequential challenge. Thus, this paper focuses on traffic incident detection with a semi-supervised learning way. It proposes a semi-supervised learning model named FPMT within the framework of MixText. The data augmentation module introduces Generative Adversarial Networks to balance and expand the dataset. During the mix-up process in the hidden space, it employs a probabilistic pseudo-mixing mechanism to enhance regularization and elevate model precision. In terms of training strategy, it initiates with unsupervised training on all data, followed by supervised fine-tuning on a subset of labeled data, and ultimately completing the goal of semi-supervised training. Through empirical validation on four authentic datasets, our FPMT model exhibits outstanding performance across various metrics. Particularly noteworthy is its robust performance even in scenarios with low label rates.",
            "id": "2409.07839",
            "link": "http://arxiv.org/abs/2409.07839v1",
            "published": "2024-09-12T08:38:42+00:00",
            "updated": "2024-09-12T08:38:42+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 10
        },
        "2409.07958": {
            "authors": [
                "Jake Street",
                "Isibor Ihianle",
                "Funminiyi Olajide",
                "Ahmad Lotfi"
            ],
            "title": "Enhanced Online Grooming Detection Employing Context Determination and Message-Level Analysis",
            "abstract": "Online Grooming (OG) is a prevalent threat facing predominately children online, with groomers using deceptive methods to prey on the vulnerability of children on social media/messaging platforms. These attacks can have severe psychological and physical impacts, including a tendency towards revictimization. Current technical measures are inadequate, especially with the advent of end-to-end encryption which hampers message monitoring. Existing solutions focus on the signature analysis of child abuse media, which does not effectively address real-time OG detection. This paper proposes that OG attacks are complex, requiring the identification of specific communication patterns between adults and children. It introduces a novel approach leveraging advanced models such as BERT and RoBERTa for Message-Level Analysis and a Context Determination approach for classifying actor interactions, including the introduction of Actor Significance Thresholds and Message Significance Thresholds. The proposed method aims to enhance accuracy and robustness in detecting OG by considering the dynamic and multi-faceted nature of these attacks. Cross-dataset experiments evaluate the robustness and versatility of our approach. This paper's contributions include improved detection methodologies and the potential for application in various scenarios, addressing gaps in current literature and practices.",
            "id": "2409.07958",
            "link": "http://arxiv.org/abs/2409.07958v1",
            "published": "2024-09-12T11:37:34+00:00",
            "updated": "2024-09-12T11:37:34+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 21
        },
        "2409.06477": {
            "authors": [
                "Atharva Gundawar",
                "Yuchao Li",
                "Dimitri Bertsekas"
            ],
            "title": "Superior Computer Chess with Model Predictive Control, Reinforcement Learning, and Rollout",
            "abstract": "In this paper we apply model predictive control (MPC), rollout, and reinforcement learning (RL) methodologies to computer chess. We introduce a new architecture for move selection, within which available chess engines are used as components. One engine is used to provide position evaluations in an approximation in value space MPC/RL scheme, while a second engine is used as nominal opponent, to emulate or approximate the moves of the true opponent player.   We show that our architecture improves substantially the performance of the position evaluation engine. In other words our architecture provides an additional layer of intelligence, on top of the intelligence of the engines on which it is based. This is true for any engine, regardless of its strength: top engines such as Stockfish and Komodo Dragon (of varying strengths), as well as weaker engines.   Structurally, our basic architecture selects moves by a one-move lookahead search, with an intermediate move generated by a nominal opponent engine, and followed by a position evaluation by another chess engine. Simpler schemes that forego the use of the nominal opponent, also perform better than the position evaluator, but not quite by as much. More complex schemes, involving multistep lookahead, may also be used and generally tend to perform better as the length of the lookahead increases.   Theoretically, our methodology relies on generic cost improvement properties and the superlinear convergence framework of Newton's method, which fundamentally underlies approximation in value space, and related MPC/RL and rollout/policy iteration schemes. A critical requirement of this framework is that the first lookahead step should be executed exactly. This fact has guided our architectural choices, and is apparently an important factor in improving the performance of even the best available chess engines.",
            "id": "2409.06477",
            "link": "http://arxiv.org/abs/2409.06477v1",
            "published": "2024-09-10T13:05:45+00:00",
            "updated": "2024-09-10T13:05:45+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ],
            "max_author_hindex": 95
        },
        "2409.08264": {
            "authors": [
                "Rogerio Bonatti",
                "Dan Zhao",
                "Francesco Bonacci",
                "Dillon Dupont",
                "Sara Abdali",
                "Yinheng Li",
                "Justin Wagle",
                "Kazuhito Koishida",
                "Arthur Bucker",
                "Lawrence Jang",
                "Zack Hui"
            ],
            "title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale",
            "abstract": "Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi's performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena.   Webpage: https://microsoft.github.io/WindowsAgentArena   Code: https://github.com/microsoft/WindowsAgentArena",
            "id": "2409.08264",
            "link": "http://arxiv.org/abs/2409.08264v1",
            "published": "2024-09-12T17:56:43+00:00",
            "updated": "2024-09-12T17:56:43+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 18
        },
        "2409.05926": {
            "authors": [
                "Chengwei Sun",
                "Jiwei Wei",
                "Yujia Wu",
                "Yiming Shi",
                "Shiyuan He",
                "Zeyu Ma",
                "Ning Xie",
                "Yang Yang"
            ],
            "title": "SVFit: Parameter-Efficient Fine-Tuning of Large Pre-Trained Models Using Singular Values",
            "abstract": "Large pre-trained models (LPMs) have demonstrated exceptional performance in diverse natural language processing and computer vision tasks. However, fully fine-tuning these models poses substantial memory challenges, particularly in resource-constrained environments. Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, mitigate this issue by adjusting only a small subset of parameters. Nevertheless, these methods typically employ random initialization for low-rank matrices, which can lead to inefficiencies in gradient descent and diminished generalizability due to suboptimal starting points. To address these limitations, we propose SVFit, a novel PEFT approach that leverages singular value decomposition (SVD) to initialize low-rank matrices using critical singular values as trainable parameters. Specifically, SVFit performs SVD on the pre-trained weight matrix to obtain the best rank-r approximation matrix, emphasizing the most critical singular values that capture over 99% of the matrix's information. These top-r singular values are then used as trainable parameters to scale the fundamental subspaces of the matrix, facilitating rapid domain adaptation. Extensive experiments across various pre-trained models in natural language understanding, text-to-image generation, and image classification tasks reveal that SVFit outperforms LoRA while requiring 16 times fewer trainable parameters.",
            "id": "2409.05926",
            "link": "http://arxiv.org/abs/2409.05926v1",
            "published": "2024-09-09T08:44:53+00:00",
            "updated": "2024-09-09T08:44:53+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 16
        },
        "2409.06033": {
            "authors": [
                "Zahra Khanjani",
                "Tolulope Ale",
                "Jianwu Wang",
                "Lavon Davis",
                "Christine Mallinson",
                "Vandana P. Janeja"
            ],
            "title": "Investigating Causal Cues: Strengthening Spoofed Audio Detection with Human-Discernible Linguistic Features",
            "abstract": "Several types of spoofed audio, such as mimicry, replay attacks, and deepfakes, have created societal challenges to information integrity. Recently, researchers have worked with sociolinguistics experts to label spoofed audio samples with Expert Defined Linguistic Features (EDLFs) that can be discerned by the human ear: pitch, pause, word-initial and word-final release bursts of consonant stops, audible intake or outtake of breath, and overall audio quality. It is established that there is an improvement in several deepfake detection algorithms when they augmented the traditional and common features of audio data with these EDLFs. In this paper, using a hybrid dataset comprised of multiple types of spoofed audio augmented with sociolinguistic annotations, we investigate causal discovery and inferences between the discernible linguistic features and the label in the audio clips, comparing the findings of the causal models with the expert ground truth validation labeling process. Our findings suggest that the causal models indicate the utility of incorporating linguistic features to help discern spoofed audio, as well as the overall need and opportunity to incorporate human knowledge into models and techniques for strengthening AI models. The causal discovery and inference can be used as a foundation of training humans to discern spoofed audio as well as automating EDLFs labeling for the purpose of performance improvement of the common AI-based spoofed audio detectors.",
            "id": "2409.06033",
            "link": "http://arxiv.org/abs/2409.06033v1",
            "published": "2024-09-09T19:47:57+00:00",
            "updated": "2024-09-09T19:47:57+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 18
        },
        "2409.06205": {
            "authors": [
                "Wanli Qian",
                "Chenfeng Gao",
                "Anup Sathya",
                "Ryo Suzuki",
                "Ken Nakagaki"
            ],
            "title": "SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs",
            "abstract": "This paper introduces text-to-shape-display, a novel approach to generating dynamic shape changes in pin-based shape displays through natural language commands. By leveraging large language models (LLMs) and AI-chaining, our approach allows users to author shape-changing behaviors on demand through text prompts without programming. We describe the foundational aspects necessary for such a system, including the identification of key generative elements (primitive, animation, and interaction) and design requirements to enhance user interaction, based on formative exploration and iterative design processes. Based on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a 24 x 24 shape display, which translates the user's textual command into executable code and allows for quick exploration through a web-based control interface. We evaluate the effectiveness of SHAPE-IT in two ways: 1) performance evaluation and 2) user evaluation (N= 10). The study conclusions highlight the ability to facilitate rapid ideation of a wide range of shape-changing behaviors with AI. However, the findings also expose accuracy-related challenges and limitations, prompting further exploration into refining the framework for leveraging AI to better suit the unique requirements of shape-changing systems.",
            "id": "2409.06205",
            "link": "http://arxiv.org/abs/2409.06205v1",
            "published": "2024-09-10T04:18:49+00:00",
            "updated": "2024-09-10T04:18:49+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.CL",
                "H.5.2"
            ],
            "max_author_hindex": 14
        },
        "2409.06211": {
            "authors": [
                "Jaeseong Lee",
                "seung-won hwang",
                "Aurick Qiao",
                "Daniel F Campos",
                "Zhewei Yao",
                "Yuxiong He"
            ],
            "title": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning",
            "abstract": "Mixture-of-experts (MoEs) have been adopted for reducing inference costs by sparsely activating experts in Large language models (LLMs). Despite this reduction, the massive number of experts in MoEs still makes them expensive to serve. In this paper, we study how to address this, by pruning MoEs. Among pruning methodologies, unstructured pruning has been known to achieve the highest performance for a given pruning ratio, compared to structured pruning, since the latter imposes constraints on the sparsification structure. This is intuitive, as the solution space of unstructured pruning subsumes that of structured pruning. However, our counterintuitive finding reveals that expert pruning, a form of structured pruning, can actually precede unstructured pruning to outperform unstructured-only pruning. As existing expert pruning, requiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot scale for recent MoEs, we propose a scalable alternative with $O(1)$ complexity, yet outperforming the more expensive methods. The key idea is leveraging a latent structure between experts, based on behavior similarity, such that the greedy decision of whether to prune closely captures the joint pruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized MoE with 128 experts, our method needs only one H100 and two hours to achieve nearly no loss in performance with 40% sparsity, even in generative tasks such as GSM8K, where state-of-the-art unstructured pruning fails to. The code will be made publicly available.",
            "id": "2409.06211",
            "link": "http://arxiv.org/abs/2409.06211v1",
            "published": "2024-09-10T04:34:42+00:00",
            "updated": "2024-09-10T04:34:42+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 38
        },
        "2409.06411": {
            "authors": [
                "Wei Liu",
                "Yang Bai",
                "Chengcheng Han",
                "Rongxiang Weng",
                "Jun Xu",
                "Xuezhi Cao",
                "Jingang Wang",
                "Xunliang Cai"
            ],
            "title": "Length Desensitization in Directed Preference Optimization",
            "abstract": "Direct Preference Optimization (DPO) is widely utilized in the Reinforcement Learning from Human Feedback (RLHF) phase to align Large Language Models (LLMs) with human preferences, thereby enhancing both their harmlessness and efficacy. However, it has been observed that DPO tends to over-optimize for verbosity, which can detrimentally affect both performance and user experience. In this paper, we conduct an in-depth theoretical analysis of DPO's optimization objective and reveal a strong correlation between its implicit reward and data length. This correlation misguides the optimization direction, resulting in length sensitivity during the DPO training and leading to verbosity. To address this issue, we propose a length-desensitization improvement method for DPO, termed LD-DPO. The proposed method aims to desensitize DPO to data length by decoupling explicit length preference, which is relatively insignificant, from the other implicit preferences, thereby enabling more effective learning of the intrinsic preferences. We utilized two settings (Base and Instruct) of Llama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various benchmarks including MT-Bench and AlpacaEval 2. The experimental results indicate that LD-DPO consistently outperforms DPO and other baseline methods, achieving more concise responses with a 10-40\\% reduction in length compared to DPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can indeed achieve length desensitization and align the model more closely with human-real preferences.",
            "id": "2409.06411",
            "link": "http://arxiv.org/abs/2409.06411v1",
            "published": "2024-09-10T10:49:38+00:00",
            "updated": "2024-09-10T10:49:38+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 34
        },
        "2409.06433": {
            "authors": [
                "Gollam Rabby",
                "S\u00f6ren Auer",
                "Jennifer D'Souza",
                "Allard Oelen"
            ],
            "title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
            "abstract": "The increasing amount of published scholarly articles, exceeding 2.5 million yearly, raises the challenge for researchers in following scientific progress. Integrating the contributions from scholarly articles into a novel type of cognitive knowledge graph (CKG) will be a crucial element for accessing and organizing scholarly knowledge, surpassing the insights provided by titles and abstracts. This research focuses on effectively conveying structured scholarly knowledge by utilizing large language models (LLMs) to categorize scholarly articles and describe their contributions in a structured and comparable manner. While previous studies explored language models within specific research domains, the extensive domain-independent knowledge captured by LLMs offers a substantial opportunity for generating structured contribution descriptions as CKGs. Additionally, LLMs offer customizable pathways through prompt engineering or fine-tuning, thus facilitating to leveraging of smaller LLMs known for their efficiency, cost-effectiveness, and environmental considerations. Our methodology involves harnessing LLM knowledge, and complementing it with domain expert-verified scholarly data sourced from a CKG. This strategic fusion significantly enhances LLM performance, especially in tasks like scholarly article categorization and predicate recommendation. Our method involves fine-tuning LLMs with CKG knowledge and additionally injecting knowledge from a CKG with a novel prompting technique significantly increasing the accuracy of scholarly knowledge extraction. We integrated our approach in the Open Research Knowledge Graph (ORKG), thus enabling precise access to organized scholarly knowledge, crucially benefiting domain-independent scholarly knowledge exchange and dissemination among policymakers, industrial practitioners, and the general public.",
            "id": "2409.06433",
            "link": "http://arxiv.org/abs/2409.06433v1",
            "published": "2024-09-10T11:31:02+00:00",
            "updated": "2024-09-10T11:31:02+00:00",
            "primary_category": "cs.DL",
            "categories": [
                "cs.DL",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 53
        },
        "2409.06656": {
            "authors": [
                "Taejin Park",
                "Ivan Medennikov",
                "Kunal Dhawan",
                "Weiqing Wang",
                "He Huang",
                "Nithin Rao Koluguri",
                "Krishna C. Puvvada",
                "Jagadeesh Balam",
                "Boris Ginsburg"
            ],
            "title": "Sortformer: Seamless Integration of Speaker Diarization and ASR by Bridging Timestamps and Tokens",
            "abstract": "We propose Sortformer, a novel neural model for speaker diarization, trained with unconventional objectives compared to existing end-to-end diarization models. The permutation problem in speaker diarization has long been regarded as a critical challenge. Most prior end-to-end diarization systems employ permutation invariant loss (PIL), which optimizes for the permutation that yields the lowest error. In contrast, we introduce Sort Loss, which enables a diarization model to autonomously resolve permutation, with or without PIL. We demonstrate that combining Sort Loss and PIL achieves performance competitive with state-of-the-art end-to-end diarization models trained exclusively with PIL. Crucially, we present a streamlined multispeaker ASR architecture that leverages Sortformer as a speaker supervision model, embedding speaker label estimation within the ASR encoder state using a sinusoidal kernel function. This approach resolves the speaker permutation problem through sorted objectives, effectively bridging speaker-label timestamps and speaker tokens. In our experiments, we show that the proposed multispeaker ASR architecture, enhanced with speaker supervision, improves performance via adapter techniques. Code and trained models will be made publicly available via the NVIDIA NeMo framework",
            "id": "2409.06656",
            "link": "http://arxiv.org/abs/2409.06656v1",
            "published": "2024-09-10T17:20:11+00:00",
            "updated": "2024-09-10T17:20:11+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.CL",
                "cs.LG",
                "cs.SD"
            ],
            "max_author_hindex": 27
        },
        "2409.06927": {
            "authors": [
                "Christopher M. Ackerman"
            ],
            "title": "Representation Tuning",
            "abstract": "Activation engineering is becoming increasingly popular as a means of online control of large language models (LLMs). In this work, I extend the idea of active steering with vectors that represent a behavioral direction of interest to tuning those vectors directly into the model, obviating the need for online control. First, I identify activation vectors related to honesty in an open-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can be made more or less honest by adding positive or negative multiples of these vectors to residual stream activations during generation. Then, I show that a similar effect can be achieved by fine-tuning the vectors directly into the model, by use of a dual loss function based on the cosine similarity of residual stream activations to the vectors combined with a standard token-based loss (\"representation tuning\"). Finally, I compare the generations in response to honesty-probing prompts from the resulting models to those from models fine-tuned with a token-based loss alone, and to those from the untuned model subjected to online steering. Overall, fine-tuning the vectors into the models using the cosine similarity plus token loss showed a stronger effect than online steering, and generalized better than using the standard loss, suggesting the potential utility of this approach as a safety measure. Code and data are available at https://github.com/cma1114/representation_tuning; tuned models are available at https://huggingface.co/collections/cackerman/ representation-tuning-66da1e5ab41cd1b824687d9f.",
            "id": "2409.06927",
            "link": "http://arxiv.org/abs/2409.06927v1",
            "published": "2024-09-11T00:56:02+00:00",
            "updated": "2024-09-11T00:56:02+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 1
        },
        "2409.07132": {
            "authors": [
                "Vojt\u011bch Balek",
                "Luk\u00e1\u0161 S\u00fdkora",
                "Vil\u00e9m Sklen\u00e1k",
                "Tom\u00e1\u0161 Kliegr"
            ],
            "title": "LLM-based feature generation from text for interpretable machine learning",
            "abstract": "Existing text representations such as embeddings and bag-of-words are not suitable for rule learning due to their high dimensionality and absent or questionable feature-level interpretability. This article explores whether large language models (LLMs) could address this by extracting a small number of interpretable features from text. We demonstrate this process on two datasets (CORD-19 and M17+) containing several thousand scientific articles from multiple disciplines and a target being a proxy for research impact. An evaluation based on testing for the statistically significant correlation with research impact has shown that LLama 2-generated features are semantically meaningful. We consequently used these generated features in text classification to predict the binary target variable representing the citation rate for the CORD-19 dataset and the ordinal 5-class target representing an expert-awarded grade in the M17+ dataset. Machine-learning models trained on the LLM-generated features provided similar predictive performance to the state-of-the-art embedding model SciBERT for scientific text. The LLM used only 62 features compared to 768 features in SciBERT embeddings, and these features were directly interpretable, corresponding to notions such as article methodological rigor, novelty, or grammatical correctness. As the final step, we extract a small number of well-interpretable action rules. Consistently competitive results obtained with the same LLM feature set across both thematically diverse datasets show that this approach generalizes across domains.",
            "id": "2409.07132",
            "link": "http://arxiv.org/abs/2409.07132v1",
            "published": "2024-09-11T09:29:28+00:00",
            "updated": "2024-09-11T09:29:28+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 15
        },
        "2409.08253": {
            "authors": [
                "Ashwini Gundappa",
                "Emilia Ellsiepen",
                "Lukas Schmitz",
                "Frederik Wiehr",
                "Vera Demberg"
            ],
            "title": "The Design of Informative Take-Over Requests for Semi-Autonomous Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a Drone-Controller Setting",
            "abstract": "The question of how cyber-physical systems should interact with human partners that can take over control or exert oversight is becoming more pressing, as these systems are deployed for an ever larger range of tasks. Drawing on the literatures on handing over control during semi-autonomous driving and human-robot interaction, we propose a design of a take-over request that combines an abstract pre-alert with an informative TOR: Relevant sensor information is highlighted on the controller's display, while a spoken message verbalizes the reason for the TOR. We conduct our study in the context of a semi-autonomous drone control scenario as our testbed. The goal of our online study is to assess in more detail what form a language-based TOR should take. Specifically, we compare a full sentence condition to shorter fragments, and test whether the visual highlighting should be done synchronously or asynchronously with the speech. Participants showed a higher accuracy in choosing the correct solution with our bi-modal TOR and felt that they were better able to recognize the critical situation. Using only fragments in the spoken message rather than full sentences did not lead to improved accuracy or faster reactions. Also, synchronizing the visual highlighting with the spoken message did not result in better accuracy and response times were even increased in this condition.",
            "id": "2409.08253",
            "link": "http://arxiv.org/abs/2409.08253v1",
            "published": "2024-09-12T17:50:05+00:00",
            "updated": "2024-09-12T17:50:05+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.CL",
                "cs.RO"
            ],
            "max_author_hindex": 30
        },
        "2409.05258": {
            "authors": [
                "Shervin Ardeshir"
            ],
            "title": "Towards Automated Machine Learning Research",
            "abstract": "This paper explores a top-down approach to automating incremental advances in machine learning research through component-level innovation, facilitated by Large Language Models (LLMs). Our framework systematically generates novel components, validates their feasibility, and evaluates their performance against existing baselines. A key distinction of this approach lies in how these novel components are generated. Unlike traditional AutoML and NAS methods, which often rely on a bottom-up combinatorial search over predefined, hardcoded base components, our method leverages the cross-domain knowledge embedded in LLMs to propose new components that may not be confined to any hard-coded predefined set. By incorporating a reward model to prioritize promising hypotheses, we aim to improve the efficiency of the hypothesis generation and evaluation process. We hope this approach offers a new avenue for exploration and contributes to the ongoing dialogue in the field.",
            "id": "2409.05258",
            "link": "http://arxiv.org/abs/2409.05258v1",
            "published": "2024-09-09T00:47:30+00:00",
            "updated": "2024-09-09T00:47:30+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 9
        },
        "2409.05265": {
            "authors": [
                "Jing Yuan",
                "Shaojie Tang"
            ],
            "title": "Learning Submodular Sequencing from Samples",
            "abstract": "This paper addresses the problem of sequential submodular maximization: selecting and ranking items in a sequence to optimize some composite submodular function. In contrast to most of the previous works, which assume access to the utility function, we assume that we are given only a set of samples. Each sample includes a random sequence of items and its associated utility. We present an algorithm that, given polynomially many samples drawn from a two-stage uniform distribution, achieves an approximation ratio dependent on the curvature of individual submodular functions. Our results apply in a wide variety of real-world scenarios, such as ranking products in online retail platforms, where complete knowledge of the utility function is often impossible to obtain. Our algorithm gives an empirically useful solution in such contexts, thus proving that limited data can be of great use in sequencing tasks. From a technical perspective, our results extend prior work on ``optimization from samples'' by generalizing from optimizing a set function to a sequence-dependent function.",
            "id": "2409.05265",
            "link": "http://arxiv.org/abs/2409.05265v1",
            "published": "2024-09-09T01:33:13+00:00",
            "updated": "2024-09-09T01:33:13+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 47
        },
        "2409.05279": {
            "authors": [
                "Minsuk Choi",
                "Hiroshi Ishikawa"
            ],
            "title": "BrainDecoder: Style-Based Visual Decoding of EEG Signals",
            "abstract": "Decoding neural representations of visual stimuli from electroencephalography (EEG) offers valuable insights into brain activity and cognition. Recent advancements in deep learning have significantly enhanced the field of visual decoding of EEG, primarily focusing on reconstructing the semantic content of visual stimuli. In this paper, we present a novel visual decoding pipeline that, in addition to recovering the content, emphasizes the reconstruction of the style, such as color and texture, of images viewed by the subject. Unlike previous methods, this ``style-based'' approach learns in the CLIP spaces of image and text separately, facilitating a more nuanced extraction of information from EEG signals. We also use captions for text alignment simpler than previously employed, which we find work better. Both quantitative and qualitative evaluations show that our method better preserves the style of visual stimuli and extracts more fine-grained semantic information from neural signals. Notably, it achieves significant improvements in quantitative results and sets a new state-of-the-art on the popular Brain2Image dataset.",
            "id": "2409.05279",
            "link": "http://arxiv.org/abs/2409.05279v1",
            "published": "2024-09-09T02:14:23+00:00",
            "updated": "2024-09-09T02:14:23+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.05294": {
            "authors": [
                "Yichuan Mo",
                "Hui Huang",
                "Mingjie Li",
                "Ang Li",
                "Yisen Wang"
            ],
            "title": "TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors",
            "abstract": "Diffusion models have achieved notable success in image generation, but they remain highly vulnerable to backdoor attacks, which compromise their integrity by producing specific undesirable outputs when presented with a pre-defined trigger. In this paper, we investigate how to protect diffusion models from this dangerous threat. Specifically, we propose TERD, a backdoor defense framework that builds unified modeling for current attacks, which enables us to derive an accessible reversed loss. A trigger reversion strategy is further employed: an initial approximation of the trigger through noise sampled from a prior distribution, followed by refinement through differential multi-step samplers. Additionally, with the reversed trigger, we propose backdoor detection from the noise space, introducing the first backdoor input detection approach for diffusion models and a novel model detection algorithm that calculates the KL divergence between reversed and benign distributions. Extensive evaluations demonstrate that TERD secures a 100% True Positive Rate (TPR) and True Negative Rate (TNR) across datasets of varying resolutions. TERD also demonstrates nice adaptability to other Stochastic Differential Equation (SDE)-based models. Our code is available at https://github.com/PKU-ML/TERD.",
            "id": "2409.05294",
            "link": "http://arxiv.org/abs/2409.05294v1",
            "published": "2024-09-09T03:02:16+00:00",
            "updated": "2024-09-09T03:02:16+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 71
        },
        "2409.05305": {
            "authors": [
                "Zakaria Patel",
                "Sebastian J. Wetzel"
            ],
            "title": "Closed-Form Interpretation of Neural Network Latent Spaces with Symbolic Gradients",
            "abstract": "It has been demonstrated in many scientific fields that artificial neural networks like autoencoders or Siamese networks encode meaningful concepts in their latent spaces. However, there does not exist a comprehensive framework for retrieving this information in a human-readable form without prior knowledge. In order to extract these concepts, we introduce a framework for finding closed-form interpretations of neurons in latent spaces of artificial neural networks. The interpretation framework is based on embedding trained neural networks into an equivalence class of functions that encode the same concept. We interpret these neural networks by finding an intersection between the equivalence class and human-readable equations defined by a symbolic search space. The approach is demonstrated by retrieving invariants of matrices and conserved quantities of dynamical systems from latent spaces of Siamese neural networks.",
            "id": "2409.05305",
            "link": "http://arxiv.org/abs/2409.05305v1",
            "published": "2024-09-09T03:26:07+00:00",
            "updated": "2024-09-09T03:26:07+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 9
        },
        "2409.05325": {
            "authors": [
                "Aryan Deshwal",
                "Sait Cakmak",
                "Yuhou Xia",
                "David Eriksson"
            ],
            "title": "Sample-Efficient Bayesian Optimization with Transfer Learning for Heterogeneous Search Spaces",
            "abstract": "Bayesian optimization (BO) is a powerful approach to sample-efficient optimization of black-box functions. However, in settings with very few function evaluations, a successful application of BO may require transferring information from historical experiments. These related experiments may not have exactly the same tunable parameters (search spaces), motivating the need for BO with transfer learning for heterogeneous search spaces. In this paper, we propose two methods for this setting. The first approach leverages a Gaussian process (GP) model with a conditional kernel to transfer information between different search spaces. Our second approach treats the missing parameters as hyperparameters of the GP model that can be inferred jointly with the other GP hyperparameters or set to fixed values. We show that these two methods perform well on several benchmark problems.",
            "id": "2409.05325",
            "link": "http://arxiv.org/abs/2409.05325v1",
            "published": "2024-09-09T04:36:06+00:00",
            "updated": "2024-09-09T04:36:06+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.05347": {
            "authors": [
                "Ahmed Imteaj",
                "Md Zarif Hossain",
                "Saika Zaman",
                "Abdur R. Shahid"
            ],
            "title": "TriplePlay: Enhancing Federated Learning with CLIP for Non-IID Data and Resource Efficiency",
            "abstract": "The rapid advancement and increasing complexity of pretrained models, exemplified by CLIP, offer significant opportunities as well as challenges for Federated Learning (FL), a critical component of privacy-preserving artificial intelligence. This research delves into the intricacies of integrating large foundation models like CLIP within FL frameworks to enhance privacy, efficiency, and adaptability across heterogeneous data landscapes. It specifically addresses the challenges posed by non-IID data distributions, the computational and communication overheads of leveraging such complex models, and the skewed representation of classes within datasets. We propose TriplePlay, a framework that integrates CLIP as an adapter to enhance FL's adaptability and performance across diverse data distributions. This approach addresses the long-tail distribution challenge to ensure fairness while reducing resource demands through quantization and low-rank adaptation techniques.Our simulation results demonstrate that TriplePlay effectively decreases GPU usage costs and speeds up the learning process, achieving convergence with reduced communication overhead.",
            "id": "2409.05347",
            "link": "http://arxiv.org/abs/2409.05347v1",
            "published": "2024-09-09T06:04:42+00:00",
            "updated": "2024-09-09T06:04:42+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.05370": {
            "authors": [
                "Yingshu Li",
                "Zhanyu Wang",
                "Yunyi Liu",
                "Lei Wang",
                "Lingqiao Liu",
                "Luping Zhou"
            ],
            "title": "KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models",
            "abstract": "Harnessing the robust capabilities of Large Language Models (LLMs) for narrative generation, logical reasoning, and common-sense knowledge integration, this study delves into utilizing LLMs to enhance automated radiology report generation (R2Gen). Despite the wealth of knowledge within LLMs, efficiently triggering relevant knowledge within these large models for specific tasks like R2Gen poses a critical research challenge. This paper presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration framework based on LLMs. Utilizing a frozen LLM to generate reports, the framework integrates a knowledge graph to unlock chest disease-related knowledge within the LLM to enhance the clinical utility of generated reports. This is achieved by leveraging the knowledge graph to distill disease-related features in a designed way. Since a radiology report encompasses both normal and disease-related findings, the extracted graph-enhanced disease-related features are integrated with regional image features, attending to both aspects. We explore two fusion methods to automatically prioritize and select the most relevant features. The fused features are employed by LLM to generate reports that are more sensitive to diseases and of improved quality. Our approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.",
            "id": "2409.05370",
            "link": "http://arxiv.org/abs/2409.05370v1",
            "published": "2024-09-09T06:57:22+00:00",
            "updated": "2024-09-09T06:57:22+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 31
        },
        "2409.05395": {
            "authors": [
                "Georgios Pantazopoulos",
                "Malvina Nikandrou",
                "Alessandro Suglia",
                "Oliver Lemon",
                "Arash Eshghi"
            ],
            "title": "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling",
            "abstract": "This study explores replacing Transformers in Visual Language Models (VLMs) with Mamba, a recent structured state space model (SSM) that demonstrates promising performance in sequence modeling. We test models up to 3B parameters under controlled conditions, showing that Mamba-based VLMs outperforms Transformers-based VLMs in captioning, question answering, and reading comprehension. However, we find that Transformers achieve greater performance in visual grounding and the performance gap widens with scale. We explore two hypotheses to explain this phenomenon: 1) the effect of task-agnostic visual encoding on the updates of the hidden states, and 2) the difficulty in performing visual grounding from the perspective of in-context multimodal retrieval. Our results indicate that a task-aware encoding yields minimal performance gains on grounding, however, Transformers significantly outperform Mamba at in-context multimodal retrieval. Overall, Mamba shows promising performance on tasks where the correct output relies on a summary of the image but struggles when retrieval of explicit information from the context is required.",
            "id": "2409.05395",
            "link": "http://arxiv.org/abs/2409.05395v1",
            "published": "2024-09-09T07:49:09+00:00",
            "updated": "2024-09-09T07:49:09+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 44
        },
        "2409.05414": {
            "authors": [
                "Xin Zhao",
                "Xiaojun Chen",
                "Xudong Chen",
                "He Li",
                "Tingyu Fan",
                "Zhendong Zhao"
            ],
            "title": "CipherDM: Secure Three-Party Inference for Diffusion Model Sampling",
            "abstract": "Diffusion Models (DMs) achieve state-of-the-art synthesis results in image generation and have been applied to various fields. However, DMs sometimes seriously violate user privacy during usage, making the protection of privacy an urgent issue. Using traditional privacy computing schemes like Secure Multi-Party Computation (MPC) directly in DMs faces significant computation and communication challenges. To address these issues, we propose CipherDM, the first novel, versatile and universal framework applying MPC technology to DMs for secure sampling, which can be widely implemented on multiple DM based tasks. We thoroughly analyze sampling latency breakdown, find time-consuming parts and design corresponding secure MPC protocols for computing nonlinear activations including SoftMax, SiLU and Mish. CipherDM is evaluated on popular architectures (DDPM, DDIM) using MNIST dataset and on SD deployed by diffusers. Compared to direct implementation on SPU, our approach improves running time by approximately 1.084\\times \\sim 2.328\\times, and reduces communication costs by approximately 1.212\\times \\sim 1.791\\times.",
            "id": "2409.05414",
            "link": "http://arxiv.org/abs/2409.05414v1",
            "published": "2024-09-09T08:16:17+00:00",
            "updated": "2024-09-09T08:16:17+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 36
        },
        "2409.05433": {
            "authors": [
                "Jianshu Hu",
                "Paul Weng",
                "Yutong Ban"
            ],
            "title": "State-Novelty Guided Action Persistence in Deep Reinforcement Learning",
            "abstract": "While a powerful and promising approach, deep reinforcement learning (DRL) still suffers from sample inefficiency, which can be notably improved by resorting to more sophisticated techniques to address the exploration-exploitation dilemma. One such technique relies on action persistence (i.e., repeating an action over multiple steps). However, previous work exploiting action persistence either applies a fixed strategy or learns additional value functions (or policy) for selecting the repetition number. In this paper, we propose a novel method to dynamically adjust the action persistence based on the current exploration status of the state space. In such a way, our method does not require training of additional value functions or policy. Moreover, the use of a smooth scheduling of the repeat probability allows a more effective balance between exploration and exploitation. Furthermore, our method can be seamlessly integrated into various basic exploration strategies to incorporate temporal persistence. Finally, extensive experiments on different DMControl tasks demonstrate that our state-novelty guided action persistence method significantly improves the sample efficiency.",
            "id": "2409.05433",
            "link": "http://arxiv.org/abs/2409.05433v1",
            "published": "2024-09-09T08:34:22+00:00",
            "updated": "2024-09-09T08:34:22+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.05457": {
            "authors": [
                "Martin N\u00f6llenburg",
                "Christian Pirker",
                "Anna Rapberger",
                "Stefan Woltran",
                "Jules Wulms"
            ],
            "title": "Visualizing Extensions of Argumentation Frameworks as Layered Graphs",
            "abstract": "The visualization of argumentation frameworks (AFs) is crucial for enabling a wide applicability of argumentative tools. However, their visualization is often considered only as an accompanying part of tools for computing semantics and standard graphical representations are used. We introduce a new visualization technique that draws an AF, together with an extension (as part of the input), as a 3-layer graph layout. Our technique supports the user to more easily explore the visualized AF, better understand extensions, and verify algorithms for computing semantics. To optimize the visual clarity and aesthetics of this layout, we propose to minimize edge crossings in our 3-layer drawing. We do so by an exact ILP-based approach, but also propose a fast heuristic pipeline. Via a quantitative evaluation, we show that the heuristic is feasible even for large instances, while producing at most twice as many crossings as an optimal drawing in most cases.",
            "id": "2409.05457",
            "link": "http://arxiv.org/abs/2409.05457v1",
            "published": "2024-09-09T09:29:53+00:00",
            "updated": "2024-09-09T09:29:53+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "max_author_hindex": 37
        },
        "2409.05466": {
            "authors": [
                "Junkun Chen",
                "Jilin Mei",
                "Liang Chen",
                "Fangzhou Zhao",
                "Yu Hu"
            ],
            "title": "Proto-OOD: Enhancing OOD Object Detection with Prototype Feature Similarity",
            "abstract": "The limited training samples for object detectors commonly result in low accuracy out-of-distribution (OOD) object detection. We have observed that feature vectors of the same class tend to cluster tightly in feature space, whereas those of different classes are more scattered. This insight motivates us to leverage feature similarity for OOD detection. Drawing on the concept of prototypes prevalent in few-shot learning, we introduce a novel network architecture, Proto-OOD, designed for this purpose. Proto-OOD enhances prototype representativeness through contrastive loss and identifies OOD data by assessing the similarity between input features and prototypes. It employs a negative embedding generator to create negative embedding, which are then used to train the similarity module. Proto-OOD achieves significantly lower FPR95 in MS-COCO dataset and higher mAP for Pascal VOC dataset, when utilizing Pascal VOC as ID dataset and MS-COCO as OOD dataset. Additionally, we identify limitations in existing evaluation metrics and propose an enhanced evaluation protocol.",
            "id": "2409.05466",
            "link": "http://arxiv.org/abs/2409.05466v1",
            "published": "2024-09-09T09:48:27+00:00",
            "updated": "2024-09-09T09:48:27+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 20
        },
        "2409.05484": {
            "authors": [
                "Seungheun Baek",
                "Soyon Park",
                "Yan Ting Chok",
                "Junhyun Lee",
                "Jueon Park",
                "Mogan Gim",
                "Jaewoo Kang"
            ],
            "title": "CRADLE-VAE: Enhancing Single-Cell Gene Perturbation Modeling with Counterfactual Reasoning-based Artifact Disentanglement",
            "abstract": "Predicting cellular responses to various perturbations is a critical focus in drug discovery and personalized therapeutics, with deep learning models playing a significant role in this endeavor. Single-cell datasets contain technical artifacts that may hinder the predictability of such models, which poses quality control issues highly regarded in this area. To address this, we propose CRADLE-VAE, a causal generative framework tailored for single-cell gene perturbation modeling, enhanced with counterfactual reasoning-based artifact disentanglement. Throughout training, CRADLE-VAE models the underlying latent distribution of technical artifacts and perturbation effects present in single-cell datasets. It employs counterfactual reasoning to effectively disentangle such artifacts by modulating the latent basal spaces and learns robust features for generating cellular response data with improved quality. Experimental results demonstrate that this approach improves not only treatment effect estimation performance but also generative quality as well. The CRADLE-VAE codebase is publicly available at https://github.com/dmis-lab/CRADLE-VAE.",
            "id": "2409.05484",
            "link": "http://arxiv.org/abs/2409.05484v2",
            "published": "2024-09-09T10:29:28+00:00",
            "updated": "2024-09-10T02:47:49+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "q-bio.GN",
                "q-bio.QM"
            ],
            "max_author_hindex": 43
        },
        "2409.05495": {
            "authors": [
                "Michael Kampouridis",
                "Nikolaos Vastardis",
                "George Rayment"
            ],
            "title": "Using machine learning for fault detection in lighthouse light sensors",
            "abstract": "Lighthouses play a crucial role in ensuring maritime safety by signaling hazardous areas such as dangerous coastlines, shoals, reefs, and rocks, along with aiding harbor entries and aerial navigation. This is achieved through the use of photoresistor sensors that activate or deactivate based on the time of day. However, a significant issue is the potential malfunction of these sensors, leading to the gradual misalignment of the light's operational timing. This paper introduces an innovative machine learning-based approach for automatically detecting such malfunctions. We evaluate four distinct algorithms: decision trees, random forest, extreme gradient boosting, and multi-layer perceptron. Our findings indicate that the multi-layer perceptron is the most effective, capable of detecting timing discrepancies as small as 10-15 minutes. This accuracy makes it a highly efficient tool for automating the detection of faults in lighthouse light sensors.",
            "id": "2409.05495",
            "link": "http://arxiv.org/abs/2409.05495v1",
            "published": "2024-09-09T10:47:41+00:00",
            "updated": "2024-09-09T10:47:41+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.05524": {
            "authors": [
                "Marco Baioletti",
                "Francesco Santini"
            ],
            "title": "An encoding of argumentation problems using quadratic unconstrained binary optimization",
            "abstract": "In this paper, we develop a way to encode several NP-Complete problems in Abstract Argumentation to Quadratic Unconstrained Binary Optimization (QUBO) problems. In this form, a solution for a QUBO problem involves minimizing a quadratic function over binary variables (0/1), where the coefficients can be represented by a symmetric square matrix (or an equivalent upper triangular version). With the QUBO formulation, exploiting new computing architectures, such as Quantum and Digital Annealers, is possible. A more conventional approach consists of developing approximate solvers, which, in this case, are used to tackle the intrinsic complexity. We performed tests to prove the correctness and applicability of classical problems in Argumentation and enforcement of argument sets. We compared our approach to two other approximate solvers in the literature during tests. In the final experimentation, we used a Simulated Annealing algorithm on a local machine. Also, we tested a Quantum Annealer from the D-Wave Ocean SDK and the Leap Quantum Cloud Service.",
            "id": "2409.05524",
            "link": "http://arxiv.org/abs/2409.05524v1",
            "published": "2024-09-09T11:29:46+00:00",
            "updated": "2024-09-09T11:29:46+00:00",
            "primary_category": "quant-ph",
            "categories": [
                "quant-ph",
                "cs.AI"
            ],
            "max_author_hindex": 0
        },
        "2409.05531": {
            "authors": [
                "Dianbo Ma",
                "Kousuke Imamura",
                "Ziyan Gao",
                "Xiangjie Wang",
                "Satoshi Yamane"
            ],
            "title": "HMAFlow: Learning More Accurate Optical Flow via Hierarchical Motion Field Alignment",
            "abstract": "Optical flow estimation is a fundamental and long-standing visual task. In this work, we present a novel method, dubbed HMAFlow, to improve optical flow estimation in these tough scenes, especially with small objects. The proposed model mainly consists of two core components: a Hierarchical Motion Field Alignment (HMA) module and a Correlation Self-Attention (CSA) module. In addition, we rebuild 4D cost volumes by employing a Multi-Scale Correlation Search (MCS) layer and replacing average pooling in common cost volumes with an search strategy using multiple search ranges. Experimental results demonstrate that our model achieves the best generalization performance in comparison to other state-of-the-art methods. Specifically, compared with RAFT, our method achieves relative error reductions of 14.2% and 3.4% on the clean pass and final pass of the Sintel online benchmark, respectively. On the KITTI test benchmark, HMAFlow surpasses RAFT and GMA in the Fl-all metric by a relative margin of 6.8% and 7.7%, respectively. To facilitate future research, our code will be made available at https://github.com/BooTurbo/HMAFlow.",
            "id": "2409.05531",
            "link": "http://arxiv.org/abs/2409.05531v1",
            "published": "2024-09-09T11:43:35+00:00",
            "updated": "2024-09-09T11:43:35+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.05558": {
            "authors": [
                "Yahya Jabary",
                "Andreas Plesner",
                "Turlan Kuzhagaliyev",
                "Roger Wattenhofer"
            ],
            "title": "Seeing Through the Mask: Rethinking Adversarial Examples for CAPTCHAs",
            "abstract": "Modern CAPTCHAs rely heavily on vision tasks that are supposedly hard for computers but easy for humans. However, advances in image recognition models pose a significant threat to such CAPTCHAs. These models can easily be fooled by generating some well-hidden \"random\" noise and adding it to the image, or hiding objects in the image. However, these methods are model-specific and thus can not aid CAPTCHAs in fooling all models. We show in this work that by allowing for more significant changes to the images while preserving the semantic information and keeping it solvable by humans, we can fool many state-of-the-art models. Specifically, we demonstrate that by adding masks of various intensities the Accuracy @ 1 (Acc@1) drops by more than 50%-points for all models, and supposedly robust models such as vision transformers see an Acc@1 drop of 80%-points.   These masks can therefore effectively fool modern image classifiers, thus showing that machines have not caught up with humans -- yet.",
            "id": "2409.05558",
            "link": "http://arxiv.org/abs/2409.05558v1",
            "published": "2024-09-09T12:29:53+00:00",
            "updated": "2024-09-09T12:29:53+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 79
        },
        "2409.05585": {
            "authors": [
                "Wei Peng",
                "Tian Xia",
                "Fabio De Sousa Ribeiro",
                "Tomas Bosschieter",
                "Ehsan Adeli",
                "Qingyu Zhao",
                "Ben Glocker",
                "Kilian M. Pohl"
            ],
            "title": "Latent 3D Brain MRI Counterfactual",
            "abstract": "The number of samples in structural brain MRI studies is often too small to properly train deep learning models. Generative models show promise in addressing this issue by effectively learning the data distribution and generating high-fidelity MRI. However, they struggle to produce diverse, high-quality data outside the distribution defined by the training data. One way to address the issue is using causal models developed for 3D volume counterfactuals. However, accurately modeling causality in high-dimensional spaces is a challenge so that these models generally generate 3D brain MRIS of lower quality. To address these challenges, we propose a two-stage method that constructs a Structural Causal Model (SCM) within the latent space. In the first stage, we employ a VQ-VAE to learn a compact embedding of the MRI volume. Subsequently, we integrate our causal model into this latent space and execute a three-step counterfactual procedure using a closed-form Generalized Linear Model (GLM). Our experiments conducted on real-world high-resolution MRI data (1mm) demonstrate that our method can generate high-quality 3D MRI counterfactuals.",
            "id": "2409.05585",
            "link": "http://arxiv.org/abs/2409.05585v1",
            "published": "2024-09-09T13:15:03+00:00",
            "updated": "2024-09-09T13:15:03+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 57
        },
        "2409.05586": {
            "authors": [
                "Arda Sarp Yenicesu",
                "Sepehr Nourmohammadi",
                "Berk Cicek",
                "Ozgur S. Oguz"
            ],
            "title": "Interpretable Responsibility Sharing as a Heuristic for Task and Motion Planning",
            "abstract": "This article introduces a novel heuristic for Task and Motion Planning (TAMP) named Interpretable Responsibility Sharing (IRS), which enhances planning efficiency in domestic robots by leveraging human-constructed environments and inherent biases. Utilizing auxiliary objects (e.g., trays and pitchers), which are commonly found in household settings, IRS systematically incorporates these elements to simplify and optimize task execution. The heuristic is rooted in the novel concept of Responsibility Sharing (RS), where auxiliary objects share the task's responsibility with the embodied agent, dividing complex tasks into manageable sub-problems. This division not only reflects human usage patterns but also aids robots in navigating and manipulating within human spaces more effectively. By integrating Optimized Rule Synthesis (ORS) for decision-making, IRS ensures that the use of auxiliary objects is both strategic and context-aware, thereby improving the interpretability and effectiveness of robotic planning. Experiments conducted across various household tasks demonstrate that IRS significantly outperforms traditional methods by reducing the effort required in task execution and enhancing the overall decision-making process. This approach not only aligns with human intuitive methods but also offers a scalable solution adaptable to diverse domestic environments. Code is available at https://github.com/asyncs/IRS.",
            "id": "2409.05586",
            "link": "http://arxiv.org/abs/2409.05586v1",
            "published": "2024-09-09T13:15:53+00:00",
            "updated": "2024-09-09T13:15:53+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 11
        },
        "2409.05595": {
            "authors": [
                "Haoyu Zhang",
                "Raghavendra Ramachandra",
                "Kiran Raja",
                "Christoph Busch"
            ],
            "title": "SynMorph: Generating Synthetic Face Morphing Dataset with Mated Samples",
            "abstract": "Face morphing attack detection (MAD) algorithms have become essential to overcome the vulnerability of face recognition systems. To solve the lack of large-scale and public-available datasets due to privacy concerns and restrictions, in this work we propose a new method to generate a synthetic face morphing dataset with 2450 identities and more than 100k morphs. The proposed synthetic face morphing dataset is unique for its high-quality samples, different types of morphing algorithms, and the generalization for both single and differential morphing attack detection algorithms. For experiments, we apply face image quality assessment and vulnerability analysis to evaluate the proposed synthetic face morphing dataset from the perspective of biometric sample quality and morphing attack potential on face recognition systems. The results are benchmarked with an existing SOTA synthetic dataset and a representative non-synthetic and indicate improvement compared with the SOTA. Additionally, we design different protocols and study the applicability of using the proposed synthetic dataset on training morphing attack detection algorithms.",
            "id": "2409.05595",
            "link": "http://arxiv.org/abs/2409.05595v1",
            "published": "2024-09-09T13:29:53+00:00",
            "updated": "2024-09-09T13:29:53+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 54
        },
        "2409.05620": {
            "authors": [
                "Shuai Wang",
                "Yibing Zhan",
                "Yong Luo",
                "Han Hu",
                "Wei Yu",
                "Yonggang Wen",
                "Dacheng Tao"
            ],
            "title": "Joint Input and Output Coordination for Class-Incremental Learning",
            "abstract": "Incremental learning is nontrivial due to severe catastrophic forgetting. Although storing a small amount of data on old tasks during incremental learning is a feasible solution, current strategies still do not 1) adequately address the class bias problem, and 2) alleviate the mutual interference between new and old tasks, and 3) consider the problem of class bias within tasks. This motivates us to propose a joint input and output coordination (JIOC) mechanism to address these issues. This mechanism assigns different weights to different categories of data according to the gradient of the output score, and uses knowledge distillation (KD) to reduce the mutual interference between the outputs of old and new tasks. The proposed mechanism is general and flexible, and can be incorporated into different incremental learning approaches that use memory storage. Extensive experiments show that our mechanism can significantly improve their performance.",
            "id": "2409.05620",
            "link": "http://arxiv.org/abs/2409.05620v1",
            "published": "2024-09-09T13:55:07+00:00",
            "updated": "2024-09-09T13:55:07+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 153
        },
        "2409.05636": {
            "authors": [
                "Grace Colverd",
                "Jumpei Takami",
                "Laura Schade",
                "Karol Bot",
                "Joseph A. Gallego-Mejia"
            ],
            "title": "3D-SAR Tomography and Machine Learning for High-Resolution Tree Height Estimation",
            "abstract": "Accurately estimating forest biomass is crucial for global carbon cycle modelling and climate change mitigation. Tree height, a key factor in biomass calculations, can be measured using Synthetic Aperture Radar (SAR) technology. This study applies machine learning to extract forest height data from two SAR products: Single Look Complex (SLC) images and tomographic cubes, in preparation for the ESA Biomass Satellite mission. We use the TomoSense dataset, containing SAR and LiDAR data from Germany's Eifel National Park, to develop and evaluate height estimation models. Our approach includes classical methods, deep learning with a 3D U-Net, and Bayesian-optimized techniques. By testing various SAR frequencies and polarimetries, we establish a baseline for future height and biomass modelling. Best-performing models predict forest height to be within 2.82m mean absolute error for canopies around 30m, advancing our ability to measure global carbon stocks and support climate action.",
            "id": "2409.05636",
            "link": "http://arxiv.org/abs/2409.05636v1",
            "published": "2024-09-09T14:07:38+00:00",
            "updated": "2024-09-09T14:07:38+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 8
        },
        "2409.05731": {
            "authors": [
                "Robert Kaufman",
                "Aaron Broukhim",
                "David Kirsh",
                "Nadir Weibel"
            ],
            "title": "What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence",
            "abstract": "Explanations for autonomous vehicle (AV) decisions may build trust, however, explanations can contain errors. In a simulated driving study (n = 232), we tested how AV explanation errors, driving context characteristics (perceived harm and driving difficulty), and personal traits (prior trust and expertise) affected a passenger's comfort in relying on an AV, preference for control, confidence in the AV's ability, and explanation satisfaction. Errors negatively affected all outcomes. Surprisingly, despite identical driving, explanation errors reduced ratings of the AV's driving ability. Severity and potential harm amplified the negative impact of errors. Contextual harm and driving difficulty directly impacted outcome ratings and influenced the relationship between errors and outcomes. Prior trust and expertise were positively associated with outcome ratings. Results emphasize the need for accurate, contextually adaptive, and personalized AV explanations to foster trust, reliance, satisfaction, and confidence. We conclude with design, research, and deployment recommendations for trustworthy AV explanation systems.",
            "id": "2409.05731",
            "link": "http://arxiv.org/abs/2409.05731v2",
            "published": "2024-09-09T15:41:53+00:00",
            "updated": "2024-09-10T16:25:58+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "max_author_hindex": 33
        },
        "2409.05749": {
            "authors": [
                "Safwen Naimi",
                "Wassim Bouachir",
                "Guillaume-Alexandre Bilodeau"
            ],
            "title": "ReL-SAR: Representation Learning for Skeleton Action Recognition with Convolutional Transformers and BYOL",
            "abstract": "To extract robust and generalizable skeleton action recognition features, large amounts of well-curated data are typically required, which is a challenging task hindered by annotation and computation costs. Therefore, unsupervised representation learning is of prime importance to leverage unlabeled skeleton data. In this work, we investigate unsupervised representation learning for skeleton action recognition. For this purpose, we designed a lightweight convolutional transformer framework, named ReL-SAR, exploiting the complementarity of convolutional and attention layers for jointly modeling spatial and temporal cues in skeleton sequences. We also use a Selection-Permutation strategy for skeleton joints to ensure more informative descriptions from skeletal data. Finally, we capitalize on Bootstrap Your Own Latent (BYOL) to learn robust representations from unlabeled skeleton sequence data. We achieved very competitive results on limited-size datasets: MCAD, IXMAS, JHMDB, and NW-UCLA, showing the effectiveness of our proposed method against state-of-the-art methods in terms of both performance and computational efficiency. To ensure reproducibility and reusability, the source code including all implementation parameters is provided at: https://github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL",
            "id": "2409.05749",
            "link": "http://arxiv.org/abs/2409.05749v1",
            "published": "2024-09-09T16:03:26+00:00",
            "updated": "2024-09-09T16:03:26+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 23
        },
        "2409.05773": {
            "authors": [
                "Ross Greer",
                "Laura Fleig",
                "Shlomo Dubnov"
            ],
            "title": "Creativity and Visual Communication from Machine to Musician: Sharing a Score through a Robotic Camera",
            "abstract": "This paper explores the integration of visual communication and musical interaction by implementing a robotic camera within a \"Guided Harmony\" musical game. We aim to examine co-creative behaviors between human musicians and robotic systems. Our research explores existing methodologies like improvisational game pieces and extends these concepts to include robotic participation using a PTZ camera. The robotic system interprets and responds to nonverbal cues from musicians, creating a collaborative and adaptive musical experience. This initial case study underscores the importance of intuitive visual communication channels. We also propose future research directions, including parameters for refining the visual cue toolkit and data collection methods to understand human-machine co-creativity further. Our findings contribute to the broader understanding of machine intelligence in augmenting human creativity, particularly in musical settings.",
            "id": "2409.05773",
            "link": "http://arxiv.org/abs/2409.05773v1",
            "published": "2024-09-09T16:34:36+00:00",
            "updated": "2024-09-09T16:34:36+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.AI",
                "cs.CV",
                "cs.RO"
            ],
            "max_author_hindex": 34
        },
        "2409.05786": {
            "authors": [
                "Bikram Boote",
                "Anh Thai",
                "Wenqi Jia",
                "Ozgur Kara",
                "Stefan Stojanov",
                "James M. Rehg",
                "Sangmin Lee"
            ],
            "title": "Leveraging Object Priors for Point Tracking",
            "abstract": "Point tracking is a fundamental problem in computer vision with numerous applications in AR and robotics. A common failure mode in long-term point tracking occurs when the predicted point leaves the object it belongs to and lands on the background or another object. We identify this as the failure to correctly capture objectness properties in learning to track. To address this limitation of prior work, we propose a novel objectness regularization approach that guides points to be aware of object priors by forcing them to stay inside the the boundaries of object instances. By capturing objectness cues at training time, we avoid the need to compute object masks during testing. In addition, we leverage contextual attention to enhance the feature representation for capturing objectness at the feature level more effectively. As a result, our approach achieves state-of-the-art performance on three point tracking benchmarks, and we further validate the effectiveness of our components via ablation studies. The source code is available at: https://github.com/RehgLab/tracking_objectness",
            "id": "2409.05786",
            "link": "http://arxiv.org/abs/2409.05786v1",
            "published": "2024-09-09T16:48:42+00:00",
            "updated": "2024-09-09T16:48:42+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.RO"
            ],
            "max_author_hindex": 80
        },
        "2409.05798": {
            "authors": [
                "Shen Li",
                "Yuyang Zhang",
                "Zhaolin Ren",
                "Claire Liang",
                "Na Li",
                "Julie A. Shah"
            ],
            "title": "Enhancing Preference-based Linear Bandits via Human Response Time",
            "abstract": "Binary human choice feedback is widely used in interactive preference learning for its simplicity, but it provides limited information about preference strength. To overcome this limitation, we leverage human response times, which inversely correlate with preference strength, as complementary information. Our work integrates the EZ-diffusion model, which jointly models human choices and response times, into preference-based linear bandits. We introduce a computationally efficient utility estimator that reformulates the utility estimation problem using both choices and response times as a linear regression problem. Theoretical and empirical comparisons with traditional choice-only estimators reveal that for queries with strong preferences (\"easy\" queries), choices alone provide limited information, while response times offer valuable complementary information about preference strength. As a result, incorporating response times makes easy queries more useful. We demonstrate this advantage in the fixed-budget best-arm identification problem, with simulations based on three real-world datasets, consistently showing accelerated learning when response times are incorporated.",
            "id": "2409.05798",
            "link": "http://arxiv.org/abs/2409.05798v1",
            "published": "2024-09-09T17:02:47+00:00",
            "updated": "2024-09-09T17:02:47+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.HC",
                "econ.EM",
                "stat.ML"
            ],
            "max_author_hindex": 44
        },
        "2409.05863": {
            "authors": [
                "Shuhan Tan",
                "Boris Ivanovic",
                "Yuxiao Chen",
                "Boyi Li",
                "Xinshuo Weng",
                "Yulong Cao",
                "Philipp Kr\u00e4henb\u00fchl",
                "Marco Pavone"
            ],
            "title": "Promptable Closed-loop Traffic Simulation",
            "abstract": "Simulation stands as a cornerstone for safe and efficient autonomous driving development. At its core a simulation system ought to produce realistic, reactive, and controllable traffic patterns. In this paper, we propose ProSim, a multimodal promptable closed-loop traffic simulation framework. ProSim allows the user to give a complex set of numerical, categorical or textual prompts to instruct each agent's behavior and intention. ProSim then rolls out a traffic scenario in a closed-loop manner, modeling each agent's interaction with other traffic participants. Our experiments show that ProSim achieves high prompt controllability given different user prompts, while reaching competitive performance on the Waymo Sim Agents Challenge when no prompt is given. To support research on promptable traffic simulation, we create ProSim-Instruct-520k, a multimodal prompt-scenario paired driving dataset with over 10M text prompts for over 520k real-world driving scenarios. We will release code of ProSim as well as data and labeling tools of ProSim-Instruct-520k at https://ariostgx.github.io/ProSim.",
            "id": "2409.05863",
            "link": "http://arxiv.org/abs/2409.05863v1",
            "published": "2024-09-09T17:59:15+00:00",
            "updated": "2024-09-09T17:59:15+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 61
        },
        "2409.05864": {
            "authors": [
                "Murtaza Dalal",
                "Jiahui Yang",
                "Russell Mendonca",
                "Youssef Khaky",
                "Ruslan Salakhutdinov",
                "Deepak Pathak"
            ],
            "title": "Neural MP: A Generalist Neural Motion Planner",
            "abstract": "The current paradigm for motion planning generates solutions from scratch for every new problem, which consumes significant amounts of time and computational resources. For complex, cluttered scenes, motion planning approaches can often take minutes to produce a solution, while humans are able to accurately and safely reach any goal in seconds by leveraging their prior experience. We seek to do the same by applying data-driven learning at scale to the problem of motion planning. Our approach builds a large number of complex scenes in simulation, collects expert data from a motion planner, then distills it into a reactive generalist policy. We then combine this with lightweight optimization to obtain a safe path for real world deployment. We perform a thorough evaluation of our method on 64 motion planning tasks across four diverse environments with randomized poses, scenes and obstacles, in the real world, demonstrating an improvement of 23%, 17% and 79% motion planning success rate over state of the art sampling, optimization and learning based planning methods. Video results available at mihdalal.github.io/neuralmotionplanner",
            "id": "2409.05864",
            "link": "http://arxiv.org/abs/2409.05864v1",
            "published": "2024-09-09T17:59:45+00:00",
            "updated": "2024-09-09T17:59:45+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ],
            "max_author_hindex": 113
        },
        "2409.05938": {
            "authors": [
                "Condy Bao",
                "Fuxiao Liu"
            ],
            "title": "DeepFM-Crispr: Prediction of CRISPR On-Target Effects via Deep Learning",
            "abstract": "Since the advent of CRISPR-Cas9, a groundbreaking gene-editing technology that enables precise genomic modifications via a short RNA guide sequence, there has been a marked increase in the accessibility and application of this technology across various fields. The success of CRISPR-Cas9 has spurred further investment and led to the discovery of additional CRISPR systems, including CRISPR-Cas13. Distinct from Cas9, which targets DNA, Cas13 targets RNA, offering unique advantages for gene modulation. We focus on Cas13d, a variant known for its collateral activity where it non-specifically cleaves adjacent RNA molecules upon activation, a feature critical to its function. We introduce DeepFM-Crispr, a novel deep learning model developed to predict the on-target efficiency and evaluate the off-target effects of Cas13d. This model harnesses a large language model to generate comprehensive representations rich in evolutionary and structural data, thereby enhancing predictions of RNA secondary structures and overall sgRNA efficacy. A transformer-based architecture processes these inputs to produce a predictive efficacy score. Comparative experiments show that DeepFM-Crispr not only surpasses traditional models but also outperforms recent state-of-the-art deep learning methods in terms of prediction accuracy and reliability.",
            "id": "2409.05938",
            "link": "http://arxiv.org/abs/2409.05938v1",
            "published": "2024-09-09T17:33:54+00:00",
            "updated": "2024-09-09T17:33:54+00:00",
            "primary_category": "q-bio.QM",
            "categories": [
                "q-bio.QM",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 12
        },
        "2409.06130": {
            "authors": [
                "Aoting Hu",
                "Yanzhi Chen",
                "Renjie Xie",
                "Adrian Weller"
            ],
            "title": "On the Weaknesses of Backdoor-based Model Watermarking: An Information-theoretic Perspective",
            "abstract": "Safeguarding the intellectual property of machine learning models has emerged as a pressing concern in AI security. Model watermarking is a powerful technique for protecting ownership of machine learning models, yet its reliability has been recently challenged by recent watermark removal attacks. In this work, we investigate why existing watermark embedding techniques particularly those based on backdooring are vulnerable. Through an information-theoretic analysis, we show that the resilience of watermarking against erasure attacks hinges on the choice of trigger-set samples, where current uses of out-distribution trigger-set are inherently vulnerable to white-box adversaries. Based on this discovery, we propose a novel model watermarking scheme, In-distribution Watermark Embedding (IWE), to overcome the limitations of existing method. To further minimise the gap to clean models, we analyze the role of logits as watermark information carriers and propose a new approach to better conceal watermark information within the logits. Experiments on real-world datasets including CIFAR-100 and Caltech-101 demonstrate that our method robustly defends against various adversaries with negligible accuracy loss (< 0.1%).",
            "id": "2409.06130",
            "link": "http://arxiv.org/abs/2409.06130v1",
            "published": "2024-09-10T00:55:21+00:00",
            "updated": "2024-09-10T00:55:21+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 41
        },
        "2409.06209": {
            "authors": [
                "Xin Zhang",
                "Deval Mehta",
                "Yanan Hu",
                "Chao Zhu",
                "David Darby",
                "Zhen Yu",
                "Daniel Merlo",
                "Melissa Gresle",
                "Anneke Van Der Walt",
                "Helmut Butzkueven",
                "Zongyuan Ge"
            ],
            "title": "Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis",
            "abstract": "Survival analysis holds a crucial role across diverse disciplines, such as economics, engineering and healthcare. It empowers researchers to analyze both time-invariant and time-varying data, encompassing phenomena like customer churn, material degradation and various medical outcomes. Given the complexity and heterogeneity of such data, recent endeavors have demonstrated successful integration of deep learning methodologies to address limitations in conventional statistical approaches. However, current methods typically involve cluttered probability distribution function (PDF), have lower sensitivity in censoring prediction, only model static datasets, or only rely on recurrent neural networks for dynamic modelling. In this paper, we propose a novel survival regression method capable of producing high-quality unimodal PDFs without any prior distribution assumption, by optimizing novel Margin-Mean-Variance loss and leveraging the flexibility of Transformer to handle both temporal and non-temporal data, coined UniSurv. Extensive experiments on several datasets demonstrate that UniSurv places a significantly higher emphasis on censoring compared to other methods.",
            "id": "2409.06209",
            "link": "http://arxiv.org/abs/2409.06209v1",
            "published": "2024-09-10T04:29:59+00:00",
            "updated": "2024-09-10T04:29:59+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 66
        },
        "2409.06214": {
            "authors": [
                "Jaewoo Kim",
                "Uehwan Kim"
            ],
            "title": "Towards Generalizable Scene Change Detection",
            "abstract": "Scene Change Detection (SCD) is vital for applications such as visual surveillance and mobile robotics. However, current SCD methods exhibit a bias to the temporal order of training datasets and limited performance on unseen domains; coventional SCD benchmarks are not able to evaluate generalization or temporal consistency. To tackle these limitations, we introduce a Generalizable Scene Change Detection Framework (GeSCF) in this work. The proposed GeSCF leverages localized semantics of a foundation model without any re-training or fine-tuning -- for generalization over unseen domains. Specifically, we design an adaptive thresholding of the similarity distribution derived from facets of the pre-trained foundation model to generate initial pseudo-change mask. We further utilize Segment Anything Model's (SAM) class-agnostic masks to refine pseudo-masks. Moreover, our proposed framework maintains commutative operations in all settings to ensure complete temporal consistency. Finally, we define new metrics, evaluation dataset, and evaluation protocol for Generalizable Scene Change Detection (GeSCD). Extensive experiments demonstrate that GeSCF excels across diverse and challenging environments -- establishing a new benchmark for SCD performance.",
            "id": "2409.06214",
            "link": "http://arxiv.org/abs/2409.06214v1",
            "published": "2024-09-10T04:45:25+00:00",
            "updated": "2024-09-10T04:45:25+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 10
        },
        "2409.06220": {
            "authors": [
                "Rashik Shahriar Akash",
                "Radiful Islam",
                "S. M. Saiful Islam Badhon",
                "K. S. M. Tozammel Hossain"
            ],
            "title": "CerviXpert: A Multi-Structural Convolutional Neural Network for Predicting Cervix Type and Cervical Cell Abnormalities",
            "abstract": "Cervical cancer affects millions of women worldwide and has a significantly higher survival rate when diagnosed early. Pap smears and cervical biopsies are vital screening tools for detecting such cancer. However, the success of these screening processes depends on the skills of cytologists. A recent trend in diagnostic cytology is to apply machine-learning-based models to classify cancer using cell images. These automated models have been shown to perform just as well as, or even better than, expert cytologists. Some notable methods for classifying cervix cancers include ResNet50, VGG16, MobileNetV2, and InceptionV3, based on deep convolutional neural networks (CNN). However, these methods are computationally expensive. We present CerviXpert, a multi-structural Convolutional Neural Network, to identify cervix cancer. We perform extensive experiments on a publicly available dataset, SiPaKMeD, to show the efficacy of our method. CerviXpert presents a promising solution for efficient cervical cancer screening and diagnosis by striking a balance between accuracy and practical feasibility.",
            "id": "2409.06220",
            "link": "http://arxiv.org/abs/2409.06220v1",
            "published": "2024-09-10T05:08:26+00:00",
            "updated": "2024-09-10T05:08:26+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 5
        },
        "2409.06241": {
            "authors": [
                "Hoang Anh Just",
                "Mahavir Dabas",
                "Lifu Huang",
                "Ming Jin",
                "Ruoxi Jia"
            ],
            "title": "DiPT: Enhancing LLM reasoning through diversified perspective-taking",
            "abstract": "Existing work on improving language model reasoning typically explores a single solution path, which can be prone to errors. Inspired by perspective-taking in social studies, this paper introduces DiPT, a novel approach that complements current reasoning methods by explicitly incorporating diversified viewpoints. This approach allows the model to gain a deeper understanding of the problem's context and identify the most effective solution path during the inference stage. Additionally, it provides a general data-centric AI recipe for augmenting existing data to improve their quality for fine-tuning.   Our empirical results demonstrate that DiPT can be flexibly integrated into existing methods that focus on a single reasoning approach, enhancing their reasoning performance and stability when presented with paraphrased problems. Furthermore, we illustrate improved context understanding by maintaining the model's safe outputs against \"jailbreaking\" prompts intentionally designed to bypass safeguards built into deployed models. Lastly, we show that fine-tuning with data enriched with diverse perspectives can boost the reasoning capabilities of the model compared to fine-tuning with raw data alone.",
            "id": "2409.06241",
            "link": "http://arxiv.org/abs/2409.06241v1",
            "published": "2024-09-10T06:17:27+00:00",
            "updated": "2024-09-10T06:17:27+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 28
        },
        "2409.06270": {
            "authors": [
                "Mulin Chen",
                "Haojian Huang",
                "Qiang Li"
            ],
            "title": "Towards Robust Uncertainty-Aware Incomplete Multi-View Classification",
            "abstract": "Handling incomplete data in multi-view classification is challenging, especially when traditional imputation methods introduce biases that compromise uncertainty estimation. Existing Evidential Deep Learning (EDL) based approaches attempt to address these issues, but they often struggle with conflicting evidence due to the limitations of the Dempster-Shafer combination rule, leading to unreliable decisions. To address these challenges, we propose the Alternating Progressive Learning Network (APLN), specifically designed to enhance EDL-based methods in incomplete MVC scenarios. Our approach mitigates bias from corrupted observed data by first applying coarse imputation, followed by mapping the data to a latent space. In this latent space, we progressively learn an evidence distribution aligned with the target domain, incorporating uncertainty considerations through EDL. Additionally, we introduce a conflict-aware Dempster-Shafer combination rule (DSCR) to better handle conflicting evidence. By sampling from the learned distribution, we optimize the latent representations of missing views, reducing bias and enhancing decision-making robustness. Extensive experiments demonstrate that APLN, combined with DSCR, significantly outperforms traditional methods, particularly in environments characterized by high uncertainty and conflicting evidence, establishing it as a promising solution for incomplete multi-view classification.",
            "id": "2409.06270",
            "link": "http://arxiv.org/abs/2409.06270v1",
            "published": "2024-09-10T07:18:57+00:00",
            "updated": "2024-09-10T07:18:57+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 51
        },
        "2409.06307": {
            "authors": [
                "Shuochen Gao",
                "Shun Lei",
                "Fan Zhuo",
                "Hangyu Liu",
                "Feng Liu",
                "Boshi Tang",
                "Qiaochu Huang",
                "Shiyin Kang",
                "Zhiyong Wu"
            ],
            "title": "An End-to-End Approach for Chord-Conditioned Song Generation",
            "abstract": "The Song Generation task aims to synthesize music composed of vocals and accompaniment from given lyrics. While the existing method, Jukebox, has explored this task, its constrained control over the generations often leads to deficiency in music performance. To mitigate the issue, we introduce an important concept from music composition, namely chords, to song generation networks. Chords form the foundation of accompaniment and provide vocal melody with associated harmony. Given the inaccuracy of automatic chord extractors, we devise a robust cross-attention mechanism augmented with dynamic weight sequence to integrate extracted chord information into song generations and reduce frame-level flaws, and propose a novel model termed Chord-Conditioned Song Generator (CSG) based on it. Experimental evidence demonstrates our proposed method outperforms other approaches in terms of musical performance and control precision of generated songs.",
            "id": "2409.06307",
            "link": "http://arxiv.org/abs/2409.06307v1",
            "published": "2024-09-10T08:07:43+00:00",
            "updated": "2024-09-10T08:07:43+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 43
        },
        "2409.06316": {
            "authors": [
                "Daniel Rose",
                "Oliver Wieder",
                "Thomas Seidel",
                "Thierry Langer"
            ],
            "title": "PharmacoMatch: Efficient 3D Pharmacophore Screening through Neural Subgraph Matching",
            "abstract": "The increasing size of screening libraries poses a significant challenge for the development of virtual screening methods for drug discovery, necessitating a re-evaluation of traditional approaches in the era of big data. Although 3D pharmacophore screening remains a prevalent technique, its application to very large datasets is limited by the computational cost associated with matching query pharmacophores to database ligands. In this study, we introduce PharmacoMatch, a novel contrastive learning approach based on neural subgraph matching. Our method reinterprets pharmacophore screening as an approximate subgraph matching problem and enables efficient querying of conformational databases by encoding query-target relationships in the embedding space. We conduct comprehensive evaluations of the learned representations and benchmark our method on virtual screening datasets in a zero-shot setting. Our findings demonstrate significantly shorter runtimes for pharmacophore matching, offering a promising speed-up for screening very large datasets.",
            "id": "2409.06316",
            "link": "http://arxiv.org/abs/2409.06316v1",
            "published": "2024-09-10T08:17:06+00:00",
            "updated": "2024-09-10T08:17:06+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "q-bio.QM"
            ],
            "max_author_hindex": 52
        },
        "2409.06336": {
            "authors": [
                "Antonin Sulc",
                "Thorsten Hellert",
                "Raimund Kammering",
                "Hayden Houscher",
                "Jason St. John"
            ],
            "title": "Towards Agentic AI on Particle Accelerators",
            "abstract": "As particle accelerators grow in complexity, traditional control methods face increasing challenges in achieving optimal performance. This paper envisions a paradigm shift: a decentralized multi-agent framework for accelerator control, powered by Large Language Models (LLMs) and distributed among autonomous agents. We present a proposition of a self-improving decentralized system where intelligent agents handle high-level tasks and communication and each agent is specialized control individual accelerator components.   This approach raises some questions: What are the future applications of AI in particle accelerators? How can we implement an autonomous complex system such as a particle accelerator where agents gradually improve through experience and human feedback? What are the implications of integrating a human-in-the-loop component for labeling operational data and providing expert guidance? We show two examples, where we demonstrate viability of such architecture.",
            "id": "2409.06336",
            "link": "http://arxiv.org/abs/2409.06336v1",
            "published": "2024-09-10T08:47:23+00:00",
            "updated": "2024-09-10T08:47:23+00:00",
            "primary_category": "physics.acc-ph",
            "categories": [
                "physics.acc-ph",
                "cs.AI"
            ],
            "max_author_hindex": 95
        },
        "2409.06343": {
            "authors": [
                "Seyed Mohammad Azimi-Abarghouyi",
                "Lav R. Varshney"
            ],
            "title": "Compute-Update Federated Learning: A Lattice Coding Approach",
            "abstract": "This paper introduces a federated learning framework that enables over-the-air computation via digital communications, using a new joint source-channel coding scheme. Without relying on channel state information at devices, this scheme employs lattice codes to both quantize model parameters and exploit interference from the devices. We propose a novel receiver structure at the server, designed to reliably decode an integer combination of the quantized model parameters as a lattice point for the purpose of aggregation. We present a mathematical approach to derive a convergence bound for the proposed scheme and offer design remarks. In this context, we suggest an aggregation metric and a corresponding algorithm to determine effective integer coefficients for the aggregation in each communication round. Our results illustrate that, regardless of channel dynamics and data heterogeneity, our scheme consistently delivers superior learning accuracy across various parameters and markedly surpasses other over-the-air methodologies.",
            "id": "2409.06343",
            "link": "http://arxiv.org/abs/2409.06343v1",
            "published": "2024-09-10T08:52:24+00:00",
            "updated": "2024-09-10T08:52:24+00:00",
            "primary_category": "cs.IT",
            "categories": [
                "cs.IT",
                "cs.AI",
                "cs.LG",
                "math.IT"
            ],
            "max_author_hindex": 31
        },
        "2409.06356": {
            "authors": [
                "Shreyas S R"
            ],
            "title": "Double Successive Over-Relaxation Q-Learning with an Extension to Deep Reinforcement Learning",
            "abstract": "Q-learning is a widely used algorithm in reinforcement learning (RL), but its convergence can be slow, especially when the discount factor is close to one. Successive Over-Relaxation (SOR) Q-learning, which introduces a relaxation factor to speed up convergence, addresses this issue but has two major limitations: In the tabular setting, the relaxation parameter depends on transition probability, making it not entirely model-free, and it suffers from overestimation bias. To overcome these limitations, we propose a sample-based, model-free double SOR Q-learning algorithm. Theoretically and empirically, this algorithm is shown to be less biased than SOR Q-learning. Further, in the tabular setting, the convergence analysis under boundedness assumptions on iterates is discussed. The proposed algorithm is extended to large-scale problems using deep RL. Finally, the tabular version of the proposed algorithm is compared using roulette and grid world environments, while the deep RL version is tested on a maximization bias example and OpenAI Gym environments.",
            "id": "2409.06356",
            "link": "http://arxiv.org/abs/2409.06356v1",
            "published": "2024-09-10T09:23:03+00:00",
            "updated": "2024-09-10T09:23:03+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 12
        },
        "2409.06362": {
            "authors": [
                "Teresa Dorszewski",
                "Lenka T\u011btkov\u00e1",
                "Lorenz Linhardt",
                "Lars Kai Hansen"
            ],
            "title": "Connecting Concept Convexity and Human-Machine Alignment in Deep Neural Networks",
            "abstract": "Understanding how neural networks align with human cognitive processes is a crucial step toward developing more interpretable and reliable AI systems. Motivated by theories of human cognition, this study examines the relationship between \\emph{convexity} in neural network representations and \\emph{human-machine alignment} based on behavioral data. We identify a correlation between these two dimensions in pretrained and fine-tuned vision transformer models. Our findings suggest that the convex regions formed in latent spaces of neural networks to some extent align with human-defined categories and reflect the similarity relations humans use in cognitive tasks. While optimizing for alignment generally enhances convexity, increasing convexity through fine-tuning yields inconsistent effects on alignment, which suggests a complex relationship between the two. This study presents a first step toward understanding the relationship between the convexity of latent representations and human-machine alignment.",
            "id": "2409.06362",
            "link": "http://arxiv.org/abs/2409.06362v1",
            "published": "2024-09-10T09:32:16+00:00",
            "updated": "2024-09-10T09:32:16+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 30
        },
        "2409.06416": {
            "authors": [
                "Ludvig Lemner",
                "Linnea Wahlgren",
                "Gregory Gay",
                "Nasser Mohammadiha",
                "Jingxiong Liu",
                "Joakim Wennerberg"
            ],
            "title": "Exploring the Integration of Large Language Models in Industrial Test Maintenance Processes",
            "abstract": "Much of the cost and effort required during the software testing process is invested in performing test maintenance - the addition, removal, or modification of test cases to keep the test suite in sync with the system-under-test or to otherwise improve its quality. Tool support could reduce the cost - and improve the quality - of test maintenance by automating aspects of the process or by providing guidance and support to developers.   In this study, we explore the capabilities and applications of large language models (LLMs) - complex machine learning models adapted to textual analysis - to support test maintenance. We conducted a case study at Ericsson AB where we explored the triggers that indicate the need for test maintenance, the actions that LLMs can take, and the considerations that must be made when deploying LLMs in an industrial setting. We also proposed and demonstrated implementations of two multi-agent architectures that can predict which test cases require maintenance following a change to the source code. Collectively, these contributions advance our theoretical and practical understanding of how LLMs can be deployed to benefit industrial test maintenance processes.",
            "id": "2409.06416",
            "link": "http://arxiv.org/abs/2409.06416v1",
            "published": "2024-09-10T10:55:48+00:00",
            "updated": "2024-09-10T10:55:48+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 18
        },
        "2409.06445": {
            "authors": [
                "Naser Kazemi",
                "Nedko Savov",
                "Danda Paudel",
                "Luc Van Gool"
            ],
            "title": "Learning Generative Interactive Environments By Trained Agent Exploration",
            "abstract": "World models are increasingly pivotal in interpreting and simulating the rules and actions of complex environments. Genie, a recent model, excels at learning from visually diverse environments but relies on costly human-collected data. We observe that their alternative method of using random agents is too limited to explore the environment. We propose to improve the model by employing reinforcement learning based agents for data generation. This approach produces diverse datasets that enhance the model's ability to adapt and perform well across various scenarios and realistic actions within the environment. In this paper, we first release the model GenieRedux - an implementation based on Genie. Additionally, we introduce GenieRedux-G, a variant that uses the agent's readily available actions to factor out action prediction uncertainty during validation. Our evaluation, including a replication of the Coinrun case study, shows that GenieRedux-G achieves superior visual fidelity and controllability using the trained agent exploration. The proposed approach is reproducable, scalable and adaptable to new types of environments. Our codebase is available at https://github.com/insait-institute/GenieRedux .",
            "id": "2409.06445",
            "link": "http://arxiv.org/abs/2409.06445v1",
            "published": "2024-09-10T12:00:40+00:00",
            "updated": "2024-09-10T12:00:40+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 171
        },
        "2409.06493": {
            "authors": [
                "Rohit Jena",
                "Ali Taghibakhshi",
                "Sahil Jain",
                "Gerald Shen",
                "Nima Tajbakhsh",
                "Arash Vahdat"
            ],
            "title": "Elucidating Optimal Reward-Diversity Tradeoffs in Text-to-Image Diffusion Models",
            "abstract": "Text-to-image (T2I) diffusion models have become prominent tools for generating high-fidelity images from text prompts. However, when trained on unfiltered internet data, these models can produce unsafe, incorrect, or stylistically undesirable images that are not aligned with human preferences. To address this, recent approaches have incorporated human preference datasets to fine-tune T2I models or to optimize reward functions that capture these preferences. Although effective, these methods are vulnerable to reward hacking, where the model overfits to the reward function, leading to a loss of diversity in the generated images. In this paper, we prove the inevitability of reward hacking and study natural regularization techniques like KL divergence and LoRA scaling, and their limitations for diffusion models. We also introduce Annealed Importance Guidance (AIG), an inference-time regularization inspired by Annealed Importance Sampling, which retains the diversity of the base model while achieving Pareto-Optimal reward-diversity tradeoffs. Our experiments demonstrate the benefits of AIG for Stable Diffusion models, striking the optimal balance between reward optimization and image diversity. Furthermore, a user study confirms that AIG improves diversity and quality of generated images across different model architectures and reward functions.",
            "id": "2409.06493",
            "link": "http://arxiv.org/abs/2409.06493v1",
            "published": "2024-09-09T16:27:26+00:00",
            "updated": "2024-09-09T16:27:26+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 29
        },
        "2409.06513": {
            "authors": [
                "Riccardo Simionato",
                "Stefano Fasciani"
            ],
            "title": "Sine, Transient, Noise Neural Modeling of Piano Notes",
            "abstract": "This paper introduces a novel method for emulating piano sounds. We propose to exploit the sine, transient, and noise decomposition to design a differentiable spectral modeling synthesizer replicating piano notes. Three sub-modules learn these components from piano recordings and generate the corresponding harmonic, transient, and noise signals. Splitting the emulation into three independently trainable models reduces the modeling tasks' complexity. The quasi-harmonic content is produced using a differentiable sinusoidal model guided by physics-derived formulas, whose parameters are automatically estimated from audio recordings. The noise sub-module uses a learnable time-varying filter, and the transients are generated using a deep convolutional network. From singular notes, we emulate the coupling between different keys in trichords with a convolutional-based network. Results show the model matches the partial distribution of the target while predicting the energy in the higher part of the spectrum presents more challenges. The energy distribution in the spectra of the transient and noise components is accurate overall. While the model is more computationally and memory efficient, perceptual tests reveal limitations in accurately modeling the attack phase of notes. Despite this, it generally achieves perceptual accuracy in emulating single notes and trichords.",
            "id": "2409.06513",
            "link": "http://arxiv.org/abs/2409.06513v1",
            "published": "2024-09-10T13:48:18+00:00",
            "updated": "2024-09-10T13:48:18+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 6
        },
        "2409.06566": {
            "authors": [
                "Tatiana V. Guy",
                "Jitka Homolov\u00e1",
                "Aleksej Gaj"
            ],
            "title": "Indirect Dynamic Negotiation in the Nash Demand Game",
            "abstract": "The paper addresses a problem of sequential bilateral bargaining with incomplete information. We proposed a decision model that helps agents to successfully bargain by performing indirect negotiation and learning the opponent's model. Methodologically the paper casts heuristically-motivated bargaining of a self-interested independent player into a framework of Bayesian learning and Markov decision processes. The special form of the reward implicitly motivates the players to negotiate indirectly, via closed-loop interaction. We illustrate the approach by applying our model to the Nash demand game, which is an abstract model of bargaining. The results indicate that the established negotiation: i) leads to coordinating players' actions; ii) results in maximising success rate of the game and iii) brings more individual profit to the players.",
            "id": "2409.06566",
            "link": "http://arxiv.org/abs/2409.06566v1",
            "published": "2024-09-10T14:58:00+00:00",
            "updated": "2024-09-10T14:58:00+00:00",
            "primary_category": "cs.GT",
            "categories": [
                "cs.GT",
                "cs.AI",
                "math.OC"
            ],
            "max_author_hindex": 12
        },
        "2409.06579": {
            "authors": [
                "Avinash Madasu",
                "Yossi Gandelsman",
                "Vasudev Lal",
                "Phillip Howard"
            ],
            "title": "Quantifying and Enabling the Interpretability of CLIP-like Models",
            "abstract": "CLIP is one of the most popular foundational models and is heavily used for many vision-language tasks. However, little is known about the inner workings of CLIP. To bridge this gap we propose a study to quantify the interpretability in CLIP like models. We conduct this study on six different CLIP models from OpenAI and OpenCLIP which vary by size, type of pre-training data and patch size. Our approach begins with using the TEXTSPAN algorithm and in-context learning to break down individual attention heads into specific properties. We then evaluate how easily these heads can be interpreted using new metrics which measure property consistency within heads and property disentanglement across heads. Our findings reveal that larger CLIP models are generally more interpretable than their smaller counterparts. To further assist users in understanding the inner workings of CLIP models, we introduce CLIP-InterpreT, a tool designed for interpretability analysis. CLIP-InterpreT offers five types of analyses: property-based nearest neighbor search, per-head topic segmentation, contrastive segmentation, per-head nearest neighbors of an image, and per-head nearest neighbors of text.",
            "id": "2409.06579",
            "link": "http://arxiv.org/abs/2409.06579v1",
            "published": "2024-09-10T15:19:40+00:00",
            "updated": "2024-09-10T15:19:40+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 10
        },
        "2409.06615": {
            "authors": [
                "Kushal Kedia",
                "Prithwish Dan",
                "Sanjiban Choudhury"
            ],
            "title": "One-Shot Imitation under Mismatched Execution",
            "abstract": "Human demonstrations as prompts are a powerful way to program robots to do long-horizon manipulation tasks. However, directly translating such demonstrations into robot-executable actions poses significant challenges due to execution mismatches, such as different movement styles and physical capabilities. Existing methods either rely on robot-demonstrator paired data, which is infeasible to scale, or overly rely on frame-level visual similarities, which fail to hold. To address these challenges, we propose RHyME, a novel framework that automatically establishes task execution correspondences between the robot and the demonstrator by using optimal transport costs. Given long-horizon robot demonstrations, RHyME synthesizes semantically equivalent human demonstrations by retrieving and composing similar short-horizon human clips, facilitating effective policy training without the need for paired data. We show that RHyME outperforms a range of baselines across various cross-embodiment datasets on all degrees of mismatches. Through detailed analysis, we uncover insights for learning and leveraging cross-embodiment visual representations.",
            "id": "2409.06615",
            "link": "http://arxiv.org/abs/2409.06615v1",
            "published": "2024-09-10T16:11:57+00:00",
            "updated": "2024-09-10T16:11:57+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 22
        },
        "2409.06672": {
            "authors": [
                "Cristian Trout"
            ],
            "title": "Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort",
            "abstract": "Many experts believe that AI systems will sooner or later pose uninsurable risks, including existential risks. This creates an extreme judgment-proof problem: few if any parties can be held accountable ex post in the event of such a catastrophe. This paper proposes a novel solution: a government-provided, mandatory indemnification program for AI developers. The program uses risk-priced indemnity fees to induce socially optimal levels of care. Risk-estimates are determined by surveying experts, including indemnified developers. The Bayesian Truth Serum mechanism is employed to incent honest and effortful responses. Compared to alternatives, this approach arguably better leverages all private information, and provides a clearer signal to indemnified developers regarding what risks they must mitigate to lower their fees. It's recommended that collected fees be used to help fund the safety research developers need, employing a fund matching mechanism (Quadratic Financing) to induce an optimal supply of this public good. Under Quadratic Financing, safety research projects would compete for private contributions from developers, signaling how much each is to be supplemented with public funds.",
            "id": "2409.06672",
            "link": "http://arxiv.org/abs/2409.06672v1",
            "published": "2024-09-10T17:41:24+00:00",
            "updated": "2024-09-10T17:41:24+00:00",
            "primary_category": "cs.CY",
            "categories": [
                "cs.CY",
                "cs.AI",
                "cs.LG",
                "q-fin.RM"
            ],
            "max_author_hindex": 0
        },
        "2409.06673": {
            "authors": [
                "Cristian Trout"
            ],
            "title": "Liability and Insurance for Catastrophic Losses: the Nuclear Power Precedent and Lessons for AI",
            "abstract": "As AI systems become more autonomous and capable, experts warn of them potentially causing catastrophic losses. Drawing on the successful precedent set by the nuclear power industry, this paper argues that developers of frontier AI models should be assigned limited, strict, and exclusive third party liability for harms resulting from Critical AI Occurrences (CAIOs) - events that cause or easily could have caused catastrophic losses. Mandatory insurance for CAIO liability is recommended to overcome developers' judgment-proofness, mitigate winner's curse dynamics, and leverage insurers' quasi-regulatory abilities. Based on theoretical arguments and observations from the analogous nuclear power context, insurers are expected to engage in a mix of causal risk-modeling, monitoring, lobbying for stricter regulation, and providing loss prevention guidance in the context of insuring against heavy-tail risks from AI. While not a substitute for regulation, clear liability assignment and mandatory insurance can help efficiently allocate resources to risk-modeling and safe design, facilitating future regulatory efforts.",
            "id": "2409.06673",
            "link": "http://arxiv.org/abs/2409.06673v1",
            "published": "2024-09-10T17:41:31+00:00",
            "updated": "2024-09-10T17:41:31+00:00",
            "primary_category": "cs.CY",
            "categories": [
                "cs.CY",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 0
        },
        "2409.06690": {
            "authors": [
                "Hongzhi Shu",
                "Xinglin Li",
                "Hongyu Jiang",
                "Minghao Fu",
                "Xinyu Li"
            ],
            "title": "Benchmarking Sub-Genre Classification For Mainstage Dance Music",
            "abstract": "Music classification, with a wide range of applications, is one of the most prominent tasks in music information retrieval. To address the absence of comprehensive datasets and high-performing methods in the classification of mainstage dance music, this work introduces a novel benchmark comprising a new dataset and a baseline. Our dataset extends the number of sub-genres to cover most recent mainstage live sets by top DJs worldwide in music festivals. A continuous soft labeling approach is employed to account for tracks that span multiple sub-genres, preserving the inherent sophistication. For the baseline, we developed deep learning models that outperform current state-of-the-art multimodel language models, which struggle to identify house music sub-genres, emphasizing the need for specialized models trained on fine-grained datasets. Our benchmark is applicable to serve for application scenarios such as music recommendation, DJ set curation, and interactive multimedia, where we also provide video demos. Our code is on \\url{https://anonymous.4open.science/r/Mainstage-EDM-Benchmark/}.",
            "id": "2409.06690",
            "link": "http://arxiv.org/abs/2409.06690v1",
            "published": "2024-09-10T17:54:00+00:00",
            "updated": "2024-09-10T17:54:00+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.MM",
                "I.2.1"
            ],
            "max_author_hindex": 28
        },
        "2409.06745": {
            "authors": [
                "Zhiyu Chen",
                "Wei Ji",
                "Jing Xiao",
                "Zitao Liu"
            ],
            "title": "Personalized Knowledge Tracing through Student Representation Reconstruction and Class Imbalance Mitigation",
            "abstract": "Knowledge tracing is a technique that predicts students' future performance by analyzing their learning process through historical interactions with intelligent educational platforms, enabling a precise evaluation of their knowledge mastery. Recent studies have achieved significant progress by leveraging powerful deep neural networks. These models construct complex input representations using questions, skills, and other auxiliary information but overlook individual student characteristics, which limits the capability for personalized assessment. Additionally, the available datasets in the field exhibit class imbalance issues. The models that simply predict all responses as correct without substantial effort can yield impressive accuracy. In this paper, we propose PKT, a novel approach for personalized knowledge tracing. PKT reconstructs representations from sequences of interactions with a tutoring platform to capture latent information about the students. Moreover, PKT incorporates focal loss to improve prioritize minority classes, thereby achieving more balanced predictions. Extensive experimental results on four publicly available educational datasets demonstrate the advanced predictive performance of PKT in comparison with 16 state-of-the-art models. To ensure the reproducibility of our research, the code is publicly available at https://anonymous.4open.science/r/PKT.",
            "id": "2409.06745",
            "link": "http://arxiv.org/abs/2409.06745v1",
            "published": "2024-09-10T07:02:46+00:00",
            "updated": "2024-09-10T07:02:46+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CY"
            ],
            "max_author_hindex": 27
        },
        "2409.06750": {
            "authors": [
                "H. Zhang",
                "J. Yin",
                "M. Jiang",
                "C. Su"
            ],
            "title": "Can Agents Spontaneously Form a Society? Introducing a Novel Architecture for Generative Multi-Agents to Elicit Social Emergence",
            "abstract": "Generative agents have demonstrated impressive capabilities in specific tasks, but most of these frameworks focus on independent tasks and lack attention to social interactions. We introduce a generative agent architecture called ITCMA-S, which includes a basic framework for individual agents and a framework called LTRHA that supports social interactions among multi-agents. This architecture enables agents to identify and filter out behaviors that are detrimental to social interactions, guiding them to choose more favorable actions. We designed a sandbox environment to simulate the natural evolution of social relationships among multiple identity-less agents for experimental evaluation. The results showed that ITCMA-S performed well on multiple evaluation indicators, demonstrating its ability to actively explore the environment, recognize new agents, and acquire new information through continuous actions and dialogue. Observations show that as agents establish connections with each other, they spontaneously form cliques with internal hierarchies around a selected leader and organize collective activities.",
            "id": "2409.06750",
            "link": "http://arxiv.org/abs/2409.06750v1",
            "published": "2024-09-10T13:39:29+00:00",
            "updated": "2024-09-10T13:39:29+00:00",
            "primary_category": "cs.MA",
            "categories": [
                "cs.MA",
                "cs.AI",
                "cs.HC",
                "cs.LG",
                "68T42",
                "I.2.7; J.4"
            ],
            "max_author_hindex": 38
        },
        "2409.06764": {
            "authors": [
                "Axel Martinez",
                "Gustavo Olague",
                "Emilio Hernandez"
            ],
            "title": "Modeling Image Tone Dichotomy with the Power Function",
            "abstract": "The primary purpose of this paper is to present the concept of dichotomy in image illumination modeling based on the power function. In particular, we review several mathematical properties of the power function to identify the limitations and propose a new mathematical model capable of abstracting illumination dichotomy. The simplicity of the equation opens new avenues for classical and modern image analysis and processing. The article provides practical and illustrative image examples to explain how the new model manages dichotomy in image perception. The article shows dichotomy image space as a viable way to extract rich information from images despite poor contrast linked to tone, lightness, and color perception. Moreover, a comparison with state-of-the-art methods in image enhancement provides evidence of the method's value.",
            "id": "2409.06764",
            "link": "http://arxiv.org/abs/2409.06764v1",
            "published": "2024-09-10T17:55:09+00:00",
            "updated": "2024-09-10T17:55:09+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 25
        },
        "2409.06800": {
            "authors": [
                "Michele Laurelli"
            ],
            "title": "Adaptive Meta-Domain Transfer Learning (AMDTL): A Novel Approach for Knowledge Transfer in AI",
            "abstract": "This paper presents Adaptive Meta-Domain Transfer Learning (AMDTL), a novel methodology that combines principles of meta-learning with domain-specific adaptations to enhance the transferability of artificial intelligence models across diverse and unknown domains. AMDTL aims to address the main challenges of transfer learning, such as domain misalignment, negative transfer, and catastrophic forgetting, through a hybrid framework that emphasizes both generalization and contextual specialization. The framework integrates a meta-learner trained on a diverse distribution of tasks, adversarial training techniques for aligning domain feature distributions, and dynamic feature regulation mechanisms based on contextual domain embeddings. Experimental results on benchmark datasets demonstrate that AMDTL outperforms existing transfer learning methodologies in terms of accuracy, adaptation efficiency, and robustness. This research provides a solid theoretical and practical foundation for the application of AMDTL in various fields, opening new perspectives for the development of more adaptable and inclusive AI systems.",
            "id": "2409.06800",
            "link": "http://arxiv.org/abs/2409.06800v1",
            "published": "2024-09-10T18:11:48+00:00",
            "updated": "2024-09-10T18:11:48+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 0
        },
        "2409.06805": {
            "authors": [
                "Azal Ahmad Khan",
                "Ahmad Faraz Khan",
                "Haider Ali",
                "Ali Anwar"
            ],
            "title": "Personalized Federated Learning Techniques: Empirical Analysis",
            "abstract": "Personalized Federated Learning (pFL) holds immense promise for tailoring machine learning models to individual users while preserving data privacy. However, achieving optimal performance in pFL often requires a careful balancing act between memory overhead costs and model accuracy. This paper delves into the trade-offs inherent in pFL, offering valuable insights for selecting the right algorithms for diverse real-world scenarios. We empirically evaluate ten prominent pFL techniques across various datasets and data splits, uncovering significant differences in their performance. Our study reveals interesting insights into how pFL methods that utilize personalized (local) aggregation exhibit the fastest convergence due to their efficiency in communication and computation. Conversely, fine-tuning methods face limitations in handling data heterogeneity and potential adversarial attacks while multi-objective learning methods achieve higher accuracy at the cost of additional training and resource consumption. Our study emphasizes the critical role of communication efficiency in scaling pFL, demonstrating how it can significantly affect resource usage in real-world deployments.",
            "id": "2409.06805",
            "link": "http://arxiv.org/abs/2409.06805v1",
            "published": "2024-09-10T18:16:28+00:00",
            "updated": "2024-09-10T18:16:28+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ],
            "max_author_hindex": 32
        },
        "2409.06941": {
            "authors": [
                "Jiashu Zhang",
                "Zihan Pan",
                "Molly",
                "Xu",
                "Khuzaima Daudjee",
                "Sihang Liu"
            ],
            "title": "FreeRide: Harvesting Bubbles in Pipeline Parallelism",
            "abstract": "The occurrence of bubbles in pipeline parallelism is an inherent limitation that can account for more than 40% of the large language model (LLM) training time and is one of the main reasons for the underutilization of GPU resources in LLM training. Harvesting these bubbles for GPU side tasks can increase resource utilization and reduce training costs but comes with challenges. First, because bubbles are discontinuous with various shapes, programming side tasks becomes difficult while requiring excessive engineering effort. Second, a side task can compete with pipeline training for GPU resources and incur significant overhead. To address these challenges, we propose FreeRide, a system designed to harvest bubbles in pipeline parallelism for side tasks. FreeRide provides programmers with interfaces to implement side tasks easily, manages bubbles and side tasks during pipeline training, and controls access to GPU resources by side tasks to reduce overhead. We demonstrate that FreeRide achieves 7.8% average cost savings with a negligible overhead of about 1% in training LLMs while serving model training, graph analytics, and image processing side tasks.",
            "id": "2409.06941",
            "link": "http://arxiv.org/abs/2409.06941v1",
            "published": "2024-09-11T01:46:49+00:00",
            "updated": "2024-09-11T01:46:49+00:00",
            "primary_category": "cs.DC",
            "categories": [
                "cs.DC",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.06953": {
            "authors": [
                "Zeno Kujawa",
                "John Poole",
                "Dobrik Georgiev",
                "Danilo Numeroso",
                "Pietro Li\u00f2"
            ],
            "title": "Neural Algorithmic Reasoning with Multiple Correct Solutions",
            "abstract": "Neural Algorithmic Reasoning (NAR) aims to optimize classical algorithms. However, canonical implementations of NAR train neural networks to return only a single solution, even when there are multiple correct solutions to a problem, such as single-source shortest paths. For some applications, it is desirable to recover more than one correct solution. To that end, we give the first method for NAR with multiple solutions. We demonstrate our method on two classical algorithms: Bellman-Ford (BF) and Depth-First Search (DFS), favouring deeper insight into two algorithms over a broader survey of algorithms. This method involves generating appropriate training data as well as sampling and validating solutions from model output. Each step of our method, which can serve as a framework for neural algorithmic reasoning beyond the tasks presented in this paper, might be of independent interest to the field and our results represent the first attempt at this task in the NAR literature.",
            "id": "2409.06953",
            "link": "http://arxiv.org/abs/2409.06953v1",
            "published": "2024-09-11T02:29:53+00:00",
            "updated": "2024-09-11T02:29:53+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 27
        },
        "2409.06997": {
            "authors": [
                "Paula Rodriguez-Diaz",
                "Lingkai Kong",
                "Kai Wang",
                "David Alvarez-Melis",
                "Milind Tambe"
            ],
            "title": "What is the Right Notion of Distance between Predict-then-Optimize Tasks?",
            "abstract": "Comparing datasets is a fundamental task in machine learning, essential for various learning paradigms; from evaluating train and test datasets for model generalization to using dataset similarity for detecting data drift. While traditional notions of dataset distances offer principled measures of similarity, their utility has largely been assessed through prediction error minimization. However, in Predict-then-Optimize (PtO) frameworks, where predictions serve as inputs for downstream optimization tasks, model performance is measured through decision regret minimization rather than prediction error minimization. In this work, we (i) show that traditional dataset distances, which rely solely on feature and label dimensions, lack informativeness in the PtO context, and (ii) propose a new dataset distance that incorporates the impacts of downstream decisions. Our results show that this decision-aware dataset distance effectively captures adaptation success in PtO contexts, providing a PtO adaptation bound in terms of dataset distance. Empirically, we show that our proposed distance measure accurately predicts transferability across three different PtO tasks from the literature.",
            "id": "2409.06997",
            "link": "http://arxiv.org/abs/2409.06997v1",
            "published": "2024-09-11T04:13:17+00:00",
            "updated": "2024-09-11T04:13:17+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 84
        },
        "2409.07012": {
            "authors": [
                "Daeun Kyung",
                "Junu Kim",
                "Tackeun Kim",
                "Edward Choi"
            ],
            "title": "Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records",
            "abstract": "Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals to assess patient conditions and monitor changes over time. Generative models, specifically diffusion-based models, have shown promise in generating realistic synthetic X-rays. However, these models mainly focus on conditional generation using single-time-point data, i.e., typically CXRs taken at a specific time with their corresponding reports, limiting their clinical utility, particularly for capturing temporal changes. To address this limitation, we propose a novel framework, EHRXDiff, which predicts future CXR images by integrating previous CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc. Our framework dynamically tracks and predicts disease progression based on a latent diffusion model, conditioned on the previous CXR image and a history of medical events. We comprehensively evaluate the performance of our framework across three key aspects, including clinical consistency, demographic consistency, and visual realism. We demonstrate that our framework generates high-quality, realistic future images that capture potential temporal changes, suggesting its potential for further development as a clinical simulation tool. This could offer valuable insights for patient monitoring and treatment planning in the medical field.",
            "id": "2409.07012",
            "link": "http://arxiv.org/abs/2409.07012v1",
            "published": "2024-09-11T04:49:44+00:00",
            "updated": "2024-09-11T04:49:44+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 22
        },
        "2409.07016": {
            "authors": [
                "Xinhu Zheng",
                "Anbai Jiang",
                "Bing Han",
                "Yanmin Qian",
                "Pingyi Fan",
                "Jia Liu",
                "Wei-Qiang Zhang"
            ],
            "title": "Improving Anomalous Sound Detection via Low-Rank Adaptation Fine-Tuning of Pre-Trained Audio Models",
            "abstract": "Anomalous Sound Detection (ASD) has gained significant interest through the application of various Artificial Intelligence (AI) technologies in industrial settings. Though possessing great potential, ASD systems can hardly be readily deployed in real production sites due to the generalization problem, which is primarily caused by the difficulty of data collection and the complexity of environmental factors. This paper introduces a robust ASD model that leverages audio pre-trained models. Specifically, we fine-tune these models using machine operation data, employing SpecAug as a data augmentation strategy. Additionally, we investigate the impact of utilizing Low-Rank Adaptation (LoRA) tuning instead of full fine-tuning to address the problem of limited data for fine-tuning. Our experiments on the DCASE2023 Task 2 dataset establish a new benchmark of 77.75% on the evaluation set, with a significant improvement of 6.48% compared with previous state-of-the-art (SOTA) models, including top-tier traditional convolutional networks and speech pre-trained models, which demonstrates the effectiveness of audio pre-trained models with LoRA tuning. Ablation studies are also conducted to showcase the efficacy of the proposed scheme.",
            "id": "2409.07016",
            "link": "http://arxiv.org/abs/2409.07016v1",
            "published": "2024-09-11T05:19:38+00:00",
            "updated": "2024-09-11T05:19:38+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 42
        },
        "2409.07078": {
            "authors": [
                "Anbin QI",
                "Zhongliang Liu",
                "Xinyong Zhou",
                "Jinba Xiao",
                "Fengrun Zhang",
                "Qi Gan",
                "Ming Tao",
                "Gaozheng Zhang",
                "Lu Zhang"
            ],
            "title": "Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout",
            "abstract": "In this paper, we present our solution for the Second Multimodal Emotion Recognition Challenge Track 1(MER2024-SEMI). To enhance the accuracy and generalization performance of emotion recognition, we propose several methods for Multimodal Emotion Recognition. Firstly, we introduce EmoVCLIP, a model fine-tuned based on CLIP using vision-language prompt learning, designed for video-based emotion recognition tasks. By leveraging prompt learning on CLIP, EmoVCLIP improves the performance of pre-trained CLIP on emotional videos. Additionally, to address the issue of modality dependence in multimodal fusion, we employ modality dropout for robust information fusion. Furthermore, to aid Baichuan in better extracting emotional information, we suggest using GPT-4 as the prompt for Baichuan. Lastly, we utilize a self-training strategy to leverage unlabeled videos. In this process, we use unlabeled videos with high-confidence pseudo-labels generated by our model and incorporate them into the training set. Experimental results demonstrate that our model ranks 1st in the MER2024-SEMI track, achieving an accuracy of 90.15% on the test set.",
            "id": "2409.07078",
            "link": "http://arxiv.org/abs/2409.07078v1",
            "published": "2024-09-11T08:06:47+00:00",
            "updated": "2024-09-11T08:06:47+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 32
        },
        "2409.07098": {
            "authors": [
                "Zehao Wang",
                "Han Zhou",
                "Matthew B. Blaschko",
                "Tinne Tuytelaars",
                "Minye Wu"
            ],
            "title": "Redundancy-Aware Camera Selection for Indoor Scene Neural Rendering",
            "abstract": "Novel view synthesis of indoor scenes can be achieved by capturing a monocular video sequence of the environment. However, redundant information caused by artificial movements in the input video data reduces the efficiency of scene modeling. In this work, we tackle this challenge from the perspective of camera selection. We begin by constructing a similarity matrix that incorporates both the spatial diversity of the cameras and the semantic variation of the images. Based on this matrix, we use the Intra-List Diversity (ILD) metric to assess camera redundancy, formulating the camera selection task as an optimization problem. Then we apply a diversity-based sampling algorithm to optimize the camera selection. We also develop a new dataset, IndoorTraj, which includes long and complex camera movements captured by humans in virtual indoor environments, closely mimicking real-world scenarios. Experimental results demonstrate that our strategy outperforms other approaches under time and memory constraints. Remarkably, our method achieves performance comparable to models trained on the full dataset, while using only an average of 15% of the frames and 75% of the allotted time.",
            "id": "2409.07098",
            "link": "http://arxiv.org/abs/2409.07098v1",
            "published": "2024-09-11T08:36:49+00:00",
            "updated": "2024-09-11T08:36:49+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 71
        },
        "2409.07109": {
            "authors": [
                "Marcus R\u00fcb",
                "Axel Sikora",
                "Daniel Mueller-Gritschneder"
            ],
            "title": "Advancing On-Device Neural Network Training with TinyPropv2: Dynamic, Sparse, and Efficient Backpropagation",
            "abstract": "This study introduces TinyPropv2, an innovative algorithm optimized for on-device learning in deep neural networks, specifically designed for low-power microcontroller units. TinyPropv2 refines sparse backpropagation by dynamically adjusting the level of sparsity, including the ability to selectively skip training steps. This feature significantly lowers computational effort without substantially compromising accuracy. Our comprehensive evaluation across diverse datasets CIFAR 10, CIFAR100, Flower, Food, Speech Command, MNIST, HAR, and DCASE2020 reveals that TinyPropv2 achieves near-parity with full training methods, with an average accuracy drop of only around 1 percent in most cases. For instance, against full training, TinyPropv2's accuracy drop is minimal, for example, only 0.82 percent on CIFAR 10 and 1.07 percent on CIFAR100. In terms of computational effort, TinyPropv2 shows a marked reduction, requiring as little as 10 percent of the computational effort needed for full training in some scenarios, and consistently outperforms other sparse training methodologies. These findings underscore TinyPropv2's capacity to efficiently manage computational resources while maintaining high accuracy, positioning it as an advantageous solution for advanced embedded device applications in the IoT ecosystem.",
            "id": "2409.07109",
            "link": "http://arxiv.org/abs/2409.07109v1",
            "published": "2024-09-11T08:56:13+00:00",
            "updated": "2024-09-11T08:56:13+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.07115": {
            "authors": [
                "Mohammed Alsaafin",
                "Musab Alsheikh",
                "Saeed Anwar",
                "Muhammad Usman"
            ],
            "title": "Attention Down-Sampling Transformer, Relative Ranking and Self-Consistency for Blind Image Quality Assessment",
            "abstract": "The no-reference image quality assessment is a challenging domain that addresses estimating image quality without the original reference. We introduce an improved mechanism to extract local and non-local information from images via different transformer encoders and CNNs. The utilization of Transformer encoders aims to mitigate locality bias and generate a non-local representation by sequentially processing CNN features, which inherently capture local visual structures. Establishing a stronger connection between subjective and objective assessments is achieved through sorting within batches of images based on relative distance information. A self-consistency approach to self-supervision is presented, explicitly addressing the degradation of no-reference image quality assessment (NR-IQA) models under equivariant transformations. Our approach ensures model robustness by maintaining consistency between an image and its horizontally flipped equivalent. Through empirical evaluation of five popular image quality assessment datasets, the proposed model outperforms alternative algorithms in the context of no-reference image quality assessment datasets, especially on smaller datasets. Codes are available at \\href{https://github.com/mas94/ADTRS}{https://github.com/mas94/ADTRS}",
            "id": "2409.07115",
            "link": "http://arxiv.org/abs/2409.07115v1",
            "published": "2024-09-11T09:08:43+00:00",
            "updated": "2024-09-11T09:08:43+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ],
            "max_author_hindex": 31
        },
        "2409.07151": {
            "authors": [
                "Tien-Hong Lo",
                "Meng-Ting Tsai",
                "Berlin Chen"
            ],
            "title": "Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment",
            "abstract": "Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).",
            "id": "2409.07151",
            "link": "http://arxiv.org/abs/2409.07151v1",
            "published": "2024-09-11T09:55:57+00:00",
            "updated": "2024-09-11T09:55:57+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.07154": {
            "authors": [
                "Kaijia Xu",
                "Petar Veli\u010dkovi\u0107"
            ],
            "title": "Recurrent Aggregators in Neural Algorithmic Reasoning",
            "abstract": "Neural algorithmic reasoning (NAR) is an emerging field that seeks to design neural networks that mimic classical algorithmic computations. Today, graph neural networks (GNNs) are widely used in neural algorithmic reasoners due to their message passing framework and permutation equivariance. In this extended abstract, we challenge this design choice, and replace the equivariant aggregation function with a recurrent neural network. While seemingly counter-intuitive, this approach has appropriate grounding when nodes have a natural ordering -- and this is the case frequently in established reasoning benchmarks like CLRS-30. Indeed, our recurrent NAR (RNAR) model performs very strongly on such tasks, while handling many others gracefully. A notable achievement of RNAR is its decisive state-of-the-art result on the Heapsort and Quickselect tasks, both deemed as a significant challenge for contemporary neural algorithmic reasoners -- especially the latter, where RNAR achieves a mean micro-F1 score of 87%.",
            "id": "2409.07154",
            "link": "http://arxiv.org/abs/2409.07154v1",
            "published": "2024-09-11T09:59:56+00:00",
            "updated": "2024-09-11T09:59:56+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 23
        },
        "2409.07165": {
            "authors": [
                "Titouan Parcollet",
                "Rogier van Dalen",
                "Shucong Zhang",
                "Sourav Batthacharya"
            ],
            "title": "Linear Time Complexity Conformers with SummaryMixing for Streaming Speech Recognition",
            "abstract": "Automatic speech recognition (ASR) with an encoder equipped with self-attention, whether streaming or non-streaming, takes quadratic time in the length of the speech utterance. This slows down training and decoding, increase their cost, and limit the deployment of the ASR in constrained devices. SummaryMixing is a promising linear-time complexity alternative to self-attention for non-streaming speech recognition that, for the first time, preserves or outperforms the accuracy of self-attention models. Unfortunately, the original definition of SummaryMixing is not suited to streaming speech recognition. Hence, this work extends SummaryMixing to a Conformer Transducer that works in both a streaming and an offline mode. It shows that this new linear-time complexity speech encoder outperforms self-attention in both scenarios while requiring less compute and memory during training and decoding.",
            "id": "2409.07165",
            "link": "http://arxiv.org/abs/2409.07165v1",
            "published": "2024-09-11T10:24:43+00:00",
            "updated": "2024-09-11T10:24:43+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 20
        },
        "2409.07194": {
            "authors": [
                "Pedro Beltr\u00e1n L\u00f3pez",
                "Manuel Gil P\u00e9rez",
                "Pantaleone Nespoli"
            ],
            "title": "Cyber Deception: State of the art, Trends and Open challenges",
            "abstract": "The growing interest in cybersecurity has significantly increased articles designing and implementing various Cyber Deception (CYDEC) mechanisms. This trend reflects the urgent need for new strategies to address cyber threats effectively. Since its emergence, CYDEC has established itself as an innovative defense against attackers, thanks to its proactive and reactive capabilities, finding applications in numerous real-life scenarios. Despite the considerable work devoted to CYDEC, the literature still presents significant gaps. In particular, there has not been (i) a comprehensive analysis of the main components characterizing CYDEC, (ii) a generic classification covering all types of solutions, nor (iii) a survey of the current state of the literature in various contexts. This article aims to fill these gaps through a detailed review of the main features that comprise CYDEC, developing a comprehensive classification taxonomy. In addition, the different frameworks used to generate CYDEC are reviewed, presenting a more comprehensive one. Existing solutions in the literature using CYDEC, both without Artificial Intelligence (AI) and with AI, are studied and compared. Finally, the most salient trends of the current state of the art are discussed, offering a list of pending challenges for future research.",
            "id": "2409.07194",
            "link": "http://arxiv.org/abs/2409.07194v1",
            "published": "2024-09-11T11:31:34+00:00",
            "updated": "2024-09-11T11:31:34+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.GT"
            ],
            "max_author_hindex": 18
        },
        "2409.07218": {
            "authors": [
                "Mustafa Yildirim",
                "Barkin Dagda",
                "Vinal Asodia",
                "Saber Fallah"
            ],
            "title": "Behavioral Cloning Models Reality Check for Autonomous Driving",
            "abstract": "How effective are recent advancements in autonomous vehicle perception systems when applied to real-world autonomous vehicle control? While numerous vision-based autonomous vehicle systems have been trained and evaluated in simulated environments, there is a notable lack of real-world validation for these systems. This paper addresses this gap by presenting the real-world validation of state-of-the-art perception systems that utilize Behavior Cloning (BC) for lateral control, processing raw image data to predict steering commands. The dataset was collected using a scaled research vehicle and tested on various track setups. Experimental results demonstrate that these methods predict steering angles with low error margins in real-time, indicating promising potential for real-world applications.",
            "id": "2409.07218",
            "link": "http://arxiv.org/abs/2409.07218v1",
            "published": "2024-09-11T12:19:38+00:00",
            "updated": "2024-09-11T12:19:38+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 22
        },
        "2409.07291": {
            "authors": [
                "Zhuohang Li",
                "Andrew Lowy",
                "Jing Liu",
                "Toshiaki Koike-Akino",
                "Bradley Malin",
                "Kieran Parsons",
                "Ye Wang"
            ],
            "title": "Exploring User-level Gradient Inversion with a Diffusion Prior",
            "abstract": "We explore user-level gradient inversion as a new attack surface in distributed learning. We first investigate existing attacks on their ability to make inferences about private information beyond training data reconstruction. Motivated by the low reconstruction quality of existing methods, we propose a novel gradient inversion attack that applies a denoising diffusion model as a strong image prior in order to enhance recovery in the large batch setting. Unlike traditional attacks, which aim to reconstruct individual samples and suffer at large batch and image sizes, our approach instead aims to recover a representative image that captures the sensitive shared semantic information corresponding to the underlying user. Our experiments with face images demonstrate the ability of our methods to recover realistic facial images along with private user attributes.",
            "id": "2409.07291",
            "link": "http://arxiv.org/abs/2409.07291v1",
            "published": "2024-09-11T14:20:47+00:00",
            "updated": "2024-09-11T14:20:47+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "cs.CV",
                "stat.ML"
            ],
            "max_author_hindex": 70
        },
        "2409.07351": {
            "authors": [
                "Sana Ayromlou",
                "Atrin Arya",
                "Armin Saadat",
                "Purang Abolmaesumi",
                "Xiaoxiao Li"
            ],
            "title": "Federated Impression for Learning with Distributed Heterogeneous Data",
            "abstract": "Standard deep learning-based classification approaches may not always be practical in real-world clinical applications, as they require a centralized collection of all samples. Federated learning (FL) provides a paradigm that can learn from distributed datasets across clients without requiring them to share data, which can help mitigate privacy and data ownership issues. In FL, sub-optimal convergence caused by data heterogeneity is common among data from different health centers due to the variety in data collection protocols and patient demographics across centers. Through experimentation in this study, we show that data heterogeneity leads to the phenomenon of catastrophic forgetting during local training. We propose FedImpres which alleviates catastrophic forgetting by restoring synthetic data that represents the global information as federated impression. To achieve this, we distill the global model resulting from each communication round. Subsequently, we use the synthetic data alongside the local data to enhance the generalization of local training. Extensive experiments show that the proposed method achieves state-of-the-art performance on both the BloodMNIST and Retina datasets, which contain label imbalance and domain shift, with an improvement in classification accuracy of up to 20%.",
            "id": "2409.07351",
            "link": "http://arxiv.org/abs/2409.07351v1",
            "published": "2024-09-11T15:37:52+00:00",
            "updated": "2024-09-11T15:37:52+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "cs.DC"
            ],
            "max_author_hindex": 46
        },
        "2409.07368": {
            "authors": [
                "Khiem Ton",
                "Nhi Nguyen",
                "Mahmoud Nazzal",
                "Abdallah Khreishah",
                "Cristian Borcea",
                "NhatHai Phan",
                "Ruoming Jin",
                "Issa Khalil",
                "Yelong Shen"
            ],
            "title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code",
            "abstract": "This paper introduces SGCode, a flexible prompt-optimizing system to generate secure code with large language models (LLMs). SGCode integrates recent prompt-optimization approaches with LLMs in a unified system accessible through front-end and back-end APIs, enabling users to 1) generate secure code, which is free of vulnerabilities, 2) review and share security analysis, and 3) easily switch from one prompt optimization approach to another, while providing insights on model and system performance. We populated SGCode on an AWS server with PromSec, an approach that optimizes prompts by combining an LLM and security tools with a lightweight generative adversarial graph neural network to detect and fix security vulnerabilities in the generated code. Extensive experiments show that SGCode is practical as a public tool to gain insights into the trade-offs between model utility, secure code generation, and system cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is available at: http://3.131.141.63:8501/.",
            "id": "2409.07368",
            "link": "http://arxiv.org/abs/2409.07368v1",
            "published": "2024-09-11T15:56:15+00:00",
            "updated": "2024-09-11T15:56:15+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 42
        },
        "2409.07407": {
            "authors": [
                "Zeqing Qin",
                "Yiwei Wu",
                "Lansheng Han"
            ],
            "title": "CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification",
            "abstract": "Large Language Models (LLMs) have shown great promise in vulnerability identification. As C/C++ comprises half of the Open-Source Software (OSS) vulnerabilities over the past decade and updates in OSS mainly occur through commits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing Commits (VCCs) is essential. However, current studies primarily focus on further pre-training LLMs on massive code datasets, which is resource-intensive and poses efficiency challenges. In this paper, we enhance the ability of BERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose CodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++ programs and LLMs. Based on commits, CLNX efficiently converts the source code into a more natural representation while preserving key details. Specifically, CLNX first applies structure-level naturalization to decompose complex programs, followed by token-level naturalization to interpret complex symbols. We evaluate CLNX on public datasets of 25,872 C/C++ functions with their commits. The results show that CLNX significantly enhances the performance of LLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new state-of-the-art and identifies 38 OSS vulnerabilities in the real world.",
            "id": "2409.07407",
            "link": "http://arxiv.org/abs/2409.07407v1",
            "published": "2024-09-11T16:49:46+00:00",
            "updated": "2024-09-11T16:49:46+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "68M25"
            ],
            "max_author_hindex": 9
        },
        "2409.07409": {
            "authors": [
                "Shaoting Zhu",
                "Runhan Huang",
                "Linzhan Mou",
                "Hang Zhao"
            ],
            "title": "Robust Robot Walker: Learning Agile Locomotion over Tiny Traps",
            "abstract": "Quadruped robots must exhibit robust walking capabilities in practical applications. In this work, we propose a novel approach that enables quadruped robots to pass various small obstacles, or \"tiny traps\". Existing methods often rely on exteroceptive sensors, which can be unreliable for detecting such tiny traps. To overcome this limitation, our approach focuses solely on proprioceptive inputs. We introduce a two-stage training framework incorporating a contact encoder and a classification head to learn implicit representations of different traps. Additionally, we design a set of tailored reward functions to improve both the stability of training and the ease of deployment for goal-tracking tasks. To benefit further research, we design a new benchmark for tiny trap task. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness and robustness of our method. Project Page: https://robust-robot-walker.github.io/",
            "id": "2409.07409",
            "link": "http://arxiv.org/abs/2409.07409v2",
            "published": "2024-09-11T16:50:29+00:00",
            "updated": "2024-09-12T15:35:49+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.07415": {
            "authors": [
                "Yuanhaur Chang",
                "Han Liu",
                "Evin Jaff",
                "Chenyang Lu",
                "Ning Zhang"
            ],
            "title": "SoK: Security and Privacy Risks of Medical AI",
            "abstract": "The integration of technology and healthcare has ushered in a new era where software systems, powered by artificial intelligence and machine learning, have become essential components of medical products and services. While these advancements hold great promise for enhancing patient care and healthcare delivery efficiency, they also expose sensitive medical data and system integrity to potential cyberattacks. This paper explores the security and privacy threats posed by AI/ML applications in healthcare. Through a thorough examination of existing research across a range of medical domains, we have identified significant gaps in understanding the adversarial attacks targeting medical AI systems. By outlining specific adversarial threat models for medical settings and identifying vulnerable application domains, we lay the groundwork for future research that investigates the security and resilience of AI-driven medical systems. Through our analysis of different threat models and feasibility studies on adversarial attacks in different medical domains, we provide compelling insights into the pressing need for cybersecurity research in the rapidly evolving field of AI healthcare technology.",
            "id": "2409.07415",
            "link": "http://arxiv.org/abs/2409.07415v1",
            "published": "2024-09-11T16:59:58+00:00",
            "updated": "2024-09-11T16:59:58+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 36
        },
        "2409.07569": {
            "authors": [
                "Guiliang Liu",
                "Sheng Xu",
                "Shicheng Liu",
                "Ashish Gaurav",
                "Sriram Ganapathi Subramanian",
                "Pascal Poupart"
            ],
            "title": "A Survey of Inverse Constrained Reinforcement Learning: Definitions, Progress and Challenges",
            "abstract": "Inverse Constrained Reinforcement Learning (ICRL) is the task of inferring the implicit constraints followed by expert agents from their demonstration data. As an emerging research topic, ICRL has received considerable attention in recent years. This article presents a categorical survey of the latest advances in ICRL. It serves as a comprehensive reference for machine learning researchers and practitioners, as well as starters seeking to comprehend the definitions, advancements, and important challenges in ICRL. We begin by formally defining the problem and outlining the algorithmic framework that facilitates constraint inference across various scenarios. These include deterministic or stochastic environments, environments with limited demonstrations, and multiple agents. For each context, we illustrate the critical challenges and introduce a series of fundamental methods to tackle these issues. This survey encompasses discrete, virtual, and realistic environments for evaluating ICRL agents. We also delve into the most pertinent applications of ICRL, such as autonomous driving, robot control, and sports analytics. To stimulate continuing research, we conclude the survey with a discussion of key unresolved questions in ICRL that can effectively foster a bridge between theoretical understanding and practical industrial applications.",
            "id": "2409.07569",
            "link": "http://arxiv.org/abs/2409.07569v1",
            "published": "2024-09-11T18:49:03+00:00",
            "updated": "2024-09-11T18:49:03+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 45
        },
        "2409.07581": {
            "authors": [
                "Abdarahmane Traor\u00e9",
                "Moulay A. Akhloufi"
            ],
            "title": "Violence detection in videos using deep recurrent and convolutional neural networks",
            "abstract": "Violence and abnormal behavior detection research have known an increase of interest in recent years, due mainly to a rise in crimes in large cities worldwide. In this work, we propose a deep learning architecture for violence detection which combines both recurrent neural networks (RNNs) and 2-dimensional convolutional neural networks (2D CNN). In addition to video frames, we use optical flow computed using the captured sequences. CNN extracts spatial characteristics in each frame, while RNN extracts temporal characteristics. The use of optical flow allows to encode the movements in the scenes. The proposed approaches reach the same level as the state-of-the-art techniques and sometime surpass them. It was validated on 3 databases achieving good results.",
            "id": "2409.07581",
            "link": "http://arxiv.org/abs/2409.07581v1",
            "published": "2024-09-11T19:21:51+00:00",
            "updated": "2024-09-11T19:21:51+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 27
        },
        "2409.07584": {
            "authors": [
                "Ke Chen",
                "Yifeng Wang",
                "Yufei Zhou",
                "Haohan Wang"
            ],
            "title": "DS-ViT: Dual-Stream Vision Transformer for Cross-Task Distillation in Alzheimer's Early Diagnosis",
            "abstract": "In the field of Alzheimer's disease diagnosis, segmentation and classification tasks are inherently interconnected. Sharing knowledge between models for these tasks can significantly improve training efficiency, particularly when training data is scarce. However, traditional knowledge distillation techniques often struggle to bridge the gap between segmentation and classification due to the distinct nature of tasks and different model architectures. To address this challenge, we propose a dual-stream pipeline that facilitates cross-task and cross-architecture knowledge sharing. Our approach introduces a dual-stream embedding module that unifies feature representations from segmentation and classification models, enabling dimensional integration of these features to guide the classification model. We validated our method on multiple 3D datasets for Alzheimer's disease diagnosis, demonstrating significant improvements in classification performance, especially on small datasets. Furthermore, we extended our pipeline with a residual temporal attention mechanism for early diagnosis, utilizing images taken before the atrophy of patients' brain mass. This advancement shows promise in enabling diagnosis approximately six months earlier in mild and asymptomatic stages, offering critical time for intervention.",
            "id": "2409.07584",
            "link": "http://arxiv.org/abs/2409.07584v1",
            "published": "2024-09-11T19:31:01+00:00",
            "updated": "2024-09-11T19:31:01+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV",
                "68T07, 92C55 (Primary) 93C85 (Secondary)"
            ],
            "max_author_hindex": 21
        },
        "2409.07606": {
            "authors": [
                "Denis Tarasov",
                "Anja Surina",
                "Caglar Gulcehre"
            ],
            "title": "The Role of Deep Learning Regularizations on Actors in Offline RL",
            "abstract": "Deep learning regularization techniques, such as \\emph{dropout}, \\emph{layer normalization}, or \\emph{weight decay}, are widely adopted in the construction of modern artificial neural networks, often resulting in more robust training processes and improved generalization capabilities. However, in the domain of \\emph{Reinforcement Learning} (RL), the application of these techniques has been limited, usually applied to value function estimators \\citep{hiraoka2021dropout, smith2022walk}, and may result in detrimental effects. This issue is even more pronounced in offline RL settings, which bear greater similarity to supervised learning but have received less attention. Recent work in continuous offline RL has demonstrated that while we can build sufficiently powerful critic networks, the generalization of actor networks remains a bottleneck. In this study, we empirically show that applying standard regularization techniques to actor networks in offline RL actor-critic algorithms yields improvements of 6\\% on average across two algorithms and three different continuous D4RL domains.",
            "id": "2409.07606",
            "link": "http://arxiv.org/abs/2409.07606v1",
            "published": "2024-09-11T20:35:29+00:00",
            "updated": "2024-09-11T20:35:29+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 13
        },
        "2409.07637": {
            "authors": [
                "Hanyu Zhang",
                "Reza Zandehshahvar",
                "Mathieu Tanneau",
                "Pascal Van Hentenryck"
            ],
            "title": "Weather-Informed Probabilistic Forecasting and Scenario Generation in Power Systems",
            "abstract": "The integration of renewable energy sources (RES) into power grids presents significant challenges due to their intrinsic stochasticity and uncertainty, necessitating the development of new techniques for reliable and efficient forecasting. This paper proposes a method combining probabilistic forecasting and Gaussian copula for day-ahead prediction and scenario generation of load, wind, and solar power in high-dimensional contexts. By incorporating weather covariates and restoring spatio-temporal correlations, the proposed method enhances the reliability of probabilistic forecasts in RES. Extensive numerical experiments compare the effectiveness of different time series models, with performance evaluated using comprehensive metrics on a real-world and high-dimensional dataset from Midcontinent Independent System Operator (MISO). The results highlight the importance of weather information and demonstrate the efficacy of the Gaussian copula in generating realistic scenarios, with the proposed weather-informed Temporal Fusion Transformer (WI-TFT) model showing superior performance.",
            "id": "2409.07637",
            "link": "http://arxiv.org/abs/2409.07637v1",
            "published": "2024-09-11T21:44:59+00:00",
            "updated": "2024-09-11T21:44:59+00:00",
            "primary_category": "stat.ML",
            "categories": [
                "stat.ML",
                "cs.AI",
                "cs.LG",
                "stat.AP"
            ],
            "max_author_hindex": 12
        },
        "2409.07704": {
            "authors": [
                "Junhyeok Lee",
                "Hyeongju Kim"
            ],
            "title": "Super Monotonic Alignment Search",
            "abstract": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most popular algorithm in TTS to estimate unknown alignments between text and speech. Since this algorithm needs to search for the most probable alignment with dynamic programming by caching all paths, the time complexity of the algorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on CPU, and while they mentioned it is difficult to parallelize, we found that MAS can be parallelized in text-length dimension and CPU execution consumes an inordinate amount of time for inter-device copy. Therefore, we implemented a Triton kernel and PyTorch JIT script to accelerate MAS on GPU without inter-device copy. As a result, Super-MAS Triton kernel is up to 72 times faster in the extreme-length case. The code is available at \\url{https://github.com/supertone-inc/super-monotonic-align}.",
            "id": "2409.07704",
            "link": "http://arxiv.org/abs/2409.07704v1",
            "published": "2024-09-12T02:13:57+00:00",
            "updated": "2024-09-12T02:13:57+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.AI"
            ],
            "max_author_hindex": 8
        },
        "2409.07706": {
            "authors": [
                "Lu Wang",
                "Tianyuan Zhang",
                "Yikai Han",
                "Muyang Fang",
                "Ting Jin",
                "Jiaqi Kang"
            ],
            "title": "Attack End-to-End Autonomous Driving through Module-Wise Noise",
            "abstract": "With recent breakthroughs in deep neural networks, numerous tasks within autonomous driving have exhibited remarkable performance. However, deep learning models are susceptible to adversarial attacks, presenting significant security risks to autonomous driving systems. Presently, end-to-end architectures have emerged as the predominant solution for autonomous driving, owing to their collaborative nature across different tasks. Yet, the implications of adversarial attacks on such models remain relatively unexplored. In this paper, we conduct comprehensive adversarial security research on the modular end-to-end autonomous driving model for the first time. We thoroughly consider the potential vulnerabilities in the model inference process and design a universal attack scheme through module-wise noise injection. We conduct large-scale experiments on the full-stack autonomous driving model and demonstrate that our attack method outperforms previous attack methods. We trust that our research will offer fresh insights into ensuring the safety and reliability of autonomous driving systems.",
            "id": "2409.07706",
            "link": "http://arxiv.org/abs/2409.07706v1",
            "published": "2024-09-12T02:19:16+00:00",
            "updated": "2024-09-12T02:19:16+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.07715": {
            "authors": [
                "Devansh Dhrafani",
                "Yifei Liu",
                "Andrew Jong",
                "Ukcheol Shin",
                "Yao He",
                "Tyler Harp",
                "Yaoyu Hu",
                "Jean Oh",
                "Sebastian Scherer"
            ],
            "title": "FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments",
            "abstract": "Robust depth perception in visually-degraded environments is crucial for autonomous aerial systems. Thermal imaging cameras, which capture infrared radiation, are robust to visual degradation. However, due to lack of a large-scale dataset, the use of thermal cameras for unmanned aerial system (UAS) depth perception has remained largely unexplored. This paper presents a stereo thermal depth perception dataset for autonomous aerial perception applications. The dataset consists of stereo thermal images, LiDAR, IMU and ground truth depth maps captured in urban and forest settings under diverse conditions like day, night, rain, and smoke. We benchmark representative stereo depth estimation algorithms, offering insights into their performance in degraded conditions. Models trained on our dataset generalize well to unseen smoky conditions, highlighting the robustness of stereo thermal imaging for depth perception. We aim for this work to enhance robotic perception in disaster scenarios, allowing for exploration and operations in previously unreachable areas. The dataset and source code are available at https://firestereo.github.io.",
            "id": "2409.07715",
            "link": "http://arxiv.org/abs/2409.07715v1",
            "published": "2024-09-12T02:51:21+00:00",
            "updated": "2024-09-12T02:51:21+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 45
        },
        "2409.07723": {
            "authors": [
                "Bojian Li",
                "Bo Liu",
                "Jinghua Yue",
                "Fugen Zhou"
            ],
            "title": "Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy",
            "abstract": "Depth estimation is a cornerstone of 3D reconstruction and plays a vital role in minimally invasive endoscopic surgeries. However, most current depth estimation networks rely on traditional convolutional neural networks, which are limited in their ability to capture global information. Foundation models offer a promising avenue for enhancing depth estimation, but those currently available are primarily trained on natural images, leading to suboptimal performance when applied to endoscopic images. In this work, we introduce a novel fine-tuning strategy for the Depth Anything Model and integrate it with an intrinsic-based unsupervised monocular depth estimation framework. Our approach includes a low-rank adaptation technique based on random vectors, which improves the model's adaptability to different scales. Additionally, we propose a residual block built on depthwise separable convolution to compensate for the transformer's limited ability to capture high-frequency details, such as edges and textures. Our experimental results on the SCARED dataset show that our method achieves state-of-the-art performance while minimizing the number of trainable parameters. Applying this method in minimally invasive endoscopic surgery could significantly enhance both the precision and safety of these procedures.",
            "id": "2409.07723",
            "link": "http://arxiv.org/abs/2409.07723v1",
            "published": "2024-09-12T03:04:43+00:00",
            "updated": "2024-09-12T03:04:43+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 50
        },
        "2409.07753": {
            "authors": [
                "Xiaotong Zhang",
                "Dingcheng Huang",
                "Kamal Youcef-Toumi"
            ],
            "title": "Relevance for Human Robot Collaboration",
            "abstract": "Effective human-robot collaboration (HRC) requires the robots to possess human-like intelligence. Inspired by the human's cognitive ability to selectively process and filter elements in complex environments, this paper introduces a novel concept and scene-understanding approach termed `relevance.' It identifies relevant components in a scene. To accurately and efficiently quantify relevance, we developed an event-based framework that selectively triggers relevance determination, along with a probabilistic methodology built on a structured scene representation. Simulation results demonstrate that the relevance framework and methodology accurately predict the relevance of a general HRC setup, achieving a precision of 0.99 and a recall of 0.94. Relevance can be broadly applied to several areas in HRC to improve task planning time by 79.56% compared with pure planning for a cereal task, reduce perception latency by up to 26.53% for an object detector, improve HRC safety by up to 13.50% and reduce the number of inquiries for HRC by 75.36%. A real-world demonstration showcases the relevance framework's ability to intelligently assist humans in everyday tasks.",
            "id": "2409.07753",
            "link": "http://arxiv.org/abs/2409.07753v1",
            "published": "2024-09-12T04:57:34+00:00",
            "updated": "2024-09-12T04:57:34+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 40
        },
        "2409.07763": {
            "authors": [
                "Sheng Shen",
                "Rabih Younes"
            ],
            "title": "Reimagining Linear Probing: Kolmogorov-Arnold Networks in Transfer Learning",
            "abstract": "This paper introduces Kolmogorov-Arnold Networks (KAN) as an enhancement to the traditional linear probing method in transfer learning. Linear probing, often applied to the final layer of pre-trained models, is limited by its inability to model complex relationships in data. To address this, we propose substituting the linear probing layer with KAN, which leverages spline-based representations to approximate intricate functions. In this study, we integrate KAN with a ResNet-50 model pre-trained on ImageNet and evaluate its performance on the CIFAR-10 dataset. We perform a systematic hyperparameter search, focusing on grid size and spline degree (k), to optimize KAN's flexibility and accuracy. Our results demonstrate that KAN consistently outperforms traditional linear probing, achieving significant improvements in accuracy and generalization across a range of configurations. These findings indicate that KAN offers a more powerful and adaptable alternative to conventional linear probing techniques in transfer learning.",
            "id": "2409.07763",
            "link": "http://arxiv.org/abs/2409.07763v1",
            "published": "2024-09-12T05:36:40+00:00",
            "updated": "2024-09-12T05:36:40+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 18
        },
        "2409.07796": {
            "authors": [
                "Mohammad Mehdi Rastikerdar",
                "Jin Huang",
                "Hui Guan",
                "Deepak Ganesan"
            ],
            "title": "In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for Efficient Adaptation",
            "abstract": "Wildlife monitoring via camera traps has become an essential tool in ecology, but the deployment of machine learning models for on-device animal classification faces significant challenges due to domain shifts and resource constraints. This paper introduces WildFit, a novel approach that reconciles the conflicting goals of achieving high domain generalization performance and ensuring efficient inference for camera trap applications. WildFit leverages continuous background-aware model fine-tuning to deploy ML models tailored to the current location and time window, allowing it to maintain robust classification accuracy in the new environment without requiring significant computational resources. This is achieved by background-aware data synthesis, which generates training images representing the new domain by blending background images with animal images from the source domain. We further enhance fine-tuning effectiveness through background drift detection and class distribution drift detection, which optimize the quality of synthesized data and improve generalization performance. Our extensive evaluation across multiple camera trap datasets demonstrates that WildFit achieves significant improvements in classification accuracy and computational efficiency compared to traditional approaches.",
            "id": "2409.07796",
            "link": "http://arxiv.org/abs/2409.07796v1",
            "published": "2024-09-12T06:56:52+00:00",
            "updated": "2024-09-12T06:56:52+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 49
        },
        "2409.07822": {
            "authors": [
                "Seyed Mohammad Azimi-Abarghouyi",
                "Leandros Tassiulas"
            ],
            "title": "Over-the-Air Federated Learning via Weighted Aggregation",
            "abstract": "This paper introduces a new federated learning scheme that leverages over-the-air computation. A novel feature of this scheme is the proposal to employ adaptive weights during aggregation, a facet treated as predefined in other over-the-air schemes. This can mitigate the impact of wireless channel conditions on learning performance, without needing channel state information at transmitter side (CSIT). We provide a mathematical methodology to derive the convergence bound for the proposed scheme in the context of computational heterogeneity and general loss functions, supplemented with design insights. Accordingly, we propose aggregation cost metrics and efficient algorithms to find optimized weights for the aggregation. Finally, through numerical experiments, we validate the effectiveness of the proposed scheme. Even with the challenges posed by channel conditions and device heterogeneity, the proposed scheme surpasses other over-the-air strategies by an accuracy improvement of 15% over the scheme using CSIT and 30% compared to the one without CSIT.",
            "id": "2409.07822",
            "link": "http://arxiv.org/abs/2409.07822v1",
            "published": "2024-09-12T08:07:11+00:00",
            "updated": "2024-09-12T08:07:11+00:00",
            "primary_category": "cs.IT",
            "categories": [
                "cs.IT",
                "cs.AI",
                "cs.LG",
                "math.IT"
            ],
            "max_author_hindex": 73
        },
        "2409.07913": {
            "authors": [
                "Inzamamul Alam",
                "Muhammad Shahid Muneer",
                "Simon S. Woo"
            ],
            "title": "UGAD: Universal Generative AI Detector utilizing Frequency Fingerprints",
            "abstract": "In the wake of a fabricated explosion image at the Pentagon, an ability to discern real images from fake counterparts has never been more critical. Our study introduces a novel multi-modal approach to detect AI-generated images amidst the proliferation of new generation methods such as Diffusion models. Our method, UGAD, encompasses three key detection steps: First, we transform the RGB images into YCbCr channels and apply an Integral Radial Operation to emphasize salient radial features. Secondly, the Spatial Fourier Extraction operation is used for a spatial shift, utilizing a pre-trained deep learning network for optimal feature extraction. Finally, the deep neural network classification stage processes the data through dense layers using softmax for classification. Our approach significantly enhances the accuracy of differentiating between real and AI-generated images, as evidenced by a 12.64% increase in accuracy and 28.43% increase in AUC compared to existing state-of-the-art methods.",
            "id": "2409.07913",
            "link": "http://arxiv.org/abs/2409.07913v1",
            "published": "2024-09-12T10:29:37+00:00",
            "updated": "2024-09-12T10:29:37+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 33
        },
        "2409.07914": {
            "authors": [
                "Andrew Lee",
                "Ian Chuang",
                "Ling-Yuan Chen",
                "Iman Soltani"
            ],
            "title": "InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation",
            "abstract": "We present InterACT: Inter-dependency aware Action Chunking with Hierarchical Attention Transformers, a novel imitation learning framework for bimanual manipulation that integrates hierarchical attention to capture inter-dependencies between dual-arm joint states and visual inputs. InterACT consists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both designed to enhance information aggregation and coordination. The encoder processes multi-modal inputs through segment-wise and cross-segment attention mechanisms, while the decoder leverages synchronization blocks to refine individual action predictions, providing the counterpart's prediction as context. Our experiments on a variety of simulated and real-world bimanual manipulation tasks demonstrate that InterACT significantly outperforms existing methods. Detailed ablation studies validate the contributions of key components of our work, including the impact of CLS tokens, cross-segment encoders, and synchronization blocks.",
            "id": "2409.07914",
            "link": "http://arxiv.org/abs/2409.07914v1",
            "published": "2024-09-12T10:30:44+00:00",
            "updated": "2024-09-12T10:30:44+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 47
        },
        "2409.07918": {
            "authors": [
                "Elizabeth Wilson",
                "Gy\u00f6rgy Fazekas",
                "Geraint Wiggins"
            ],
            "title": "Tidal MerzA: Combining affective modelling and autonomous code generation through Reinforcement Learning",
            "abstract": "This paper presents Tidal-MerzA, a novel system designed for collaborative performances between humans and a machine agent in the context of live coding, specifically focusing on the generation of musical patterns. Tidal-MerzA fuses two foundational models: ALCAA (Affective Live Coding Autonomous Agent) and Tidal Fuzz, a computational framework. By integrating affective modelling with computational generation, this system leverages reinforcement learning techniques to dynamically adapt music composition parameters within the TidalCycles framework, ensuring both affective qualities to the patterns and syntactical correctness. The development of Tidal-MerzA introduces two distinct agents: one focusing on the generation of mini-notation strings for musical expression, and another on the alignment of music with targeted affective states through reinforcement learning. This approach enhances the adaptability and creative potential of live coding practices and allows exploration of human-machine creative interactions. Tidal-MerzA advances the field of computational music generation, presenting a novel methodology for incorporating artificial intelligence into artistic practices.",
            "id": "2409.07918",
            "link": "http://arxiv.org/abs/2409.07918v1",
            "published": "2024-09-12T10:38:55+00:00",
            "updated": "2024-09-12T10:38:55+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 43
        },
        "2409.07925": {
            "authors": [
                "Eduardo Cueto-Mendoza",
                "John D. Kelleher"
            ],
            "title": "A framework for measuring the training efficiency of a neural architecture",
            "abstract": "Measuring Efficiency in neural network system development is an open research problem. This paper presents an experimental framework to measure the training efficiency of a neural architecture. To demonstrate our approach, we analyze the training efficiency of Convolutional Neural Networks and Bayesian equivalents on the MNIST and CIFAR-10 tasks. Our results show that training efficiency decays as training progresses and varies across different stopping criteria for a given neural model and learning task. We also find a non-linear relationship between training stopping criteria, training Efficiency, model size, and training Efficiency.   Furthermore, we illustrate the potential confounding effects of overtraining on measuring the training efficiency of a neural architecture. Regarding relative training efficiency across different architectures, our results indicate that CNNs are more efficient than BCNNs on both datasets. More generally, as a learning task becomes more complex, the relative difference in training efficiency between different architectures becomes more pronounced.",
            "id": "2409.07925",
            "link": "http://arxiv.org/abs/2409.07925v1",
            "published": "2024-09-12T10:45:38+00:00",
            "updated": "2024-09-12T10:45:38+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 24
        },
        "2409.07932": {
            "authors": [
                "Alexei Pisacane",
                "Victor-Alexandru Darvariu",
                "Mirco Musolesi"
            ],
            "title": "Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies",
            "abstract": "Graph path search is a classic computer science problem that has been recently approached with Reinforcement Learning (RL) due to its potential to outperform prior methods. Existing RL techniques typically assume a global view of the network, which is not suitable for large-scale, dynamic, and privacy-sensitive settings. An area of particular interest is search in social networks due to its numerous applications. Inspired by seminal work in experimental sociology, which showed that decentralized yet efficient search is possible in social networks, we frame the problem as a collaborative task between multiple agents equipped with a limited local view of the network. We propose a multi-agent approach for graph path search that successfully leverages both homophily and structural heterogeneity. Our experiments, carried out over synthetic and real-world social networks, demonstrate that our model significantly outperforms learned and heuristic baselines. Furthermore, our results show that meaningful embeddings for graph navigation can be constructed using reward-driven learning.",
            "id": "2409.07932",
            "link": "http://arxiv.org/abs/2409.07932v1",
            "published": "2024-09-12T10:56:38+00:00",
            "updated": "2024-09-12T10:56:38+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.MA",
                "cs.SI"
            ],
            "max_author_hindex": 53
        },
        "2409.07964": {
            "authors": [
                "Jingwen Tong",
                "Jiawei Shao",
                "Qiong Wu",
                "Wei Guo",
                "Zijian Li",
                "Zehong Lin",
                "Jun Zhang"
            ],
            "title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks",
            "abstract": "Wireless networks are increasingly facing challenges due to their expanding scale and complexity. These challenges underscore the need for advanced AI-driven strategies, particularly in the upcoming 6G networks. In this article, we introduce WirelessAgent, a novel approach leveraging large language models (LLMs) to develop AI agents capable of managing complex tasks in wireless networks. It can effectively improve network performance through advanced reasoning, multimodal data processing, and autonomous decision making. Thereafter, we demonstrate the practical applicability and benefits of WirelessAgent for network slicing management. The experimental results show that WirelessAgent is capable of accurately understanding user intent, effectively allocating slice resources, and consistently maintaining optimal performance.",
            "id": "2409.07964",
            "link": "http://arxiv.org/abs/2409.07964v1",
            "published": "2024-09-12T11:48:01+00:00",
            "updated": "2024-09-12T11:48:01+00:00",
            "primary_category": "cs.NI",
            "categories": [
                "cs.NI",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 26
        },
        "2409.08012": {
            "authors": [
                "Ivan Ovinnikov",
                "Eugene Bykovets",
                "Joachim M. Buhmann"
            ],
            "title": "Learning Causally Invariant Reward Functions from Diverse Demonstrations",
            "abstract": "Inverse reinforcement learning methods aim to retrieve the reward function of a Markov decision process based on a dataset of expert demonstrations. The commonplace scarcity and heterogeneous sources of such demonstrations can lead to the absorption of spurious correlations in the data by the learned reward function. Consequently, this adaptation often exhibits behavioural overfitting to the expert data set when a policy is trained on the obtained reward function under distribution shift of the environment dynamics. In this work, we explore a novel regularization approach for inverse reinforcement learning methods based on the causal invariance principle with the goal of improved reward function generalization. By applying this regularization to both exact and approximate formulations of the learning task, we demonstrate superior policy performance when trained using the recovered reward functions in a transfer setting",
            "id": "2409.08012",
            "link": "http://arxiv.org/abs/2409.08012v1",
            "published": "2024-09-12T12:56:24+00:00",
            "updated": "2024-09-12T12:56:24+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 74
        },
        "2409.08111": {
            "authors": [
                "Louis Van Langendonck",
                "Ismael Castell-Uroz",
                "Pere Barlet-Ros"
            ],
            "title": "Towards a graph-based foundation model for network traffic analysis",
            "abstract": "Foundation models have shown great promise in various fields of study. A potential application of such models is in computer network traffic analysis, where these models can grasp the complexities of network traffic dynamics and adapt to any specific task or network environment with minimal fine-tuning. Previous approaches have used tokenized hex-level packet data and the model architecture of large language transformer models. We propose a new, efficient graph-based alternative at the flow-level. Our approach represents network traffic as a dynamic spatio-temporal graph, employing a self-supervised link prediction pretraining task to capture the spatial and temporal dynamics in this network graph framework. To evaluate the effectiveness of our approach, we conduct a few-shot learning experiment for three distinct downstream network tasks: intrusion detection, traffic classification, and botnet classification. Models finetuned from our pretrained base achieve an average performance increase of 6.87\\% over training from scratch, demonstrating their ability to effectively learn general network traffic dynamics during pretraining. This success suggests the potential for a large-scale version to serve as an operational foundational model.",
            "id": "2409.08111",
            "link": "http://arxiv.org/abs/2409.08111v1",
            "published": "2024-09-12T15:04:34+00:00",
            "updated": "2024-09-12T15:04:34+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "cs.NI"
            ],
            "max_author_hindex": 27
        },
        "2409.08215": {
            "authors": [
                "Quan Meng",
                "Lei Li",
                "Matthias Nie\u00dfner",
                "Angela Dai"
            ],
            "title": "LT3SD: Latent Trees for 3D Scene Diffusion",
            "abstract": "We present LT3SD, a novel latent diffusion model for large-scale 3D scene generation. Recent advances in diffusion models have shown impressive results in 3D object generation, but are limited in spatial extent and quality when extended to 3D scenes. To generate complex and diverse 3D scene structures, we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy. We can then learn a generative diffusion process in this latent 3D scene space, modeling the latent components of a scene at each resolution level. To synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3D scenes through shared diffusion generation across multiple scene patches. Through extensive experiments, we demonstrate the efficacy and benefits of LT3SD for large-scale, high-quality unconditional 3D scene generation and for probabilistic completion for partial scene observations.",
            "id": "2409.08215",
            "link": "http://arxiv.org/abs/2409.08215v1",
            "published": "2024-09-12T16:55:51+00:00",
            "updated": "2024-09-12T16:55:51+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 73
        },
        "2409.08217": {
            "authors": [
                "Davide Buffelli",
                "Farzin Soleymani",
                "Bastian Rieck"
            ],
            "title": "CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs",
            "abstract": "Graph neural networks have become the default choice by practitioners for graph learning tasks such as graph classification and node classification. Nevertheless, popular graph neural network models still struggle to capture higher-order information, i.e., information that goes \\emph{beyond} pairwise interactions. Recent work has shown that persistent homology, a tool from topological data analysis, can enrich graph neural networks with topological information that they otherwise could not capture. Calculating such features is efficient for dimension 0 (connected components) and dimension 1 (cycles). However, when it comes to higher-order structures, it does not scale well, with a complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the order of the structures. In this work, we introduce a novel method that extracts information about higher-order structures in the graph while still using the efficient low-dimensional persistent homology algorithm. On standard benchmark datasets, we show that our method can lead to up to $31\\%$ improvements in test accuracy.",
            "id": "2409.08217",
            "link": "http://arxiv.org/abs/2409.08217v1",
            "published": "2024-09-12T16:56:26+00:00",
            "updated": "2024-09-12T16:56:26+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 25
        },
        "2409.08240": {
            "authors": [
                "Yinwei Wu",
                "Xianpan Zhou",
                "Bing Ma",
                "Xuefeng Su",
                "Kai Ma",
                "Xinchao Wang"
            ],
            "title": "IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation",
            "abstract": "While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models' abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations.",
            "id": "2409.08240",
            "link": "http://arxiv.org/abs/2409.08240v1",
            "published": "2024-09-12T17:39:23+00:00",
            "updated": "2024-09-12T17:39:23+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 52
        },
        "2409.08250": {
            "authors": [
                "Jiahao Nick Li",
                "Zhuohao Jerry Zhang",
                "Jiaju Ma"
            ],
            "title": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering",
            "abstract": "People often capture memories through photos, screenshots, and videos. While existing AI-based tools enable querying this data using natural language, they mostly only support retrieving individual pieces of information like certain objects in photos and struggle with answering more complex queries that involve interpreting interconnected memories like event sequences. We conducted a one-month diary study to collect realistic user queries and generated a taxonomy of necessary contextual information for integrating with captured memories. We then introduce OmniQuery, a novel system that is able to answer complex personal memory-related questions that require extracting and inferring contextual information. OmniQuery augments single captured memories through integrating scattered contextual information from multiple interconnected memories, retrieves relevant memories, and uses a large language model (LLM) to comprehensive answers. In human evaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG system, winning or tying in 74.5% of the time.",
            "id": "2409.08250",
            "link": "http://arxiv.org/abs/2409.08250v1",
            "published": "2024-09-12T17:48:08+00:00",
            "updated": "2024-09-12T17:48:08+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.08255": {
            "authors": [
                "Geigh Zollicoffer",
                "Minh Vu",
                "Ben Nebgen",
                "Juan Castorena",
                "Boian Alexandrov",
                "Manish Bhattarai"
            ],
            "title": "LoRID: Low-Rank Iterative Diffusion for Adversarial Purification",
            "abstract": "This work presents an information-theoretic examination of diffusion-based purification methods, the state-of-the-art adversarial defenses that utilize diffusion models to remove malicious perturbations in adversarial examples. By theoretically characterizing the inherent purification errors associated with the Markov-based diffusion purifications, we introduce LoRID, a novel Low-Rank Iterative Diffusion purification method designed to remove adversarial perturbation with low intrinsic purification errors. LoRID centers around a multi-stage purification process that leverages multiple rounds of diffusion-denoising loops at the early time-steps of the diffusion models, and the integration of Tucker decomposition, an extension of matrix factorization, to remove adversarial noise at high-noise regimes. Consequently, LoRID increases the effective diffusion time-steps and overcomes strong adversarial attacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ, and ImageNet datasets under both white-box and black-box settings.",
            "id": "2409.08255",
            "link": "http://arxiv.org/abs/2409.08255v1",
            "published": "2024-09-12T17:51:25+00:00",
            "updated": "2024-09-12T17:51:25+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ],
            "max_author_hindex": 22
        },
        "2409.08270": {
            "authors": [
                "Qiuhong Shen",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "title": "FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally",
            "abstract": "This study addresses the challenge of accurately segmenting 3D Gaussian Splatting from 2D masks. Conventional methods often rely on iterative gradient descent to assign each Gaussian a unique label, leading to lengthy optimization and sub-optimal solutions. Instead, we propose a straightforward yet globally optimal solver for 3D-GS segmentation. The core insight of our method is that, with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially a linear function with respect to the labels of each Gaussian. As such, the optimal label assignment can be solved via linear programming in closed form. This solution capitalizes on the alpha blending characteristic of the splatting process for single step optimization. By incorporating the background bias in our objective function, our method shows superior robustness in 3D segmentation against noises. Remarkably, our optimization completes within 30 seconds, about 50$\\times$ faster than the best existing methods. Extensive experiments demonstrate the efficiency and robustness of our method in segmenting various scenes, and its superior performance in downstream tasks such as object removal and inpainting. Demos and code will be available at https://github.com/florinshen/FlashSplat.",
            "id": "2409.08270",
            "link": "http://arxiv.org/abs/2409.08270v1",
            "published": "2024-09-12T17:58:13+00:00",
            "updated": "2024-09-12T17:58:13+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.GR",
                "cs.MM"
            ],
            "max_author_hindex": 52
        },
        "2409.08273": {
            "authors": [
                "Himanshu Gaurav Singh",
                "Antonio Loquercio",
                "Carmelo Sferrazza",
                "Jane Wu",
                "Haozhi Qi",
                "Pieter Abbeel",
                "Jitendra Malik"
            ],
            "title": "Hand-Object Interaction Pretraining from Videos",
            "abstract": "We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: \\url{https://hgaurav2k.github.io/hop/}.",
            "id": "2409.08273",
            "link": "http://arxiv.org/abs/2409.08273v1",
            "published": "2024-09-12T17:59:07+00:00",
            "updated": "2024-09-12T17:59:07+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 150
        },
        "2409.08276": {
            "authors": [
                "Raunaq Bhirangi",
                "Venkatesh Pattabiraman",
                "Enes Erciyes",
                "Yifeng Cao",
                "Tess Hellebrekers",
                "Lerrel Pinto"
            ],
            "title": "AnySkin: Plug-and-play Skin Sensing for Robotic Touch",
            "abstract": "While tactile sensing is widely accepted as an important and useful sensing modality, its use pales in comparison to other sensory modalities like vision and proprioception. AnySkin addresses the critical challenges that impede the use of tactile sensing -- versatility, replaceability, and data reusability. Building on the simplistic design of ReSkin, and decoupling the sensing electronics from the sensing interface, AnySkin simplifies integration making it as straightforward as putting on a phone case and connecting a charger. Furthermore, AnySkin is the first uncalibrated tactile-sensor with cross-instance generalizability of learned manipulation policies. To summarize, this work makes three key contributions: first, we introduce a streamlined fabrication process and a design tool for creating an adhesive-free, durable and easily replaceable magnetic tactile sensor; second, we characterize slip detection and policy learning with the AnySkin sensor; and third, we demonstrate zero-shot generalization of models trained on one instance of AnySkin to new instances, and compare it with popular existing tactile solutions like DIGIT and ReSkin.https://any-skin.github.io/",
            "id": "2409.08276",
            "link": "http://arxiv.org/abs/2409.08276v1",
            "published": "2024-09-12T17:59:44+00:00",
            "updated": "2024-09-12T17:59:44+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 36
        },
        "2409.05277": {
            "authors": [
                "Chanho Eom",
                "Wonkyung Lee",
                "Geon Lee",
                "Bumsub Ham"
            ],
            "title": "Disentangled Representations for Short-Term and Long-Term Person Re-Identification",
            "abstract": "We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons could have the same attribute, and persons' appearances look different, e.g., with viewpoint changes. Recent reID methods focus on learning person features discriminative only for a particular factor of variations (e.g., human pose), which also requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to factorize person images into identity-related and unrelated features. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose). To this end, we propose a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN). It disentangles identity-related and unrelated features from person images through an identity-shuffling technique that exploits identification labels alone without any auxiliary supervisory signals. We restrict the distribution of identity-unrelated features or encourage the identity-related and unrelated features to be uncorrelated, facilitating the disentanglement process. Experimental results validate the effectiveness of IS-GAN, showing state-of-the-art performance on standard reID benchmarks, including Market-1501, CUHK03, and DukeMTMC-reID. We further demonstrate the advantages of disentangling person representations on a long-term reID task, setting a new state of the art on a Celeb-reID dataset.",
            "id": "2409.05277",
            "link": "http://arxiv.org/abs/2409.05277v1",
            "published": "2024-09-09T02:09:49+00:00",
            "updated": "2024-09-09T02:09:49+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 30
        },
        "2409.05280": {
            "authors": [
                "Quoc-Bao Nguyen-Le",
                "Tuan-Hy Le",
                "Anh-Triet Do",
                "Quoc-Huy Trinh"
            ],
            "title": "RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation",
            "abstract": "Cardiovascular disease is a major global health concern, contributing significantly to global mortality. Accurately segmenting cardiac medical imaging data is crucial for reducing fatality rates associated with these conditions. However, current state-of-the-art (SOTA) neural networks, including CNN-based and Transformer-based approaches, face challenges in capturing both inter-slice connections and intra-slice details, especially in datasets featuring intricate, long-range details along the z-axis like coronary arteries. Existing methods also struggle with differentiating non-cardiac components from the myocardium, resulting in segmentation inaccuracies and the \"spraying\" phenomenon. To address these issues, we introduce RotCAtt-TransUNet++, a novel architecture designed for robust segmentation of intricate cardiac structures. Our approach enhances global context modeling through multiscale feature aggregation and nested skip connections in the encoder. Transformer layers facilitate capturing intra-slice interactions, while a rotatory attention mechanism handles inter-slice connectivity. A channel-wise cross-attention gate integrates multiscale information and decoder features, effectively bridging semantic gaps. Experimental results across multiple datasets demonstrate superior performance over current methods, achieving near-perfect annotation of coronary arteries and myocardium. Ablation studies confirm that our rotatory attention mechanism significantly improves segmentation accuracy by transforming embedded vectorized patches in semantic dimensional space.",
            "id": "2409.05280",
            "link": "http://arxiv.org/abs/2409.05280v1",
            "published": "2024-09-09T02:18:50+00:00",
            "updated": "2024-09-09T02:18:50+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 12
        },
        "2409.05303": {
            "authors": [
                "Yuxin Liang",
                "Peng Yang",
                "Yuanyuan He",
                "Feng Lyu"
            ],
            "title": "Resource-Efficient Generative AI Model Deployment in Mobile Edge Networks",
            "abstract": "The surging development of Artificial Intelligence-Generated Content (AIGC) marks a transformative era of the content creation and production. Edge servers promise attractive benefits, e.g., reduced service delay and backhaul traffic load, for hosting AIGC services compared to cloud-based solutions. However, the scarcity of available resources on the edge pose significant challenges in deploying generative AI models. In this paper, by characterizing the resource and delay demands of typical generative AI models, we find that the consumption of storage and GPU memory, as well as the model switching delay represented by I/O delay during the preloading phase, are significant and vary across models. These multidimensional coupling factors render it difficult to make efficient edge model deployment decisions. Hence, we present a collaborative edge-cloud framework aiming to properly manage generative AI model deployment on the edge. Specifically, we formulate edge model deployment problem considering heterogeneous features of models as an optimization problem, and propose a model-level decision selection algorithm to solve it. It enables pooled resource sharing and optimizes the trade-off between resource consumption and delay in edge generative AI model deployment. Simulation results validate the efficacy of the proposed algorithm compared with baselines, demonstrating its potential to reduce overall costs by providing feature-aware model deployment decisions.",
            "id": "2409.05303",
            "link": "http://arxiv.org/abs/2409.05303v1",
            "published": "2024-09-09T03:17:28+00:00",
            "updated": "2024-09-09T03:17:28+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 36
        },
        "2409.05314": {
            "authors": [
                "Ali Maatouk",
                "Kenny Chirino Ampudia",
                "Rex Ying",
                "Leandros Tassiulas"
            ],
            "title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications",
            "abstract": "The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in telecommunications remain limited, often relying on general-purpose models that lack domain-specific specialization. This lack of specialization results in underperformance, particularly when dealing with telecommunications-specific technical terminology and their associated mathematical representations. This paper addresses this gap by first creating and disseminating Tele-Data, a comprehensive dataset of telecommunications material curated from relevant sources, and Tele-Eval, a large-scale question-and-answer dataset tailored to the domain. Through extensive experiments, we explore the most effective training techniques for adapting LLMs to the telecommunications domain, ranging from examining the division of expertise across various telecommunications aspects to employing parameter-efficient techniques. We also investigate how models of different sizes behave during adaptation and analyze the impact of their training data on this behavior. Leveraging these findings, we develop and open-source Tele-LLMs, the first series of language models ranging from 1B to 8B parameters, specifically tailored for telecommunications. Our evaluations demonstrate that these models outperform their general-purpose counterparts on Tele-Eval while retaining their previously acquired capabilities, thus avoiding the catastrophic forgetting phenomenon.",
            "id": "2409.05314",
            "link": "http://arxiv.org/abs/2409.05314v1",
            "published": "2024-09-09T03:58:51+00:00",
            "updated": "2024-09-09T03:58:51+00:00",
            "primary_category": "cs.IT",
            "categories": [
                "cs.IT",
                "cs.AI",
                "cs.LG",
                "math.IT"
            ],
            "max_author_hindex": 73
        },
        "2409.05319": {
            "authors": [
                "Kai Li",
                "Khalid Zaman",
                "Xingfeng Li",
                "Masato Akagi",
                "Masashi Unoki"
            ],
            "title": "Machine Anomalous Sound Detection Using Spectral-temporal Modulation Representations Derived from Machine-specific Filterbanks",
            "abstract": "Early detection of factory machinery malfunctions is crucial in industrial applications. In machine anomalous sound detection (ASD), different machines exhibit unique vibration-frequency ranges based on their physical properties. Meanwhile, the human auditory system is adept at tracking both temporal and spectral dynamics of machine sounds. Consequently, integrating the computational auditory models of the human auditory system with machine-specific properties can be an effective approach to machine ASD. We first quantified the frequency importances of four types of machines using the Fisher ratio (F-ratio). The quantified frequency importances were then used to design machine-specific non-uniform filterbanks (NUFBs), which extract the log non-uniform spectrum (LNS) feature. The designed NUFBs have a narrower bandwidth and higher filter distribution density in frequency regions with relatively high F-ratios. Finally, spectral and temporal modulation representations derived from the LNS feature were proposed. These proposed LNS feature and modulation representations are input into an autoencoder neural-network-based detector for ASD. The quantification results from the training set of the Malfunctioning Industrial Machine Investigation and Inspection dataset with a signal-to-noise (SNR) of 6 dB reveal that the distinguishing information between normal and anomalous sounds of different machines is encoded non-uniformly in the frequency domain. By highlighting these important frequency regions using NUFBs, the LNS feature can significantly enhance performance using the metric of AUC (area under the receiver operating characteristic curve) under various SNR conditions. Furthermore, modulation representations can further improve performance. Specifically, temporal modulation is effective for fans, pumps, and sliders, while spectral modulation is particularly effective for valves.",
            "id": "2409.05319",
            "link": "http://arxiv.org/abs/2409.05319v1",
            "published": "2024-09-09T04:27:17+00:00",
            "updated": "2024-09-09T04:27:17+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI"
            ],
            "max_author_hindex": 60
        },
        "2409.05335": {
            "authors": [
                "Md Hasebul Hasan",
                "Md Abid Jahan",
                "Mohammed Eunus Ali",
                "Yuan-Fang Li",
                "Timos Sellis"
            ],
            "title": "A Multi-Modal Deep Learning Based Approach for House Price Prediction",
            "abstract": "Accurate prediction of house price, a vital aspect of the residential real estate sector, is of substantial interest for a wide range of stakeholders. However, predicting house prices is a complex task due to the significant variability influenced by factors such as house features, location, neighborhood, and many others. Despite numerous attempts utilizing a wide array of algorithms, including recent deep learning techniques, to predict house prices accurately, existing approaches have fallen short of considering a wide range of factors such as textual and visual features. This paper addresses this gap by comprehensively incorporating attributes, such as features, textual descriptions, geo-spatial neighborhood, and house images, typically showcased in real estate listings in a house price prediction system. Specifically, we propose a multi-modal deep learning approach that leverages different types of data to learn more accurate representation of the house. In particular, we learn a joint embedding of raw house attributes, geo-spatial neighborhood, and most importantly from textual description and images representing the house; and finally use a downstream regression model to predict the house price from this jointly learned embedding vector. Our experimental results with a real-world dataset show that the text embedding of the house advertisement description and image embedding of the house pictures in addition to raw attributes and geo-spatial embedding, can significantly improve the house price prediction accuracy. The relevant source code and dataset are publicly accessible at the following URL: https://github.com/4P0N/mhpp",
            "id": "2409.05335",
            "link": "http://arxiv.org/abs/2409.05335v1",
            "published": "2024-09-09T05:26:33+00:00",
            "updated": "2024-09-09T05:26:33+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "I.2.7; I.2.10"
            ],
            "max_author_hindex": 55
        },
        "2409.05336": {
            "authors": [
                "Edanur Demir",
                "Emre Akbas"
            ],
            "title": "Early-exit Convolutional Neural Networks",
            "abstract": "This paper is aimed at developing a method that reduces the computational cost of convolutional neural networks (CNN) during inference. Conventionally, the input data pass through a fixed neural network architecture. However, easy examples can be classified at early stages of processing and conventional networks do not take this into account. In this paper, we introduce 'Early-exit CNNs', EENets for short, which adapt their computational cost based on the input by stopping the inference process at certain exit locations. In EENets, there are a number of exit blocks each of which consists of a confidence branch and a softmax branch. The confidence branch computes the confidence score of exiting (i.e. stopping the inference process) at that location; while the softmax branch outputs a classification probability vector. Both branches are learnable and their parameters are separate. During training of EENets, in addition to the classical classification loss, the computational cost of inference is taken into account as well. As a result, the network adapts its many confidence branches to the inputs so that less computation is spent for easy examples. Inference works as in conventional feed-forward networks, however, when the output of a confidence branch is larger than a certain threshold, the inference stops for that specific example. The idea of EENets is applicable to available CNN architectures such as ResNets. Through comprehensive experiments on MNIST, SVHN, CIFAR10 and Tiny-ImageNet datasets, we show that early-exit (EE) ResNets achieve similar accuracy with their non-EE versions while reducing the computational cost to 20% of the original. Code is available at https://github.com/eksuas/eenets.pytorch",
            "id": "2409.05336",
            "link": "http://arxiv.org/abs/2409.05336v1",
            "published": "2024-09-09T05:29:38+00:00",
            "updated": "2024-09-09T05:29:38+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 10
        },
        "2409.05346": {
            "authors": [
                "Kangjun Lee",
                "Minha Kim",
                "Youngho Jun",
                "Simon S. Woo"
            ],
            "title": "GDFlow: Anomaly Detection with NCDE-based Normalizing Flow for Advanced Driver Assistance System",
            "abstract": "For electric vehicles, the Adaptive Cruise Control (ACC) in Advanced Driver Assistance Systems (ADAS) is designed to assist braking based on driving conditions, road inclines, predefined deceleration strengths, and user braking patterns. However, the driving data collected during the development of ADAS are generally limited and lack diversity. This deficiency leads to late or aggressive braking for different users. Crucially, it is necessary to effectively identify anomalies, such as unexpected or inconsistent braking patterns in ADAS, especially given the challenge of working with unlabelled, limited, and noisy datasets from real-world electric vehicles. In order to tackle the aforementioned challenges in ADAS, we propose Graph Neural Controlled Differential Equation Normalizing Flow (GDFlow), a model that leverages Normalizing Flow (NF) with Neural Controlled Differential Equations (NCDE) to learn the distribution of normal driving patterns continuously. Compared to the traditional clustering or anomaly detection algorithms, our approach effectively captures the spatio-temporal information from different sensor data and more accurately models continuous changes in driving patterns. Additionally, we introduce a quantile-based maximum likelihood objective to improve the likelihood estimate of the normal data near the boundary of the distribution, enhancing the model's ability to distinguish between normal and anomalous patterns. We validate GDFlow using real-world electric vehicle driving data that we collected from Hyundai IONIQ5 and GV80EV, achieving state-of-the-art performance compared to six baselines across four dataset configurations of different vehicle types and drivers. Furthermore, our model outperforms the latest anomaly detection methods across four time series benchmark datasets. Our approach demonstrates superior efficiency in inference time compared to existing methods.",
            "id": "2409.05346",
            "link": "http://arxiv.org/abs/2409.05346v1",
            "published": "2024-09-09T06:04:41+00:00",
            "updated": "2024-09-09T06:04:41+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 33
        },
        "2409.05358": {
            "authors": [
                "Aly Lidayan",
                "Michael Dennis",
                "Stuart Russell"
            ],
            "title": "BAMDP Shaping: a Unified Theoretical Framework for Intrinsic Motivation and Reward Shaping",
            "abstract": "Intrinsic motivation (IM) and reward shaping are common methods for guiding the exploration of reinforcement learning (RL) agents by adding pseudo-rewards. Designing these rewards is challenging, however, and they can counter-intuitively harm performance. To address this, we characterize them as reward shaping in Bayes-Adaptive Markov Decision Processes (BAMDPs), which formalizes the value of exploration by formulating the RL process as updating a prior over possible MDPs through experience. RL algorithms can be viewed as BAMDP policies; instead of attempting to find optimal algorithms by solving BAMDPs directly, we use it at a theoretical framework for understanding how pseudo-rewards guide suboptimal algorithms. By decomposing BAMDP state value into the value of the information collected plus the prior value of the physical state, we show how psuedo-rewards can help by compensating for RL algorithms' misestimation of these two terms, yielding a new typology of IM and reward shaping approaches. We carefully extend the potential-based shaping theorem to BAMDPs to prove that when pseudo-rewards are BAMDP Potential-based shaping Functions (BAMPFs), they preserve optimal, or approximately optimal, behavior of RL algorithms; otherwise, they can corrupt even optimal learners. We finally give guidance on how to design or convert existing pseudo-rewards to BAMPFs by expressing assumptions about the environment as potential functions on BAMDP states.",
            "id": "2409.05358",
            "link": "http://arxiv.org/abs/2409.05358v1",
            "published": "2024-09-09T06:39:56+00:00",
            "updated": "2024-09-09T06:39:56+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 12
        },
        "2409.05379": {
            "authors": [
                "Longhao Zhang",
                "Shuang Liang",
                "Zhipeng Ge",
                "Tianshu Hu"
            ],
            "title": "PersonaTalk: Bring Attention to Your Persona in Visual Dubbing",
            "abstract": "For audio-driven visual dubbing, it remains a considerable challenge to uphold and highlight speaker's persona while synthesizing accurate lip synchronization. Existing methods fall short of capturing speaker's unique speaking style or preserving facial details. In this paper, we present PersonaTalk, an attention-based two-stage framework, including geometry construction and face rendering, for high-fidelity and personalized visual dubbing. In the first stage, we propose a style-aware audio encoding module that injects speaking style into audio features through a cross-attention layer. The stylized audio features are then used to drive speaker's template geometry to obtain lip-synced geometries. In the second stage, a dual-attention face renderer is introduced to render textures for the target geometries. It consists of two parallel cross-attention layers, namely Lip-Attention and Face-Attention, which respectively sample textures from different reference frames to render the entire face. With our innovative design, intricate facial details can be well preserved. Comprehensive experiments and user studies demonstrate our advantages over other state-of-the-art methods in terms of visual quality, lip-sync accuracy and persona preservation. Furthermore, as a person-generic framework, PersonaTalk can achieve competitive performance as state-of-the-art person-specific methods. Project Page: https://grisoon.github.io/PersonaTalk/.",
            "id": "2409.05379",
            "link": "http://arxiv.org/abs/2409.05379v1",
            "published": "2024-09-09T07:23:28+00:00",
            "updated": "2024-09-09T07:23:28+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ],
            "max_author_hindex": 20
        },
        "2409.05383": {
            "authors": [
                "Peng Wu",
                "Chengyu Pan",
                "Yuting Yan",
                "Guansong Pang",
                "Peng Wang",
                "Yanning Zhang"
            ],
            "title": "Deep Learning for Video Anomaly Detection: A Review",
            "abstract": "Video anomaly detection (VAD) aims to discover behaviors or events deviating from the normality in videos. As a long-standing task in the field of computer vision, VAD has witnessed much good progress. In the era of deep learning, with the explosion of architectures of continuously growing capability and capacity, a great variety of deep learning based methods are constantly emerging for the VAD task, greatly improving the generalization ability of detection algorithms and broadening the application scenarios. Therefore, such a multitude of methods and a large body of literature make a comprehensive survey a pressing necessity. In this paper, we present an extensive and comprehensive research review, covering the spectrum of five different categories, namely, semi-supervised, weakly supervised, fully supervised, unsupervised and open-set supervised VAD, and we also delve into the latest VAD works based on pre-trained large models, remedying the limitations of past reviews in terms of only focusing on semi-supervised VAD and small model based methods. For the VAD task with different levels of supervision, we construct a well-organized taxonomy, profoundly discuss the characteristics of different types of methods, and show their performance comparisons. In addition, this review involves the public datasets, open-source codes, and evaluation metrics covering all the aforementioned VAD tasks. Finally, we provide several important research directions for the VAD community.",
            "id": "2409.05383",
            "link": "http://arxiv.org/abs/2409.05383v1",
            "published": "2024-09-09T07:31:16+00:00",
            "updated": "2024-09-09T07:31:16+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 54
        },
        "2409.05384": {
            "authors": [
                "Shiming Ge",
                "Kangkai Zhang",
                "Haolin Liu",
                "Yingying Hua",
                "Shengwei Zhao",
                "Xin Jin",
                "Hao Wen"
            ],
            "title": "Look One and More: Distilling Hybrid Order Relational Knowledge for Cross-Resolution Image Recognition",
            "abstract": "In spite of great success in many image recognition tasks achieved by recent deep models, directly applying them to recognize low-resolution images may suffer from low accuracy due to the missing of informative details during resolution degradation. However, these images are still recognizable for subjects who are familiar with the corresponding high-resolution ones. Inspired by that, we propose a teacher-student learning approach to facilitate low-resolution image recognition via hybrid order relational knowledge distillation. The approach refers to three streams: the teacher stream is pretrained to recognize high-resolution images in high accuracy, the student stream is learned to identify low-resolution images by mimicking the teacher's behaviors, and the extra assistant stream is introduced as bridge to help knowledge transfer across the teacher to the student. To extract sufficient knowledge for reducing the loss in accuracy, the learning of student is supervised with multiple losses, which preserves the similarities in various order relational structures. In this way, the capability of recovering missing details of familiar low-resolution images can be effectively enhanced, leading to a better knowledge transfer. Extensive experiments on metric learning, low-resolution image classification and low-resolution face recognition tasks show the effectiveness of our approach, while taking reduced models.",
            "id": "2409.05384",
            "link": "http://arxiv.org/abs/2409.05384v1",
            "published": "2024-09-09T07:32:18+00:00",
            "updated": "2024-09-09T07:32:18+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ],
            "max_author_hindex": 23
        },
        "2409.05396": {
            "authors": [
                "Jianzhi Lu",
                "Ruian He",
                "Shili Zhou",
                "Weimin Tan",
                "Bo Yan"
            ],
            "title": "FacialFlowNet: Advancing Facial Optical Flow Estimation with a Diverse Dataset and a Decomposed Model",
            "abstract": "Facial movements play a crucial role in conveying altitude and intentions, and facial optical flow provides a dynamic and detailed representation of it. However, the scarcity of datasets and a modern baseline hinders the progress in facial optical flow research. This paper proposes FacialFlowNet (FFN), a novel large-scale facial optical flow dataset, and the Decomposed Facial Flow Model (DecFlow), the first method capable of decomposing facial flow. FFN comprises 9,635 identities and 105,970 image pairs, offering unprecedented diversity for detailed facial and head motion analysis. DecFlow features a facial semantic-aware encoder and a decomposed flow decoder, excelling in accurately estimating and decomposing facial flow into head and expression components. Comprehensive experiments demonstrate that FFN significantly enhances the accuracy of facial flow estimation across various optical flow methods, achieving up to an 11% reduction in Endpoint Error (EPE) (from 3.91 to 3.48). Moreover, DecFlow, when coupled with FFN, outperforms existing methods in both synthetic and real-world scenarios, enhancing facial expression analysis. The decomposed expression flow achieves a substantial accuracy improvement of 18% (from 69.1% to 82.1%) in micro-expressions recognition. These contributions represent a significant advancement in facial motion analysis and optical flow estimation. Codes and datasets can be found.",
            "id": "2409.05396",
            "link": "http://arxiv.org/abs/2409.05396v1",
            "published": "2024-09-09T07:49:13+00:00",
            "updated": "2024-09-09T07:49:13+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 16
        },
        "2409.05402": {
            "authors": [
                "Ziming Zhao",
                "Tiehua Zhang",
                "Zijian Yi",
                "Zhishu Shen"
            ],
            "title": "HyperSMOTE: A Hypergraph-based Oversampling Approach for Imbalanced Node Classifications",
            "abstract": "Hypergraphs are increasingly utilized in both unimodal and multimodal data scenarios due to their superior ability to model and extract higher-order relationships among nodes, compared to traditional graphs. However, current hypergraph models are encountering challenges related to imbalanced data, as this imbalance can lead to biases in the model towards the more prevalent classes. While the existing techniques, such as GraphSMOTE, have improved classification accuracy for minority samples in graph data, they still fall short when addressing the unique structure of hypergraphs. Inspired by SMOTE concept, we propose HyperSMOTE as a solution to alleviate the class imbalance issue in hypergraph learning. This method involves a two-step process: initially synthesizing minority class nodes, followed by the nodes integration into the original hypergraph. We synthesize new nodes based on samples from minority classes and their neighbors. At the same time, in order to solve the problem on integrating the new node into the hypergraph, we train a decoder based on the original hypergraph incidence matrix to adaptively associate the augmented node to hyperedges. We conduct extensive evaluation on multiple single-modality datasets, such as Cora, Cora-CA and Citeseer, as well as multimodal conversation dataset MELD to verify the effectiveness of HyperSMOTE, showing an average performance gain of 3.38% and 2.97% on accuracy, respectively.",
            "id": "2409.05402",
            "link": "http://arxiv.org/abs/2409.05402v1",
            "published": "2024-09-09T08:01:28+00:00",
            "updated": "2024-09-09T08:01:28+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 29
        },
        "2409.05564": {
            "authors": [
                "Patrick Palmer",
                "Martin Kr\u00fcger",
                "Stefan Sch\u00fctte",
                "Richard Altendorfer",
                "Ganesh Adam",
                "Torsten Bertram"
            ],
            "title": "LEROjD: Lidar Extended Radar-Only Object Detection",
            "abstract": "Accurate 3D object detection is vital for automated driving. While lidar sensors are well suited for this task, they are expensive and have limitations in adverse weather conditions. 3+1D imaging radar sensors offer a cost-effective, robust alternative but face challenges due to their low resolution and high measurement noise. Existing 3+1D imaging radar datasets include radar and lidar data, enabling cross-modal model improvements. Although lidar should not be used during inference, it can aid the training of radar-only object detectors. We explore two strategies to transfer knowledge from the lidar to the radar domain and radar-only object detectors: 1. multi-stage training with sequential lidar point cloud thin-out, and 2. cross-modal knowledge distillation. In the multi-stage process, three thin-out methods are examined. Our results show significant performance gains of up to 4.2 percentage points in mean Average Precision with multi-stage training and up to 3.9 percentage points with knowledge distillation by initializing the student with the teacher's weights. The main benefit of these approaches is their applicability to other 3D object detection networks without altering their architecture, as we show by analyzing it on two different object detectors. Our code is available at https://github.com/rst-tu-dortmund/lerojd",
            "id": "2409.05564",
            "link": "http://arxiv.org/abs/2409.05564v1",
            "published": "2024-09-09T12:43:25+00:00",
            "updated": "2024-09-09T12:43:25+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 41
        },
        "2409.05573": {
            "authors": [
                "Lirong Wu",
                "Haitao Lin",
                "Guojiang Zhao",
                "Cheng Tan",
                "Stan Z. Li"
            ],
            "title": "Learning to Model Graph Structural Information on MLPs via Graph Structure Self-Contrasting",
            "abstract": "Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). However, most existing GNNs are based on message passing to perform feature aggregation and transformation, where the structural information is explicitly involved in the forward propagation by coupling with node features through graph convolution at each layer. As a result, subtle feature noise or structure perturbation may cause severe error propagation, resulting in extremely poor robustness. In this paper, we rethink the roles played by graph structural information in graph data training and identify that message passing is not the only path to modeling structural information. Inspired by this, we propose a simple but effective Graph Structure Self-Contrasting (GSSC) framework that learns graph structural information without message passing. The proposed framework is based purely on Multi-Layer Perceptrons (MLPs), where the structural information is only implicitly incorporated as prior knowledge to guide the computation of supervision signals, substituting the explicit message propagation as in GNNs. Specifically, it first applies structural sparsification to remove potentially uninformative or noisy edges in the neighborhood, and then performs structural self-contrasting in the sparsified neighborhood to learn robust node representations. Finally, structural sparsification and self-contrasting are formulated as a bi-level optimization problem and solved in a unified framework. Extensive experiments have qualitatively and quantitatively demonstrated that the GSSC framework can produce truly encouraging performance with better generalization and robustness than other leading competitors.",
            "id": "2409.05573",
            "link": "http://arxiv.org/abs/2409.05573v1",
            "published": "2024-09-09T12:56:02+00:00",
            "updated": "2024-09-09T12:56:02+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 34
        },
        "2409.05611": {
            "authors": [
                "Tianwu Lei",
                "Silin Chen",
                "Bohan Wang",
                "Zhengkai Jiang",
                "Ningmu Zou"
            ],
            "title": "Adapted-MoE: Mixture of Experts with Test-Time Adaption for Anomaly Detection",
            "abstract": "Most unsupervised anomaly detection methods based on representations of normal samples to distinguish anomalies have recently made remarkable progress. However, existing methods only learn a single decision boundary for distinguishing the samples within the training dataset, neglecting the variation in feature distribution for normal samples even in the same category in the real world. Furthermore, it was not considered that a distribution bias still exists between the test set and the train set. Therefore, we propose an Adapted-MoE which contains a routing network and a series of expert models to handle multiple distributions of same-category samples by divide and conquer. Specifically, we propose a routing network based on representation learning to route same-category samples into the subclasses feature space. Then, a series of expert models are utilized to learn the representation of various normal samples and construct several independent decision boundaries. We propose the test-time adaption to eliminate the bias between the unseen test sample representation and the feature distribution learned by the expert model. Our experiments are conducted on a dataset that provides multiple subclasses from three categories, namely Texture AD benchmark. The Adapted-MoE significantly improves the performance of the baseline model, achieving 2.18%-7.20% and 1.57%-16.30% increase in I-AUROC and P-AUROC, which outperforms the current state-of-the-art methods. Our code is available at https://github.com/.",
            "id": "2409.05611",
            "link": "http://arxiv.org/abs/2409.05611v1",
            "published": "2024-09-09T13:49:09+00:00",
            "updated": "2024-09-09T13:49:09+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 43
        },
        "2409.05650": {
            "authors": [
                "Riccardo De Monte",
                "Davide Dalle Pezze",
                "Marina Ceccon",
                "Francesco Pasti",
                "Francesco Paissan",
                "Elisabetta Farella",
                "Gian Antonio Susto",
                "Nicola Bellotto"
            ],
            "title": "Replay Consolidation with Label Propagation for Continual Object Detection",
            "abstract": "Object Detection is a highly relevant computer vision problem with many applications such as robotics and autonomous driving. Continual Learning~(CL) considers a setting where a model incrementally learns new information while retaining previously acquired knowledge. This is particularly challenging since Deep Learning models tend to catastrophically forget old knowledge while training on new data. In particular, Continual Learning for Object Detection~(CLOD) poses additional difficulties compared to CL for Classification. In CLOD, images from previous tasks may contain unknown classes that could reappear labeled in future tasks. These missing annotations cause task interference issues for replay-based approaches. As a result, most works in the literature have focused on distillation-based approaches. However, these approaches are effective only when there is a strong overlap of classes across tasks. To address the issues of current methodologies, we propose a novel technique to solve CLOD called Replay Consolidation with Label Propagation for Object Detection (RCLPOD). Based on the replay method, our solution avoids task interference issues by enhancing the buffer memory samples. Our method is evaluated against existing techniques in CLOD literature, demonstrating its superior performance on established benchmarks like VOC and COCO.",
            "id": "2409.05650",
            "link": "http://arxiv.org/abs/2409.05650v1",
            "published": "2024-09-09T14:16:27+00:00",
            "updated": "2024-09-09T14:16:27+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 34
        },
        "2409.05655": {
            "authors": [
                "Markus Knauer",
                "Alin Albu-Sch\u00e4ffer",
                "Freek Stulp",
                "Jo\u00e3o Silv\u00e9rio"
            ],
            "title": "Interactive incremental learning of generalizable skills with local trajectory modulation",
            "abstract": "The problem of generalization in learning from demonstration (LfD) has received considerable attention over the years, particularly within the context of movement primitives, where a number of approaches have emerged. Recently, two important approaches have gained recognition. While one leverages via-points to adapt skills locally by modulating demonstrated trajectories, another relies on so-called task-parameterized models that encode movements with respect to different coordinate systems, using a product of probabilities for generalization. While the former are well-suited to precise, local modulations, the latter aim at generalizing over large regions of the workspace and often involve multiple objects. Addressing the quality of generalization by leveraging both approaches simultaneously has received little attention. In this work, we propose an interactive imitation learning framework that simultaneously leverages local and global modulations of trajectory distributions. Building on the kernelized movement primitives (KMP) framework, we introduce novel mechanisms for skill modulation from direct human corrective feedback. Our approach particularly exploits the concept of via-points to incrementally and interactively 1) improve the model accuracy locally, 2) add new objects to the task during execution and 3) extend the skill into regions where demonstrations were not provided. We evaluate our method on a bearing ring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.",
            "id": "2409.05655",
            "link": "http://arxiv.org/abs/2409.05655v1",
            "published": "2024-09-09T14:22:19+00:00",
            "updated": "2024-09-09T14:22:19+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 67
        },
        "2409.05662": {
            "authors": [
                "Ruiqi Wang",
                "Zichen Wang",
                "Peiqi Gao",
                "Mingzhen Li",
                "Jaehwan Jeong",
                "Yihang Xu",
                "Yejin Lee",
                "Carolyn M. Baum",
                "Lisa Tabor Connor",
                "Chenyang Lu"
            ],
            "title": "Real-Time Human Action Recognition on Embedded Platforms",
            "abstract": "With advancements in computer vision and deep learning, video-based human action recognition (HAR) has become practical. However, due to the complexity of the computation pipeline, running HAR on live video streams incurs excessive delays on embedded platforms. This work tackles the real-time performance challenges of HAR with four contributions: 1) an experimental study identifying a standard Optical Flow (OF) extraction technique as the latency bottleneck in a state-of-the-art HAR pipeline, 2) an exploration of the latency-accuracy tradeoff between the standard and deep learning approaches to OF extraction, which highlights the need for a novel, efficient motion feature extractor, 3) the design of Integrated Motion Feature Extractor (IMFE), a novel single-shot neural network architecture for motion feature extraction with drastic improvement in latency, 4) the development of RT-HARE, a real-time HAR system tailored for embedded platforms. Experimental results on an Nvidia Jetson Xavier NX platform demonstrated that RT-HARE realizes real-time HAR at a video frame rate of 30 frames per second while delivering high levels of recognition accuracy.",
            "id": "2409.05662",
            "link": "http://arxiv.org/abs/2409.05662v2",
            "published": "2024-09-09T14:35:23+00:00",
            "updated": "2024-09-11T14:21:29+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 36
        },
        "2409.05672": {
            "authors": [
                "Yuchen Shen",
                "Haomin Wen",
                "Leman Akoglu"
            ],
            "title": "Zero-shot Outlier Detection via Prior-data Fitted Networks: Model Selection Bygone!",
            "abstract": "Outlier detection (OD) has a vast literature as it finds numerous applications in environmental monitoring, cybersecurity, finance, and medicine to name a few. Being an inherently unsupervised task, model selection is a key bottleneck for OD (both algorithm and hyperparameter selection) without label supervision. There is a long list of techniques to choose from -- both classical algorithms and deep neural architectures -- and while several studies report their hyperparameter sensitivity, the literature is quite slim on unsupervised model selection -- limiting the effective use of OD in practice. In this paper we present FoMo-0D, for zero/0-shot OD exploring a transformative new direction that bypasses the hurdle of model selection altogether (!), thus breaking new ground. The fundamental idea behind FoMo-0D is the Prior-data Fitted Networks, recently introduced by Muller et al.(2022), which trains a Transformer model on a large body of synthetically generated data from a prior data distribution. In essence, FoMo-0D is a pretrained Foundation Model for zero/0-shot OD on tabular data, which can directly predict the (outlier/inlier) label of any test data at inference time, by merely a single forward pass -- making obsolete the need for choosing an algorithm/architecture, tuning its associated hyperparameters, and even training any model parameters when given a new OD dataset. Extensive experiments on 57 public benchmark datasets against 26 baseline methods show that FoMo-0D performs statistically no different from the top 2nd baseline, while significantly outperforming the majority of the baselines, with an average inference time of 7.7 ms per test sample.",
            "id": "2409.05672",
            "link": "http://arxiv.org/abs/2409.05672v1",
            "published": "2024-09-09T14:41:24+00:00",
            "updated": "2024-09-09T14:41:24+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 53
        },
        "2409.05698": {
            "authors": [
                "Mengyu Wang",
                "Tiejun Ma"
            ],
            "title": "MANA-Net: Mitigating Aggregated Sentiment Homogenization with News Weighting for Enhanced Market Prediction",
            "abstract": "It is widely acknowledged that extracting market sentiments from news data benefits market predictions. However, existing methods of using financial sentiments remain simplistic, relying on equal-weight and static aggregation to manage sentiments from multiple news items. This leads to a critical issue termed ``Aggregated Sentiment Homogenization'', which has been explored through our analysis of a large financial news dataset from industry practice. This phenomenon occurs when aggregating numerous sentiments, causing representations to converge towards the mean values of sentiment distributions and thereby smoothing out unique and important information. Consequently, the aggregated sentiment representations lose much predictive value of news data. To address this problem, we introduce the Market Attention-weighted News Aggregation Network (MANA-Net), a novel method that leverages a dynamic market-news attention mechanism to aggregate news sentiments for market prediction. MANA-Net learns the relevance of news sentiments to price changes and assigns varying weights to individual news items. By integrating the news aggregation step into the networks for market prediction, MANA-Net allows for trainable sentiment representations that are optimized directly for prediction. We evaluate MANA-Net using the S&P 500 and NASDAQ 100 indices, along with financial news spanning from 2003 to 2018. Experimental results demonstrate that MANA-Net outperforms various recent market prediction methods, enhancing Profit & Loss by 1.1% and the daily Sharpe ratio by 0.252.",
            "id": "2409.05698",
            "link": "http://arxiv.org/abs/2409.05698v1",
            "published": "2024-09-09T15:12:24+00:00",
            "updated": "2024-09-09T15:12:24+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CE",
                "q-fin.CP"
            ],
            "max_author_hindex": 8
        },
        "2409.05701": {
            "authors": [
                "Jiahao Lai",
                "Jiaqi Li",
                "Jian Xu",
                "Yanru Wu",
                "Boshi Tang",
                "Siqi Chen",
                "Yongfeng Huang",
                "Wenbo Ding",
                "Yang Li"
            ],
            "title": "pFedGPA: Diffusion-based Generative Parameter Aggregation for Personalized Federated Learning",
            "abstract": "Federated Learning (FL) offers a decentralized approach to model training, where data remains local and only model parameters are shared between the clients and the central server. Traditional methods, such as Federated Averaging (FedAvg), linearly aggregate these parameters which are usually trained on heterogeneous data distributions, potentially overlooking the complex, high-dimensional nature of the parameter space. This can result in degraded performance of the aggregated model. While personalized FL approaches can mitigate the heterogeneous data issue to some extent, the limitation of linear aggregation remains unresolved. To alleviate this issue, we investigate the generative approach of diffusion model and propose a novel generative parameter aggregation framework for personalized FL, \\texttt{pFedGPA}. In this framework, we deploy a diffusion model on the server to integrate the diverse parameter distributions and propose a parameter inversion method to efficiently generate a set of personalized parameters for each client. This inversion method transforms the uploaded parameters into a latent code, which is then aggregated through denoising sampling to produce the final personalized parameters. By encoding the dependence of a client's model parameters on the specific data distribution using the high-capacity diffusion model, \\texttt{pFedGPA} can effectively decouple the complexity of the overall distribution of all clients' model parameters from the complexity of each individual client's parameter distribution. Our experimental results consistently demonstrate the superior performance of the proposed method across multiple datasets, surpassing baseline approaches.",
            "id": "2409.05701",
            "link": "http://arxiv.org/abs/2409.05701v1",
            "published": "2024-09-09T15:13:56+00:00",
            "updated": "2024-09-09T15:13:56+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 49
        },
        "2409.05747": {
            "authors": [
                "B. Sankar",
                "Dibakar Sen"
            ],
            "title": "A Novel Idea Generation Tool using a Structured Conversational AI (CAI) System",
            "abstract": "This paper presents a novel conversational AI-enabled active ideation interface as a creative idea-generation tool to assist novice designers in mitigating the initial latency and ideation bottlenecks that are commonly observed. It is a dynamic, interactive, and contextually responsive approach, actively involving a large language model (LLM) from the domain of natural language processing (NLP) in artificial intelligence (AI) to produce multiple statements of potential ideas for different design problems. Integrating such AI models with ideation creates what we refer to as an Active Ideation scenario, which helps foster continuous dialogue-based interaction, context-sensitive conversation, and prolific idea generation. A pilot study was conducted with thirty novice designers to generate ideas for given problems using traditional methods and the new CAI-based interface. The key parameters of fluency, novelty, and variety were used to compare the outcomes qualitatively by a panel of experts. The findings demonstrated the effectiveness of the proposed tool for generating prolific, diverse and novel ideas. The interface was enhanced by incorporating a prompt-engineered structured dialogue style for each ideation stage to make it uniform and more convenient for the designers. The resulting responses of such a structured CAI interface were found to be more succinct and aligned towards the subsequent design stage, namely conceptualization. The paper thus established the rich potential of using Generative AI (Gen-AI) for the early ill-structured phase of the creative product design process.",
            "id": "2409.05747",
            "link": "http://arxiv.org/abs/2409.05747v1",
            "published": "2024-09-09T16:02:27+00:00",
            "updated": "2024-09-09T16:02:27+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.AI",
                "I.2; J.6"
            ],
            "max_author_hindex": 39
        },
        "2409.05785": {
            "authors": [
                "Wenqi Jia",
                "Youyuan Liu",
                "Zhewen Hu",
                "Jinzhen Wang",
                "Boyuan Zhang",
                "Wei Niu",
                "Junzhou Huang",
                "Stavros Kalafatis",
                "Sian Jin",
                "Miao Yin"
            ],
            "title": "NeurLZ: On Enhancing Lossy Compression Performance based on Error-Controlled Neural Learning for Scientific Data",
            "abstract": "Large-scale scientific simulations generate massive datasets that pose significant challenges for storage and I/O. While traditional lossy compression techniques can improve performance, balancing compression ratio, data quality, and throughput remains difficult. To address this, we propose NeurLZ, a novel cross-field learning-based and error-controlled compression framework for scientific data. By integrating skipping DNN models, cross-field learning, and error control, our framework aims to substantially enhance lossy compression performance. Our contributions are three-fold: (1) We design a lightweight skipping model to provide high-fidelity detail retention, further improving prediction accuracy. (2) We adopt a cross-field learning approach to significantly improve data prediction accuracy, resulting in a substantially improved compression ratio. (3) We develop an error control approach to provide strict error bounds according to user requirements. We evaluated NeurLZ on several real-world HPC application datasets, including Nyx (cosmological simulation), Miranda (large turbulence simulation), and Hurricane (weather simulation). Experiments demonstrate that our framework achieves up to a 90% relative reduction in bit rate under the same data distortion, compared to the best existing approach.",
            "id": "2409.05785",
            "link": "http://arxiv.org/abs/2409.05785v2",
            "published": "2024-09-09T16:48:09+00:00",
            "updated": "2024-09-10T02:02:12+00:00",
            "primary_category": "cs.DC",
            "categories": [
                "cs.DC",
                "cs.AI"
            ],
            "max_author_hindex": 76
        },
        "2409.05808": {
            "authors": [
                "Mohammad Baqar",
                "Rajat Khanda"
            ],
            "title": "The Future of Software Testing: AI-Powered Test Case Generation and Validation",
            "abstract": "Software testing is a crucial phase in the software development lifecycle (SDLC), ensuring that products meet necessary functional, performance, and quality benchmarks before release. Despite advancements in automation, traditional methods of generating and validating test cases still face significant challenges, including prolonged timelines, human error, incomplete test coverage, and high costs of manual intervention. These limitations often lead to delayed product launches and undetected defects that compromise software quality and user satisfaction. The integration of artificial intelligence (AI) into software testing presents a promising solution to these persistent challenges. AI-driven testing methods automate the creation of comprehensive test cases, dynamically adapt to changes, and leverage machine learning to identify high-risk areas in the codebase. This approach enhances regression testing efficiency while expanding overall test coverage. Furthermore, AI-powered tools enable continuous testing and self-healing test cases, significantly reducing manual oversight and accelerating feedback loops, ultimately leading to faster and more reliable software releases. This paper explores the transformative potential of AI in improving test case generation and validation, focusing on its ability to enhance efficiency, accuracy, and scalability in testing processes. It also addresses key challenges associated with adapting AI for testing, including the need for high quality training data, ensuring model transparency, and maintaining a balance between automation and human oversight. Through case studies and examples of real-world applications, this paper illustrates how AI can significantly enhance testing efficiency across both legacy and modern software systems.",
            "id": "2409.05808",
            "link": "http://arxiv.org/abs/2409.05808v1",
            "published": "2024-09-09T17:12:40+00:00",
            "updated": "2024-09-09T17:12:40+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 10
        },
        "2409.05923": {
            "authors": [
                "Shuai Wang",
                "Liang Ding",
                "Li Shen",
                "Yong Luo",
                "Zheng He",
                "Wei Yu",
                "Dacheng Tao"
            ],
            "title": "$\\mathbb{USCD}$: Improving Code Generation of LLMs by Uncertainty-Aware Selective Contrastive Decoding",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities in code generation. However, the effects of hallucinations (e.g., output noise) make it particularly challenging for LLMs to generate high-quality code in one pass. In this work, we propose a simple and effective \\textbf{u}ncertainty-aware \\textbf{s}elective \\textbf{c}ontrastive \\textbf{d}ecoding ($\\mathbb{USCD}$) mechanism to improve the quality of one-pass code generation in LLMs and reduce the impact of output noise. To be specific, we first elaborately designed a negative prompt (namely lame prompt) to output noise by removing input-output examples from the standard few-shot prompt. Our preliminary study shows that the Jensen-Shannon divergence (JS divergence) between token distribution uncertainty and the output noise is relatively low (approximately $0.25$), indicating their high relevance. Then, we selectively eliminate output noise induced by lame prompts based on the uncertainty of the prediction distribution from the standard prompt. Notably, our proposed plug-and-play mechanism is an inference-only method, enjoying appealing flexibility. Extensive experiments on widely used benchmarks, e.g., HumanEval, MBPP, and MultiPL-E, upon several LLMs (i.e., Inocder-6b, CodeLlama-7b, WizardCoder-15b, StarCoder, and Llama2-7b), demonstrate that our proposed USCD significantly improves one-pass code generation, with an average \\textit{pass@$1$} scores increase of 16.59\\%. We will release code and data on GitHub.",
            "id": "2409.05923",
            "link": "http://arxiv.org/abs/2409.05923v1",
            "published": "2024-09-09T02:07:41+00:00",
            "updated": "2024-09-09T02:07:41+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 153
        },
        "2409.05929": {
            "authors": [
                "Hongyang Lei",
                "Xiaolong Cheng",
                "Dan Wang",
                "Qi Qin",
                "Huazhen Huang",
                "Yetao Wu",
                "Qingqing Gu",
                "Zhonglin Jiang",
                "Yong Chen",
                "Luo Ji"
            ],
            "title": "Alt-MoE: Multimodal Alignment via Alternating Optimization of Multi-directional MoE with Unimodal Models",
            "abstract": "Recent Large Multi-Modal Models (LMMs) have made significant advancements in multi-modal alignment by employing lightweight connection modules to facilitate the representation and fusion of knowledge from existing pre-trained uni-modal models. However, these methods still rely on modality-specific and direction-specific connectors, leading to compartmentalized knowledge representations and reduced computational efficiency, which limits the model's ability to form unified multi-modal representations. To address these issues, we introduce a novel training framework, Alt-MoE, which employs the Mixture of Experts (MoE) as a unified multi-directional connector across modalities, and employs a multi-step sequential alternating unidirectional alignment strategy, which converges to bidirectional alignment over iterations. The extensive empirical studies revealed the following key points: 1) Alt-MoE achieves competitive results by integrating diverse knowledge representations from uni-modal models. This approach seamlessly fuses the specialized expertise of existing high-performance uni-modal models, effectively synthesizing their domain-specific knowledge into a cohesive multi-modal representation. 2) Alt-MoE efficiently scales to new tasks and modalities without altering its model architecture or training strategy. Furthermore, Alt-MoE operates in latent space, supporting vector pre-storage and real-time retrieval via lightweight multi-directional MoE, thereby facilitating massive data processing. Our methodology has been validated on several well-performing uni-modal models (LLAMA3, Qwen2, and DINOv2), achieving competitive results on a wide range of downstream tasks and datasets.",
            "id": "2409.05929",
            "link": "http://arxiv.org/abs/2409.05929v1",
            "published": "2024-09-09T10:40:50+00:00",
            "updated": "2024-09-09T10:40:50+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 60
        },
        "2409.06029": {
            "authors": [
                "Shun Lei",
                "Yixuan Zhou",
                "Boshi Tang",
                "Max W. Y. Lam",
                "Feng Liu",
                "Hangyu Liu",
                "Jingcheng Wu",
                "Shiyin Kang",
                "Zhiyong Wu",
                "Helen Meng"
            ],
            "title": "SongCreator: Lyrics-based Universal Song Generation",
            "abstract": "Music is an integral part of human culture, embodying human intelligence and creativity, of which songs compose an essential part. While various aspects of song generation have been explored by previous works, such as singing voice, vocal composition and instrumental arrangement, etc., generating songs with both vocals and accompaniment given lyrics remains a significant challenge, hindering the application of music generation models in the real world. In this light, we propose SongCreator, a song-generation system designed to tackle this challenge. The model features two novel designs: a meticulously designed dual-sequence language model (DSLM) to capture the information of vocals and accompaniment for song generation, and an additional attention mask strategy for DSLM, which allows our model to understand, generate and edit songs, making it suitable for various song-related generation tasks. Extensive experiments demonstrate the effectiveness of SongCreator by achieving state-of-the-art or competitive performances on all eight tasks. Notably, it surpasses previous works by a large margin in lyrics-to-song and lyrics-to-vocals. Additionally, it is able to independently control the acoustic conditions of the vocals and accompaniment in the generated song through different prompts, exhibiting its potential applicability. Our samples are available at https://songcreator.github.io/.",
            "id": "2409.06029",
            "link": "http://arxiv.org/abs/2409.06029v1",
            "published": "2024-09-09T19:37:07+00:00",
            "updated": "2024-09-09T19:37:07+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 43
        },
        "2409.06077": {
            "authors": [
                "Faezeh Faez",
                "Raika Karimi",
                "Yingxue Zhang",
                "Xing Li",
                "Lei Chen",
                "Mingxuan Yuan",
                "Mahdi Biparva"
            ],
            "title": "MTLSO: A Multi-Task Learning Approach for Logic Synthesis Optimization",
            "abstract": "Electronic Design Automation (EDA) is essential for IC design and has recently benefited from AI-based techniques to improve efficiency. Logic synthesis, a key EDA stage, transforms high-level hardware descriptions into optimized netlists. Recent research has employed machine learning to predict Quality of Results (QoR) for pairs of And-Inverter Graphs (AIGs) and synthesis recipes. However, the severe scarcity of data due to a very limited number of available AIGs results in overfitting, significantly hindering performance. Additionally, the complexity and large number of nodes in AIGs make plain GNNs less effective for learning expressive graph-level representations. To tackle these challenges, we propose MTLSO - a Multi-Task Learning approach for Logic Synthesis Optimization. On one hand, it maximizes the use of limited data by training the model across different tasks. This includes introducing an auxiliary task of binary multi-label graph classification alongside the primary regression task, allowing the model to benefit from diverse supervision sources. On the other hand, we employ a hierarchical graph representation learning strategy to improve the model's capacity for learning expressive graph-level representations of large AIGs, surpassing traditional plain GNNs. Extensive experiments across multiple datasets and against state-of-the-art baselines demonstrate the superiority of our method, achieving an average performance gain of 8.22\\% for delay and 5.95\\% for area.",
            "id": "2409.06077",
            "link": "http://arxiv.org/abs/2409.06077v1",
            "published": "2024-09-09T21:20:36+00:00",
            "updated": "2024-09-09T21:20:36+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 24
        },
        "2409.06111": {
            "authors": [
                "Sara Pohland",
                "Claire Tomlin"
            ],
            "title": "PaRCE: Probabilistic and Reconstruction-Based Competency Estimation for Safe Navigation Under Perception Uncertainty",
            "abstract": "Perception-based navigation systems are useful for unmanned ground vehicle (UGV) navigation in complex terrains, where traditional depth-based navigation schemes are insufficient. However, these data-driven methods are highly dependent on their training data and can fail in surprising and dramatic ways with little warning. To ensure the safety of the vehicle and the surrounding environment, it is imperative that the navigation system is able to recognize the predictive uncertainty of the perception model and respond safely and effectively in the face of uncertainty. In an effort to enable safe navigation under perception uncertainty, we develop a probabilistic and reconstruction-based competency estimation (PaRCE) method to estimate the model's level of familiarity with an input image as a whole and with specific regions in the image. We find that the overall competency score can correctly predict correctly classified, misclassified, and out-of-distribution (OOD) samples. We also confirm that the regional competency maps can accurately distinguish between familiar and unfamiliar regions across images. We then use this competency information to develop a planning and control scheme that enables effective navigation while maintaining a low probability of error. We find that the competency-aware scheme greatly reduces the number of collisions with unfamiliar obstacles, compared to a baseline controller with no competency awareness. Furthermore, the regional competency information is very valuable in enabling efficient navigation.",
            "id": "2409.06111",
            "link": "http://arxiv.org/abs/2409.06111v1",
            "published": "2024-09-09T23:34:24+00:00",
            "updated": "2024-09-09T23:34:24+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.SY",
                "eess.SY"
            ],
            "max_author_hindex": 79
        },
        "2409.06147": {
            "authors": [
                "Dong Han",
                "Jihye Moon",
                "Lu\u00eds Roberto Mercado D\u00edaz",
                "Darren Chen",
                "Devan Williams",
                "Eric Y. Ding",
                "Khanh-Van Tran",
                "David D. McManus",
                "Ki H. Chon"
            ],
            "title": "Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings",
            "abstract": "Most deep learning models of multiclass arrhythmia classification are tested on fingertip photoplethysmographic (PPG) data, which has higher signal-to-noise ratios compared to smartwatch-derived PPG, and the best reported sensitivity value for premature atrial/ventricular contraction (PAC/PVC) detection is only 75%. To improve upon PAC/PVC detection sensitivity while maintaining high AF detection, we use multi-modal data which incorporates 1D PPG, accelerometers, and heart rate data as the inputs to a computationally efficient 1D bi-directional Gated Recurrent Unit (1D-Bi-GRU) model to detect three arrhythmia classes. We used motion-artifact prone smartwatch PPG data from the NIH-funded Pulsewatch clinical trial. Our multimodal model tested on 72 subjects achieved an unprecedented 83% sensitivity for PAC/PVC detection while maintaining a high accuracy of 97.31% for AF detection. These results outperformed the best state-of-the-art model by 20.81% for PAC/PVC and 2.55% for AF detection even while our model was computationally more efficient (14 times lighter and 2.7 faster).",
            "id": "2409.06147",
            "link": "http://arxiv.org/abs/2409.06147v1",
            "published": "2024-09-10T01:44:56+00:00",
            "updated": "2024-09-10T01:44:56+00:00",
            "primary_category": "eess.SP",
            "categories": [
                "eess.SP",
                "cs.AI"
            ],
            "max_author_hindex": 65
        },
        "2409.06163": {
            "authors": [
                "Peng Wang",
                "Xin Wen",
                "Ruochen Cao",
                "Chengxin Gao",
                "Yanrong Hao",
                "Rui Cao"
            ],
            "title": "MCDGLN: Masked Connection-based Dynamic Graph Learning Network for Autism Spectrum Disorder",
            "abstract": "Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by complex physiological processes. Previous research has predominantly focused on static cerebral interactions, often neglecting the brain's dynamic nature and the challenges posed by network noise. To address these gaps, we introduce the Masked Connection-based Dynamic Graph Learning Network (MCDGLN). Our approach first segments BOLD signals using sliding temporal windows to capture dynamic brain characteristics. We then employ a specialized weighted edge aggregation (WEA) module, which uses the cross convolution with channel-wise element-wise convolutional kernel, to integrate dynamic functional connectivity and to isolating task-relevant connections. This is followed by topological feature extraction via a hierarchical graph convolutional network (HGCN), with key attributes highlighted by a self-attention module. Crucially, we refine static functional connections using a customized task-specific mask, reducing noise and pruning irrelevant links. The attention-based connection encoder (ACE) then enhances critical connections and compresses static features. The combined features are subsequently used for classification. Applied to the Autism Brain Imaging Data Exchange I (ABIDE I) dataset, our framework achieves a 73.3\\% classification accuracy between ASD and Typical Control (TC) groups among 1,035 subjects. The pivotal roles of WEA and ACE in refining connectivity and enhancing classification accuracy underscore their importance in capturing ASD-specific features, offering new insights into the disorder.",
            "id": "2409.06163",
            "link": "http://arxiv.org/abs/2409.06163v1",
            "published": "2024-09-10T02:21:29+00:00",
            "updated": "2024-09-10T02:21:29+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 54
        },
        "2409.06277": {
            "authors": [
                "Yao Shu",
                "Wenyang Hu",
                "See-Kiong Ng",
                "Bryan Kian Hsiang Low",
                "Fei Richard Yu"
            ],
            "title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models",
            "abstract": "Large Language Models (LLMs) have become indispensable in numerous real-world applications. Unfortunately, fine-tuning these models at scale, especially in federated settings where data privacy and communication efficiency are critical, presents significant challenges. Existing methods often resort to parameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but this typically comes at the cost of model accuracy. To address these limitations, we propose federated full-parameter tuning at scale for LLMs (Ferret), the first first-order method with shared randomness to enable scalable full-parameter tuning of LLMs across decentralized data sources while maintaining competitive model accuracy. Ferret accomplishes this through three aspects: (1) it employs widely applied first-order methods for efficient local updates; (2) it projects these updates into a low-dimensional space to considerably reduce communication overhead; and (3) it reconstructs local updates from this low-dimensional space with shared randomness to facilitate effective full-parameter global aggregation, ensuring fast convergence and competitive final performance. Our rigorous theoretical analyses and insights along with extensive experiments, show that Ferret significantly enhances the scalability of existing federated full-parameter tuning approaches by achieving high computational efficiency, reduced communication overhead, and fast convergence, all while maintaining competitive model accuracy. Our implementation is available at https://github.com/allen4747/Ferret.",
            "id": "2409.06277",
            "link": "http://arxiv.org/abs/2409.06277v2",
            "published": "2024-09-10T07:28:13+00:00",
            "updated": "2024-09-11T01:47:48+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 72
        },
        "2409.06280": {
            "authors": [
                "Zitao Chen",
                "Karthik Pattabiraman"
            ],
            "title": "Catch Me if You Can: Detecting Unauthorized Data Use in Deep Learning Models",
            "abstract": "The rise of deep learning (DL) has led to a surging demand for training data, which incentivizes the creators of DL models to trawl through the Internet for training materials. Meanwhile, users often have limited control over whether their data (e.g., facial images) are used to train DL models without their consent, which has engendered pressing concerns.   This work proposes MembershipTracker, a practical data provenance tool that can empower ordinary users to take agency in detecting the unauthorized use of their data in training DL models. We view tracing data provenance through the lens of membership inference (MI). MembershipTracker consists of a lightweight data marking component to mark the target data with small and targeted changes, which can be strongly memorized by the model trained on them; and a specialized MI-based verification process to audit whether the model exhibits strong memorization on the target samples.   Overall, MembershipTracker only requires the users to mark a small fraction of data (0.005% to 0.1% in proportion to the training set), and it enables the users to reliably detect the unauthorized use of their data (average 0% FPR@100% TPR). We show that MembershipTracker is highly effective across various settings, including industry-scale training on the full-size ImageNet-1k dataset. We finally evaluate MembershipTracker under multiple classes of countermeasures.",
            "id": "2409.06280",
            "link": "http://arxiv.org/abs/2409.06280v1",
            "published": "2024-09-10T07:31:56+00:00",
            "updated": "2024-09-10T07:31:56+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 39
        },
        "2409.06299": {
            "authors": [
                "Dingxin Cheng",
                "Mingda Li",
                "Jingyu Liu",
                "Yongxin Guo",
                "Bin Jiang",
                "Qingbin Liu",
                "Xi Chen",
                "Bo Zhao"
            ],
            "title": "Enhancing Long Video Understanding via Hierarchical Event-Based Memory",
            "abstract": "Recently, integrating visual foundation models into large language models (LLMs) to form video understanding systems has attracted widespread attention. Most of the existing models compress diverse semantic information within the whole video and feed it into LLMs for content comprehension. While this method excels in short video understanding, it may result in a blend of multiple event information in long videos due to coarse compression, which causes information redundancy. Consequently, the semantics of key events might be obscured within the vast information that hinders the model's understanding capabilities. To address this issue, we propose a Hierarchical Event-based Memory-enhanced LLM (HEM-LLM) for better understanding of long videos. Firstly, we design a novel adaptive sequence segmentation scheme to divide multiple events within long videos. In this way, we can perform individual memory modeling for each event to establish intra-event contextual connections, thereby reducing information redundancy. Secondly, while modeling current event, we compress and inject the information of the previous event to enhance the long-term inter-event dependencies in videos. Finally, we perform extensive experiments on various video understanding tasks and the results show that our model achieves state-of-the-art performances.",
            "id": "2409.06299",
            "link": "http://arxiv.org/abs/2409.06299v1",
            "published": "2024-09-10T07:53:10+00:00",
            "updated": "2024-09-10T07:53:10+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.06323": {
            "authors": [
                "Siqing Li",
                "Jin-Duk Park",
                "Wei Huang",
                "Xin Cao",
                "Won-Yong Shin",
                "Zhiqiang Xu"
            ],
            "title": "LAMP: Learnable Meta-Path Guided Adversarial Contrastive Learning for Heterogeneous Graphs",
            "abstract": "Heterogeneous graph neural networks (HGNNs) have significantly propelled the information retrieval (IR) field. Still, the effectiveness of HGNNs heavily relies on high-quality labels, which are often expensive to acquire. This challenge has shifted attention towards Heterogeneous Graph Contrastive Learning (HGCL), which usually requires pre-defined meta-paths. However, our findings reveal that meta-path combinations significantly affect performance in unsupervised settings, an aspect often overlooked in current literature. Existing HGCL methods have considerable variability in outcomes across different meta-path combinations, thereby challenging the optimization process to achieve consistent and high performance. In response, we introduce \\textsf{LAMP} (\\underline{\\textbf{L}}earn\\underline{\\textbf{A}}ble \\underline{\\textbf{M}}eta-\\underline{\\textbf{P}}ath), a novel adversarial contrastive learning approach that integrates various meta-path sub-graphs into a unified and stable structure, leveraging the overlap among these sub-graphs. To address the denseness of this integrated sub-graph, we propose an adversarial training strategy for edge pruning, maintaining sparsity to enhance model performance and robustness. \\textsf{LAMP} aims to maximize the difference between meta-path and network schema views for guiding contrastive learning to capture the most meaningful information. Our extensive experimental study conducted on four diverse datasets from the Heterogeneous Graph Benchmark (HGB) demonstrates that \\textsf{LAMP} significantly outperforms existing state-of-the-art unsupervised models in terms of accuracy and robustness.",
            "id": "2409.06323",
            "link": "http://arxiv.org/abs/2409.06323v1",
            "published": "2024-09-10T08:27:39+00:00",
            "updated": "2024-09-10T08:27:39+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ],
            "max_author_hindex": 24
        },
        "2409.06348": {
            "authors": [
                "Ziwei Yan",
                "Yanjie Zhao",
                "Haoyu Wang"
            ],
            "title": "VoiceWukong: Benchmarking Deepfake Voice Detection",
            "abstract": "With the rapid advancement of technologies like text-to-speech (TTS) and voice conversion (VC), detecting deepfake voices has become increasingly crucial. However, both academia and industry lack a comprehensive and intuitive benchmark for evaluating detectors. Existing datasets are limited in language diversity and lack many manipulations encountered in real-world production environments.   To fill this gap, we propose VoiceWukong, a benchmark designed to evaluate the performance of deepfake voice detectors. To build the dataset, we first collected deepfake voices generated by 19 advanced and widely recognized commercial tools and 15 open-source tools. We then created 38 data variants covering six types of manipulations, constructing the evaluation dataset for deepfake voice detection. VoiceWukong thus includes 265,200 English and 148,200 Chinese deepfake voice samples. Using VoiceWukong, we evaluated 12 state-of-the-art detectors. AASIST2 achieved the best equal error rate (EER) of 13.50%, while all others exceeded 20%. Our findings reveal that these detectors face significant challenges in real-world applications, with dramatically declining performance. In addition, we conducted a user study with more than 300 participants. The results are compared with the performance of the 12 detectors and a multimodel large language model (MLLM), i.e., Qwen2-Audio, where different detectors and humans exhibit varying identification capabilities for deepfake voices at different deception levels, while the LALM demonstrates no detection ability at all. Furthermore, we provide a leaderboard for deepfake voice detection, publicly available at {https://voicewukong.github.io}.",
            "id": "2409.06348",
            "link": "http://arxiv.org/abs/2409.06348v1",
            "published": "2024-09-10T09:07:12+00:00",
            "updated": "2024-09-10T09:07:12+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.CR",
                "eess.AS"
            ],
            "max_author_hindex": 17
        },
        "2409.06367": {
            "authors": [
                "Tianwu Lei",
                "Bohan Wang",
                "Silin Chen",
                "Shurong Cao",
                "Ningmu Zou"
            ],
            "title": "Texture-AD: An Anomaly Detection Dataset and Benchmark for Real Algorithm Development",
            "abstract": "Anomaly detection is a crucial process in industrial manufacturing and has made significant advancements recently. However, there is a large variance between the data used in the development and the data collected by the production environment. Therefore, we present the Texture-AD benchmark based on representative texture-based anomaly detection to evaluate the effectiveness of unsupervised anomaly detection algorithms in real-world applications. This dataset includes images of 15 different cloth, 14 semiconductor wafers and 10 metal plates acquired under different optical schemes. In addition, it includes more than 10 different types of defects produced during real manufacturing processes, such as scratches, wrinkles, color variations and point defects, which are often more difficult to detect than existing datasets. All anomalous areas are provided with pixel-level annotations to facilitate comprehensive evaluation using anomaly detection models. Specifically, to adapt to diverse products in automated pipelines, we present a new evaluation method and results of baseline algorithms. The experimental results show that Texture-AD is a difficult challenge for state-of-the-art algorithms. To our knowledge, Texture-AD is the first dataset to be devoted to evaluating industrial defect detection algorithms in the real world. The dataset is available at https://XXX.",
            "id": "2409.06367",
            "link": "http://arxiv.org/abs/2409.06367v1",
            "published": "2024-09-10T09:44:38+00:00",
            "updated": "2024-09-10T09:44:38+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 9
        },
        "2409.06371": {
            "authors": [
                "Junzheng Zhang",
                "Weijia Guo",
                "Bochao Liu",
                "Ruixin Shi",
                "Yong Li",
                "Shiming Ge"
            ],
            "title": "Distilling Generative-Discriminative Representations for Very Low-Resolution Face Recognition",
            "abstract": "Very low-resolution face recognition is challenging due to the serious loss of informative facial details in resolution degradation. In this paper, we propose a generative-discriminative representation distillation approach that combines generative representation with cross-resolution aligned knowledge distillation. This approach facilitates very low-resolution face recognition by jointly distilling generative and discriminative models via two distillation modules. Firstly, the generative representation distillation takes the encoder of a diffusion model pretrained for face super-resolution as the generative teacher to supervise the learning of the student backbone via feature regression, and then freezes the student backbone. After that, the discriminative representation distillation further considers a pretrained face recognizer as the discriminative teacher to supervise the learning of the student head via cross-resolution relational contrastive distillation. In this way, the general backbone representation can be transformed into discriminative head representation, leading to a robust and discriminative student model for very low-resolution face recognition. Our approach improves the recovery of the missing details in very low-resolution faces and achieves better knowledge transfer. Extensive experiments on face datasets demonstrate that our approach enhances the recognition accuracy of very low-resolution faces, showcasing its effectiveness and adaptability.",
            "id": "2409.06371",
            "link": "http://arxiv.org/abs/2409.06371v1",
            "published": "2024-09-10T09:53:06+00:00",
            "updated": "2024-09-10T09:53:06+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ],
            "max_author_hindex": 44
        },
        "2409.06402": {
            "authors": [
                "Jun-Jie Zhang",
                "Nan Cheng",
                "Fu-Peng Li",
                "Xiu-Cheng Wang",
                "Jian-Nan Chen",
                "Long-Gang Pang",
                "Deyu Meng"
            ],
            "title": "Symmetry Breaking in Neural Network Optimization: Insights from Input Dimension Expansion",
            "abstract": "Understanding the mechanisms behind neural network optimization is crucial for improving network design and performance. While various optimization techniques have been developed, a comprehensive understanding of the underlying principles that govern these techniques remains elusive. Specifically, the role of symmetry breaking, a fundamental concept in physics, has not been fully explored in neural network optimization. This gap in knowledge limits our ability to design networks that are both efficient and effective. Here, we propose the symmetry breaking hypothesis to elucidate the significance of symmetry breaking in enhancing neural network optimization. We demonstrate that a simple input expansion can significantly improve network performance across various tasks, and we show that this improvement can be attributed to the underlying symmetry breaking mechanism. We further develop a metric to quantify the degree of symmetry breaking in neural networks, providing a practical approach to evaluate and guide network design. Our findings confirm that symmetry breaking is a fundamental principle that underpins various optimization techniques, including dropout, batch normalization, and equivariance. By quantifying the degree of symmetry breaking, our work offers a practical technique for performance enhancement and a metric to guide network design without the need for complete datasets and extensive training processes.",
            "id": "2409.06402",
            "link": "http://arxiv.org/abs/2409.06402v2",
            "published": "2024-09-10T10:36:40+00:00",
            "updated": "2024-09-12T10:47:35+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "math-ph",
                "math.MP"
            ],
            "max_author_hindex": 69
        },
        "2409.06427": {
            "authors": [
                "Kento Kawaharazuka",
                "Kei Okada",
                "Masayuki Inaba"
            ],
            "title": "GeMuCo: Generalized Multisensory Correlational Model for Body Schema Learning",
            "abstract": "Humans can autonomously learn the relationship between sensation and motion in their own bodies, estimate and control their own body states, and move while continuously adapting to the current environment. On the other hand, current robots control their bodies by learning the network structure described by humans from their experiences, making certain assumptions on the relationship between sensors and actuators. In addition, the network model does not adapt to changes in the robot's body, the tools that are grasped, or the environment, and there is no unified theory, not only for control but also for state estimation, anomaly detection, simulation, and so on. In this study, we propose a Generalized Multisensory Correlational Model (GeMuCo), in which the robot itself acquires a body schema describing the correlation between sensors and actuators from its own experience, including model structures such as network input/output. The robot adapts to the current environment by updating this body schema model online, estimates and controls its body state, and even performs anomaly detection and simulation. We demonstrate the effectiveness of this method by applying it to tool-use considering changes in grasping state for an axis-driven robot, to joint-muscle mapping learning for a musculoskeletal robot, and to full-body tool manipulation for a low-rigidity plastic-made humanoid.",
            "id": "2409.06427",
            "link": "http://arxiv.org/abs/2409.06427v1",
            "published": "2024-09-10T11:19:13+00:00",
            "updated": "2024-09-10T11:19:13+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 52
        },
        "2409.06450": {
            "authors": [
                "Qiujing Lu",
                "Xuanhan Wang",
                "Yiwei Jiang",
                "Guangming Zhao",
                "Mingyue Ma",
                "Shuo Feng"
            ],
            "title": "Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles",
            "abstract": "The generation of corner cases has become increasingly crucial for efficiently testing autonomous vehicles prior to road deployment. However, existing methods struggle to accommodate diverse testing requirements and often lack the ability to generalize to unseen situations, thereby reducing the convenience and usability of the generated scenarios. A method that facilitates easily controllable scenario generation for efficient autonomous vehicles (AV) testing with realistic and challenging situations is greatly needed. To address this, we proposed OmniTester: a multimodal Large Language Model (LLM) based framework that fully leverages the extensive world knowledge and reasoning capabilities of LLMs. OmniTester is designed to generate realistic and diverse scenarios within a simulation environment, offering a robust solution for testing and evaluating AVs. In addition to prompt engineering, we employ tools from Simulation of Urban Mobility to simplify the complexity of codes generated by LLMs. Furthermore, we incorporate Retrieval-Augmented Generation and a self-improvement mechanism to enhance the LLM's understanding of scenarios, thereby increasing its ability to produce more realistic scenes. In the experiments, we demonstrated the controllability and realism of our approaches in generating three types of challenging and complex scenarios. Additionally, we showcased its effectiveness in reconstructing new scenarios described in crash report, driven by the generalization capability of LLMs.",
            "id": "2409.06450",
            "link": "http://arxiv.org/abs/2409.06450v1",
            "published": "2024-09-10T12:12:09+00:00",
            "updated": "2024-09-10T12:12:09+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.ET"
            ],
            "max_author_hindex": 12
        },
        "2409.06509": {
            "authors": [
                "Lukas Muttenthaler",
                "Klaus Greff",
                "Frieda Born",
                "Bernhard Spitzer",
                "Simon Kornblith",
                "Michael C. Mozer",
                "Klaus-Robert M\u00fcller",
                "Thomas Unterthiner",
                "Andrew K. Lampinen"
            ],
            "title": "Aligning Machine and Human Visual Representations across Abstraction Levels",
            "abstract": "Deep neural networks have achieved success across a wide range of applications, including as models of human behavior in vision tasks. However, neural network training and human learning differ in fundamental ways, and neural networks often fail to generalize as robustly as humans do, raising questions regarding the similarity of their underlying representations. What is missing for modern learning systems to exhibit more human-like behavior? We highlight a key misalignment between vision models and humans: whereas human conceptual knowledge is hierarchically organized from fine- to coarse-scale distinctions, model representations do not accurately capture all these levels of abstraction. To address this misalignment, we first train a teacher model to imitate human judgments, then transfer human-like structure from its representations into pretrained state-of-the-art vision foundation models. These human-aligned models more accurately approximate human behavior and uncertainty across a wide range of similarity tasks, including a new dataset of human judgments spanning multiple levels of semantic abstractions. They also perform better on a diverse set of machine learning tasks, increasing generalization and out-of-distribution robustness. Thus, infusing neural networks with additional human knowledge yields a best-of-both-worlds representation that is both more consistent with human cognition and more practically useful, thus paving the way toward more robust, interpretable, and human-like artificial intelligence systems.",
            "id": "2409.06509",
            "link": "http://arxiv.org/abs/2409.06509v1",
            "published": "2024-09-10T13:41:08+00:00",
            "updated": "2024-09-10T13:41:08+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 128
        },
        "2409.06585": {
            "authors": [
                "Zoe Hancox",
                "Sarah R. Kingsbury",
                "Andrew Clegg",
                "Philip G. Conaghan",
                "Samuel D. Relton"
            ],
            "title": "Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records",
            "abstract": "Background: Hip replacement procedures improve patient lives by relieving pain and restoring mobility. Predicting hip replacement in advance could reduce pain by enabling timely interventions, prioritising individuals for surgery or rehabilitation, and utilising physiotherapy to potentially delay the need for joint replacement. This study predicts hip replacement a year in advance to enhance quality of life and health service efficiency. Methods: Adapting previous work using Temporal Graph Convolutional Neural Network (TG-CNN) models, we construct temporal graphs from primary care medical event codes, sourced from ResearchOne EHRs of 40-75-year-old patients, to predict hip replacement risk. We match hip replacement cases to controls by age, sex, and Index of Multiple Deprivation. The model, trained on 9,187 cases and 9,187 controls, predicts hip replacement one year in advance. We validate the model on two unseen datasets, recalibrating for class imbalance. Additionally, we conduct an ablation study and compare against four baseline models. Results: Our best model predicts hip replacement risk one year in advance with an AUROC of 0.724 (95% CI: 0.715-0.733) and an AUPRC of 0.185 (95% CI: 0.160-0.209), achieving a calibration slope of 1.107 (95% CI: 1.074-1.139) after recalibration. Conclusions: The TG-CNN model effectively predicts hip replacement risk by identifying patterns in patient trajectories, potentially improving understanding and management of hip-related conditions.",
            "id": "2409.06585",
            "link": "http://arxiv.org/abs/2409.06585v1",
            "published": "2024-09-10T15:26:58+00:00",
            "updated": "2024-09-10T15:26:58+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 107
        },
        "2409.06607": {
            "authors": [
                "Nayel Fabian Salem",
                "Marcus Nolte",
                "Veronica Haber",
                "Till Menzel",
                "Hans Steege",
                "Robert Graubohm",
                "Markus Maurer"
            ],
            "title": "An Ontology-based Approach Towards Traceable Behavior Specifications in Automated Driving",
            "abstract": "Vehicles in public traffic that are equipped with Automated Driving Systems are subject to a number of expectations: Among other aspects, their behavior should be safe, conforming to the rules of the road and provide mobility to their users. This poses challenges for the developers of such systems: Developers are responsible for specifying this behavior, for example, in terms of requirements at system design time. As we will discuss in the article, this specification always involves the need for assumptions and trade-offs. As a result, insufficiencies in such a behavior specification can occur that can potentially lead to unsafe system behavior. In order to support the identification of specification insufficiencies, requirements and respective assumptions need to be made explicit. In this article, we propose the Semantic Norm Behavior Analysis as an ontology-based approach to specify the behavior for an Automated Driving System equipped vehicle. We use ontologies to formally represent specified behavior for a targeted operational environment, and to establish traceability between specified behavior and the addressed stakeholder needs. Furthermore, we illustrate the application of the Semantic Norm Behavior Analysis in two example scenarios and evaluate our results.",
            "id": "2409.06607",
            "link": "http://arxiv.org/abs/2409.06607v1",
            "published": "2024-09-10T16:00:22+00:00",
            "updated": "2024-09-10T16:00:22+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ],
            "max_author_hindex": 40
        },
        "2409.06608": {
            "authors": [
                "Hambisa Keno",
                "Nicholas J. Pioch",
                "Christopher Guagliano",
                "Timothy H. Chung"
            ],
            "title": "Simulation-based Scenario Generation for Robust Hybrid AI for Autonomy",
            "abstract": "Application of Unmanned Aerial Vehicles (UAVs) in search and rescue, emergency management, and law enforcement has gained traction with the advent of low-cost platforms and sensor payloads. The emergence of hybrid neural and symbolic AI approaches for complex reasoning is expected to further push the boundaries of these applications with decreasing levels of human intervention. However, current UAV simulation environments lack semantic context suited to this hybrid approach. To address this gap, HAMERITT (Hybrid Ai Mission Environment for RapId Training and Testing) provides a simulation-based autonomy software framework that supports the training, testing and assurance of neuro-symbolic algorithms for autonomous maneuver and perception reasoning. HAMERITT includes scenario generation capabilities that offer mission-relevant contextual symbolic information in addition to raw sensor data. Scenarios include symbolic descriptions for entities of interest and their relations to scene elements, as well as spatial-temporal constraints in the form of time-bounded areas of interest with prior probabilities and restricted zones within those areas. HAMERITT also features support for training distinct algorithm threads for maneuver vs. perception within an end-to-end mission run. Future work includes improving scenario realism and scaling symbolic context generation through automated workflow.",
            "id": "2409.06608",
            "link": "http://arxiv.org/abs/2409.06608v1",
            "published": "2024-09-10T16:00:26+00:00",
            "updated": "2024-09-10T16:00:26+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "68T20, 68T45, 68T40",
                "J.7; C.3"
            ],
            "max_author_hindex": 22
        },
        "2409.06612": {
            "authors": [
                "Isaac Xu",
                "Scott Lowe",
                "Thomas Trappenberg"
            ],
            "title": "Label-free Monitoring of Self-Supervised Learning Progress",
            "abstract": "Self-supervised learning (SSL) is an effective method for exploiting unlabelled data to learn a high-level embedding space that can be used for various downstream tasks. However, existing methods to monitor the quality of the encoder -- either during training for one model or to compare several trained models -- still rely on access to annotated data. When SSL methodologies are applied to new data domains, a sufficiently large labelled dataset may not always be available. In this study, we propose several evaluation metrics which can be applied on the embeddings of unlabelled data and investigate their viability by comparing them to linear probe accuracy (a common metric which utilizes an annotated dataset). In particular, we apply $k$-means clustering and measure the clustering quality with the silhouette score and clustering agreement. We also measure the entropy of the embedding distribution. We find that while the clusters did correspond better to the ground truth annotations as training of the network progressed, label-free clustering metrics correlated with the linear probe accuracy only when training with SSL methods SimCLR and MoCo-v2, but not with SimSiam. Additionally, although entropy did not always have strong correlations with LP accuracy, this appears to be due to instability arising from early training, with the metric stabilizing and becoming more reliable at later stages of learning. Furthermore, while entropy generally decreases as learning progresses, this trend reverses for SimSiam. More research is required to establish the cause for this unexpected behaviour. Lastly, we find that while clustering based approaches are likely only viable for same-architecture comparisons, entropy may be architecture-independent.",
            "id": "2409.06612",
            "link": "http://arxiv.org/abs/2409.06612v1",
            "published": "2024-09-10T16:04:10+00:00",
            "updated": "2024-09-10T16:04:10+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 143
        },
        "2409.06644": {
            "authors": [
                "Danli Shi",
                "Weiyi Zhang",
                "Jiancheng Yang",
                "Siyu Huang",
                "Xiaolan Chen",
                "Mayinuer Yusufu",
                "Kai Jin",
                "Shan Lin",
                "Shunming Liu",
                "Qing Zhang",
                "Mingguang He"
            ],
            "title": "EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis",
            "abstract": "Early detection of eye diseases like glaucoma, macular degeneration, and diabetic retinopathy is crucial for preventing vision loss. While artificial intelligence (AI) foundation models hold significant promise for addressing these challenges, existing ophthalmic foundation models primarily focus on a single modality, whereas diagnosing eye diseases requires multiple modalities. A critical yet often overlooked aspect is harnessing the multi-view information across various modalities for the same patient. Additionally, due to the long-tail nature of ophthalmic diseases, standard fully supervised or unsupervised learning approaches often struggle. Therefore, it is essential to integrate clinical text to capture a broader spectrum of diseases. We propose EyeCLIP, a visual-language foundation model developed using over 2.77 million multi-modal ophthalmology images with partial text data. To fully leverage the large multi-modal unlabeled and labeled data, we introduced a pretraining strategy that combines self-supervised reconstructions, multi-modal image contrastive learning, and image-text contrastive learning to learn a shared representation of multiple modalities. Through evaluation using 14 benchmark datasets, EyeCLIP can be transferred to a wide range of downstream tasks involving ocular and systemic diseases, achieving state-of-the-art performance in disease classification, visual question answering, and cross-modal retrieval. EyeCLIP represents a significant advancement over previous methods, especially showcasing few-shot, even zero-shot capabilities in real-world long-tail scenarios.",
            "id": "2409.06644",
            "link": "http://arxiv.org/abs/2409.06644v2",
            "published": "2024-09-10T17:00:19+00:00",
            "updated": "2024-09-11T17:00:09+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 47
        },
        "2409.06662": {
            "authors": [
                "Zehong Shen",
                "Huaijin Pi",
                "Yan Xia",
                "Zhi Cen",
                "Sida Peng",
                "Zechen Hu",
                "Hujun Bao",
                "Ruizhen Hu",
                "Xiaowei Zhou"
            ],
            "title": "World-Grounded Human Motion Recovery via Gravity-View Coordinates",
            "abstract": "We present a novel method for recovering world-grounded human motion from monocular video. The main challenge lies in the ambiguity of defining the world coordinate system, which varies between sequences. Previous approaches attempt to alleviate this issue by predicting relative motion in an autoregressive manner, but are prone to accumulating errors. Instead, we propose estimating human poses in a novel Gravity-View (GV) coordinate system, which is defined by the world gravity and the camera view direction. The proposed GV system is naturally gravity-aligned and uniquely defined for each video frame, largely reducing the ambiguity of learning image-pose mapping. The estimated poses can be transformed back to the world coordinate system using camera rotations, forming a global motion sequence. Additionally, the per-frame estimation avoids error accumulation in the autoregressive methods. Experiments on in-the-wild benchmarks demonstrate that our method recovers more realistic motion in both the camera space and world-grounded settings, outperforming state-of-the-art methods in both accuracy and speed. The code is available at https://zju3dv.github.io/gvhmr/.",
            "id": "2409.06662",
            "link": "http://arxiv.org/abs/2409.06662v1",
            "published": "2024-09-10T17:25:47+00:00",
            "updated": "2024-09-10T17:25:47+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 64
        },
        "2409.06702": {
            "authors": [
                "Kairui Ding",
                "Boyuan Chen",
                "Yuchen Su",
                "Huan-ang Gao",
                "Bu Jin",
                "Chonghao Sima",
                "Wuqiang Zhang",
                "Xiaohui Li",
                "Paul Barsch",
                "Hongyang Li",
                "Hao Zhao"
            ],
            "title": "Hint-AD: Holistically Aligned Interpretability in End-to-End Autonomous Driving",
            "abstract": "End-to-end architectures in autonomous driving (AD) face a significant challenge in interpretability, impeding human-AI trust. Human-friendly natural language has been explored for tasks such as driving explanation and 3D captioning. However, previous works primarily focused on the paradigm of declarative interpretability, where the natural language interpretations are not grounded in the intermediate outputs of AD systems, making the interpretations only declarative. In contrast, aligned interpretability establishes a connection between language and the intermediate outputs of AD systems. Here we introduce Hint-AD, an integrated AD-language system that generates language aligned with the holistic perception-prediction-planning outputs of the AD model. By incorporating the intermediate outputs and a holistic token mixer sub-network for effective feature adaptation, Hint-AD achieves desirable accuracy, achieving state-of-the-art results in driving language tasks including driving explanation, 3D dense captioning, and command prediction. To facilitate further study on driving explanation task on nuScenes, we also introduce a human-labeled dataset, Nu-X. Codes, dataset, and models will be publicly available.",
            "id": "2409.06702",
            "link": "http://arxiv.org/abs/2409.06702v1",
            "published": "2024-09-10T17:59:40+00:00",
            "updated": "2024-09-10T17:59:40+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.06741": {
            "authors": [
                "Haowei Cheng",
                "Jati H. Husen",
                "Sien Reeve Peralta",
                "Bowen Jiang",
                "Nobukazu Yoshioka",
                "Naoyasu Ubayashi",
                "Hironori Washizaki"
            ],
            "title": "Generative AI for Requirements Engineering: A Systematic Literature Review",
            "abstract": "Context: Generative AI (GenAI) has emerged as a transformative tool in software engineering, with requirements engineering (RE) actively exploring its potential to revolutionize processes and outcomes. The integration of GenAI into RE presents both promising opportunities and significant challenges that necessitate systematic analysis and evaluation. Objective: This paper presents a comprehensive systematic literature review (SLR) analyzing state-of-the-art applications and innovative proposals leveraging GenAI in RE. It surveys studies focusing on the utilization of GenAI to enhance RE processes while identifying key challenges and opportunities in this rapidly evolving field. Method: A rigorous SLR methodology was used to analyze 27 carefully selected primary studies in-depth. The review examined research questions pertaining to the application of GenAI across various RE phases, the models and techniques used, and the challenges encountered in implementation and adoption. Results: The most salient findings include i) a predominant focus on the early stages of RE, particularly the elicitation and analysis of requirements, indicating potential for expansion into later phases; ii) the dominance of large language models, especially the GPT series, highlighting the need for diverse AI approaches; and iii) persistent challenges in domain-specific applications and the interpretability of AI-generated outputs, underscoring areas requiring further research and development. Conclusions: The results highlight the critical need for comprehensive evaluation frameworks, improved human-AI collaboration models, and thorough consideration of ethical implications in GenAI-assisted RE. Future research should prioritize extending GenAI applications across the entire RE lifecycle, enhancing domain-specific capabilities, and developing strategies for responsible AI integration in RE practices.",
            "id": "2409.06741",
            "link": "http://arxiv.org/abs/2409.06741v1",
            "published": "2024-09-10T02:44:39+00:00",
            "updated": "2024-09-10T02:44:39+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 25
        },
        "2409.06744": {
            "authors": [
                "Fei Ye",
                "Zaixiang Zheng",
                "Dongyu Xue",
                "Yuning Shen",
                "Lihao Wang",
                "Yiming Ma",
                "Yan Wang",
                "Xinyou Wang",
                "Xiangxin Zhou",
                "Quanquan Gu"
            ],
            "title": "ProteinBench: A Holistic Evaluation of Protein Foundation Models",
            "abstract": "Recent years have witnessed a surge in the development of protein foundation models, significantly improving performance in protein prediction and generative tasks ranging from 3D structure prediction and protein design to conformational dynamics. However, the capabilities and limitations associated with these models remain poorly understood due to the absence of a unified evaluation framework. To fill this gap, we introduce ProteinBench, a holistic evaluation framework designed to enhance the transparency of protein foundation models. Our approach consists of three key components: (i) A taxonomic classification of tasks that broadly encompass the main challenges in the protein domain, based on the relationships between different protein modalities; (ii) A multi-metric evaluation approach that assesses performance across four key dimensions: quality, novelty, diversity, and robustness; and (iii) In-depth analyses from various user objectives, providing a holistic view of model performance. Our comprehensive evaluation of protein foundation models reveals several key findings that shed light on their current capabilities and limitations. To promote transparency and facilitate further research, we release the evaluation dataset, code, and a public leaderboard publicly for further analysis and a general modular toolkit. We intend for ProteinBench to be a living benchmark for establishing a standardized, in-depth evaluation framework for protein foundation models, driving their development and application while fostering collaboration within the field.",
            "id": "2409.06744",
            "link": "http://arxiv.org/abs/2409.06744v1",
            "published": "2024-09-10T06:52:33+00:00",
            "updated": "2024-09-10T06:52:33+00:00",
            "primary_category": "q-bio.QM",
            "categories": [
                "q-bio.QM",
                "cs.AI",
                "cs.LG",
                "q-bio.BM"
            ],
            "max_author_hindex": 58
        },
        "2409.06748": {
            "authors": [
                "Jiabin Tang",
                "Wei Wei",
                "Lianghao Xia",
                "Chao Huang"
            ],
            "title": "EasyST: A Simple Framework for Spatio-Temporal Prediction",
            "abstract": "Spatio-temporal prediction is a crucial research area in data-driven urban computing, with implications for transportation, public safety, and environmental monitoring. However, scalability and generalization challenges remain significant obstacles. Advanced models often rely on Graph Neural Networks to encode spatial and temporal correlations, but struggle with the increased complexity of large-scale datasets. The recursive GNN-based message passing schemes used in these models hinder their training and deployment in real-life urban sensing scenarios. Moreover, long-spanning large-scale spatio-temporal data introduce distribution shifts, necessitating improved generalization performance. To address these challenges, we propose a simple framework for spatio-temporal prediction - EasyST paradigm. It learns lightweight and robust Multi-Layer Perceptrons (MLPs) by effectively distilling knowledge from complex spatio-temporal GNNs. We ensure robust knowledge distillation by integrating the spatio-temporal information bottleneck with teacher-bounded regression loss, filtering out task-irrelevant noise and avoiding erroneous guidance. We further enhance the generalization ability of the student model by incorporating spatial and temporal prompts to provide downstream task contexts. Evaluation on three spatio-temporal datasets for urban computing tasks demonstrates that EasyST surpasses state-of-the-art approaches in terms of efficiency and accuracy. The implementation code is available at: https://github.com/HKUDS/EasyST.",
            "id": "2409.06748",
            "link": "http://arxiv.org/abs/2409.06748v1",
            "published": "2024-09-10T11:40:01+00:00",
            "updated": "2024-09-10T11:40:01+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 39
        },
        "2409.06756": {
            "authors": [
                "Quanliang Liu",
                "Maciej P. Polak",
                "So Yeon Kim",
                "MD Al Amin Shuvo",
                "Hrishikesh Shridhar Deodhar",
                "Jeongsoo Han",
                "Dane Morgan",
                "Hyunseok Oh"
            ],
            "title": "Beyond designer's knowledge: Generating materials design hypotheses via large language models",
            "abstract": "Materials design often relies on human-generated hypotheses, a process inherently limited by cognitive constraints such as knowledge gaps and limited ability to integrate and extract knowledge implications, particularly when multidisciplinary expertise is required. This work demonstrates that large language models (LLMs), coupled with prompt engineering, can effectively generate non-trivial materials hypotheses by integrating scientific principles from diverse sources without explicit design guidance by human experts. These include design ideas for high-entropy alloys with superior cryogenic properties and halide solid electrolytes with enhanced ionic conductivity and formability. These design ideas have been experimentally validated in high-impact publications in 2023 not available in the LLM training data, demonstrating the LLM's ability to generate highly valuable and realizable innovative ideas not established in the literature. Our approach primarily leverages materials system charts encoding processing-structure-property relationships, enabling more effective data integration by condensing key information from numerous papers, and evaluation and categorization of numerous hypotheses for human cognition, both through the LLM. This LLM-driven approach opens the door to new avenues of artificial intelligence-driven materials discovery by accelerating design, democratizing innovation, and expanding capabilities beyond the designer's direct knowledge.",
            "id": "2409.06756",
            "link": "http://arxiv.org/abs/2409.06756v1",
            "published": "2024-09-10T16:28:50+00:00",
            "updated": "2024-09-10T16:28:50+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cond-mat.mtrl-sci",
                "cs.AI"
            ],
            "max_author_hindex": 64
        },
        "2409.06762": {
            "authors": [
                "Sherry Yang",
                "Simon Batzner",
                "Ruiqi Gao",
                "Muratahan Aykol",
                "Alexander L. Gaunt",
                "Brendan McMorrow",
                "Danilo J. Rezende",
                "Dale Schuurmans",
                "Igor Mordatch",
                "Ekin D. Cubuk"
            ],
            "title": "Generative Hierarchical Materials Search",
            "abstract": "Generative models trained at scale can now produce text, video, and more recently, scientific data such as crystal structures. In applications of generative approaches to materials science, and in particular to crystal structures, the guidance from the domain expert in the form of high-level instructions can be essential for an automated system to output candidate crystals that are viable for downstream research. In this work, we formulate end-to-end language-to-structure generation as a multi-objective optimization problem, and propose Generative Hierarchical Materials Search (GenMS) for controllable generation of crystal structures. GenMS consists of (1) a language model that takes high-level natural language as input and generates intermediate textual information about a crystal (e.g., chemical formulae), and (2) a diffusion model that takes intermediate information as input and generates low-level continuous value crystal structures. GenMS additionally uses a graph neural network to predict properties (e.g., formation energy) from the generated crystal structures. During inference, GenMS leverages all three components to conduct a forward tree search over the space of possible structures. Experiments show that GenMS outperforms other alternatives of directly using language models to generate structures both in satisfying user request and in generating low-energy structures. We confirm that GenMS is able to generate common crystal structures such as double perovskites, or spinels, solely from natural language input, and hence can form the foundation for more complex structure generation in near future.",
            "id": "2409.06762",
            "link": "http://arxiv.org/abs/2409.06762v1",
            "published": "2024-09-10T17:51:28+00:00",
            "updated": "2024-09-10T17:51:28+00:00",
            "primary_category": "cond-mat.mtrl-sci",
            "categories": [
                "cond-mat.mtrl-sci",
                "cs.AI"
            ],
            "max_author_hindex": 52
        },
        "2409.06817": {
            "authors": [
                "Cecilia G. Morales",
                "Dhruv Srikanth",
                "Jack H. Good",
                "Keith A. Dufendach",
                "Artur Dubrawski"
            ],
            "title": "Bifurcation Identification for Ultrasound-driven Robotic Cannulation",
            "abstract": "In trauma and critical care settings, rapid and precise intravascular access is key to patients' survival. Our research aims at ensuring this access, even when skilled medical personnel are not readily available. Vessel bifurcations are anatomical landmarks that can guide the safe placement of catheters or needles during medical procedures. Although ultrasound is advantageous in navigating anatomical landmarks in emergency scenarios due to its portability and safety, to our knowledge no existing algorithm can autonomously extract vessel bifurcations using ultrasound images. This is primarily due to the limited availability of ground truth data, in particular, data from live subjects, needed for training and validating reliable models. Researchers often resort to using data from anatomical phantoms or simulations. We introduce BIFURC, Bifurcation Identification for Ultrasound-driven Robot Cannulation, a novel algorithm that identifies vessel bifurcations and provides optimal needle insertion sites for an autonomous robotic cannulation system. BIFURC integrates expert knowledge with deep learning techniques to efficiently detect vessel bifurcations within the femoral region and can be trained on a limited amount of in-vivo data. We evaluated our algorithm using a medical phantom as well as real-world experiments involving live pigs. In all cases, BIFURC consistently identified bifurcation points and needle insertion locations in alignment with those identified by expert clinicians.",
            "id": "2409.06817",
            "link": "http://arxiv.org/abs/2409.06817v1",
            "published": "2024-09-10T18:53:52+00:00",
            "updated": "2024-09-10T18:53:52+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ],
            "max_author_hindex": 27
        },
        "2409.06904": {
            "authors": [
                "Ilias Siniosoglou",
                "Vasileios Argyriou",
                "George Fragulis",
                "Panagiotis Fouliras",
                "Georgios Th. Papadopoulos",
                "Anastasios Lytos",
                "Panagiotis Sarigiannidis"
            ],
            "title": "Applied Federated Model Personalisation in the Industrial Domain: A Comparative Study",
            "abstract": "The time-consuming nature of training and deploying complicated Machine and Deep Learning (DL) models for a variety of applications continues to pose significant challenges in the field of Machine Learning (ML). These challenges are particularly pronounced in the federated domain, where optimizing models for individual nodes poses significant difficulty. Many methods have been developed to tackle this problem, aiming to reduce training expenses and time while maintaining efficient optimisation. Three suggested strategies to tackle this challenge include Active Learning, Knowledge Distillation, and Local Memorization. These methods enable the adoption of smaller models that require fewer computational resources and allow for model personalization with local insights, thereby improving the effectiveness of current models. The present study delves into the fundamental principles of these three approaches and proposes an advanced Federated Learning System that utilises different Personalisation methods towards improving the accuracy of AI models and enhancing user experience in real-time NG-IoT applications, investigating the efficacy of these techniques in the local and federated domain. The results of the original and optimised models are then compared in both local and federated contexts using a comparison analysis. The post-analysis shows encouraging outcomes when it comes to optimising and personalising the models with the suggested techniques.",
            "id": "2409.06904",
            "link": "http://arxiv.org/abs/2409.06904v1",
            "published": "2024-09-10T23:00:19+00:00",
            "updated": "2024-09-10T23:00:19+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.PF"
            ],
            "max_author_hindex": 35
        },
        "2409.06912": {
            "authors": [
                "Haodong Zheng",
                "Andrei Jalba",
                "Raymond H. Cuijpers",
                "Wijnand IJsselsteijn",
                "Sanne Schoenmakers"
            ],
            "title": "A Bayesian framework for active object recognition, pose estimation and shape transfer learning through touch",
            "abstract": "As humans can explore and understand the world through the sense of touch, tactile sensing is also an important aspect of robotic perception. In unstructured environments, robots can encounter both known and novel objects, this calls for a method to address both known and novel objects. In this study, we combine a particle filter (PF) and Gaussian process implicit surface (GPIS) in a unified Bayesian framework. The framework can differentiate between known and novel objects, perform object recognition, estimate pose for known objects, and reconstruct shapes for unknown objects, in an active learning fashion. By grounding the selection of the GPIS prior with the maximum-likelihood-estimation (MLE) shape from the PF, the knowledge about known objects' shapes can be transferred to learn novel shapes. An exploration procedure with global shape estimation is proposed to guide active data acquisition and conclude the exploration when sufficient information is obtained. The performance of the proposed Bayesian framework is evaluated through simulations on known and novel objects, initialized with random poses and is compared with a rapidly explore random tree (RRT).The results show that the proposed exploration procedure, utilizing global shape estimation, achieves faster exploration than the RRT-based local exploration procedure. Overall, results indicate that the proposed framework is effective and efficient in object recognition, pose estimation and shape reconstruction. Moreover, we show that a learned shape can be included as a new prior and used effectively for future object recognition and pose estimation of novel objects.",
            "id": "2409.06912",
            "link": "http://arxiv.org/abs/2409.06912v1",
            "published": "2024-09-10T23:35:30+00:00",
            "updated": "2024-09-10T23:35:30+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 74
        },
        "2409.06928": {
            "authors": [
                "Jianmei Jiang",
                "Huijin Wang",
                "Jieyun Bai",
                "Shun Long",
                "Shuangping Chen",
                "Victor M. Campello",
                "Karim Lekadir"
            ],
            "title": "Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning",
            "abstract": "The segmentation of the pubic symphysis and fetal head (PSFH) constitutes a pivotal step in monitoring labor progression and identifying potential delivery complications. Despite the advances in deep learning, the lack of annotated medical images hinders the training of segmentation. Traditional semi-supervised learning approaches primarily utilize a unified network model based on Convolutional Neural Networks (CNNs) and apply consistency regularization to mitigate the reliance on extensive annotated data. However, these methods often fall short in capturing the discriminative features of unlabeled data and in delineating the long-range dependencies inherent in the ambiguous boundaries of PSFH within ultrasound images. To address these limitations, we introduce a novel framework, the Dual-Student and Teacher Combining CNN and Transformer (DSTCT), which synergistically integrates the capabilities of CNNs and Transformers. Our framework comprises a Vision Transformer (ViT) as the teacher and two student mod ls one ViT and one CNN. This dual-student setup enables mutual supervision through the generation of both hard and soft pseudo-labels, with the consistency in their predictions being refined by minimizing the classifier determinacy discrepancy. The teacher model further reinforces learning within this architecture through the imposition of consistency regularization constraints. To augment the generalization abilities of our approach, we employ a blend of data and model perturbation techniques. Comprehensive evaluations on the benchmark dataset of the PSFH Segmentation Grand Challenge at MICCAI 2023 demonstrate our DSTCT framework outperformed ten contemporary semi-supervised segmentation methods. Code available at https://github.com/jjm1589/DSTCT.",
            "id": "2409.06928",
            "link": "http://arxiv.org/abs/2409.06928v1",
            "published": "2024-09-11T00:57:31+00:00",
            "updated": "2024-09-11T00:57:31+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.06945": {
            "authors": [
                "Tianran Liu",
                "Morteza Mousa Pasandi",
                "Robert Laganiere"
            ],
            "title": "FSMDet: Vision-guided feature diffusion for fully sparse 3D detector",
            "abstract": "Fully sparse 3D detection has attracted an increasing interest in the recent years. However, the sparsity of the features in these frameworks challenges the generation of proposals because of the limited diffusion process. In addition, the quest for efficiency has led to only few work on vision-assisted fully sparse models. In this paper, we propose FSMDet (Fully Sparse Multi-modal Detection), which use visual information to guide the LiDAR feature diffusion process while still maintaining the efficiency of the pipeline. Specifically, most of fully sparse works focus on complex customized center fusion diffusion/regression operators. However, we observed that if the adequate object completion is performed, even the simplest interpolation operator leads to satisfactory results. Inspired by this observation, we split the vision-guided diffusion process into two modules: a Shape Recover Layer (SRLayer) and a Self Diffusion Layer (SDLayer). The former uses RGB information to recover the shape of the visible part of an object, and the latter uses a visual prior to further spread the features to the center region. Experiments demonstrate that our approach successfully improves the performance of previous fully sparse models that use LiDAR only and reaches SOTA performance in multimodal models. At the same time, thanks to the sparse architecture, our method can be up to 5 times more efficient than previous SOTA methods in the inference process.",
            "id": "2409.06945",
            "link": "http://arxiv.org/abs/2409.06945v1",
            "published": "2024-09-11T01:55:45+00:00",
            "updated": "2024-09-11T01:55:45+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 11
        },
        "2409.06957": {
            "authors": [
                "Wei Shen",
                "Chuheng Zhang"
            ],
            "title": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation",
            "abstract": "Reinforcement learning from human feedback (RLHF) is one of the key techniques that helps large language models (LLMs) to follow instructions and provide helpful and harmless responses. While direct policy optimization methods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in RLHF to train the policy to generate good responses guided by a reward model learned from preference data. The main challenge of these methods is the inaccuracy of the intermediate reward model, especially in code generation tasks that require long and complex reasoning to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtration strategy for a given reward model, the coefficient of determination ($R^2$) between rewards and actual scores on filtered samples serves as a good metrics and helps us find several promising strategies. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation tasks, and find that some variants of PF-PPO are highly effective and achieve new state-of-the-art performance across 7-billion-parameter models on HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.",
            "id": "2409.06957",
            "link": "http://arxiv.org/abs/2409.06957v1",
            "published": "2024-09-11T02:40:38+00:00",
            "updated": "2024-09-11T02:40:38+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.06978": {
            "authors": [
                "Ji\u0159\u00ed Wiedermann",
                "Jan van Leeuwen"
            ],
            "title": "Large Language Models and the Extended Church-Turing Thesis",
            "abstract": "The Extended Church-Turing Thesis (ECTT) posits that all effective information processing, including unbounded and non-uniform interactive computations, can be described in terms of interactive Turing machines with advice. Does this assertion also apply to the abilities of contemporary large language models (LLMs)? From a broader perspective, this question calls for an investigation of the computational power of LLMs by the classical means of computability and computational complexity theory, especially the theory of automata. Along these lines, we establish a number of fundamental results. Firstly, we argue that any fixed (non-adaptive) LLM is computationally equivalent to a, possibly very large, deterministic finite-state transducer. This characterizes the base level of LLMs. We extend this to a key result concerning the simulation of space-bounded Turing machines by LLMs. Secondly, we show that lineages of evolving LLMs are computationally equivalent to interactive Turing machines with advice. The latter finding confirms the validity of the ECTT for lineages of LLMs. From a computability viewpoint, it also suggests that lineages of LLMs possess super-Turing computational power. Consequently, in our computational model knowledge generation is in general a non-algorithmic process realized by lineages of LLMs. Finally, we discuss the merits of our findings in the broader context of several related disciplines and philosophies.",
            "id": "2409.06978",
            "link": "http://arxiv.org/abs/2409.06978v1",
            "published": "2024-09-11T03:09:55+00:00",
            "updated": "2024-09-11T03:09:55+00:00",
            "primary_category": "cs.FL",
            "categories": [
                "cs.FL",
                "cs.AI"
            ],
            "max_author_hindex": 54
        },
        "2409.07092": {
            "authors": [
                "Feiyang Jia",
                "Zhineng Chen",
                "Ziying Song",
                "Lin Liu",
                "Caiyan Jia"
            ],
            "title": "CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer",
            "abstract": "Super-resolution (SR) aims to enhance the quality of low-resolution images and has been widely applied in medical imaging. We found that the design principles of most existing methods are influenced by SR tasks based on real-world images and do not take into account the significance of the multi-level structure in pathological images, even if they can achieve respectable objective metric evaluations. In this work, we delve into two super-resolution working paradigms and propose a novel network called CWT-Net, which leverages cross-scale image wavelet transform and Transformer architecture. Our network consists of two branches: one dedicated to learning super-resolution and the other to high-frequency wavelet features. To generate high-resolution histopathology images, the Transformer module shares and fuses features from both branches at various stages. Notably, we have designed a specialized wavelet reconstruction module to effectively enhance the wavelet domain features and enable the network to operate in different modes, allowing for the introduction of additional relevant information from cross-scale images. Our experimental results demonstrate that our model significantly outperforms state-of-the-art methods in both performance and visualization evaluations and can substantially boost the accuracy of image diagnostic networks.",
            "id": "2409.07092",
            "link": "http://arxiv.org/abs/2409.07092v1",
            "published": "2024-09-11T08:26:28+00:00",
            "updated": "2024-09-11T08:26:28+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 23
        },
        "2409.07114": {
            "authors": [
                "Marcus R\u00fcb",
                "Philipp Tuchel",
                "Axel Sikora",
                "Daniel Mueller-Gritschneder"
            ],
            "title": "A Continual and Incremental Learning Approach for TinyML On-device Training Using Dataset Distillation and Model Size Adaption",
            "abstract": "A new algorithm for incremental learning in the context of Tiny Machine learning (TinyML) is presented, which is optimized for low-performance and energy efficient embedded devices. TinyML is an emerging field that deploys machine learning models on resource-constrained devices such as microcontrollers, enabling intelligent applications like voice recognition, anomaly detection, predictive maintenance, and sensor data processing in environments where traditional machine learning models are not feasible. The algorithm solve the challenge of catastrophic forgetting through the use of knowledge distillation to create a small, distilled dataset. The novelty of the method is that the size of the model can be adjusted dynamically, so that the complexity of the model can be adapted to the requirements of the task. This offers a solution for incremental learning in resource-constrained environments, where both model size and computational efficiency are critical factors. Results show that the proposed algorithm offers a promising approach for TinyML incremental learning on embedded devices. The algorithm was tested on five datasets including: CIFAR10, MNIST, CORE50, HAR, Speech Commands. The findings indicated that, despite using only 43% of Floating Point Operations (FLOPs) compared to a larger fixed model, the algorithm experienced a negligible accuracy loss of just 1%. In addition, the presented method is memory efficient. While state-of-the-art incremental learning is usually very memory intensive, the method requires only 1% of the original data set.",
            "id": "2409.07114",
            "link": "http://arxiv.org/abs/2409.07114v1",
            "published": "2024-09-11T09:02:33+00:00",
            "updated": "2024-09-11T09:02:33+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.07128": {
            "authors": [
                "Mustapha Hemis",
                "Hamza Kheddar",
                "Sami Bourouis",
                "Nasir Saleem"
            ],
            "title": "Deep Learning Techniques for Hand Vein Biometrics: A Comprehensive Review",
            "abstract": "Biometric authentication has garnered significant attention as a secure and efficient method of identity verification. Among the various modalities, hand vein biometrics, including finger vein, palm vein, and dorsal hand vein recognition, offer unique advantages due to their high accuracy, low susceptibility to forgery, and non-intrusiveness. The vein patterns within the hand are highly complex and distinct for each individual, making them an ideal biometric identifier. Additionally, hand vein recognition is contactless, enhancing user convenience and hygiene compared to other modalities such as fingerprint or iris recognition. Furthermore, the veins are internally located, rendering them less susceptible to damage or alteration, thus enhancing the security and reliability of the biometric system. The combination of these factors makes hand vein biometrics a highly effective and secure method for identity verification. This review paper delves into the latest advancements in deep learning techniques applied to finger vein, palm vein, and dorsal hand vein recognition. It encompasses all essential fundamentals of hand vein biometrics, summarizes publicly available datasets, and discusses state-of-the-art metrics used for evaluating the three modes. Moreover, it provides a comprehensive overview of suggested approaches for finger, palm, dorsal, and multimodal vein techniques, offering insights into the best performance achieved, data augmentation techniques, and effective transfer learning methods, along with associated pretrained deep learning models. Additionally, the review addresses research challenges faced and outlines future directions and perspectives, encouraging researchers to enhance existing methods and propose innovative techniques.",
            "id": "2409.07128",
            "link": "http://arxiv.org/abs/2409.07128v1",
            "published": "2024-09-11T09:25:05+00:00",
            "updated": "2024-09-11T09:25:05+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CR",
                "cs.CV"
            ],
            "max_author_hindex": 29
        },
        "2409.07186": {
            "authors": [
                "Sheng Chen",
                "Zihao Tang",
                "Mariano Cabezas",
                "Xinyi Wang",
                "Arkiev D'Souza",
                "Michael Barnett",
                "Fernando Calamante",
                "Weidong Cai",
                "Chenyu Wang"
            ],
            "title": "Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging",
            "abstract": "Diffusion-weighted imaging (DWI) is a type of Magnetic Resonance Imaging (MRI) technique sensitised to the diffusivity of water molecules, offering the capability to inspect tissue microstructures and is the only in-vivo method to reconstruct white matter fiber tracts non-invasively. The DWI signal can be analysed with the diffusion tensor imaging (DTI) model to estimate the directionality of water diffusion within voxels. Several scalar metrics, including axial diffusivity (AD), mean diffusivity (MD), radial diffusivity (RD), and fractional anisotropy (FA), can be further derived from DTI to quantitatively summarise the microstructural integrity of brain tissue. These scalar metrics have played an important role in understanding the organisation and health of brain tissue at a microscopic level in clinical studies. However, reliable DTI metrics rely on DWI acquisitions with high gradient directions, which often go beyond the commonly used clinical protocols. To enhance the utility of clinically acquired DWI and save scanning time for robust DTI analysis, this work proposes DirGeo-DTI, a deep learning-based method to estimate reliable DTI metrics even from a set of DWIs acquired with the minimum theoretical number (6) of gradient directions. DirGeo-DTI leverages directional encoding and geometric constraints to facilitate the training process. Two public DWI datasets were used for evaluation, demonstrating the effectiveness of the proposed method. Extensive experimental results show that the proposed method achieves the best performance compared to existing DTI enhancement methods and potentially reveals further clinical insights with routine clinical DWI scans.",
            "id": "2409.07186",
            "link": "http://arxiv.org/abs/2409.07186v1",
            "published": "2024-09-11T11:12:26+00:00",
            "updated": "2024-09-11T11:12:26+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 60
        },
        "2409.07192": {
            "authors": [
                "Umm-e- Habiba",
                "Markus Haug",
                "Justus Bogner",
                "Stefan Wagner"
            ],
            "title": "How Mature is Requirements Engineering for AI-based Systems? A Systematic Mapping Study on Practices, Challenges, and Future Research Directions",
            "abstract": "Artificial intelligence (AI) permeates all fields of life, which resulted in new challenges in requirements engineering for artificial intelligence (RE4AI), e.g., the difficulty in specifying and validating requirements for AI or considering new quality requirements due to emerging ethical implications. It is currently unclear if existing RE methods are sufficient or if new ones are needed to address these challenges. Therefore, our goal is to provide a comprehensive overview of RE4AI to researchers and practitioners. What has been achieved so far, i.e., what practices are available, and what research gaps and challenges still need to be addressed? To achieve this, we conducted a systematic mapping study combining query string search and extensive snowballing. The extracted data was aggregated, and results were synthesized using thematic analysis. Our selection process led to the inclusion of 126 primary studies. Existing RE4AI research focuses mainly on requirements analysis and elicitation, with most practices applied in these areas. Furthermore, we identified requirements specification, explainability, and the gap between machine learning engineers and end-users as the most prevalent challenges, along with a few others. Additionally, we proposed seven potential research directions to address these challenges. Practitioners can use our results to identify and select suitable RE methods for working on their AI-based systems, while researchers can build on the identified gaps and research directions to push the field forward.",
            "id": "2409.07192",
            "link": "http://arxiv.org/abs/2409.07192v1",
            "published": "2024-09-11T11:28:16+00:00",
            "updated": "2024-09-11T11:28:16+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 18
        },
        "2409.07200": {
            "authors": [
                "Rongfeng Lu",
                "Hangyu Chen",
                "Zunjie Zhu",
                "Yuhang Qin",
                "Ming Lu",
                "Le Zhang",
                "Chenggang Yan",
                "Anke Xue"
            ],
            "title": "ThermalGaussian: Thermal 3D Gaussian Splatting",
            "abstract": "Thermography is especially valuable for the military and other users of surveillance cameras. Some recent methods based on Neural Radiance Fields (NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of thermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS) prevails due to its rapid training and real-time rendering. In this work, we propose ThermalGaussian, the first thermal 3DGS approach capable of rendering high-quality images in RGB and thermal modalities. We first calibrate the RGB camera and the thermal camera to ensure that both modalities are accurately aligned. Subsequently, we use the registered images to learn the multimodal 3D Gaussians. To prevent the overfitting of any single modality, we introduce several multimodal regularization constraints. We also develop smoothing constraints tailored to the physical characteristics of the thermal modality. Besides, we contribute a real-world dataset named RGBT-Scenes, captured by a hand-hold thermal-infrared camera, facilitating future research on thermal scene reconstruction. We conduct comprehensive experiments to show that ThermalGaussian achieves photorealistic rendering of thermal images and improves the rendering quality of RGB images. With the proposed multimodal regularization constraints, we also reduced the model's storage cost by 90\\%. The code and dataset will be released.",
            "id": "2409.07200",
            "link": "http://arxiv.org/abs/2409.07200v1",
            "published": "2024-09-11T11:45:57+00:00",
            "updated": "2024-09-11T11:45:57+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 33
        },
        "2409.07341": {
            "authors": [
                "Luo Ji",
                "Runji Lin"
            ],
            "title": "Online Decision MetaMorphFormer: A Casual Transformer-Based Reinforcement Learning Framework of Universal Embodied Intelligence",
            "abstract": "Interactive artificial intelligence in the motion control field is an interesting topic, especially when universal knowledge is adaptive to multiple tasks and universal environments. Despite there being increasing efforts in the field of Reinforcement Learning (RL) with the aid of transformers, most of them might be limited by the offline training pipeline, which prohibits exploration and generalization abilities. To address this limitation, we propose the framework of Online Decision MetaMorphFormer (ODM) which aims to achieve self-awareness, environment recognition, and action planning through a unified model architecture. Motivated by cognitive and behavioral psychology, an ODM agent is able to learn from others, recognize the world, and practice itself based on its own experience. ODM can also be applied to any arbitrary agent with a multi-joint body, located in different environments, and trained with different types of tasks using large-scale pre-trained datasets. Through the use of pre-trained datasets, ODM can quickly warm up and learn the necessary knowledge to perform the desired task, while the target environment continues to reinforce the universal policy. Extensive online experiments as well as few-shot and zero-shot environmental tests are used to verify ODM's performance and generalization ability. The results of our study contribute to the study of general artificial intelligence in embodied and cognitive fields. Code, results, and video examples can be found on the website \\url{https://rlodm.github.io/odm/}.",
            "id": "2409.07341",
            "link": "http://arxiv.org/abs/2409.07341v1",
            "published": "2024-09-11T15:22:43+00:00",
            "updated": "2024-09-11T15:22:43+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 13
        },
        "2409.07353": {
            "authors": [
                "Md Zarif Hossain",
                "Ahmed Imteaj"
            ],
            "title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks",
            "abstract": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced AI by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which bypass safety protocols and cause the model to generate misleading or harmful responses. This vulnerability stems from both the inherent susceptibilities of LLMs and the expanded attack surface introduced by the visual modality. We propose Sim-CLIP+, a novel defense mechanism that adversarially fine-tunes the CLIP vision encoder by leveraging a Siamese architecture. This approach maximizes cosine similarity between perturbed and clean samples, facilitating resilience against adversarial manipulations. Sim-CLIP+ offers a plug-and-play solution, allowing seamless integration into existing LVLM architectures as a robust vision encoder. Unlike previous defenses, our method requires no structural modifications to the LVLM and incurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness against both gradient-based adversarial attacks and various jailbreak techniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack strategies and perform clean evaluations using standard downstream datasets, including COCO for image captioning and OKVQA for visual question answering. Extensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy while substantially improving robustness against both gradient-based adversarial attacks and jailbreak techniques. Our code and robust vision encoders are available at https://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.",
            "id": "2409.07353",
            "link": "http://arxiv.org/abs/2409.07353v1",
            "published": "2024-09-11T15:39:42+00:00",
            "updated": "2024-09-11T15:39:42+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.07493": {
            "authors": [
                "Javad Hassannataj Joloudari",
                "Mohammad Maftoun",
                "Bahareh Nakisa",
                "Roohallah Alizadehsani",
                "Meisam Yadollahzadeh-Tabari"
            ],
            "title": "Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review",
            "abstract": "The Complex Emotion Recognition System (CERS) deciphers complex emotional states by examining combinations of basic emotions expressed, their interconnections, and the dynamic variations. Through the utilization of advanced algorithms, CERS provides profound insights into emotional dynamics, facilitating a nuanced understanding and customized responses. The attainment of such a level of emotional recognition in machines necessitates the knowledge distillation and the comprehension of novel concepts akin to human cognition. The development of AI systems for discerning complex emotions poses a substantial challenge with significant implications for affective computing. Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task due to the intricacies involved in capturing subtle emotions, necessitating specialized methods for data collection and processing. Incorporating physiological signals such as Electrocardiogram (ECG) and Electroencephalogram (EEG) can notably enhance CERS by furnishing valuable insights into the user's emotional state, enhancing the quality of datasets, and fortifying system dependability. A comprehensive literature review was conducted in this study to assess the efficacy of machine learning, deep learning, and meta-learning approaches in both basic and complex emotion recognition utilizing EEG, ECG signals, and facial expression datasets. The chosen research papers offer perspectives on potential applications, clinical implications, and results of CERSs, with the objective of promoting their acceptance and integration into clinical decision-making processes. This study highlights research gaps and challenges in understanding CERSs, encouraging further investigation by relevant studies and organizations. Lastly, the significance of meta-learning approaches in improving CERS performance and guiding future research endeavors is underscored.",
            "id": "2409.07493",
            "link": "http://arxiv.org/abs/2409.07493v1",
            "published": "2024-09-09T05:06:10+00:00",
            "updated": "2024-09-09T05:06:10+00:00",
            "primary_category": "eess.SP",
            "categories": [
                "eess.SP",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ],
            "max_author_hindex": 37
        },
        "2409.07505": {
            "authors": [
                "\u00d6vg\u00fc \u00d6zdemir",
                "M. Tu\u011fberk \u0130\u015fyapar",
                "P\u0131nar Karag\u00f6z",
                "Klaus Werner Schmidt",
                "Demet Demir",
                "N. Alpay Karag\u00f6z"
            ],
            "title": "A Survey of Anomaly Detection in In-Vehicle Networks",
            "abstract": "Modern vehicles are equipped with Electronic Control Units (ECU) that are used for controlling important vehicle functions including safety-critical operations. ECUs exchange information via in-vehicle communication buses, of which the Controller Area Network (CAN bus) is by far the most widespread representative. Problems that may occur in the vehicle's physical parts or malicious attacks may cause anomalies in the CAN traffic, impairing the correct vehicle operation. Therefore, the detection of such anomalies is vital for vehicle safety. This paper reviews the research on anomaly detection for in-vehicle networks, more specifically for the CAN bus. Our main focus is the evaluation of methods used for CAN bus anomaly detection together with the datasets used in such analysis. To provide the reader with a more comprehensive understanding of the subject, we first give a brief review of related studies on time series-based anomaly detection. Then, we conduct an extensive survey of recent deep learning-based techniques as well as conventional techniques for CAN bus anomaly detection. Our comprehensive analysis delves into anomaly detection algorithms employed in in-vehicle networks, specifically focusing on their learning paradigms, inherent strengths, and weaknesses, as well as their efficacy when applied to CAN bus datasets. Lastly, we highlight challenges and open research problems in CAN bus anomaly detection.",
            "id": "2409.07505",
            "link": "http://arxiv.org/abs/2409.07505v1",
            "published": "2024-09-11T11:45:18+00:00",
            "updated": "2024-09-11T11:45:18+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "eess.SP"
            ],
            "max_author_hindex": 18
        },
        "2409.07585": {
            "authors": [
                "Muhammad Akhtar Munir",
                "Fahad Shahbaz Khan",
                "Salman Khan"
            ],
            "title": "Efficient Localized Adaptation of Neural Weather Forecasting: A Case Study in the MENA Region",
            "abstract": "Accurate weather and climate modeling is critical for both scientific advancement and safeguarding communities against environmental risks. Traditional approaches rely heavily on Numerical Weather Prediction (NWP) models, which simulate energy and matter flow across Earth's systems. However, heavy computational requirements and low efficiency restrict the suitability of NWP, leading to a pressing need for enhanced modeling techniques. Neural network-based models have emerged as promising alternatives, leveraging data-driven approaches to forecast atmospheric variables. In this work, we focus on limited-area modeling and train our model specifically for localized region-level downstream tasks. As a case study, we consider the MENA region due to its unique climatic challenges, where accurate localized weather forecasting is crucial for managing water resources, agriculture and mitigating the impacts of extreme weather events. This targeted approach allows us to tailor the model's capabilities to the unique conditions of the region of interest. Our study aims to validate the effectiveness of integrating parameter-efficient fine-tuning (PEFT) methodologies, specifically Low-Rank Adaptation (LoRA) and its variants, to enhance forecast accuracy, as well as training speed, computational resource utilization, and memory efficiency in weather and climate modeling for specific regions.",
            "id": "2409.07585",
            "link": "http://arxiv.org/abs/2409.07585v1",
            "published": "2024-09-11T19:31:56+00:00",
            "updated": "2024-09-11T19:31:56+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "physics.ao-ph"
            ],
            "max_author_hindex": 78
        },
        "2409.07619": {
            "authors": [
                "Maxime Kawawa-Beaudan",
                "Srijan Sood",
                "Soham Palande",
                "Ganapathy Mani",
                "Tucker Balch",
                "Manuela Veloso"
            ],
            "title": "Ensemble Methods for Sequence Classification with Hidden Markov Models",
            "abstract": "We present a lightweight approach to sequence classification using Ensemble Methods for Hidden Markov Models (HMMs). HMMs offer significant advantages in scenarios with imbalanced or smaller datasets due to their simplicity, interpretability, and efficiency. These models are particularly effective in domains such as finance and biology, where traditional methods struggle with high feature dimensionality and varied sequence lengths. Our ensemble-based scoring method enables the comparison of sequences of any length and improves performance on imbalanced datasets.   This study focuses on the binary classification problem, particularly in scenarios with data imbalance, where the negative class is the majority (e.g., normal data) and the positive class is the minority (e.g., anomalous data), often with extreme distribution skews. We propose a novel training approach for HMM Ensembles that generalizes to multi-class problems and supports classification and anomaly detection. Our method fits class-specific groups of diverse models using random data subsets, and compares likelihoods across classes to produce composite scores, achieving high average precisions and AUCs.   In addition, we compare our approach with neural network-based methods such as Convolutional Neural Networks (CNNs) and Long Short-Term Memory networks (LSTMs), highlighting the efficiency and robustness of HMMs in data-scarce environments. Motivated by real-world use cases, our method demonstrates robust performance across various benchmarks, offering a flexible framework for diverse applications.",
            "id": "2409.07619",
            "link": "http://arxiv.org/abs/2409.07619v1",
            "published": "2024-09-11T20:59:32+00:00",
            "updated": "2024-09-11T20:59:32+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 77
        },
        "2409.07645": {
            "authors": [
                "Mohsen Azarmi",
                "Mahdi Rezaei",
                "He Wang",
                "Ali Arabian"
            ],
            "title": "Feature Importance in Pedestrian Intention Prediction: A Context-Aware Review",
            "abstract": "Recent advancements in predicting pedestrian crossing intentions for Autonomous Vehicles using Computer Vision and Deep Neural Networks are promising. However, the black-box nature of DNNs poses challenges in understanding how the model works and how input features contribute to final predictions. This lack of interpretability delimits the trust in model performance and hinders informed decisions on feature selection, representation, and model optimisation; thereby affecting the efficacy of future research in the field. To address this, we introduce Context-aware Permutation Feature Importance (CAPFI), a novel approach tailored for pedestrian intention prediction. CAPFI enables more interpretability and reliable assessments of feature importance by leveraging subdivided scenario contexts, mitigating the randomness of feature values through targeted shuffling. This aims to reduce variance and prevent biased estimations in importance scores during permutations. We divide the Pedestrian Intention Estimation (PIE) dataset into 16 comparable context sets, measure the baseline performance of five distinct neural network architectures for intention prediction in each context, and assess input feature importance using CAPFI. We observed nuanced differences among models across various contextual characteristics. The research reveals the critical role of pedestrian bounding boxes and ego-vehicle speed in predicting pedestrian intentions, and potential prediction biases due to the speed feature through cross-context permutation evaluation. We propose an alternative feature representation by considering proximity change rate for rendering dynamic pedestrian-vehicle locomotion, thereby enhancing the contributions of input features to intention prediction. These findings underscore the importance of contextual features and their diversity to develop accurate and robust intent-predictive models.",
            "id": "2409.07645",
            "link": "http://arxiv.org/abs/2409.07645v1",
            "published": "2024-09-11T22:13:01+00:00",
            "updated": "2024-09-11T22:13:01+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.RO",
                "eess.IV"
            ],
            "max_author_hindex": 10
        },
        "2409.07683": {
            "authors": [
                "Qinglong Cao",
                "Yuntian Chen",
                "Chao Ma",
                "Xiaokang Yang"
            ],
            "title": "Open-Vocabulary Remote Sensing Image Semantic Segmentation",
            "abstract": "Open-vocabulary image semantic segmentation (OVS) seeks to segment images into semantic regions across an open set of categories. Existing OVS methods commonly depend on foundational vision-language models and utilize similarity computation to tackle OVS tasks. However, these approaches are predominantly tailored to natural images and struggle with the unique characteristics of remote sensing images, such as rapidly changing orientations and significant scale variations. These challenges complicate OVS tasks in earth vision, requiring specialized approaches. To tackle this dilemma, we propose the first OVS framework specifically designed for remote sensing imagery, drawing inspiration from the distinct remote sensing traits. Particularly, to address the varying orientations, we introduce a rotation-aggregative similarity computation module that generates orientation-adaptive similarity maps as initial semantic maps. These maps are subsequently refined at both spatial and categorical levels to produce more accurate semantic maps. Additionally, to manage significant scale changes, we integrate multi-scale image features into the upsampling process, resulting in the final scale-aware semantic masks. To advance OVS in earth vision and encourage reproducible research, we establish the first open-sourced OVS benchmark for remote sensing imagery, including four public remote sensing datasets. Extensive experiments on this benchmark demonstrate our proposed method achieves state-of-the-art performance. All codes and datasets are available at https://github.com/caoql98/OVRS.",
            "id": "2409.07683",
            "link": "http://arxiv.org/abs/2409.07683v1",
            "published": "2024-09-12T01:16:25+00:00",
            "updated": "2024-09-12T01:16:25+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.07684": {
            "authors": [
                "Patrick Gerard",
                "Svitlana Volkova",
                "Louis Penafiel",
                "Kristina Lerman",
                "Tim Weninger"
            ],
            "title": "Modeling Information Narrative Detection and Evolution on Telegram during the Russia-Ukraine War",
            "abstract": "Following the Russian Federation's full-scale invasion of Ukraine in February 2022, a multitude of information narratives emerged within both pro-Russian and pro-Ukrainian communities online. As the conflict progresses, so too do the information narratives, constantly adapting and influencing local and global community perceptions and attitudes. This dynamic nature of the evolving information environment (IE) underscores a critical need to fully discern how narratives evolve and affect online communities. Existing research, however, often fails to capture information narrative evolution, overlooking both the fluid nature of narratives and the internal mechanisms that drive their evolution. Recognizing this, we introduce a novel approach designed to both model narrative evolution and uncover the underlying mechanisms driving them. In this work we perform a comparative discourse analysis across communities on Telegram covering the initial three months following the invasion. First, we uncover substantial disparities in narratives and perceptions between pro-Russian and pro-Ukrainian communities. Then, we probe deeper into prevalent narratives of each group, identifying key themes and examining the underlying mechanisms fueling their evolution. Finally, we explore influences and factors that may shape the development and spread of narratives.",
            "id": "2409.07684",
            "link": "http://arxiv.org/abs/2409.07684v1",
            "published": "2024-09-12T01:18:57+00:00",
            "updated": "2024-09-12T01:18:57+00:00",
            "primary_category": "cs.SI",
            "categories": [
                "cs.SI",
                "cs.AI"
            ],
            "max_author_hindex": 60
        },
        "2409.07725": {
            "authors": [
                "Kaizhe Fan",
                "Quanjun Li"
            ],
            "title": "GRE^2-MDCL: Graph Representation Embedding Enhanced via Multidimensional Contrastive Learning",
            "abstract": "Graph representation learning has emerged as a powerful tool for preserving graph topology when mapping nodes to vector representations, enabling various downstream tasks such as node classification and community detection. However, most current graph neural network models face the challenge of requiring extensive labeled data, which limits their practical applicability in real-world scenarios where labeled data is scarce. To address this challenge, researchers have explored Graph Contrastive Learning (GCL), which leverages enhanced graph data and contrastive learning techniques. While promising, existing GCL methods often struggle with effectively capturing both local and global graph structures, and balancing the trade-off between nodelevel and graph-level representations. In this work, we propose Graph Representation Embedding Enhanced via Multidimensional Contrastive Learning (GRE2-MDCL). Our model introduces a novel triple network architecture with a multi-head attention GNN as the core. GRE2-MDCL first globally and locally augments the input graph using SVD and LAGNN techniques. It then constructs a multidimensional contrastive loss, incorporating cross-network, cross-view, and neighbor contrast, to optimize the model. Extensive experiments on benchmark datasets Cora, Citeseer, and PubMed demonstrate that GRE2-MDCL achieves state-of-the-art performance, with average accuracies of 82.5%, 72.5%, and 81.6% respectively. Visualizations further show tighter intra-cluster aggregation and clearer inter-cluster boundaries, highlighting the effectiveness of our framework in improving upon baseline GCL models.",
            "id": "2409.07725",
            "link": "http://arxiv.org/abs/2409.07725v1",
            "published": "2024-09-12T03:09:05+00:00",
            "updated": "2024-09-12T03:09:05+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 28
        },
        "2409.07770": {
            "authors": [
                "Jin Sob Kim",
                "Hyun Joon Park",
                "Wooseok Shin",
                "Sung Won Han"
            ],
            "title": "Universal Pooling Method of Multi-layer Features from Pretrained Models for Speaker Verification",
            "abstract": "Recent advancements in automatic speaker verification (ASV) studies have been achieved by leveraging large-scale pretrained networks. In this study, we analyze the approaches toward such a paradigm and underline the significance of interlayer information processing as a result. Accordingly, we present a novel approach for exploiting the multilayered nature of pretrained models for ASV, which comprises a layer/frame-level network and two steps of pooling architectures for each layer and frame axis. Specifically, we let convolutional architecture directly processes a stack of layer outputs.Then, we present a channel attention-based scheme of gauging layer significance and squeeze the layer level with the most representative value. Finally, attentive statistics over frame-level representations yield a single vector speaker embedding. Comparative experiments are designed using versatile data environments and diverse pretraining models to validate the proposed approach. The experimental results demonstrate the stability of the approach using multi-layer outputs in leveraging pretrained architectures. Then, we verify the superiority of the proposed ASV backend structure, which involves layer-wise operations, in terms of performance improvement along with cost efficiency compared to the conventional method. The ablation study shows how the proposed interlayer processing aids in maximizing the advantage of utilizing pretrained models.",
            "id": "2409.07770",
            "link": "http://arxiv.org/abs/2409.07770v1",
            "published": "2024-09-12T05:55:32+00:00",
            "updated": "2024-09-12T05:55:32+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.AI"
            ],
            "max_author_hindex": 16
        },
        "2409.07776": {
            "authors": [
                "Yongbo Zhang",
                "Katsuma Inoue",
                "Mitsumasa Nakajima",
                "Toshikazu Hashimoto",
                "Yasuo Kuniyoshi",
                "Kohei Nakajima"
            ],
            "title": "Training Spiking Neural Networks via Augmented Direct Feedback Alignment",
            "abstract": "Spiking neural networks (SNNs), the models inspired by the mechanisms of real neurons in the brain, transmit and represent information by employing discrete action potentials or spikes. The sparse, asynchronous properties of information processing make SNNs highly energy efficient, leading to SNNs being promising solutions for implementing neural networks in neuromorphic devices. However, the nondifferentiable nature of SNN neurons makes it a challenge to train them. The current training methods of SNNs that are based on error backpropagation (BP) and precisely designing surrogate gradient are difficult to implement and biologically implausible, hindering the implementation of SNNs on neuromorphic devices. Thus, it is important to train SNNs with a method that is both physically implementatable and biologically plausible. In this paper, we propose using augmented direct feedback alignment (aDFA), a gradient-free approach based on random projection, to train SNNs. This method requires only partial information of the forward process during training, so it is easy to implement and biologically plausible. We systematically demonstrate the feasibility of the proposed aDFA-SNNs scheme, propose its effective working range, and analyze its well-performing settings by employing genetic algorithm. We also analyze the impact of crucial features of SNNs on the scheme, thus demonstrating its superiority and stability over BP and conventional direct feedback alignment. Our scheme can achieve competitive performance without accurate prior knowledge about the utilized system, thus providing a valuable reference for physically training SNNs.",
            "id": "2409.07776",
            "link": "http://arxiv.org/abs/2409.07776v1",
            "published": "2024-09-12T06:22:44+00:00",
            "updated": "2024-09-12T06:22:44+00:00",
            "primary_category": "cs.NE",
            "categories": [
                "cs.NE",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 48
        },
        "2409.07779": {
            "authors": [
                "Fuchen Zheng",
                "Xinyi Chen",
                "Xuhang Chen",
                "Haolun Li",
                "Xiaojiao Guo",
                "Guoheng Huang",
                "Chi-Man Pun",
                "Shoujun Zhou"
            ],
            "title": "ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation",
            "abstract": "Medical image segmentation, a crucial task in computer vision, facilitates the automated delineation of anatomical structures and pathologies, supporting clinicians in diagnosis, treatment planning, and disease monitoring. Notably, transformers employing shifted window-based self-attention have demonstrated exceptional performance. However, their reliance on local window attention limits the fusion of local and global contextual information, crucial for segmenting microtumors and miniature organs. To address this limitation, we propose the Adaptive Semantic Segmentation Network (ASSNet), a transformer architecture that effectively integrates local and global features for precise medical image segmentation. ASSNet comprises a transformer-based U-shaped encoder-decoder network. The encoder utilizes shifted window self-attention across five resolutions to extract multi-scale features, which are then propagated to the decoder through skip connections. We introduce an augmented multi-layer perceptron within the encoder to explicitly model long-range dependencies during feature extraction. Recognizing the constraints of conventional symmetrical encoder-decoder designs, we propose an Adaptive Feature Fusion (AFF) decoder to complement our encoder. This decoder incorporates three key components: the Long Range Dependencies (LRD) block, the Multi-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC) block. These components synergistically facilitate the effective fusion of multi-scale features extracted by the decoder while capturing long-range dependencies and refining object boundaries. Comprehensive experiments on diverse medical image segmentation tasks, including multi-organ, liver tumor, and bladder tumor segmentation, demonstrate that ASSNet achieves state-of-the-art results. Code and models are available at: \\url{https://github.com/lzeeorno/ASSNet}.",
            "id": "2409.07779",
            "link": "http://arxiv.org/abs/2409.07779v1",
            "published": "2024-09-12T06:25:44+00:00",
            "updated": "2024-09-12T06:25:44+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 38
        },
        "2409.07793": {
            "authors": [
                "Fuchen Zheng",
                "Quanjun Li",
                "Weixuan Li",
                "Xuhang Chen",
                "Yihang Dong",
                "Guoheng Huang",
                "Chi-Man Pun",
                "Shoujun Zhou"
            ],
            "title": "Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation",
            "abstract": "Medical image segmentation, a critical application of semantic segmentation in healthcare, has seen significant advancements through specialized computer vision techniques. While deep learning-based medical image segmentation is essential for assisting in medical diagnosis, the lack of diverse training data causes the long-tail problem. Moreover, most previous hybrid CNN-ViT architectures have limited ability to combine various attentions in different layers of the Convolutional Neural Network. To address these issues, we propose a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware Contrastive Loss, as the overall training objective for semi-supervised learning to mitigate the long-tail problem. Additionally, we introduce CMAformer, a novel network that synergizes the strengths of ResUNet and Transformer. The cross-attention block in CMAformer effectively integrates spatial attention and channel attention for multi-scale feature fusion. Overall, our results indicate that CMAformer, combined with the feature fusion framework and the new consistency loss, demonstrates strong complementarity in semi-supervised learning ensembles. We achieve state-of-the-art results on multiple public medical image datasets. Example code are available at: \\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.",
            "id": "2409.07793",
            "link": "http://arxiv.org/abs/2409.07793v1",
            "published": "2024-09-12T06:52:46+00:00",
            "updated": "2024-09-12T06:52:46+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 38
        },
        "2409.07930": {
            "authors": [
                "Jing Sun",
                "Sigmund Slang",
                "Thomas Elboth",
                "Thomas Larsen Greiner",
                "Steven McDonald",
                "Leiv-J Gelius"
            ],
            "title": "A convolutional neural network approach to deblending seismic data",
            "abstract": "For economic and efficiency reasons, blended acquisition of seismic data is becoming more and more commonplace. Seismic deblending methods are always computationally demanding and normally consist of multiple processing steps. Besides, the parameter setting is not always trivial. Machine learning-based processing has the potential to significantly reduce processing time and to change the way seismic deblending is carried out. We present a data-driven deep learning-based method for fast and efficient seismic deblending. The blended data are sorted from the common source to the common channel domain to transform the character of the blending noise from coherent events to incoherent distributions. A convolutional neural network (CNN) is designed according to the special character of seismic data, and performs deblending with comparable results to those obtained with conventional industry deblending algorithms. To ensure authenticity, the blending was done numerically and only field seismic data were employed, including more than 20000 training examples. After training and validation of the network, seismic deblending can be performed in near real time. Experiments also show that the initial signal to noise ratio (SNR) is the major factor controlling the quality of the final deblended result. The network is also demonstrated to be robust and adaptive by using the trained model to firstly deblend a new data set from a different geological area with a slightly different delay time setting, and secondly deblend shots with blending noise in the top part of the data.",
            "id": "2409.07930",
            "link": "http://arxiv.org/abs/2409.07930v1",
            "published": "2024-09-12T10:54:35+00:00",
            "updated": "2024-09-12T10:54:35+00:00",
            "primary_category": "physics.geo-ph",
            "categories": [
                "physics.geo-ph",
                "cs.AI"
            ],
            "max_author_hindex": 47
        },
        "2409.07957": {
            "authors": [
                "Bo Liang",
                "Hong Guo",
                "Tianyu Zhao",
                "He wang",
                "Herik Evangelinelis",
                "Yuxiang Xu",
                "Chang liu",
                "Manjia Liang",
                "Xiaotong Wei",
                "Yong Yuan",
                "Peng Xu",
                "Minghui Du",
                "Wei-Liang Qian",
                "Ziren Luo"
            ],
            "title": "Rapid Parameter Estimation for Extreme Mass Ratio Inspirals Using Machine Learning",
            "abstract": "Extreme-mass-ratio inspiral (EMRI) signals pose significant challenges in gravitational wave (GW) astronomy owing to their low-frequency nature and highly complex waveforms, which occupy a high-dimensional parameter space with numerous variables. Given their extended inspiral timescales and low signal-to-noise ratios, EMRI signals warrant prolonged observation periods. Parameter estimation becomes particularly challenging due to non-local parameter degeneracies, arising from multiple local maxima, as well as flat regions and ridges inherent in the likelihood function. These factors lead to exceptionally high time complexity for parameter analysis while employing traditional matched filtering and random sampling methods. To address these challenges, the present study applies machine learning to Bayesian posterior estimation of EMRI signals, leveraging the recently developed flow matching technique based on ODE neural networks. Our approach demonstrates computational efficiency several orders of magnitude faster than the traditional Markov Chain Monte Carlo (MCMC) methods, while preserving the unbiasedness of parameter estimation. We show that machine learning technology has the potential to efficiently handle the vast parameter space, involving up to seventeen parameters, associated with EMRI signals. Furthermore, to our knowledge, this is the first instance of applying machine learning, specifically the Continuous Normalizing Flows (CNFs), to EMRI signal analysis. Our findings highlight the promising potential of machine learning in EMRI waveform analysis, offering new perspectives for the advancement of space-based GW detection and GW astronomy.",
            "id": "2409.07957",
            "link": "http://arxiv.org/abs/2409.07957v1",
            "published": "2024-09-12T11:36:23+00:00",
            "updated": "2024-09-12T11:36:23+00:00",
            "primary_category": "physics.comp-ph",
            "categories": [
                "physics.comp-ph",
                "astro-ph.IM",
                "cs.AI"
            ],
            "max_author_hindex": 51
        },
        "2409.07989": {
            "authors": [
                "Fatemeh Askari",
                "Amirreza Fateh",
                "Mohammad Reza Mohammadi"
            ],
            "title": "Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms",
            "abstract": "In the context of few-shot classification, the goal is to train a classifier using a limited number of samples while maintaining satisfactory performance. However, traditional metric-based methods exhibit certain limitations in achieving this objective. These methods typically rely on a single distance value between the query feature and support feature, thereby overlooking the contribution of shallow features. To overcome this challenge, we propose a novel approach in this paper. Our approach involves utilizing multi-output embedding network that maps samples into distinct feature spaces. The proposed method extract feature vectors at different stages, enabling the model to capture both global and abstract features. By utilizing these diverse feature spaces, our model enhances its performance. Moreover, employing a self-attention mechanism improves the refinement of features at each stage, leading to even more robust representations and improved overall performance. Furthermore, assigning learnable weights to each stage significantly improved performance and results. We conducted comprehensive evaluations on the MiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way 5-shot scenarios. Additionally, we performed a cross-domain task from MiniImageNet to the CUB dataset, achieving high accuracy in the testing domain. These evaluations demonstrate the efficacy of our proposed method in comparison to state-of-the-art approaches. https://github.com/FatemehAskari/MSENet",
            "id": "2409.07989",
            "link": "http://arxiv.org/abs/2409.07989v1",
            "published": "2024-09-12T12:34:29+00:00",
            "updated": "2024-09-12T12:34:29+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.08065": {
            "authors": [
                "Xiao-Qi Han",
                "Zhenfeng Ouyang",
                "Peng-Jie Guo",
                "Hao Sun",
                "Ze-Feng Gao",
                "Zhong-Yi Lu"
            ],
            "title": "AI-accelerated discovery of high critical temperature superconductors",
            "abstract": "The discovery of new superconducting materials, particularly those exhibiting high critical temperature ($T_c$), has been a vibrant area of study within the field of condensed matter physics. Conventional approaches primarily rely on physical intuition to search for potential superconductors within the existing databases. However, the known materials only scratch the surface of the extensive array of possibilities within the realm of materials. Here, we develop an AI search engine that integrates deep model pre-training and fine-tuning techniques, diffusion models, and physics-based approaches (e.g., first-principles electronic structure calculation) for discovery of high-$T_c$ superconductors. Utilizing this AI search engine, we have obtained 74 dynamically stable materials with critical temperatures predicted by the AI model to be $T_c \\geq$ 15 K based on a very small set of samples. Notably, these materials are not contained in any existing dataset. Furthermore, we analyze trends in our dataset and individual materials including B$_4$CN$_3$ and B$_5$CN$_2$ whose $T_c$s are 24.08 K and 15.93 K, respectively. We demonstrate that AI technique can discover a set of new high-$T_c$ superconductors, outline its potential for accelerating discovery of the materials with targeted properties.",
            "id": "2409.08065",
            "link": "http://arxiv.org/abs/2409.08065v1",
            "published": "2024-09-12T14:16:56+00:00",
            "updated": "2024-09-12T14:16:56+00:00",
            "primary_category": "cond-mat.supr-con",
            "categories": [
                "cond-mat.supr-con",
                "cond-mat.mtrl-sci",
                "cs.AI",
                "physics.comp-ph"
            ],
            "max_author_hindex": 68
        },
        "2409.05420": {
            "authors": [
                "Asim Naveed",
                "Syed S. Naqvi",
                "Tariq M. Khan",
                "Shahzaib Iqbal",
                "M. Yaqoob Wani",
                "Haroon Ahmed Khan"
            ],
            "title": "AD-Net: Attention-based dilated convolutional residual network with guided decoder for robust skin lesion segmentation",
            "abstract": "In computer-aided diagnosis tools employed for skin cancer treatment and early diagnosis, skin lesion segmentation is important. However, achieving precise segmentation is challenging due to inherent variations in appearance, contrast, texture, and blurry lesion boundaries. This research presents a robust approach utilizing a dilated convolutional residual network, which incorporates an attention-based spatial feature enhancement block (ASFEB) and employs a guided decoder strategy. In each dilated convolutional residual block, dilated convolution is employed to broaden the receptive field with varying dilation rates. To improve the spatial feature information of the encoder, we employed an attention-based spatial feature enhancement block in the skip connections. The ASFEB in our proposed method combines feature maps obtained from average and maximum-pooling operations. These combined features are then weighted using the active outcome of global average pooling and convolution operations. Additionally, we have incorporated a guided decoder strategy, where each decoder block is optimized using an individual loss function to enhance the feature learning process in the proposed AD-Net. The proposed AD-Net presents a significant benefit by necessitating fewer model parameters compared to its peer methods. This reduction in parameters directly impacts the number of labeled data required for training, facilitating faster convergence during the training process. The effectiveness of the proposed AD-Net was evaluated using four public benchmark datasets. We conducted a Wilcoxon signed-rank test to verify the efficiency of the AD-Net. The outcomes suggest that our method surpasses other cutting-edge methods in performance, even without the implementation of data augmentation strategies.",
            "id": "2409.05420",
            "link": "http://arxiv.org/abs/2409.05420v1",
            "published": "2024-09-09T08:21:17+00:00",
            "updated": "2024-09-09T08:21:17+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 39
        },
        "2409.05565": {
            "authors": [
                "Xudong Gao",
                "Xiao Guang Gao",
                "Jia Rong",
                "Ni Li",
                "Yifeng Niu",
                "Jun Chen"
            ],
            "title": "On the Convergence of Sigmoid and tanh Fuzzy General Grey Cognitive Maps",
            "abstract": "Fuzzy General Grey Cognitive Map (FGGCM) and Fuzzy Grey Cognitive Map (FGCM) are extensions of Fuzzy Cognitive Map (FCM) in terms of uncertainty. FGGCM allows for the processing of general grey number with multiple intervals, enabling FCM to better address uncertain situations. Although the convergence of FCM and FGCM has been discussed in many literature, the convergence of FGGCM has not been thoroughly explored. This paper aims to fill this research gap. First, metrics for the general grey number space and its vector space is given and proved using the Minkowski inequality. By utilizing the characteristic that Cauchy sequences are convergent sequences, the completeness of these two space is demonstrated. On this premise, utilizing Banach fixed point theorem and Browder-Gohde-Kirk fixed point theorem, combined with Lagrange's mean value theorem and Cauchy's inequality, deduces the sufficient conditions for FGGCM to converge to a unique fixed point when using tanh and sigmoid functions as activation functions. The sufficient conditions for the kernels and greyness of FGGCM to converge to a unique fixed point are also provided separately. Finally, based on Web Experience and Civil engineering FCM, designed corresponding FGGCM with sigmoid and tanh as activation functions by modifying the weights to general grey numbers. By comparing with the convergence theorems of FCM and FGCM, the effectiveness of the theorems proposed in this paper was verified. It was also demonstrated that the convergence theorems of FCM are special cases of the theorems proposed in this paper. The study for convergence of FGGCM is of great significance for guiding the learning algorithm of FGGCM, which is needed for designing FGGCM with specific fixed points, lays a solid theoretical foundation for the application of FGGCM in fields such as control, prediction, and decision support systems.",
            "id": "2409.05565",
            "link": "http://arxiv.org/abs/2409.05565v1",
            "published": "2024-09-09T12:46:03+00:00",
            "updated": "2024-09-09T12:46:03+00:00",
            "primary_category": "eess.SY",
            "categories": [
                "eess.SY",
                "cs.AI",
                "cs.SY"
            ],
            "max_author_hindex": 24
        },
        "2409.06091": {
            "authors": [
                "Dongyue Li",
                "Aneesh Sharma",
                "Hongyang R. Zhang"
            ],
            "title": "Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity",
            "abstract": "Multitask learning is a widely used paradigm for training models on diverse tasks, with applications ranging from graph neural networks to language model fine-tuning. Since tasks may interfere with each other, a key notion for modeling their relationships is task affinity. This includes pairwise task affinity, computed among pairs of tasks, and higher-order affinity, computed among subsets of tasks. Naively computing either of them requires repeatedly training on data from various task combinations, which is computationally intensive. We present a new algorithm Grad-TAG that can estimate task affinities without this repeated training.   The key idea of Grad-TAG is to train a \"base\" model for all tasks and then use a linearization technique to estimate the loss of the model for a specific task combination. The linearization works by computing a gradient-based approximation of the loss, using low-dimensional projections of gradients as features in a logistic regression to predict labels for the task combination. We show that the linearized model can provably approximate the loss when the gradient-based approximation is accurate, and also empirically verify that on several large models. Then, given the estimated task affinity, we design a semi-definite program for clustering similar tasks by maximizing the average density of clusters.   We evaluate Grad-TAG's performance across seven datasets, including multi-label classification on graphs, and instruction fine-tuning of language models. Our task affinity estimates are within 2.7% distance to the true affinities while needing only 3% of FLOPs in full training. On our largest graph with 21M edges and 500 labeling tasks, our algorithm delivers estimates within 5% distance to the true affinities, using only 112 GPU hours. Our results show that Grad-TAG achieves excellent performance and runtime tradeoffs compared to existing approaches.",
            "id": "2409.06091",
            "link": "http://arxiv.org/abs/2409.06091v1",
            "published": "2024-09-09T21:59:27+00:00",
            "updated": "2024-09-09T21:59:27+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.SI",
                "stat.ML"
            ],
            "max_author_hindex": 27
        },
        "2409.06851": {
            "authors": [
                "Kang Zhu",
                "Qianbo Zang",
                "Shian Jia",
                "Siwei Wu",
                "Feiteng Fang",
                "Yizhi Li",
                "Shuyue Guo",
                "Tianyu Zheng",
                "Bo Li",
                "Haoning Wu",
                "Xingwei Qu",
                "Jian Yang",
                "Zachary Liu",
                "Xiang Yue",
                "J. H. Liu",
                "Chenghua Lin",
                "Min Yang",
                "Shiwen Ni",
                "Wenhao Huang",
                "Ge Zhang"
            ],
            "title": "LIME-M: Less Is More for Evaluation of MLLMs",
            "abstract": "With the remarkable success achieved by Multimodal Large Language Models (MLLMs), numerous benchmarks have been designed to assess MLLMs' ability to guide their development in image perception tasks (e.g., image captioning and visual question answering). However, the existence of numerous benchmarks results in a substantial computational burden when evaluating model performance across all of them. Moreover, these benchmarks contain many overly simple problems or challenging samples, which do not effectively differentiate the capabilities among various MLLMs. To address these challenges, we propose a pipeline to process the existing benchmarks, which consists of two modules: (1) Semi-Automated Screening Process and (2) Eliminating Answer Leakage. The Semi-Automated Screening Process filters out samples that cannot distinguish the model's capabilities by synthesizing various MLLMs and manually evaluating them. The Eliminate Answer Leakage module filters samples whose answers can be inferred without images. Finally, we curate the LIME-M: Less Is More for Evaluation of Multimodal LLMs, a lightweight Multimodal benchmark that can more effectively evaluate the performance of different models. Our experiments demonstrate that: LIME-M can better distinguish the performance of different MLLMs with fewer samples (24% of the original) and reduced time (23% of the original); LIME-M eliminates answer leakage, focusing mainly on the information within images; The current automatic metric (i.e., CIDEr) is insufficient for evaluating MLLMs' capabilities in captioning. Moreover, removing the caption task score when calculating the overall score provides a more accurate reflection of model performance differences. All our codes and data are released at https://github.com/kangreen0210/LIME-M.",
            "id": "2409.06851",
            "link": "http://arxiv.org/abs/2409.06851v1",
            "published": "2024-09-10T20:19:14+00:00",
            "updated": "2024-09-10T20:19:14+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 32
        },
        "2409.07189": {
            "authors": [
                "Mohamed Dhouioui",
                "Jonathan Barnoud",
                "Rhoslyn Roebuck Williams",
                "Harry J. Stroud",
                "Phil Bates",
                "David R. Glowacki"
            ],
            "title": "A Perspective on AI-Guided Molecular Simulations in VR: Exploring Strategies for Imitation Learning in Hyperdimensional Molecular Systems",
            "abstract": "Molecular dynamics simulations are a crucial computational tool for researchers to understand and engineer molecular structure and function in areas such as drug discovery, protein engineering, and material design. Despite their utility, MD simulations are expensive, owing to the high dimensionality of molecular systems. Interactive molecular dynamics in virtual reality (iMD-VR) has recently been developed as a 'human-in-the-loop' strategy, which leverages high-performance computing to accelerate the researcher's ability to solve the hyperdimensional sampling problem. By providing an immersive 3D environment that enables visualization and manipulation of real-time molecular motion, iMD-VR enables researchers and students to efficiently and intuitively explore and navigate these complex, high-dimensional systems. iMD-VR platforms offer a unique opportunity to quickly generate rich datasets that capture human experts' spatial insight regarding molecular structure and function. This paper explores the possibility of employing user-generated iMD-VR datasets to train AI agents via imitation learning (IL). IL is an important technique in robotics that enables agents to mimic complex behaviors from expert demonstrations, thus circumventing the need for explicit programming or intricate reward design. We review the utilization of IL for manipulation tasks in robotics and discuss how iMD-VR recordings could be used to train IL models for solving specific molecular 'tasks'. We then investigate how such approaches could be applied to the data captured from iMD-VR recordings. Finally, we outline the future research directions and potential challenges of using AI agents to augment human expertise to efficiently navigate conformational spaces, highlighting how this approach could provide valuable insight across domains such as materials science, protein engineering, and computer-aided drug design.",
            "id": "2409.07189",
            "link": "http://arxiv.org/abs/2409.07189v1",
            "published": "2024-09-11T11:21:02+00:00",
            "updated": "2024-09-11T11:21:02+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.HC",
                "q-bio.BM"
            ],
            "max_author_hindex": 34
        },
        "2409.07202": {
            "authors": [
                "Shichen Zhan",
                "Yebo Wu",
                "Chunlin Tian",
                "Yan Zhao",
                "Li Li"
            ],
            "title": "Heterogeneity-Aware Coordination for Federated Learning via Stitching Pre-trained blocks",
            "abstract": "Federated learning (FL) coordinates multiple devices to collaboratively train a shared model while preserving data privacy. However, large memory footprint and high energy consumption during the training process excludes the low-end devices from contributing to the global model with their own data, which severely deteriorates the model performance in real-world scenarios. In this paper, we propose FedStitch, a hierarchical coordination framework for heterogeneous federated learning with pre-trained blocks. Unlike the traditional approaches that train the global model from scratch, for a new task, FedStitch composes the global model via stitching pre-trained blocks. Specifically, each participating client selects the most suitable block based on their local data from the candidate pool composed of blocks from pre-trained models. The server then aggregates the optimal block for stitching. This process iterates until a new stitched network is generated. Except for the new training paradigm, FedStitch consists of the following three core components: 1) an RL-weighted aggregator, 2) a search space optimizer deployed on the server side, and 3) a local energy optimizer deployed on each participating client. The RL-weighted aggregator helps to select the right block in the non-IID scenario, while the search space optimizer continuously reduces the size of the candidate block pool during stitching. Meanwhile, the local energy optimizer is designed to minimize energy consumption of each client while guaranteeing the overall training progress. The results demonstrate that compared to existing approaches, FedStitch improves the model accuracy up to 20.93%. At the same time, it achieves up to 8.12% speedup, reduces the memory footprint up to 79.5%, and achieves 89.41% energy saving at most during the learning procedure.",
            "id": "2409.07202",
            "link": "http://arxiv.org/abs/2409.07202v1",
            "published": "2024-09-11T11:47:50+00:00",
            "updated": "2024-09-11T11:47:50+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 18
        },
        "2409.07321": {
            "authors": [
                "Tianyuan Zhang",
                "Lu Wang",
                "Jiaqi Kang",
                "Xinwei Zhang",
                "Siyuan Liang",
                "Yuwei Chen",
                "Aishan Liu",
                "Xianglong Liu"
            ],
            "title": "Module-wise Adaptive Adversarial Training for End-to-end Autonomous Driving",
            "abstract": "Recent advances in deep learning have markedly improved autonomous driving (AD) models, particularly end-to-end systems that integrate perception, prediction, and planning stages, achieving state-of-the-art performance. However, these models remain vulnerable to adversarial attacks, where human-imperceptible perturbations can disrupt decision-making processes. While adversarial training is an effective method for enhancing model robustness against such attacks, no prior studies have focused on its application to end-to-end AD models. In this paper, we take the first step in adversarial training for end-to-end AD models and present a novel Module-wise Adaptive Adversarial Training (MA2T). However, extending conventional adversarial training to this context is highly non-trivial, as different stages within the model have distinct objectives and are strongly interconnected. To address these challenges, MA2T first introduces Module-wise Noise Injection, which injects noise before the input of different modules, targeting training models with the guidance of overall objectives rather than each independent module loss. Additionally, we introduce Dynamic Weight Accumulation Adaptation, which incorporates accumulated weight changes to adaptively learn and adjust the loss weights of each module based on their contributions (accumulated reduction rates) for better balance and robust training. To demonstrate the efficacy of our defense, we conduct extensive experiments on the widely-used nuScenes dataset across several end-to-end AD models under both white-box and black-box attacks, where our method outperforms other baselines by large margins (+5-10%). Moreover, we validate the robustness of our defense through closed-loop evaluation in the CARLA simulation environment, showing improved resilience even against natural corruption.",
            "id": "2409.07321",
            "link": "http://arxiv.org/abs/2409.07321v1",
            "published": "2024-09-11T15:00:18+00:00",
            "updated": "2024-09-11T15:00:18+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 53
        },
        "2409.07629": {
            "authors": [
                "Jingzhi Gong",
                "Tao Chen",
                "Rami Bahsoon"
            ],
            "title": "Dividable Configuration Performance Learning",
            "abstract": "Machine/deep learning models have been widely adopted for predicting the configuration performance of software systems. However, a crucial yet unaddressed challenge is how to cater for the sparsity inherited from the configuration landscape: the influence of configuration options (features) and the distribution of data samples are highly sparse. In this paper, we propose a model-agnostic and sparsity-robust framework for predicting configuration performance, dubbed DaL, based on the new paradigm of dividable learning that builds a model via \"divide-and-learn\". To handle sample sparsity, the samples from the configuration landscape are divided into distant divisions, for each of which we build a sparse local model, e.g., regularized Hierarchical Interaction Neural Network, to deal with the feature sparsity. A newly given configuration would then be assigned to the right model of division for the final prediction. Further, DaL adaptively determines the optimal number of divisions required for a system and sample size without any extra training or profiling. Experiment results from 12 real-world systems and five sets of training data reveal that, compared with the state-of-the-art approaches, DaL performs no worse than the best counterpart on 44 out of 60 cases with up to 1.61x improvement on accuracy; requires fewer samples to reach the same/better accuracy; and producing acceptable training overhead. In particular, the mechanism that adapted the parameter d can reach the optimal value for 76.43% of the individual runs. The result also confirms that the paradigm of dividable learning is more suitable than other similar paradigms such as ensemble learning for predicting configuration performance. Practically, DaL considerably improves different global models when using them as the underlying local models, which further strengthens its flexibility.",
            "id": "2409.07629",
            "link": "http://arxiv.org/abs/2409.07629v1",
            "published": "2024-09-11T21:23:23+00:00",
            "updated": "2024-09-11T21:23:23+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 33
        },
        "2409.07966": {
            "authors": [
                "Sichun Wu",
                "Kazi Injamamul Haque",
                "Zerrin Yumak"
            ],
            "title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE",
            "abstract": "Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (https://github.com/uuembodiedsocialai/ProbTalk3D/).",
            "id": "2409.07966",
            "link": "http://arxiv.org/abs/2409.07966v1",
            "published": "2024-09-12T11:53:05+00:00",
            "updated": "2024-09-12T11:53:05+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 11
        },
        "2409.08045": {
            "authors": [
                "Stav Cohen",
                "Ron Bitton",
                "Ben Nassi"
            ],
            "title": "Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking",
            "abstract": "In this paper, we show that with the ability to jailbreak a GenAI model, attackers can escalate the outcome of attacks against RAG-based GenAI-powered applications in severity and scale. In the first part of the paper, we show that attackers can escalate RAG membership inference attacks and RAG entity extraction attacks to RAG documents extraction attacks, forcing a more severe outcome compared to existing attacks. We evaluate the results obtained from three extraction methods, the influence of the type and the size of five embeddings algorithms employed, the size of the provided context, and the GenAI engine. We show that attackers can extract 80%-99.8% of the data stored in the database used by the RAG of a Q&A chatbot. In the second part of the paper, we show that attackers can escalate the scale of RAG data poisoning attacks from compromising a single GenAI-powered application to compromising the entire GenAI ecosystem, forcing a greater scale of damage. This is done by crafting an adversarial self-replicating prompt that triggers a chain reaction of a computer worm within the ecosystem and forces each affected application to perform a malicious activity and compromise the RAG of additional applications. We evaluate the performance of the worm in creating a chain of confidential data extraction about users within a GenAI ecosystem of GenAI-powered email assistants and analyze how the performance of the worm is affected by the size of the context, the adversarial self-replicating prompt used, the type and size of the embeddings algorithm employed, and the number of hops in the propagation. Finally, we review and analyze guardrails to protect RAG-based inference and discuss the tradeoffs.",
            "id": "2409.08045",
            "link": "http://arxiv.org/abs/2409.08045v1",
            "published": "2024-09-12T13:50:22+00:00",
            "updated": "2024-09-12T13:50:22+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 13
        },
        "2409.05152": {
            "authors": [
                "Jintian Zhang",
                "Cheng Peng",
                "Mengshu Sun",
                "Xiang Chen",
                "Lei Liang",
                "Zhiqiang Zhang",
                "Jun Zhou",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "title": "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs",
            "abstract": "Despite the recent advancements in Large Language Models (LLMs), which have significantly enhanced the generative capabilities for various NLP tasks, LLMs still face limitations in directly handling retrieval tasks. However, many practical applications demand the seamless integration of both retrieval and generation. This paper introduces a novel and efficient One-pass Generation and retrieval framework (OneGen), designed to improve LLMs' performance on tasks that require both generation and retrieval. The proposed framework bridges the traditionally separate training approaches for generation and retrieval by incorporating retrieval tokens generated autoregressively. This enables a single LLM to handle both tasks simultaneously in a unified forward pass. We conduct experiments on two distinct types of composite tasks, RAG and Entity Linking, to validate the pluggability, effectiveness, and efficiency of OneGen in training and inference. Furthermore, our results show that integrating generation and retrieval within the same context preserves the generative capabilities of LLMs while improving retrieval performance. To the best of our knowledge, OneGen is the first to enable LLMs to conduct vector retrieval during the generation.",
            "id": "2409.05152",
            "link": "http://arxiv.org/abs/2409.05152v1",
            "published": "2024-09-08T16:35:19+00:00",
            "updated": "2024-09-08T16:35:19+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.DB",
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 35
        },
        "2409.00847": {
            "authors": [
                "Eric Anderson",
                "Jonathan Fritz",
                "Austin Lee",
                "Bohou Li",
                "Mark Lindblad",
                "Henry Lindeman",
                "Alex Meyer",
                "Parth Parmar",
                "Tanvi Ranade",
                "Mehul A. Shah",
                "Benjamin Sowell",
                "Dan Tecuci",
                "Vinayak Thapliyal",
                "Matt Welsh"
            ],
            "title": "The Design of an LLM-powered Unstructured Analytics System",
            "abstract": "LLMs demonstrate an uncanny ability to process unstructured data, and as such, have the potential to go beyond search and run complex, semantic analyses at scale. We describe the design of an unstructured analytics system, Aryn, and the tenets and use cases that motivate its design. With Aryn, users can specify queries in natural language and the system automatically determines a semantic plan and executes it to compute an answer from a large collection of unstructured documents using LLMs. At the core of Aryn is Sycamore, a declarative document processing engine, built using Ray, that provides a reliable distributed abstraction called DocSets. Sycamore allows users to analyze, enrich, and transform complex documents at scale. Aryn also comprises Luna, a query planner that translates natural language queries to Sycamore scripts, and the Aryn Partitioner, which takes raw PDFs and document images, and converts them to DocSets for downstream processing. Using Aryn, we demonstrate a real world use case for analyzing accident reports from the National Transportation Safety Board (NTSB), and discuss some of the major challenges we encountered in deploying Aryn in the wild.",
            "id": "2409.00847",
            "link": "http://arxiv.org/abs/2409.00847v2",
            "published": "2024-09-01T21:30:14+00:00",
            "updated": "2024-09-04T16:39:22+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.AI",
                "cs.IR"
            ],
            "max_author_hindex": 53
        },
        "2409.03753": {
            "authors": [
                "Yuntian Deng",
                "Wenting Zhao",
                "Jack Hessel",
                "Xiang Ren",
                "Claire Cardie",
                "Yejin Choi"
            ],
            "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
            "abstract": "The increasing availability of real-world conversation data offers exciting opportunities for researchers to study user-chatbot interactions. However, the sheer volume of this data makes manually examining individual conversations impractical. To overcome this challenge, we introduce WildVis, an interactive tool that enables fast, versatile, and large-scale conversation analysis. WildVis provides search and visualization capabilities in the text and embedding spaces based on a list of criteria. To manage million-scale datasets, we implemented optimizations including search index construction, embedding precomputation and compression, and caching to ensure responsive user interactions within seconds. We demonstrate WildVis' utility through three case studies: facilitating chatbot misuse research, visualizing and comparing topic distributions across datasets, and characterizing user-specific conversation patterns. WildVis is open-source and designed to be extendable, supporting additional datasets and customized search and visualization functionalities.",
            "id": "2409.03753",
            "link": "http://arxiv.org/abs/2409.03753v2",
            "published": "2024-09-05T17:59:15+00:00",
            "updated": "2024-09-09T10:04:00+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.HC",
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 75
        },
        "2409.04056": {
            "authors": [
                "Yiwen Peng",
                "Thomas Bonald",
                "Mehwish Alam"
            ],
            "title": "Refining Wikidata Taxonomy using Large Language Models",
            "abstract": "Due to its collaborative nature, Wikidata is known to have a complex taxonomy, with recurrent issues like the ambiguity between instances and classes, the inaccuracy of some taxonomic paths, the presence of cycles, and the high level of redundancy across classes. Manual efforts to clean up this taxonomy are time-consuming and prone to errors or subjective decisions. We present WiKC, a new version of Wikidata taxonomy cleaned automatically using a combination of Large Language Models (LLMs) and graph mining techniques. Operations on the taxonomy, such as cutting links or merging classes, are performed with the help of zero-shot prompting on an open-source LLM. The quality of the refined taxonomy is evaluated from both intrinsic and extrinsic perspectives, on a task of entity typing for the latter, showing the practical interest of WiKC.",
            "id": "2409.04056",
            "link": "http://arxiv.org/abs/2409.04056v1",
            "published": "2024-09-06T06:53:45+00:00",
            "updated": "2024-09-06T06:53:45+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.IR"
            ],
            "max_author_hindex": 35
        },
        "2409.02038": {
            "authors": [
                "Peter Baile Chen",
                "Fabian Wenz",
                "Yi Zhang",
                "Moe Kayali",
                "Nesime Tatbul",
                "Michael Cafarella",
                "\u00c7a\u011fatay Demiralp",
                "Michael Stonebraker"
            ],
            "title": "BEAVER: An Enterprise Benchmark for Text-to-SQL",
            "abstract": "Existing text-to-SQL benchmarks have largely been constructed using publicly available tables from the web with human-generated tests containing question and SQL statement pairs. They typically show very good results and lead people to think that LLMs are effective at text-to-SQL tasks. In this paper, we apply off-the-shelf LLMs to a benchmark containing enterprise data warehouse data. In this environment, LLMs perform poorly, even when standard prompt engineering and RAG techniques are utilized. As we will show, the reasons for poor performance are largely due to three characteristics: (1) public LLMs cannot train on enterprise data warehouses because they are largely in the \"dark web\", (2) schemas of enterprise tables are more complex than the schemas in public data, which leads the SQL-generation task innately harder, and (3) business-oriented questions are often more complex, requiring joins over multiple tables and aggregations. As a result, we propose a new dataset BEAVER, sourced from real enterprise data warehouses together with natural language queries and their correct SQL statements which we collected from actual user history. We evaluated this dataset using recent LLMs and demonstrated their poor performance on this task. We hope this dataset will facilitate future researchers building more sophisticated text-to-SQL systems which can do better on this important class of data.",
            "id": "2409.02038",
            "link": "http://arxiv.org/abs/2409.02038v1",
            "published": "2024-09-03T16:37:45+00:00",
            "updated": "2024-09-03T16:37:45+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.DB"
            ],
            "max_author_hindex": 96
        },
        "2409.00856": {
            "authors": [
                "William Zhang",
                "Maria Leon",
                "Ryan Xu",
                "Adrian Cardenas",
                "Amelia Wissink",
                "Hanna Martin",
                "Maya Srikanth",
                "Kaya Dorogi",
                "Christian Valadez",
                "Pedro Perez",
                "Citlalli Grijalva",
                "Corey Zhang",
                "Mark Santolucito"
            ],
            "title": "Benchmarking LLM Code Generation for Audio Programming with Visual Dataflow Languages",
            "abstract": "Node-based programming languages are increasingly popular in media arts coding domains. These languages are designed to be accessible to users with limited coding experience, allowing them to achieve creative output without an extensive programming background. Using LLM-based code generation to further lower the barrier to creative output is an exciting opportunity. However, the best strategy for code generation for visual node-based programming languages is still an open question. In particular, such languages have multiple levels of representation in text, each of which may be used for code generation. In this work, we explore the performance of LLM code generation in audio programming tasks in visual programming languages at multiple levels of representation. We explore code generation through metaprogramming code representations for these languages (i.e., coding the language using a different high-level text-based programming language), as well as through direct node generation with JSON. We evaluate code generated in this way for two visual languages for audio programming on a benchmark set of coding problems. We measure both correctness and complexity of the generated code. We find that metaprogramming results in more semantically correct generated code, given that the code is well-formed (i.e., is syntactically correct and runs). We also find that prompting for richer metaprogramming using randomness and loops led to more complex code.",
            "id": "2409.00856",
            "link": "http://arxiv.org/abs/2409.00856v1",
            "published": "2024-09-01T22:11:23+00:00",
            "updated": "2024-09-01T22:11:23+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.CL",
                "cs.PL"
            ],
            "max_author_hindex": 45
        },
        "2409.00727": {
            "authors": [
                "Yuxiang Wang",
                "Xiao Yan",
                "Shiyu Jin",
                "Quanqing Xu",
                "Chuanhui Yang",
                "Yuanyuan Zhu",
                "Chuang Hu",
                "Bo Du",
                "Jiawei Jiang"
            ],
            "title": "Hound: Hunting Supervision Signals for Few and Zero Shot Node Classification on Text-attributed Graph",
            "abstract": "Text-attributed graph (TAG) is an important type of graph structured data with text descriptions for each node. Few- and zero-shot node classification on TAGs have many applications in fields such as academia and social networks. However, the two tasks are challenging due to the lack of supervision signals, and existing methods only use the contrastive loss to align graph-based node embedding and language-based text embedding. In this paper, we propose Hound to improve accuracy by introducing more supervision signals, and the core idea is to go beyond the node-text pairs that come with data. Specifically, we design three augmentation techniques, i.e., node perturbation, text matching, and semantics negation to provide more reference nodes for each text and vice versa. Node perturbation adds/drops edges to produce diversified node embeddings that can be matched with a text. Text matching retrieves texts with similar embeddings to match with a node. Semantics negation uses a negative prompt to construct a negative text with the opposite semantics, which is contrasted with the original node and text. We evaluate Hound on 5 datasets and compare with 13 state-of-the-art baselines. The results show that Hound consistently outperforms all baselines, and its accuracy improvements over the best-performing baseline are usually over 5%.",
            "id": "2409.00727",
            "link": "http://arxiv.org/abs/2409.00727v1",
            "published": "2024-09-01T14:20:01+00:00",
            "updated": "2024-09-01T14:20:01+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.IR"
            ],
            "max_author_hindex": 33
        },
        "2409.00830": {
            "authors": [
                "Saransh Kumar Gupta",
                "Lipika Dey",
                "Partha Pratim Das",
                "Ramesh Jain"
            ],
            "title": "Building FKG.in: a Knowledge Graph for Indian Food",
            "abstract": "This paper presents an ontology design along with knowledge engineering, and multilingual semantic reasoning techniques to build an automated system for assimilating culinary information for Indian food in the form of a knowledge graph. The main focus is on designing intelligent methods to derive ontology designs and capture all-encompassing knowledge about food, recipes, ingredients, cooking characteristics, and most importantly, nutrition, at scale. We present our ongoing work in this workshop paper, describe in some detail the relevant challenges in curating knowledge of Indian food, and propose our high-level ontology design. We also present a novel workflow that uses AI, LLM, and language technology to curate information from recipe blog sites in the public domain to build knowledge graphs for Indian food. The methods for knowledge curation proposed in this paper are generic and can be replicated for any domain. The design is application-agnostic and can be used for AI-driven smart analysis, building recommendation systems for Personalized Digital Health, and complementing the knowledge graph for Indian food with contextual information such as user information, food biochemistry, geographic information, agricultural information, etc.",
            "id": "2409.00830",
            "link": "http://arxiv.org/abs/2409.00830v1",
            "published": "2024-09-01T20:18:36+00:00",
            "updated": "2024-09-01T20:18:36+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.IR"
            ],
            "max_author_hindex": 21
        },
        "2409.03284": {
            "authors": [
                "Yassir Lairgi",
                "Ludovic Moncla",
                "R\u00e9my Cazabet",
                "Khalid Benabdeslem",
                "Pierre Cl\u00e9au"
            ],
            "title": "iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models",
            "abstract": "Most available data is unstructured, making it challenging to access valuable information. Automatically building Knowledge Graphs (KGs) is crucial for structuring data and making it accessible, allowing users to search for information effectively. KGs also facilitate insights, inference, and reasoning. Traditional NLP methods, such as named entity recognition and relation extraction, are key in information retrieval but face limitations, including the use of predefined entity types and the need for supervised learning. Current research leverages large language models' capabilities, such as zero- or few-shot learning. However, unresolved and semantically duplicated entities and relations still pose challenges, leading to inconsistent graphs and requiring extensive post-processing. Additionally, most approaches are topic-dependent. In this paper, we propose iText2KG, a method for incremental, topic-independent KG construction without post-processing. This plug-and-play, zero-shot method is applicable across a wide range of KG construction scenarios and comprises four modules: Document Distiller, Incremental Entity Extractor, Incremental Relation Extractor, and Graph Integrator and Visualization. Our method demonstrates superior performance compared to baseline methods across three scenarios: converting scientific papers to graphs, websites to graphs, and CVs to graphs.",
            "id": "2409.03284",
            "link": "http://arxiv.org/abs/2409.03284v1",
            "published": "2024-09-05T06:49:14+00:00",
            "updated": "2024-09-05T06:49:14+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.IR"
            ],
            "max_author_hindex": 13
        },
        "2409.04073": {
            "authors": [
                "Zeyu Zhang",
                "Paul Groth",
                "Iacer Calixto",
                "Sebastian Schelter"
            ],
            "title": "AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language Model",
            "abstract": "Entity matching (EM) is the problem of determining whether two records refer to same real-world entity, which is crucial in data integration, e.g., for product catalogs or address databases. A major drawback of many EM approaches is their dependence on labelled examples. We thus focus on the challenging setting of zero-shot entity matching where no labelled examples are available for an unseen target dataset. Recently, large language models (LLMs) have shown promising results for zero-shot EM, but their low throughput and high deployment cost limit their applicability and scalability.   We revisit the zero-shot EM problem with AnyMatch, a small language model fine-tuned in a transfer learning setup. We propose several novel data selection techniques to generate fine-tuning data for our model, e.g., by selecting difficult pairs to match via an AutoML filter, by generating additional attribute-level examples, and by controlling label imbalance in the data.   We conduct an extensive evaluation of the prediction quality and deployment cost of our model, in a comparison to thirteen baselines on nine benchmark datasets. We find that AnyMatch provides competitive prediction quality despite its small parameter size: it achieves the second-highest F1 score overall, and outperforms several other approaches that employ models with hundreds of billions of parameters. Furthermore, our approach exhibits major cost benefits: the average prediction quality of AnyMatch is within 4.4% of the state-of-the-art method MatchGPT with the proprietary trillion-parameter model GPT-4, yet AnyMatch requires four orders of magnitude less parameters and incurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).",
            "id": "2409.04073",
            "link": "http://arxiv.org/abs/2409.04073v2",
            "published": "2024-09-06T07:29:01+00:00",
            "updated": "2024-09-09T11:33:00+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.DB"
            ],
            "max_author_hindex": 36
        },
        "2409.02343": {
            "authors": [
                "Sepanta Zeighami",
                "Zac Wellmer",
                "Aditya Parameswaran"
            ],
            "title": "NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for Retrieval",
            "abstract": "$k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval) from pre-trained embedding models is the predominant retrieval method for text and images, as well as Retrieval-Augmented Generation (RAG) pipelines. In practice, application developers often fine-tune the embeddings to improve their accuracy on the dataset and query workload in hand. Existing approaches either fine-tune the pre-trained model itself or, more efficiently, but at the cost of accuracy, train adaptor models to transform the output of the pre-trained model. We present NUDGE, a family of novel non-parametric embedding fine-tuning approaches that are significantly more accurate and efficient than both sets of existing approaches. NUDGE directly modifies the embeddings of data records to maximize the accuracy of $k$-NN retrieval. We present a thorough theoretical and experimental study of NUDGE's non-parametric approach. We show that even though the underlying problem is NP-Hard, constrained variations can be solved efficiently. These constraints additionally ensure that the changes to the embeddings are modest, avoiding large distortions to the semantics learned during pre-training. In experiments across five pre-trained models and nine standard text and image retrieval datasets, NUDGE runs in minutes and often improves NDCG@10 by more than 10% over existing fine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase in accuracy and runs 200x and 3x faster, respectively, over fine-tuning the pre-trained model and training adaptors.",
            "id": "2409.02343",
            "link": "http://arxiv.org/abs/2409.02343v1",
            "published": "2024-09-04T00:10:36+00:00",
            "updated": "2024-09-04T00:10:36+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.IR"
            ],
            "max_author_hindex": 41
        },
        "2409.01736": {
            "authors": [
                "Dean Light",
                "Ahmad Aiashy",
                "Mahmoud Diab",
                "Daniel Nachmias",
                "Stijn Vansummeren",
                "Benny Kimelfeld"
            ],
            "title": "SpannerLib: Embedding Declarative Information Extraction in an Imperative Workflow",
            "abstract": "Document spanners have been proposed as a formal framework for declarative Information Extraction (IE) from text, following IE products from the industry and academia. Over the past decade, the framework has been studied thoroughly in terms of expressive power, complexity, and the ability to naturally combine text analysis with relational querying. This demonstration presents SpannerLib a library for embedding document spanners in Python code. SpannerLib facilitates the development of IE programs by providing an implementation of Spannerlog (Datalog-based documentspanners) that interacts with the Python code in two directions: rules can be embedded inside Python, and they can invoke custom Python code (e.g., calls to ML-based NLP models) via user-defined functions. The demonstration scenarios showcase IE programs, with increasing levels of complexity, within Jupyter Notebook.",
            "id": "2409.01736",
            "link": "http://arxiv.org/abs/2409.01736v1",
            "published": "2024-09-03T09:25:26+00:00",
            "updated": "2024-09-03T09:25:26+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.IR",
                "H.4"
            ],
            "max_author_hindex": 30
        },
        "2409.02571": {
            "authors": [
                "Yuexuan Xu",
                "Jianyang Gao",
                "Yutong Gou",
                "Cheng Long",
                "Christian S. Jensen"
            ],
            "title": "iRangeGraph: Improvising Range-dedicated Graphs for Range-filtering Nearest Neighbor Search",
            "abstract": "Range-filtering approximate nearest neighbor (RFANN) search is attracting increasing attention in academia and industry. Given a set of data objects, each being a pair of a high-dimensional vector and a numeric value, an RFANN query with a vector and a numeric range as parameters returns the data object whose numeric value is in the query range and whose vector is nearest to the query vector. To process this query, a recent study proposes to build $O(n^2)$ dedicated graph-based indexes for all possible query ranges to enable efficient processing on a database of $n$ objects. As storing all these indexes is prohibitively expensive, the study constructs compressed indexes instead, which reduces the memory consumption considerably. However, this incurs suboptimal performance because the compression is lossy. In this study, instead of materializing a compressed index for every possible query range in preparation for querying, we materialize graph-based indexes, called elemental graphs, for a moderate number of ranges. We then provide an effective and efficient algorithm that during querying can construct an index for any query range using the elemental graphs. We prove that the time needed to construct such an index is low. We also cover an experimental study on real-world datasets that provides evidence that the materialized elemental graphs only consume moderate space and that the proposed method is capable of superior and stable query performance across different query workloads.",
            "id": "2409.02571",
            "link": "http://arxiv.org/abs/2409.02571v1",
            "published": "2024-09-04T09:41:52+00:00",
            "updated": "2024-09-04T09:41:52+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.DS",
                "cs.IR"
            ],
            "max_author_hindex": 27
        },
        "2409.02808": {
            "authors": [
                "Danilo Fernandes",
                "Douglas L. L. Moura",
                "Gean Santos",
                "Geymerson S. Ramos",
                "Fabiane Queiroz",
                "Andre L. L. Aquino"
            ],
            "title": "Towards Edge-Based Data Lake Architecture for Intelligent Transportation System",
            "abstract": "The rapid urbanization growth has underscored the need for innovative solutions to enhance transportation efficiency and safety. Intelligent Transportation Systems (ITS) have emerged as a promising solution in this context. However, analyzing and processing the massive and intricate data generated by ITS presents significant challenges for traditional data processing systems. This work proposes an Edge-based Data Lake Architecture to integrate and analyze the complex data from ITS efficiently. The architecture offers scalability, fault tolerance, and performance, improving decision-making and enhancing innovative services for a more intelligent transportation ecosystem. We demonstrate the effectiveness of the architecture through an analysis of three different use cases: (i) Vehicular Sensor Network, (ii) Mobile Network, and (iii) Driver Identification applications.",
            "id": "2409.02808",
            "link": "http://arxiv.org/abs/2409.02808v1",
            "published": "2024-09-04T15:25:28+00:00",
            "updated": "2024-09-04T15:25:28+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.AI",
                "cs.NI"
            ],
            "max_author_hindex": 25
        },
        "2409.03707": {
            "authors": [
                "Qingwen Fu"
            ],
            "title": "A Different Level Text Protection Mechanism With Differential Privacy",
            "abstract": "The article introduces a method for extracting words of different degrees of importance based on the BERT pre-training model and proves the effectiveness of this method. The article also discusses the impact of maintaining the same perturbation results for words of different importance on the overall text utility. This method can be applied to long text protection.",
            "id": "2409.03707",
            "link": "http://arxiv.org/abs/2409.03707v1",
            "published": "2024-09-05T17:13:38+00:00",
            "updated": "2024-09-05T17:13:38+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 2
        },
        "2409.03140": {
            "authors": [
                "Ashirbad Mishra",
                "Soumik Dey",
                "Marshall Wu",
                "Jinyu Zhao",
                "He Yu",
                "Kaichen Ni",
                "Binbin Li",
                "Kamesh Madduri"
            ],
            "title": "GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation",
            "abstract": "Online sellers and advertisers are recommended keyphrases for their listed products, which they bid on to enhance their sales. One popular paradigm that generates such recommendations is Extreme Multi-Label Classification (XMC), which involves tagging/mapping keyphrases to items. We outline the limitations of using traditional item-query based tagging or mapping techniques for keyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an innovative graph-based approach that recommends keyphrases to sellers using extraction of token permutations from item titles. Additionally, we demonstrate that relying on traditional metrics such as precision/recall can be misleading in practical applications, thereby necessitating a combination of metrics to evaluate performance in real-world scenarios. These metrics are designed to assess the relevance of keyphrases to items and the potential for buyer outreach. GraphEx outperforms production models at eBay, achieving the objectives mentioned above. It supports near real-time inferencing in resource-constrained production environments and scales effectively for billions of items.",
            "id": "2409.03140",
            "link": "http://arxiv.org/abs/2409.03140v2",
            "published": "2024-09-05T00:25:37+00:00",
            "updated": "2024-09-06T18:41:50+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 31
        },
        "2409.04475": {
            "authors": [
                "Yihang Zheng",
                "Bo Li",
                "Zhenghao Lin",
                "Yi Luo",
                "Xuanhe Zhou",
                "Chen Lin",
                "Jinsong Su",
                "Guoliang Li",
                "Shifu Li"
            ],
            "title": "Revolutionizing Database Q&A with Large Language Models: Comprehensive Benchmark and Evaluation",
            "abstract": "The development of Large Language Models (LLMs) has revolutionized Q&A across various industries, including the database domain. However, there is still a lack of a comprehensive benchmark to evaluate the capabilities of different LLMs and their modular components in database Q&A. To this end, we introduce DQA, the first comprehensive database Q&A benchmark. DQA features an innovative LLM-based method for automating the generation, cleaning, and rewriting of database Q&A, resulting in over 240,000 Q&A pairs in English and Chinese. These Q&A pairs cover nearly all aspects of database knowledge, including database manuals, database blogs, and database tools. This inclusion allows for additional assessment of LLMs' Retrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG) capabilities in the database Q&A task. Furthermore, we propose a comprehensive LLM-based database Q&A testbed on DQA. This testbed is highly modular and scalable, with both basic and advanced components like Question Classification Routing (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Besides, DQA provides a complete evaluation pipeline, featuring diverse metrics and a standardized evaluation process to ensure comprehensiveness, accuracy, and fairness. We use DQA to evaluate the database Q&A capabilities under the proposed testbed comprehensively. The evaluation reveals findings like (i) the strengths and limitations of nine different LLM-based Q&A bots and (ii) the performance impact and potential improvements of various service components (e.g., QCR, RAG, TIG). We hope our benchmark and findings will better guide the future development of LLM-based database Q&A research.",
            "id": "2409.04475",
            "link": "http://arxiv.org/abs/2409.04475v1",
            "published": "2024-09-05T13:45:42+00:00",
            "updated": "2024-09-05T13:45:42+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.AI"
            ],
            "max_author_hindex": 47
        },
        "2409.04194": {
            "authors": [
                "Malte Luttermann",
                "Ralf M\u00f6ller",
                "Mattis Hartwig"
            ],
            "title": "Towards Privacy-Preserving Relational Data Synthesis via Probabilistic Relational Models",
            "abstract": "Probabilistic relational models provide a well-established formalism to combine first-order logic and probabilistic models, thereby allowing to represent relationships between objects in a relational domain. At the same time, the field of artificial intelligence requires increasingly large amounts of relational training data for various machine learning tasks. Collecting real-world data, however, is often challenging due to privacy concerns, data protection regulations, high costs, and so on. To mitigate these challenges, the generation of synthetic data is a promising approach. In this paper, we solve the problem of generating synthetic relational data via probabilistic relational models. In particular, we propose a fully-fledged pipeline to go from relational database to probabilistic relational model, which can then be used to sample new synthetic relational data points from its underlying probability distribution. As part of our proposed pipeline, we introduce a learning algorithm to construct a probabilistic relational model from a given relational database.",
            "id": "2409.04194",
            "link": "http://arxiv.org/abs/2409.04194v1",
            "published": "2024-09-06T11:24:25+00:00",
            "updated": "2024-09-06T11:24:25+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.DB",
                "cs.LG"
            ],
            "max_author_hindex": 25
        },
        "2409.05022": {
            "authors": [
                "Linsey Pang",
                "Amir Hossein Raffiee",
                "Wei Liu",
                "Keld Lundgaard"
            ],
            "title": "Sequential Recommendation via Adaptive Robust Attention with Multi-dimensional Embeddings",
            "abstract": "Sequential recommendation models have achieved state-of-the-art performance using self-attention mechanism. It has since been found that moving beyond only using item ID and positional embeddings leads to a significant accuracy boost when predicting the next item. In recent literature, it was reported that a multi-dimensional kernel embedding with temporal contextual kernels to capture users' diverse behavioral patterns results in a substantial performance improvement. In this study, we further improve the sequential recommender model's robustness and generalization by introducing a mix-attention mechanism with a layer-wise noise injection (LNI) regularization. We refer to our proposed model as adaptive robust sequential recommendation framework (ADRRec), and demonstrate through extensive experiments that our model outperforms existing self-attention architectures.",
            "id": "2409.05022",
            "link": "http://arxiv.org/abs/2409.05022v1",
            "published": "2024-09-08T08:27:22+00:00",
            "updated": "2024-09-08T08:27:22+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 29
        },
        "2409.01152": {
            "authors": [
                "Mohanna Hoveyda",
                "Arjen P. de Vries",
                "Maarten de Rijke",
                "Faegheh Hasibi"
            ],
            "title": "Real World Conversational Entity Linking Requires More Than Zeroshots",
            "abstract": "Entity linking (EL) in conversations faces notable challenges in practical applications, primarily due to the scarcity of entity-annotated conversational datasets and sparse knowledge bases (KB) containing domain-specific, long-tail entities. We designed targeted evaluation scenarios to measure the efficacy of EL models under resource constraints. Our evaluation employs two KBs: Fandom, exemplifying real-world EL complexities, and the widely used Wikipedia. First, we assess EL models' ability to generalize to a new unfamiliar KB using Fandom and a novel zero-shot conversational entity linking dataset that we curated based on Reddit discussions on Fandom entities. We then evaluate the adaptability of EL models to conversational settings without prior training. Our results indicate that current zero-shot EL models falter when introduced to new, domain-specific KBs without prior training, significantly dropping in performance. Our findings reveal that previous evaluation approaches fall short of capturing real-world complexities for zero-shot EL, highlighting the necessity for new approaches to design and assess conversational EL models to adapt to limited resources. The evaluation setup and the dataset proposed in this research are made publicly available.",
            "id": "2409.01152",
            "link": "http://arxiv.org/abs/2409.01152v1",
            "published": "2024-09-02T10:37:53+00:00",
            "updated": "2024-09-02T10:37:53+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.IR"
            ],
            "max_author_hindex": 77
        },
        "2409.01357": {
            "authors": [
                "Antoine Louis",
                "Gijs van Dijck",
                "Gerasimos Spanakis"
            ],
            "title": "Know When to Fuse: Investigating Non-English Hybrid Retrieval in the Legal Domain",
            "abstract": "Hybrid search has emerged as an effective strategy to offset the limitations of different matching paradigms, especially in out-of-domain contexts where notable improvements in retrieval quality have been observed. However, existing research predominantly focuses on a limited set of retrieval methods, evaluated in pairs on domain-general datasets exclusively in English. In this work, we study the efficacy of hybrid search across a variety of prominent retrieval models within the unexplored field of law in the French language, assessing both zero-shot and in-domain scenarios. Our findings reveal that in a zero-shot context, fusing different domain-general models consistently enhances performance compared to using a standalone model, regardless of the fusion method. Surprisingly, when models are trained in-domain, we find that fusion generally diminishes performance relative to using the best single system, unless fusing scores with carefully tuned weights. These novel insights, among others, expand the applicability of prior findings across a new field and language, and contribute to a deeper understanding of hybrid search in non-English specialized domains.",
            "id": "2409.01357",
            "link": "http://arxiv.org/abs/2409.01357v1",
            "published": "2024-09-02T16:19:13+00:00",
            "updated": "2024-09-02T16:19:13+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.IR"
            ],
            "max_author_hindex": 17
        },
        "2409.04701": {
            "authors": [
                "Michael G\u00fcnther",
                "Isabelle Mohr",
                "Bo Wang",
                "Han Xiao"
            ],
            "title": "Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models",
            "abstract": "Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be \"over-compressed\" in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in suboptimal representations. In this paper, we introduce a novel method called \"late chunking,\" which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks without the need for additional training. Moreover, our method is generic enough to be applied to any long-context embedding model.",
            "id": "2409.04701",
            "link": "http://arxiv.org/abs/2409.04701v1",
            "published": "2024-09-07T03:54:46+00:00",
            "updated": "2024-09-07T03:54:46+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.IR",
                "68T50",
                "I.2.7"
            ],
            "max_author_hindex": 33
        },
        "2409.00921": {
            "authors": [
                "Andrew Blinn",
                "Xiang Li",
                "June Hyung Kim",
                "Cyrus Omar"
            ],
            "title": "Statically Contextualizing Large Language Models with Typed Holes",
            "abstract": "Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate context, particularly when working with definitions not in the training data nor near the cursor. This paper demonstrates that tight integration with the type and binding structure of a language, as exposed by its language server, can address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server identifies the type and typing context of the hole being filled, even in the presence of errors, ensuring that a meaningful program sketch is always available. This allows prompting with codebase-wide contextual information not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications. These applications serve as challenge problems due to their reliance on application-specific data structures. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.",
            "id": "2409.00921",
            "link": "http://arxiv.org/abs/2409.00921v1",
            "published": "2024-09-02T03:29:00+00:00",
            "updated": "2024-09-02T03:29:00+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL",
                "cs.AI",
                "cs.SE",
                "D.3.0"
            ],
            "max_author_hindex": 20
        },
        "2409.02864": {
            "authors": [
                "Joshua Pickard",
                "Marc Andrew Choi",
                "Natalie Oliven",
                "Cooper Stansbury",
                "Jillian Cwycyshyn",
                "Nicholas Galioto",
                "Alex Gorodetsky",
                "Alvaro Velasquez",
                "Indika Rajapakse"
            ],
            "title": "Bioinformatics Retrieval Augmentation Data (BRAD) Digital Assistant",
            "abstract": "We present a prototype for a Bioinformatics Retrieval Augmentation Data (BRAD) digital assistant. BRAD integrates a suite of tools to handle a wide range of bioinformatics tasks, from code execution to online search. We demonstrate BRAD's capabilities through (1) improved question-and-answering with retrieval augmented generation (RAG), (2) BRAD's ability to run and write complex software pipelines, and (3) BRAD's ability to organize and distribute tasks across individual and teams of agents. We use BRAD for automation of bioinformatics workflows, performing tasks ranging from gene enrichment and searching the archive to automatic code generation and running biomarker identification pipelines. BRAD is a step toward the ultimate goal to develop a digital twin of laboratories driven by self-contained loops for hypothesis generation and testing of digital biology experiments.",
            "id": "2409.02864",
            "link": "http://arxiv.org/abs/2409.02864v1",
            "published": "2024-09-04T16:43:14+00:00",
            "updated": "2024-09-04T16:43:14+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.IR",
                "cs.SE"
            ],
            "max_author_hindex": 22
        },
        "2409.01137": {
            "authors": [
                "M. Badouch",
                "M. Boutaounte"
            ],
            "title": "Smart E-commerce Recommendations with Semantic AI",
            "abstract": "In e-commerce, web mining for page recommendations is widely used but often fails to meet user needs. To address this, we propose a novel solution combining semantic web mining with BP neural networks. We process user search logs to extract five key features: content priority, time spent, user feedback, recommendation semantics, and input deviation. These features are then fed into a BP neural network to classify and prioritize web pages. The prioritized pages are recommended to users. Using book sales pages for testing, our results demonstrate that this solution can quickly and accurately identify the pages users need. Our approach ensures that recommendations are more relevant and tailored to individual preferences, enhancing the online shopping experience. By leveraging advanced semantic analysis and neural network techniques, we bridge the gap between user expectations and actual recommendations. This innovative method not only improves accuracy but also speeds up the recommendation process, making it a valuable tool for e-commerce platforms aiming to boost user satisfaction and engagement. Additionally, our system ability to handle large datasets and provide real-time recommendations makes it a scalable and efficient solution for modern e-commerce challenges.",
            "id": "2409.01137",
            "link": "http://arxiv.org/abs/2409.01137v3",
            "published": "2024-09-02T10:19:31+00:00",
            "updated": "2024-09-11T22:59:34+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 6
        },
        "2409.01605": {
            "authors": [
                "Xinyu Zhang",
                "Linmei Hu",
                "Luhao Zhang",
                "Dandan Song",
                "Heyan Huang",
                "Liqiang Nie"
            ],
            "title": "Laser: Parameter-Efficient LLM Bi-Tuning for Sequential Recommendation with Collaborative Information",
            "abstract": "Sequential recommender systems are essential for discerning user preferences from historical interactions and facilitating targeted recommendations. Recent innovations employing Large Language Models (LLMs) have advanced the field by encoding item semantics, yet they often necessitate substantial parameter tuning and are resource-demanding. Moreover, these works fails to consider the diverse characteristics of different types of users and thus diminishes the recommendation accuracy. In this paper, we propose a parameter-efficient Large Language Model Bi-Tuning framework for sequential recommendation with collaborative information (Laser). Specifically, Bi-Tuning works by inserting trainable virtual tokens at both the prefix and suffix of the input sequence and freezing the LLM parameters, thus optimizing the LLM for the sequential recommendation. In our Laser, the prefix is utilized to incorporate user-item collaborative information and adapt the LLM to the recommendation task, while the suffix converts the output embeddings of the LLM from the language space to the recommendation space for the follow-up item recommendation. Furthermore, to capture the characteristics of different types of users when integrating the collaborative information via the prefix, we introduce M-Former, a lightweight MoE-based querying transformer that uses a set of query experts to integrate diverse user-specific collaborative information encoded by frozen ID-based sequential recommender systems, significantly improving the accuracy of recommendations. Extensive experiments on real-world datasets demonstrate that Laser can parameter-efficiently adapt LLMs to effective recommender systems, significantly outperforming state-of-the-art methods.",
            "id": "2409.01605",
            "link": "http://arxiv.org/abs/2409.01605v1",
            "published": "2024-09-03T04:55:03+00:00",
            "updated": "2024-09-03T04:55:03+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "max_author_hindex": 66
        },
        "2409.02580": {
            "authors": [
                "Jinfeng Xu",
                "Zheyu Chen",
                "Jinze Li",
                "Shuo Yang",
                "Hewei Wang",
                "Edith C. -H. Ngai"
            ],
            "title": "AlignGroup: Learning and Aligning Group Consensus with Member Preferences for Group Recommendation",
            "abstract": "Group activities are important behaviors in human society, providing personalized recommendations for groups is referred to as the group recommendation task. Existing methods can usually be categorized into two strategies to infer group preferences: 1) determining group preferences by aggregating members' personalized preferences, and 2) inferring group consensus by capturing group members' coherent decisions after common compromises. However, the former would suffer from the lack of group-level considerations, and the latter overlooks the fine-grained preferences of individual users. To this end, we propose a novel group recommendation method AlignGroup, which focuses on both group consensus and individual preferences of group members to infer the group decision-making. Specifically, AlignGroup explores group consensus through a well-designed hypergraph neural network that efficiently learns intra- and inter-group relationships. Moreover, AlignGroup innovatively utilizes a self-supervised alignment task to capture fine-grained group decision-making by aligning the group consensus with members' common preferences. Extensive experiments on two real-world datasets validate that our AlignGroup outperforms the state-of-the-art on both the group recommendation task and the user recommendation task, as well as outperforms the efficiency of most baselines.",
            "id": "2409.02580",
            "link": "http://arxiv.org/abs/2409.02580v1",
            "published": "2024-09-04T10:03:09+00:00",
            "updated": "2024-09-04T10:03:09+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "max_author_hindex": 32
        },
        "2409.02685": {
            "authors": [
                "Hyunji Lee",
                "Luca Soldaini",
                "Arman Cohan",
                "Minjoon Seo",
                "Kyle Lo"
            ],
            "title": "RouterRetriever: Exploring the Benefits of Routing over Multiple Expert Embedding Models",
            "abstract": "Information retrieval methods often rely on a single embedding model trained on large, general-domain datasets like MSMARCO. While this approach can produce a retriever with reasonable overall performance, models trained on domain-specific data often yield better results within their respective domains. While prior work in information retrieval has tackled this through multi-task training, the topic of combining multiple domain-specific expert retrievers remains unexplored, despite its popularity in language model generation. In this work, we introduce RouterRetriever, a retrieval model that leverages multiple domain-specific experts along with a routing mechanism to select the most appropriate expert for each query. It is lightweight and allows easy addition or removal of experts without additional training. Evaluation on the BEIR benchmark demonstrates that RouterRetriever outperforms both MSMARCO-trained (+2.1 absolute nDCG@10) and multi-task trained (+3.2) models. This is achieved by employing our routing mechanism, which surpasses other routing techniques (+1.8 on average) commonly used in language modeling. Furthermore, the benefit generalizes well to other datasets, even in the absence of a specific expert on the dataset. To our knowledge, RouterRetriever is the first work to demonstrate the advantages of using multiple domain-specific expert embedding models with effective routing over a single, general-purpose embedding model in retrieval tasks.",
            "id": "2409.02685",
            "link": "http://arxiv.org/abs/2409.02685v1",
            "published": "2024-09-04T13:16:55+00:00",
            "updated": "2024-09-04T13:16:55+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "max_author_hindex": 34
        },
        "2409.05033": {
            "authors": [
                "Jianghao Lin",
                "Jiaqi Liu",
                "Jiachen Zhu",
                "Yunjia Xi",
                "Chengkai Liu",
                "Yangtian Zhang",
                "Yong Yu",
                "Weinan Zhang"
            ],
            "title": "A Survey on Diffusion Models for Recommender Systems",
            "abstract": "While traditional recommendation techniques have made significant strides in the past decades, they still suffer from limited generalization performance caused by factors like inadequate collaborative signals, weak latent representations, and noisy data. In response, diffusion models (DMs) have emerged as promising solutions for recommender systems due to their robust generative capabilities, solid theoretical foundations, and improved training stability. To this end, in this paper, we present the first comprehensive survey on diffusion models for recommendation, and draw a bird's-eye view from the perspective of the whole pipeline in real-world recommender systems. We systematically categorize existing research works into three primary domains: (1) diffusion for data engineering & encoding, focusing on data augmentation and representation enhancement; (2) diffusion as recommender models, employing diffusion models to directly estimate user preferences and rank items; and (3) diffusion for content presentation, utilizing diffusion models to generate personalized content such as fashion and advertisement creatives. Our taxonomy highlights the unique strengths of diffusion models in capturing complex data distributions and generating high-quality, diverse samples that closely align with user preferences. We also summarize the core characteristics of the adapting diffusion models for recommendation, and further identify key areas for future exploration, which helps establish a roadmap for researchers and practitioners seeking to advance recommender systems through the innovative application of diffusion models. To further facilitate the research community of recommender systems based on diffusion models, we actively maintain a GitHub repository for papers and other related resources in this rising direction https://github.com/CHIANGEL/Awesome-Diffusion-for-RecSys.",
            "id": "2409.05033",
            "link": "http://arxiv.org/abs/2409.05033v1",
            "published": "2024-09-08T08:57:12+00:00",
            "updated": "2024-09-08T08:57:12+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "max_author_hindex": 42
        },
        "2409.02727": {
            "authors": [
                "Yixuan Tang",
                "Yi Yang"
            ],
            "title": "Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?",
            "abstract": "The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.",
            "id": "2409.02727",
            "link": "http://arxiv.org/abs/2409.02727v2",
            "published": "2024-09-04T14:01:48+00:00",
            "updated": "2024-09-05T07:17:59+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.IR"
            ],
            "max_author_hindex": 27
        },
        "2409.03708": {
            "authors": [
                "Sriram Veturi",
                "Saurabh Vaichal",
                "Reshma Lal Jagadheesh",
                "Nafis Irtiza Tripto",
                "Nian Yan"
            ],
            "title": "RAG based Question-Answering for Contextual Response Prediction System",
            "abstract": "Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems. However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations. Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge. Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation. In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases. Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company. Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance. Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload.",
            "id": "2409.03708",
            "link": "http://arxiv.org/abs/2409.03708v2",
            "published": "2024-09-05T17:14:23+00:00",
            "updated": "2024-09-06T14:18:20+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.IR"
            ],
            "max_author_hindex": 37
        },
        "2409.03397": {
            "authors": [
                "Marcus Mohr"
            ],
            "title": "Dynamic String Generation and C++-style Output in Fortran",
            "abstract": "Using standard components of modern Fortran we present a technique to dynamically generate strings with as little coding overhead as possible on the application side. Additionally we demonstrate how this can be extended to allow for output generation with a C++ stream-like look and feel.",
            "id": "2409.03397",
            "link": "http://arxiv.org/abs/2409.03397v1",
            "published": "2024-09-05T10:14:59+00:00",
            "updated": "2024-09-05T10:14:59+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL",
                "D.3.3"
            ],
            "max_author_hindex": 14
        },
        "2409.03439": {
            "authors": [
                "Wei Gao",
                "Jingqiang Wang",
                "Xinv Zhu",
                "Jun Zhong",
                "Yue Shen",
                "Youshuang Ding"
            ],
            "title": "KiloBot: A Programming Language for Deploying Perception-Guided Industrial Manipulators at Scale",
            "abstract": "We would like industrial robots to handle unstructured environments with cameras and perception pipelines. In contrast to traditional industrial robots that replay offline-crafted trajectories, online behavior planning is required for these perception-guided industrial applications. Aside from perception and planning algorithms, deploying perception-guided manipulators also requires substantial effort in integration. One approach is writing scripts in a traditional language (such as Python) to construct the planning problem and perform integration with other algorithmic modules & external devices. While scripting in Python is feasible for a handful of robots and applications, deploying perception-guided manipulation at scale (e.g., more than 10000 robot workstations in over 2000 customer sites) becomes intractable. To resolve this challenge, we propose a Domain-Specific Language (DSL) for perception-guided manipulation applications. To scale up the deployment,our DSL provides: 1) an easily accessible interface to construct & solve a sub-class of Task and Motion Planning (TAMP) problems that are important in practical applications; and 2) a mechanism to implement flexible control flow to perform integration and address customized requirements of distinct industrial application. Combined with an intuitive graphical programming frontend, our DSL is mainly used by machine operators without coding experience in traditional programming languages. Within hours of training, operators are capable of orchestrating interesting sophisticated manipulation behaviors with our DSL. Extensive practical deployments demonstrate the efficacy of our method.",
            "id": "2409.03439",
            "link": "http://arxiv.org/abs/2409.03439v1",
            "published": "2024-09-05T11:42:08+00:00",
            "updated": "2024-09-05T11:42:08+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.PL"
            ],
            "max_author_hindex": 37
        },
        "2409.04667": {
            "authors": [
                "Hemanth Kandula",
                "Damianos Karakos",
                "Haoling Qiu",
                "Benjamin Rozonoyer",
                "Ian Soboroff",
                "Lee Tarlin",
                "Bonan Min"
            ],
            "title": "QueryBuilder: Human-in-the-Loop Query Development for Information Retrieval",
            "abstract": "Frequently, users of an Information Retrieval (IR) system start with an overarching information need (a.k.a., an analytic task) and proceed to define finer-grained queries covering various important aspects (i.e., sub-topics) of that analytic task. We present a novel, interactive system called $\\textit{QueryBuilder}$, which allows a novice, English-speaking user to create queries with a small amount of effort, through efficient exploration of an English development corpus in order to rapidly develop cross-lingual information retrieval queries corresponding to the user's information needs. QueryBuilder performs near real-time retrieval of documents based on user-entered search terms; the user looks through the retrieved documents and marks sentences as relevant to the information needed. The marked sentences are used by the system as additional information in query formation and refinement: query terms (and, optionally, event features, which capture event $'triggers'$ (indicator terms) and agent/patient roles) are appropriately weighted, and a neural-based system, which better captures textual meaning, retrieves other relevant content. The process of retrieval and marking is repeated as many times as desired, giving rise to increasingly refined queries in each iteration. The final product is a fine-grained query used in Cross-Lingual Information Retrieval (CLIR). Our experiments using analytic tasks and requests from the IARPA BETTER IR datasets show that with a small amount of effort (at most 10 minutes per sub-topic), novice users can form $\\textit{useful}$ fine-grained queries including in languages they don't understand. QueryBuilder also provides beneficial capabilities to the traditional corpus exploration and query formation process. A demonstration video is released at https://vimeo.com/734795835",
            "id": "2409.04667",
            "link": "http://arxiv.org/abs/2409.04667v2",
            "published": "2024-09-07T00:46:58+00:00",
            "updated": "2024-09-10T17:56:52+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 43
        },
        "2409.00448": {
            "authors": [
                "Jialiang Wang",
                "Yan Xia",
                "Ye Yuan"
            ],
            "title": "PSLF: A PID Controller-incorporated Second-order Latent Factor Analysis Model for Recommender System",
            "abstract": "A second-order-based latent factor (SLF) analysis model demonstrates superior performance in graph representation learning, particularly for high-dimensional and incomplete (HDI) interaction data, by incorporating the curvature information of the loss landscape. However, its objective function is commonly bi-linear and non-convex, causing the SLF model to suffer from a low convergence rate. To address this issue, this paper proposes a PID controller-incorporated SLF (PSLF) model, leveraging two key strategies: a) refining learning error estimation by incorporating the PID controller principles, and b) acquiring second-order information insights through Hessian-vector products. Experimental results on multiple HDI datasets indicate that the proposed PSLF model outperforms four state-of-the-art latent factor models based on advanced optimizers regarding convergence rates and generalization performance.",
            "id": "2409.00448",
            "link": "http://arxiv.org/abs/2409.00448v1",
            "published": "2024-08-31T13:01:58+00:00",
            "updated": "2024-08-31T13:01:58+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.IR"
            ],
            "max_author_hindex": 24
        },
        "2409.04432": {
            "authors": [
                "Angelo Salatino",
                "Tanay Aggarwal",
                "Andrea Mannocci",
                "Francesco Osborne",
                "Enrico Motta"
            ],
            "title": "A Survey on Knowledge Organization Systems of Research Fields: Resources and Challenges",
            "abstract": "Knowledge Organization Systems (KOSs), such as term lists, thesauri, taxonomies, and ontologies, play a fundamental role in categorising, managing, and retrieving information. In the academic domain, KOSs are often adopted for representing research areas and their relationships, primarily aiming to classify research articles, academic courses, patents, books, scientific venues, domain experts, grants, software, experiment materials, and several other relevant products and agents. These structured representations of research areas, widely embraced by many academic fields, have proven effective in empowering AI-based systems to i) enhance retrievability of relevant documents, ii) enable advanced analytic solutions to quantify the impact of academic research, and iii) analyse and forecast research dynamics. This paper aims to present a comprehensive survey of the current KOS for academic disciplines. We analysed and compared 45 KOSs according to five main dimensions: scope, structure, curation, usage, and links to other KOSs. Our results reveal a very heterogeneous scenario in terms of scope, scale, quality, and usage, highlighting the need for more integrated solutions for representing research knowledge across academic fields. We conclude by discussing the main challenges and the most promising future directions.",
            "id": "2409.04432",
            "link": "http://arxiv.org/abs/2409.04432v1",
            "published": "2024-09-06T17:54:43+00:00",
            "updated": "2024-09-06T17:54:43+00:00",
            "primary_category": "cs.DL",
            "categories": [
                "cs.DL",
                "cs.AI",
                "cs.IR"
            ],
            "max_author_hindex": 64
        },
        "2409.00315": {
            "authors": [
                "Xinyi Shen",
                "Zuoquan Lin"
            ],
            "title": "An Empirical Study on Context Length for Open-Domain Dialog Generation",
            "abstract": "Transformer-based open-domain dialog models have become increasingly popular in recent years. These models typically represent context as a concatenation of a dialog history. However, there is no criterion to decide how many utterances should be kept adequate in a context. We try to figure out how the choice of context length affects the model. We experiment on three questions from coarse to fine: (i) Does longer context help model training? (ii) Is it necessary to change the training context length when dealing with dialogs of different context lengths? (iii) Do different dialog samples have the same preference for context length? Our experimental results show that context length, an often overlooked setting, deserves attention when implementing Transformer-based dialog models.",
            "id": "2409.00315",
            "link": "http://arxiv.org/abs/2409.00315v1",
            "published": "2024-08-31T00:56:36+00:00",
            "updated": "2024-08-31T00:56:36+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 22
        },
        "2409.00331": {
            "authors": [
                "Oktie Hassanzadeh"
            ],
            "title": "WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction",
            "abstract": "Recently, there has been an increasing interest in the construction of general-domain and domain-specific causal knowledge graphs. Such knowledge graphs enable reasoning for causal analysis and event prediction, and so have a range of applications across different domains. While great progress has been made toward automated construction of causal knowledge graphs, the evaluation of such solutions has either focused on low-level tasks (e.g., cause-effect phrase extraction) or on ad hoc evaluation data and small manual evaluations. In this paper, we present a corpus, task, and evaluation framework for causal knowledge graph construction. Our corpus consists of Wikipedia articles for a collection of event-related concepts in Wikidata. The task is to extract causal relations between event concepts from the corpus. The evaluation is performed in part using existing causal relations in Wikidata to measure recall, and in part using Large Language Models to avoid the need for manual or crowd-sourced evaluation. We evaluate a pipeline for causal knowledge graph construction that relies on neural models for question answering and concept linking, and show how the corpus and the evaluation framework allow us to effectively find the right model for each task. The corpus and the evaluation framework are publicly available.",
            "id": "2409.00331",
            "link": "http://arxiv.org/abs/2409.00331v1",
            "published": "2024-08-31T02:21:39+00:00",
            "updated": "2024-08-31T02:21:39+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 27
        },
        "2409.00335": {
            "authors": [
                "Yuhan Ji",
                "Song Gao"
            ],
            "title": "Evaluating the Effectiveness of Large Language Models in Representing and Understanding Movement Trajectories",
            "abstract": "This research focuses on assessing the ability of AI foundation models in representing the trajectories of movements. We utilize one of the large language models (LLMs) (i.e., GPT-J) to encode the string format of trajectories and then evaluate the effectiveness of the LLM-based representation for trajectory data analysis. The experiments demonstrate that while the LLM-based embeddings can preserve certain trajectory distance metrics (i.e., the correlation coefficients exceed 0.74 between the Cosine distance derived from GPT-J embeddings and the Hausdorff and Dynamic Time Warping distances on raw trajectories), challenges remain in restoring numeric values and retrieving spatial neighbors in movement trajectory analytics. In addition, the LLMs can understand the spatiotemporal dependency contained in trajectories and have good accuracy in location prediction tasks. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using LLMs.",
            "id": "2409.00335",
            "link": "http://arxiv.org/abs/2409.00335v1",
            "published": "2024-08-31T02:57:25+00:00",
            "updated": "2024-08-31T02:57:25+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "I.2; E.2"
            ],
            "max_author_hindex": 31
        },
        "2409.00358": {
            "authors": [
                "Dipankar Srirag",
                "Aditya Joshi",
                "Jacob Eisenstein"
            ],
            "title": "Predicting the Target Word of Game-playing Conversations using a Low-Rank Dialect Adapter for Decoder Models",
            "abstract": "Dialect adapters that improve the performance of LLMs for NLU tasks on certain sociolects/dialects/national varieties ('dialects' for the sake of brevity) have been reported for encoder models. In this paper, we extend the idea of dialect adapters to decoder models in our architecture called LoRDD. Using MD-3, a publicly available dataset of word game-playing conversations between dialectal speakers, our task is Target Word Prediction (TWP) from a masked conversation. LoRDD combines task adapters and dialect adapters where the latter employ contrastive learning on pseudo-parallel conversations from MD-3. Our results for en-IN conversations on two models (Mistral and Gemma) show that LoRDD outperforms four baselines on TWP, while bridging the performance gap with en-US by 12% on word similarity and 25% on accuracy. The focused contribution of LoRDD is in its promise for dialect adaptation of decoder models.",
            "id": "2409.00358",
            "link": "http://arxiv.org/abs/2409.00358v1",
            "published": "2024-08-31T05:53:39+00:00",
            "updated": "2024-08-31T05:53:39+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 52
        },
        "2409.00551": {
            "authors": [
                "Wenxuan Wang"
            ],
            "title": "Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness",
            "abstract": "Large language models (LLMs), such as ChatGPT, have rapidly penetrated into people's work and daily lives over the past few years, due to their extraordinary conversational skills and intelligence. ChatGPT has become the fastest-growing software in terms of user numbers in human history and become an important foundational model for the next generation of artificial intelligence applications. However, the generations of LLMs are not entirely reliable, often producing content with factual errors, biases, and toxicity. Given their vast number of users and wide range of application scenarios, these unreliable responses can lead to many serious negative impacts. This thesis introduces the exploratory works in the field of language model reliability during the PhD study, focusing on the correctness, non-toxicity, and fairness of LLMs from both software testing and natural language processing perspectives. First, to measure the correctness of LLMs, we introduce two testing frameworks, FactChecker and LogicAsker, to evaluate factual knowledge and logical reasoning accuracy, respectively. Second, for the non-toxicity of LLMs, we introduce two works for red-teaming LLMs. Third, to evaluate the fairness of LLMs, we introduce two evaluation frameworks, BiasAsker and XCulturalBench, to measure the social bias and cultural bias of LLMs, respectively.",
            "id": "2409.00551",
            "link": "http://arxiv.org/abs/2409.00551v1",
            "published": "2024-08-31T22:21:04+00:00",
            "updated": "2024-08-31T22:21:04+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.SE"
            ],
            "max_author_hindex": 0
        },
        "2409.00614": {
            "authors": [
                "Xiaoyan Yu",
                "Yifan Wei",
                "Pu Li",
                "Shuaishuai Zhou",
                "Hao Peng",
                "Li Sun",
                "Liehuang Zhu",
                "Philip S. Yu"
            ],
            "title": "DAMe: Personalized Federated Social Event Detection with Dual Aggregation Mechanism",
            "abstract": "Training social event detection models through federated learning (FedSED) aims to improve participants' performance on the task. However, existing federated learning paradigms are inadequate for achieving FedSED's objective and exhibit limitations in handling the inherent heterogeneity in social data. This paper proposes a personalized federated learning framework with a dual aggregation mechanism for social event detection, namely DAMe. We present a novel local aggregation strategy utilizing Bayesian optimization to incorporate global knowledge while retaining local characteristics. Moreover, we introduce a global aggregation strategy to provide clients with maximum external knowledge of their preferences. In addition, we incorporate a global-local event-centric constraint to prevent local overfitting and ``client-drift''. Experiments within a realistic simulation of a natural federated setting, utilizing six social event datasets spanning six languages and two social media platforms, along with an ablation study, have demonstrated the effectiveness of the proposed framework. Further robustness analyses have shown that DAMe is resistant to injection attacks.",
            "id": "2409.00614",
            "link": "http://arxiv.org/abs/2409.00614v1",
            "published": "2024-09-01T04:56:41+00:00",
            "updated": "2024-09-01T04:56:41+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 45
        },
        "2409.00617": {
            "authors": [
                "Yifan Wei",
                "Xiaoyan Yu",
                "Yixuan Weng",
                "Huanhuan Ma",
                "Yuanzhe Zhang",
                "Jun Zhao",
                "Kang Liu"
            ],
            "title": "Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models",
            "abstract": "Large language models encapsulate knowledge and have demonstrated superior performance on various natural language processing tasks. Recent studies have localized this knowledge to specific model parameters, such as the MLP weights in intermediate layers. This study investigates the differences between entity and relational knowledge through knowledge editing. Our findings reveal that entity and relational knowledge cannot be directly transferred or mapped to each other. This result is unexpected, as logically, modifying the entity or the relation within the same knowledge triplet should yield equivalent outcomes. To further elucidate the differences between entity and relational knowledge, we employ causal analysis to investigate how relational knowledge is stored in pre-trained models. Contrary to prior research suggesting that knowledge is stored in MLP weights, our experiments demonstrate that relational knowledge is also significantly encoded in attention modules. This insight highlights the multifaceted nature of knowledge storage in language models, underscoring the complexity of manipulating specific types of knowledge within these models.",
            "id": "2409.00617",
            "link": "http://arxiv.org/abs/2409.00617v1",
            "published": "2024-09-01T05:09:11+00:00",
            "updated": "2024-09-01T05:09:11+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 40
        },
        "2409.00625": {
            "authors": [
                "Xinyi Bai"
            ],
            "title": "Entity-Aware Biaffine Attention Model for Improved Constituent Parsing with Reduced Entity Violations",
            "abstract": "Constituency parsing involves analyzing a sentence by breaking it into sub-phrases, or constituents. While many deep neural models have achieved state-of-the-art performance in this task, they often overlook the entity-violating issue, where an entity fails to form a complete sub-tree in the resultant parsing tree. To address this, we propose an entity-aware biaffine attention model for constituent parsing. This model incorporates entity information into the biaffine attention mechanism by using additional entity role vectors for potential phrases, which enhances the parsing accuracy. We introduce a new metric, the Entity Violating Rate (EVR), to quantify the extent of entity violations in parsing results. Experiments on three popular datasets-ONTONOTES, PTB, and CTB-demonstrate that our model achieves the lowest EVR while maintaining high precision, recall, and F1-scores comparable to existing models. Further evaluation in downstream tasks, such as sentence sentiment analysis, highlights the effectiveness of our model and the validity of the proposed EVR metric.",
            "id": "2409.00625",
            "link": "http://arxiv.org/abs/2409.00625v1",
            "published": "2024-09-01T05:59:54+00:00",
            "updated": "2024-09-01T05:59:54+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 4
        },
        "2409.00696": {
            "authors": [
                "Jasper Dekoninck",
                "Maximilian Baader",
                "Martin Vechev"
            ],
            "title": "Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM Evaluation",
            "abstract": "Rating-based human evaluation has become an essential tool to accurately evaluate the impressive performance of Large language models (LLMs). However, current rating systems suffer from several critical limitations. Specifically, they fail to account for human biases that significantly influence evaluation results, require large and expensive preference datasets to obtain accurate ratings, and do not facilitate meaningful comparisons of model ratings across different tasks. To address these issues, we introduce Polyrating, an expressive and flexible rating system based on maximum a posteriori estimation that enables a more nuanced and thorough analysis of model performance at lower costs. Polyrating can detect and quantify biases affecting human preferences, ensuring fairer model comparisons. Furthermore, Polyrating can reduce the cost of human evaluations by up to $41\\%$ for new models and up to $77\\%$ for new tasks by leveraging existing benchmark scores. Lastly, Polyrating enables direct comparisons of ratings across different tasks, providing a comprehensive understanding of an LLMs' strengths, weaknesses, and relative performance across different applications.",
            "id": "2409.00696",
            "link": "http://arxiv.org/abs/2409.00696v1",
            "published": "2024-09-01T11:24:54+00:00",
            "updated": "2024-09-01T11:24:54+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 55
        },
        "2409.00940": {
            "authors": [
                "Ruoyu Wen",
                "Stephanie Elena Crowe",
                "Kunal Gupta",
                "Xinyue Li",
                "Mark Billinghurst",
                "Simon Hoermann",
                "Dwain Allan",
                "Alaeddin Nassani",
                "Thammathip Piumsomboon"
            ],
            "title": "Large Language Models for Automatic Detection of Sensitive Topics",
            "abstract": "Sensitive information detection is crucial in content moderation to maintain safe online communities. Assisting in this traditionally manual process could relieve human moderators from overwhelming and tedious tasks, allowing them to focus solely on flagged content that may pose potential risks. Rapidly advancing large language models (LLMs) are known for their capability to understand and process natural language and so present a potential solution to support this process. This study explores the capabilities of five LLMs for detecting sensitive messages in the mental well-being domain within two online datasets and assesses their performance in terms of accuracy, precision, recall, F1 scores, and consistency. Our findings indicate that LLMs have the potential to be integrated into the moderation workflow as a convenient and precise detection tool. The best-performing model, GPT-4o, achieved an average accuracy of 99.5\\% and an F1-score of 0.99. We discuss the advantages and potential challenges of using LLMs in the moderation workflow and suggest that future research should address the ethical considerations of utilising this technology.",
            "id": "2409.00940",
            "link": "http://arxiv.org/abs/2409.00940v1",
            "published": "2024-09-02T04:50:42+00:00",
            "updated": "2024-09-02T04:50:42+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "J.6"
            ],
            "max_author_hindex": 83
        },
        "2409.01053": {
            "authors": [
                "Imke van Heerden",
                "Anil Bas"
            ],
            "title": "A Perspective on Literary Metaphor in the Context of Generative AI",
            "abstract": "At the intersection of creative text generation and literary theory, this study explores the role of literary metaphor and its capacity to generate a range of meanings. In this regard, literary metaphor is vital to the development of any particular language. To investigate whether the inclusion of original figurative language improves textual quality, we trained an LSTM-based language model in Afrikaans. The network produces phrases containing compellingly novel figures of speech. Specifically, the emphasis falls on how AI might be utilised as a defamiliarisation technique, which disrupts expected uses of language to augment poetic expression. Providing a literary perspective on text generation, the paper raises thought-provoking questions on aesthetic value, interpretation and evaluation.",
            "id": "2409.01053",
            "link": "http://arxiv.org/abs/2409.01053v1",
            "published": "2024-09-02T08:27:29+00:00",
            "updated": "2024-09-02T08:27:29+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 2
        },
        "2409.01087": {
            "authors": [
                "Muhammad Umair",
                "Tangina Sultana",
                "Young-Koo Lee"
            ],
            "title": "Pre-Trained Language Models for Keyphrase Prediction: A Review",
            "abstract": "Keyphrase Prediction (KP) is essential for identifying keyphrases in a document that can summarize its content. However, recent Natural Language Processing (NLP) advances have developed more efficient KP models using deep learning techniques. The limitation of a comprehensive exploration jointly both keyphrase extraction and generation using pre-trained language models spotlights a critical gap in the literature, compelling our survey paper to bridge this deficiency and offer a unified and in-depth analysis to address limitations in previous surveys. This paper extensively examines the topic of pre-trained language models for keyphrase prediction (PLM-KP), which are trained on large text corpora via different learning (supervisor, unsupervised, semi-supervised, and self-supervised) techniques, to provide respective insights into these two types of tasks in NLP, precisely, Keyphrase Extraction (KPE) and Keyphrase Generation (KPG). We introduce appropriate taxonomies for PLM-KPE and KPG to highlight these two main tasks of NLP. Moreover, we point out some promising future directions for predicting keyphrases.",
            "id": "2409.01087",
            "link": "http://arxiv.org/abs/2409.01087v1",
            "published": "2024-09-02T09:15:44+00:00",
            "updated": "2024-09-02T09:15:44+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 41
        },
        "2409.01366": {
            "authors": [
                "Junhui He",
                "Shangyu Wu",
                "Weidong Wen",
                "Chun Jason Xue",
                "Qingan Li"
            ],
            "title": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification",
            "abstract": "Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not explicitly model the impact of activation sparsification on performance, leading to suboptimal performance degradation. To address this issue, this paper reformulates the activation sparsification problem by introducing a new objective that optimizes the sparsification decisions. Building on this reformulation, we propose CHESS, a general activation sparsification approach via CHannel-wise thrEsholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters compared to existing methods, thus speeding up the LLM inference by up to 1.27x.",
            "id": "2409.01366",
            "link": "http://arxiv.org/abs/2409.01366v1",
            "published": "2024-09-02T16:41:44+00:00",
            "updated": "2024-09-02T16:41:44+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 57
        },
        "2409.01392": {
            "authors": [
                "Xiangyuan Xue",
                "Zeyu Lu",
                "Di Huang",
                "Wanli Ouyang",
                "Lei Bai"
            ],
            "title": "GenAgent: Build Collaborative AI Systems with Automated Workflow Generation -- Case Studies on ComfyUI",
            "abstract": "Much previous AI research has focused on developing monolithic models to maximize their intelligence and capability, with the primary goal of enhancing performance on specific tasks. In contrast, this paper explores an alternative approach: collaborative AI systems that use workflows to integrate models, data sources, and pipelines to solve complex and diverse tasks. We introduce GenAgent, an LLM-based framework that automatically generates complex workflows, offering greater flexibility and scalability compared to monolithic models. The core innovation of GenAgent lies in representing workflows with code, alongside constructing workflows with collaborative agents in a step-by-step manner. We implement GenAgent on the ComfyUI platform and propose a new benchmark, OpenComfy. The results demonstrate that GenAgent outperforms baseline approaches in both run-level and task-level evaluations, showing its capability to generate complex workflows with superior effectiveness and stability.",
            "id": "2409.01392",
            "link": "http://arxiv.org/abs/2409.01392v1",
            "published": "2024-09-02T17:44:10+00:00",
            "updated": "2024-09-02T17:44:10+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 87
        },
        "2409.01466": {
            "authors": [
                "Menglin Liu",
                "Ge Shi"
            ],
            "title": "PoliPrompt: A High-Performance Cost-Effective LLM-Based Text Classification Framework for Political Science",
            "abstract": "Recent advancements in large language models (LLMs) have opened new avenues for enhancing text classification efficiency in political science, surpassing traditional machine learning methods that often require extensive feature engineering, human labeling, and task-specific training. However, their effectiveness in achieving high classification accuracy remains questionable. This paper introduces a three-stage in-context learning approach that leverages LLMs to improve classification accuracy while minimizing experimental costs. Our method incorporates automatic enhanced prompt generation, adaptive exemplar selection, and a consensus mechanism that resolves discrepancies between two weaker LLMs, refined by an advanced LLM. We validate our approach using datasets from the BBC news reports, Kavanaugh Supreme Court confirmation, and 2018 election campaign ads. The results show significant improvements in classification F1 score (+0.36 for zero-shot classification) with manageable economic costs (-78% compared with human labeling), demonstrating that our method effectively addresses the limitations of traditional machine learning while offering a scalable and reliable solution for text analysis in political science.",
            "id": "2409.01466",
            "link": "http://arxiv.org/abs/2409.01466v1",
            "published": "2024-09-02T21:05:31+00:00",
            "updated": "2024-09-02T21:05:31+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 13
        },
        "2409.01586": {
            "authors": [
                "Tiansheng Huang",
                "Sihao Hu",
                "Fatih Ilhan",
                "Selim Furkan Tekin",
                "Ling Liu"
            ],
            "title": "Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation",
            "abstract": "Harmful fine-tuning issue \\citep{qi2023fine} poses serious safety concerns for Large language models' fine-tuning-as-a-service. While existing defenses \\citep{huang2024vaccine,rosati2024representation} have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. For the first time in the literature, we in this paper show that \\textit{harmful perturbation} over the model weights should be the root cause of alignment-broken of harmful fine-tuning. In order to attenuate the negative impact of harmful perturbation, we propose an alignment-stage solution, dubbed Booster. Technically, along with the original alignment loss, we append a loss regularizer in the alignment stage's optimization. The regularizer ensures that the model's harmful loss reduction before/after simulated harmful perturbation is attenuated, thereby mitigating the subsequent fine-tuning risk. Empirical results show that Booster can effectively reduce the harmful score of the fine-tuned models while maintaining the performance of downstream tasks. Our code is available at \\url{https://github.com/git-disl/Booster}.",
            "id": "2409.01586",
            "link": "http://arxiv.org/abs/2409.01586v2",
            "published": "2024-09-03T03:59:22+00:00",
            "updated": "2024-09-04T19:30:59+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 39
        },
        "2409.02060": {
            "authors": [
                "Niklas Muennighoff",
                "Luca Soldaini",
                "Dirk Groeneveld",
                "Kyle Lo",
                "Jacob Morrison",
                "Sewon Min",
                "Weijia Shi",
                "Pete Walsh",
                "Oyvind Tafjord",
                "Nathan Lambert",
                "Yuling Gu",
                "Shane Arora",
                "Akshita Bhagia",
                "Dustin Schwenk",
                "David Wadden",
                "Alexander Wettig",
                "Binyuan Hui",
                "Tim Dettmers",
                "Douwe Kiela",
                "Ali Farhadi",
                "Noah A. Smith",
                "Pang Wei Koh",
                "Amanpreet Singh",
                "Hannaneh Hajishirzi"
            ],
            "title": "OLMoE: Open Mixture-of-Experts Language Models",
            "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.",
            "id": "2409.02060",
            "link": "http://arxiv.org/abs/2409.02060v1",
            "published": "2024-09-03T17:08:20+00:00",
            "updated": "2024-09-03T17:08:20+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 107
        },
        "2409.02098": {
            "authors": [
                "Ingo Ziegler",
                "Abdullatif K\u00f6ksal",
                "Desmond Elliott",
                "Hinrich Sch\u00fctze"
            ],
            "title": "CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation",
            "abstract": "Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given the few-shot examples, we use large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents. Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning. We demonstrate that CRAFT can efficiently generate large-scale task-specific training datasets for four diverse tasks: biology question-answering (QA), medicine QA and commonsense QA as well as summarization. Our experiments show that CRAFT-based models outperform or achieve comparable performance to general LLMs for QA tasks, while CRAFT-based summarization models outperform models trained on human-curated data by 46 preference points.",
            "id": "2409.02098",
            "link": "http://arxiv.org/abs/2409.02098v1",
            "published": "2024-09-03T17:54:40+00:00",
            "updated": "2024-09-03T17:54:40+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 65
        },
        "2409.02449": {
            "authors": [
                "Kavya Manohar",
                "Leena G Pillai"
            ],
            "title": "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations",
            "abstract": "This paper explores the pitfalls in evaluating multilingual automatic speech recognition (ASR) models, with a particular focus on Indic language scripts. We investigate the text normalization routine employed by leading ASR models, including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer, and their unintended consequences on performance metrics. Our research reveals that current text normalization practices, while aiming to standardize ASR outputs for fair comparison, by removing inconsistencies such as variations in spelling, punctuation, and special characters, are fundamentally flawed when applied to Indic scripts. Through empirical analysis using text similarity scores and in-depth linguistic examination, we demonstrate that these flaws lead to artificially inflated performance metrics for Indic languages. We conclude by proposing a shift towards developing normalization routines that leverage native linguistic expertise, ensuring more robust and accurate evaluations of multilingual ASR models.",
            "id": "2409.02449",
            "link": "http://arxiv.org/abs/2409.02449v1",
            "published": "2024-09-04T05:08:23+00:00",
            "updated": "2024-09-04T05:08:23+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.HC",
                "68T50, 91F20, 68T10",
                "I.2.1; I.2.7"
            ],
            "max_author_hindex": 2
        },
        "2409.02649": {
            "authors": [
                "W\u0142odzimierz Lewoniewski",
                "Piotr Stolarski",
                "Milena Str\u00f3\u017cyna",
                "Elzbieta Lewa\u0144ska",
                "Aleksandra Wojewoda",
                "Ewelina Ksi\u0119\u017cniak",
                "Marcin Sawi\u0144ski"
            ],
            "title": "OpenFact at CheckThat! 2024: Combining Multiple Attack Methods for Effective Adversarial Text Generation",
            "abstract": "This paper presents the experiments and results for the CheckThat! Lab at CLEF 2024 Task 6: Robustness of Credibility Assessment with Adversarial Examples (InCrediblAE). The primary objective of this task was to generate adversarial examples in five problem domains in order to evaluate the robustness of widely used text classification methods (fine-tuned BERT, BiLSTM, and RoBERTa) when applied to credibility assessment issues.   This study explores the application of ensemble learning to enhance adversarial attacks on natural language processing (NLP) models. We systematically tested and refined several adversarial attack methods, including BERT-Attack, Genetic algorithms, TextFooler, and CLARE, on five datasets across various misinformation tasks. By developing modified versions of BERT-Attack and hybrid methods, we achieved significant improvements in attack effectiveness. Our results demonstrate the potential of modification and combining multiple methods to create more sophisticated and effective adversarial attack strategies, contributing to the development of more robust and secure systems.",
            "id": "2409.02649",
            "link": "http://arxiv.org/abs/2409.02649v2",
            "published": "2024-09-04T12:26:26+00:00",
            "updated": "2024-09-05T06:20:36+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 10
        },
        "2409.02836": {
            "authors": [
                "Moein Shahiki Tash",
                "Zahra Ahani",
                "Mohim Tash",
                "Olga Kolesnikova",
                "Grigori Sidorov"
            ],
            "title": "Exploring Sentiment Dynamics and Predictive Behaviors in Cryptocurrency Discussions by Few-Shot Learning with Large Language Models",
            "abstract": "This study performs analysis of Predictive statements, Hope speech, and Regret Detection behaviors within cryptocurrency-related discussions, leveraging advanced natural language processing techniques. We introduce a novel classification scheme named \"Prediction statements,\" categorizing comments into Predictive Incremental, Predictive Decremental, Predictive Neutral, or Non-Predictive categories. Employing GPT-4o, a cutting-edge large language model, we explore sentiment dynamics across five prominent cryptocurrencies: Cardano, Binance, Matic, Fantom, and Ripple. Our analysis reveals distinct patterns in predictive sentiments, with Matic demonstrating a notably higher propensity for optimistic predictions. Additionally, we investigate hope and regret sentiments, uncovering nuanced interplay between these emotions and predictive behaviors. Despite encountering limitations related to data volume and resource availability, our study reports valuable discoveries concerning investor behavior and sentiment trends within the cryptocurrency market, informing strategic decision-making and future research endeavors.",
            "id": "2409.02836",
            "link": "http://arxiv.org/abs/2409.02836v1",
            "published": "2024-09-04T16:02:30+00:00",
            "updated": "2024-09-04T16:02:30+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CE",
                "cs.LG"
            ],
            "max_author_hindex": 30
        },
        "2409.02889": {
            "authors": [
                "Xidong Wang",
                "Dingjie Song",
                "Shunian Chen",
                "Chen Zhang",
                "Benyou Wang"
            ],
            "title": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture",
            "abstract": "Expanding the long-context capabilities of Multi-modal Large Language Models~(MLLMs) is crucial for video understanding, high-resolution image understanding, and multi-modal agents. This involves a series of systematic optimizations, including model architecture, data construction and training strategy, particularly addressing challenges such as \\textit{degraded performance with more images} and \\textit{high computational costs}. In this paper, we adapt the model architecture to a hybrid of Mamba and Transformer blocks, approach data construction with both temporal and spatial dependencies among multiple images and employ a progressive training strategy. The released model \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge \\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first hybrid MLLM, which achieved a better balance between efficiency and effectiveness. LongLLaVA not only achieves competitive results across various benchmarks, but also maintains high throughput and low memory consumption. Especially, it could process nearly a thousand images on a single A100 80GB GPU, showing promising application prospects for a wide range of tasks.",
            "id": "2409.02889",
            "link": "http://arxiv.org/abs/2409.02889v1",
            "published": "2024-09-04T17:25:21+00:00",
            "updated": "2024-09-04T17:25:21+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CV",
                "cs.MM"
            ],
            "max_author_hindex": 37
        },
        "2409.03203": {
            "authors": [
                "Zhuowei Chen",
                "Lianxi Wang",
                "Yuben Wu",
                "Xinfeng Liao",
                "Yujia Tian",
                "Junyang Zhong"
            ],
            "title": "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification",
            "abstract": "Sentiment classification (SC) often suffers from low-resource challenges such as domain-specific contexts, imbalanced label distributions, and few-shot scenarios. The potential of the diffusion language model (LM) for textual data augmentation (DA) remains unexplored, moreover, textual DA methods struggle to balance the diversity and consistency of new samples. Most DA methods either perform logical modifications or rephrase less important tokens in the original sequence with the language model. In the context of SC, strong emotional tokens could act critically on the sentiment of the whole sequence. Therefore, contrary to rephrasing less important context, we propose DiffusionCLS to leverage a diffusion LM to capture in-domain knowledge and generate pseudo samples by reconstructing strong label-related tokens. This approach ensures a balance between consistency and diversity, avoiding the introduction of noise and augmenting crucial features of datasets. DiffusionCLS also comprises a Noise-Resistant Training objective to help the model generalize. Experiments demonstrate the effectiveness of our method in various low-resource scenarios including domain-specific and domain-general problems. Ablation studies confirm the effectiveness of our framework's modules, and visualization studies highlight optimal deployment conditions, reinforcing our conclusions.",
            "id": "2409.03203",
            "link": "http://arxiv.org/abs/2409.03203v1",
            "published": "2024-09-05T02:51:28+00:00",
            "updated": "2024-09-05T02:51:28+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 8
        },
        "2409.03256": {
            "authors": [
                "Hanlin Wang",
                "Chak Tou Leong",
                "Jian Wang",
                "Wenjie Li"
            ],
            "title": "E2CL: Exploration-based Error Correction Learning for Embodied Agents",
            "abstract": "Language models are exhibiting increasing capability in knowledge utilization and reasoning. However, when applied as agents in embodied environments, they often suffer from misalignment between their intrinsic knowledge and environmental knowledge, leading to infeasible actions. Traditional environment alignment methods, such as supervised learning on expert trajectories and reinforcement learning, face limitations in covering environmental knowledge and achieving efficient convergence, respectively. Inspired by human learning, we propose Exploration-based Error Correction Learning (E2CL), a novel framework that leverages exploration-induced errors and environmental feedback to enhance environment alignment for LM-based agents. E2CL incorporates teacher-guided and teacher-free exploration to gather environmental feedback and correct erroneous actions. The agent learns to provide feedback and self-correct, thereby enhancing its adaptability to target environments. Evaluations in the Virtualhome environment demonstrate that E2CL-trained agents outperform those trained by baseline methods and exhibit superior self-correction capabilities.",
            "id": "2409.03256",
            "link": "http://arxiv.org/abs/2409.03256v1",
            "published": "2024-09-05T05:22:27+00:00",
            "updated": "2024-09-05T05:22:27+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 44
        },
        "2409.03257": {
            "authors": [
                "Chanjun Park",
                "Hyeonwoo Kim"
            ],
            "title": "Understanding LLM Development Through Longitudinal Study: Insights from the Open Ko-LLM Leaderboard",
            "abstract": "This paper conducts a longitudinal study over eleven months to address the limitations of prior research on the Open Ko-LLM Leaderboard, which have relied on empirical studies with restricted observation periods of only five months. By extending the analysis duration, we aim to provide a more comprehensive understanding of the progression in developing Korean large language models (LLMs). Our study is guided by three primary research questions: (1) What are the specific challenges in improving LLM performance across diverse tasks on the Open Ko-LLM Leaderboard over time? (2) How does model size impact task performance correlations across various benchmarks? (3) How have the patterns in leaderboard rankings shifted over time on the Open Ko-LLM Leaderboard?. By analyzing 1,769 models over this period, our research offers a comprehensive examination of the ongoing advancements in LLMs and the evolving nature of evaluation frameworks.",
            "id": "2409.03257",
            "link": "http://arxiv.org/abs/2409.03257v1",
            "published": "2024-09-05T05:31:29+00:00",
            "updated": "2024-09-05T05:31:29+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 48
        },
        "2409.03295": {
            "authors": [
                "DongNyeong Heo",
                "Daniela Noemi Rim",
                "Heeyoul Choi"
            ],
            "title": "N-gram Prediction and Word Difference Representations for Language Modeling",
            "abstract": "Causal language modeling (CLM) serves as the foundational framework underpinning remarkable successes of recent large language models (LLMs). Despite its success, the training approach for next word prediction poses a potential risk of causing the model to overly focus on local dependencies within a sentence. While prior studies have been introduced to predict future N words simultaneously, they were primarily applied to tasks such as masked language modeling (MLM) and neural machine translation (NMT). In this study, we introduce a simple N-gram prediction framework for the CLM task. Moreover, we introduce word difference representation (WDR) as a surrogate and contextualized target representation during model training on the basis of N-gram prediction framework. To further enhance the quality of next word prediction, we propose an ensemble method that incorporates the future N words' prediction results. Empirical evaluations across multiple benchmark datasets encompassing CLM and NMT tasks demonstrate the significant advantages of our proposed methods over the conventional CLM.",
            "id": "2409.03295",
            "link": "http://arxiv.org/abs/2409.03295v1",
            "published": "2024-09-05T07:03:23+00:00",
            "updated": "2024-09-05T07:03:23+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 16
        },
        "2409.03346": {
            "authors": [
                "Xin Jiang",
                "Xiang Li",
                "Wenjia Ma",
                "Xuezhi Fang",
                "Yiqun Yao",
                "Naitong Yu",
                "Xuying Meng",
                "Peng Han",
                "Jing Li",
                "Aixin Sun",
                "Yequan Wang"
            ],
            "title": "Sketch: A Toolkit for Streamlining LLM Operations",
            "abstract": "Large language models (LLMs) represented by GPT family have achieved remarkable success. The characteristics of LLMs lie in their ability to accommodate a wide range of tasks through a generative approach. However, the flexibility of their output format poses challenges in controlling and harnessing the model's outputs, thereby constraining the application of LLMs in various domains. In this work, we present Sketch, an innovative toolkit designed to streamline LLM operations across diverse fields. Sketch comprises the following components: (1) a suite of task description schemas and prompt templates encompassing various NLP tasks; (2) a user-friendly, interactive process for building structured output LLM services tailored to various NLP tasks; (3) an open-source dataset for output format control, along with tools for dataset construction; and (4) an open-source model based on LLaMA3-8B-Instruct that adeptly comprehends and adheres to output formatting instructions. We anticipate this initiative to bring considerable convenience to LLM users, achieving the goal of ''plug-and-play'' for various applications. The components of Sketch will be progressively open-sourced at https://github.com/cofe-ai/Sketch.",
            "id": "2409.03346",
            "link": "http://arxiv.org/abs/2409.03346v1",
            "published": "2024-09-05T08:45:44+00:00",
            "updated": "2024-09-05T08:45:44+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 69
        },
        "2409.04318": {
            "authors": [
                "Aliakbar Nafar",
                "Kristen Brent Venable",
                "Parisa Kordjamshidi"
            ],
            "title": "Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs",
            "abstract": "Generative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. First, we show that LLMs can perform regression on real-world datasets and then design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples. We argue that this process lies on a spectrum between these two extremes. We provide an in-depth analysis of the degrees to which these mechanisms are triggered depending on various factors, such as prior knowledge about the tasks and the type and richness of the information provided by the in-context examples. We employ three LLMs and utilize multiple datasets to corroborate the robustness of our findings. Our results shed light on how to engineer prompts to leverage meta-learning from in-context examples and foster knowledge retrieval depending on the problem being addressed.",
            "id": "2409.04318",
            "link": "http://arxiv.org/abs/2409.04318v1",
            "published": "2024-09-06T14:46:37+00:00",
            "updated": "2024-09-06T14:46:37+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ],
            "max_author_hindex": 29
        },
        "2409.04795": {
            "authors": [
                "Haddad Philip",
                "Tsegaye Misikir Tashu"
            ],
            "title": "Phrase-Level Adversarial Training for Mitigating Bias in Neural Network-based Automatic Essay Scoring",
            "abstract": "Automatic Essay Scoring (AES) is widely used to evaluate candidates for educational purposes. However, due to the lack of representative data, most existing AES systems are not robust, and their scoring predictions are biased towards the most represented data samples. In this study, we propose a model-agnostic phrase-level method to generate an adversarial essay set to address the biases and robustness of AES models. Specifically, we construct an attack test set comprising samples from the original test set and adversarially generated samples using our proposed method. To evaluate the effectiveness of the attack strategy and data augmentation, we conducted a comprehensive analysis utilizing various neural network scoring models. Experimental results show that the proposed approach significantly improves AES model performance in the presence of adversarial examples and scenarios without such attacks.",
            "id": "2409.04795",
            "link": "http://arxiv.org/abs/2409.04795v1",
            "published": "2024-09-07T11:22:35+00:00",
            "updated": "2024-09-07T11:22:35+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 7
        },
        "2409.04822": {
            "authors": [
                "George Kour",
                "Naama Zwerdling",
                "Marcel Zalmanovici",
                "Ateret Anaby-Tavor",
                "Ora Nova Fandina",
                "Eitan Farchi"
            ],
            "title": "Exploring Straightforward Conversational Red-Teaming",
            "abstract": "Large language models (LLMs) are increasingly used in business dialogue systems but they pose security and ethical risks. Multi-turn conversations, where context influences the model's behavior, can be exploited to produce undesired responses. In this paper, we examine the effectiveness of utilizing off-the-shelf LLMs in straightforward red-teaming approaches, where an attacker LLM aims to elicit undesired output from a target LLM, comparing both single-turn and conversational red-teaming tactics. Our experiments offer insights into various usage strategies that significantly affect their performance as red teamers. They suggest that off-the-shelf models can act as effective red teamers and even adjust their attack strategy based on past attempts, although their effectiveness decreases with greater alignment.",
            "id": "2409.04822",
            "link": "http://arxiv.org/abs/2409.04822v1",
            "published": "2024-09-07T13:28:01+00:00",
            "updated": "2024-09-07T13:28:01+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.05105": {
            "authors": [
                "Lei Sheng",
                "Shuai-Shuai Xu"
            ],
            "title": "EdaCSC: Two Easy Data Augmentation Methods for Chinese Spelling Correction",
            "abstract": "Chinese Spelling Correction (CSC) aims to detect and correct spelling errors in Chinese sentences caused by phonetic or visual similarities. While current CSC models integrate pinyin or glyph features and have shown significant progress,they still face challenges when dealing with sentences containing multiple typos and are susceptible to overcorrection in real-world scenarios. In contrast to existing model-centric approaches, we propose two data augmentation methods to address these limitations. Firstly, we augment the dataset by either splitting long sentences into shorter ones or reducing typos in sentences with multiple typos. Subsequently, we employ different training processes to select the optimal model. Experimental evaluations on the SIGHAN benchmarks demonstrate the superiority of our approach over most existing models, achieving state-of-the-art performance on the SIGHAN15 test set.",
            "id": "2409.05105",
            "link": "http://arxiv.org/abs/2409.05105v1",
            "published": "2024-09-08T14:29:10+00:00",
            "updated": "2024-09-08T14:29:10+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 37
        },
        "2409.04244": {
            "authors": [
                "Chengxi Pan",
                "Junshang Chen",
                "Jingrui Ye"
            ],
            "title": "WarpAdam: A new Adam optimizer based on Meta-Learning approach",
            "abstract": "Optimal selection of optimization algorithms is crucial for training deep learning models. The Adam optimizer has gained significant attention due to its efficiency and wide applicability. However, to enhance the adaptability of optimizers across diverse datasets, we propose an innovative optimization strategy by integrating the 'warped gradient descend'concept from Meta Learning into the Adam optimizer. In the conventional Adam optimizer, gradients are utilized to compute estimates of gradient mean and variance, subsequently updating model parameters. Our approach introduces a learnable distortion matrix, denoted as P, which is employed for linearly transforming gradients. This transformation slightly adjusts gradients during each iteration, enabling the optimizer to better adapt to distinct dataset characteristics. By learning an appropriate distortion matrix P, our method aims to adaptively adjust gradient information across different data distributions, thereby enhancing optimization performance. Our research showcases the potential of this novel approach through theoretical insights and empirical evaluations. Experimental results across various tasks and datasets validate the superiority of our optimizer that integrates the 'warped gradient descend' concept in terms of adaptability. Furthermore, we explore effective strategies for training the adaptation matrix P and identify scenarios where this method can yield optimal results. In summary, this study introduces an innovative approach that merges the 'warped gradient descend' concept from Meta Learning with the Adam optimizer. By introducing a learnable distortion matrix P within the optimizer, we aim to enhance the model's generalization capability across diverse data distributions, thus opening up new possibilities in the field of deep learning optimization.",
            "id": "2409.04244",
            "link": "http://arxiv.org/abs/2409.04244v1",
            "published": "2024-09-06T12:51:10+00:00",
            "updated": "2024-09-06T12:51:10+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.IR"
            ],
            "max_author_hindex": 25
        },
        "2409.01344": {
            "authors": [
                "K Roth",
                "Rushil Gupta",
                "Simon Halle",
                "Bang Liu"
            ],
            "title": "Pairing Analogy-Augmented Generation with Procedural Memory for Procedural Q&A",
            "abstract": "While LLMs in the RAG paradigm have shown remarkable performance on a variety of tasks, they still under-perform on unseen domains, especially on complex tasks like procedural question answering. In this work, we introduce a novel formalism and structure for manipulating text-based procedures. Based on this formalism, we further present a novel dataset called LCStep, scraped from the LangChain Python docs. Moreover, we extend the traditional RAG system to propose a novel system called analogy-augmented generation (AAG), that draws inspiration from human analogical reasoning and ability to assimilate past experiences to solve unseen problems. The proposed method uses a frozen language model with a custom procedure memory store to adapt to specialized knowledge. We demonstrate that AAG outperforms few-shot and RAG baselines on LCStep, RecipeNLG, and CHAMP datasets under a pairwise LLM-based evaluation, corroborated by human evaluation in the case of RecipeNLG.",
            "id": "2409.01344",
            "link": "http://arxiv.org/abs/2409.01344v1",
            "published": "2024-09-02T15:58:24+00:00",
            "updated": "2024-09-02T15:58:24+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 28
        },
        "2409.01806": {
            "authors": [
                "Haoming Li",
                "Zhaoliang Chen",
                "Jonathan Zhang",
                "Fei Liu"
            ],
            "title": "LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning",
            "abstract": "Effective planning is essential for the success of any task, from organizing a vacation to routing autonomous vehicles and developing corporate strategies. It involves setting goals, formulating plans, and allocating resources to achieve them. LLMs are particularly well-suited for automated planning due to their strong capabilities in commonsense reasoning. They can deduce a sequence of actions needed to achieve a goal from a given state and identify an effective course of action. However, it is frequently observed that plans generated through direct prompting often fail upon execution. Our survey aims to highlight the existing challenges in planning with language models, focusing on key areas such as embodied environments, optimal scheduling, competitive and cooperative games, task decomposition, reasoning, and planning. Through this study, we explore how LLMs transform AI planning and provide unique insights into the future of LM-assisted planning.",
            "id": "2409.01806",
            "link": "http://arxiv.org/abs/2409.01806v1",
            "published": "2024-09-03T11:39:52+00:00",
            "updated": "2024-09-03T11:39:52+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 22
        },
        "2409.02387": {
            "authors": [
                "Qian Niu",
                "Junyu Liu",
                "Ziqian Bi",
                "Pohsun Feng",
                "Benji Peng",
                "Keyu Chen",
                "Ming Li"
            ],
            "title": "Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges",
            "abstract": "This comprehensive review explores the intersection of Large Language Models (LLMs) and cognitive science, examining similarities and differences between LLMs and human cognitive processes. We analyze methods for evaluating LLMs cognitive abilities and discuss their potential as cognitive models. The review covers applications of LLMs in various cognitive fields, highlighting insights gained for cognitive science research. We assess cognitive biases and limitations of LLMs, along with proposed methods for improving their performance. The integration of LLMs with cognitive architectures is examined, revealing promising avenues for enhancing artificial intelligence (AI) capabilities. Key challenges and future research directions are identified, emphasizing the need for continued refinement of LLMs to better align with human cognition. This review provides a balanced perspective on the current state and future potential of LLMs in advancing our understanding of both artificial and human intelligence.",
            "id": "2409.02387",
            "link": "http://arxiv.org/abs/2409.02387v3",
            "published": "2024-09-04T02:30:12+00:00",
            "updated": "2024-09-12T14:56:35+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 22
        },
        "2409.03277": {
            "authors": [
                "Zhengzhuo Xu",
                "Bowen Qu",
                "Yiyan Qi",
                "Sinan Du",
                "Chengjin Xu",
                "Chun Yuan",
                "Jian Guo"
            ],
            "title": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding",
            "abstract": "Automatic chart understanding is crucial for content comprehension and document parsing. Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in chart understanding through domain-specific alignment and fine-tuning. However, the application of alignment training within the chart domain is still underexplored. To address this, we propose ChartMoE, which employs the mixture of expert (MoE) architecture to replace the traditional linear projector to bridge the modality gap. Specifically, we train multiple linear connectors through distinct alignment tasks, which are utilized as the foundational initialization parameters for different experts. Additionally, we introduce ChartMoE-Align, a dataset with over 900K chart-table-JSON-code quadruples to conduct three alignment tasks (chart-table/JSON/code). Combined with the vanilla connector, we initialize different experts in four distinct ways and adopt high-quality knowledge learning to further refine the MoE connector and LLM parameters. Extensive experiments demonstrate the effectiveness of the MoE connector and our initialization strategy, e.g., ChartMoE improves the accuracy of the previous state-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.",
            "id": "2409.03277",
            "link": "http://arxiv.org/abs/2409.03277v1",
            "published": "2024-09-05T06:41:02+00:00",
            "updated": "2024-09-05T06:41:02+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.CV"
            ],
            "max_author_hindex": 24
        },
        "2409.04267": {
            "authors": [
                "Haolong Chen",
                "Hanzhi Chen",
                "Zijian Zhao",
                "Kaifeng Han",
                "Guangxu Zhu",
                "Yichen Zhao",
                "Ying Du",
                "Wei Xu",
                "Qingjiang Shi"
            ],
            "title": "An overview of domain-specific foundation model: key technologies, applications and challenges",
            "abstract": "The impressive performance of ChatGPT and other foundation-model-based products in human language understanding has prompted both academia and industry to explore how these models can be tailored for specific industries and application scenarios. This process, known as the customization of domain-specific foundation models, addresses the limitations of general-purpose models, which may not fully capture the unique patterns and requirements of domain-specific data. Despite its importance, there is a notable lack of comprehensive overview papers on building domain-specific foundation models, while numerous resources exist for general-purpose models. To bridge this gap, this article provides a timely and thorough overview of the methodology for customizing domain-specific foundation models. It introduces basic concepts, outlines the general architecture, and surveys key methods for constructing domain-specific models. Furthermore, the article discusses various domains that can benefit from these specialized models and highlights the challenges ahead. Through this overview, we aim to offer valuable guidance and reference for researchers and practitioners from diverse fields to develop their own customized foundation models.",
            "id": "2409.04267",
            "link": "http://arxiv.org/abs/2409.04267v1",
            "published": "2024-09-06T13:24:22+00:00",
            "updated": "2024-09-06T13:24:22+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 68
        },
        "2409.04286": {
            "authors": [
                "Desiree Heim",
                "Christian Jilek",
                "Adrian Ulges",
                "Andreas Dengel"
            ],
            "title": "Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets",
            "abstract": "Current publicly available knowledge work data collections lack diversity, extensive annotations, and contextual information about the users and their documents. These issues hinder objective and comparable data-driven evaluations and optimizations of knowledge work assistance systems. Due to the considerable resources needed to collect such data in real-life settings and the necessity of data censorship, collecting such a dataset appears nearly impossible. For this reason, we propose a configurable, multi-agent knowledge work dataset generator. This system simulates collaborative knowledge work among agents producing Large Language Model-generated documents and accompanying data traces. Additionally, the generator captures all background information, given in its configuration or created during the simulation process, in a knowledge graph. Finally, the resulting dataset can be utilized and shared without privacy or confidentiality concerns.   This paper introduces our approach's design and vision and focuses on generating authentic knowledge work documents using Large Language Models. Our study involving human raters who assessed 53% of the generated and 74% of the real documents as realistic demonstrates the potential of our approach. Furthermore, we analyze the authenticity criteria mentioned in the participants' comments and elaborate on potential improvements for identified common issues.",
            "id": "2409.04286",
            "link": "http://arxiv.org/abs/2409.04286v1",
            "published": "2024-09-06T13:53:28+00:00",
            "updated": "2024-09-06T13:53:28+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 55
        },
        "2409.00544": {
            "authors": [
                "Jacqueline Lammert",
                "Nicole Pfarr",
                "Leonid Kuligin",
                "Sonja Mathes",
                "Tobias Dreyer",
                "Luise Modersohn",
                "Patrick Metzger",
                "Dyke Ferber",
                "Jakob Nikolas Kather",
                "Daniel Truhn",
                "Lisa Christine Adams",
                "Keno Kyrill Bressem",
                "Sebastian Lange",
                "Kristina Schwamborn",
                "Martin Boeker",
                "Marion Kiechle",
                "Ulrich A. Schatz",
                "Holger Bronger",
                "Maximilian Tschochohei"
            ],
            "title": "Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors",
            "abstract": "Rare gynecological tumors (RGTs) present major clinical challenges due to their low incidence and heterogeneity. The lack of clear guidelines leads to suboptimal management and poor prognosis. Molecular tumor boards accelerate access to effective therapies by tailoring treatment based on biomarkers, beyond cancer type. Unstructured data that requires manual curation hinders efficient use of biomarker profiling for therapy matching. This study explores the use of large language models (LLMs) to construct digital twins for precision medicine in RGTs.   Our proof-of-concept digital twin system integrates clinical and biomarker data from institutional and published cases (n=21) and literature-derived data (n=655 publications with n=404,265 patients) to create tailored treatment plans for metastatic uterine carcinosarcoma, identifying options potentially missed by traditional, single-source analysis. LLM-enabled digital twins efficiently model individual patient trajectories. Shifting to a biology-based rather than organ-based tumor definition enables personalized care that could advance RGT management and thus enhance patient outcomes.",
            "id": "2409.00544",
            "link": "http://arxiv.org/abs/2409.00544v1",
            "published": "2024-08-31T21:14:09+00:00",
            "updated": "2024-08-31T21:14:09+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "q-bio.QM",
                "stat.ML"
            ],
            "max_author_hindex": 47
        },
        "2409.00557": {
            "authors": [
                "Wenxuan Wang",
                "Juluan Shi",
                "Chaozheng Wang",
                "Cheryl Lee",
                "Youliang Yuan",
                "Jen-tse Huang",
                "Michael R. Lyu"
            ],
            "title": "Learning to Ask: When LLMs Meet Unclear Instruction",
            "abstract": "Equipped with the capability to call functions, modern large language models (LLMs) can leverage external tools for addressing a range of tasks unattainable through language skills alone. However, the effective execution of these tools relies heavily not just on the advanced capabilities of LLMs but also on precise user instructions, which often cannot be ensured in the real world. To evaluate the performance of LLMs tool-use under imperfect instructions, we meticulously examine the real-world instructions queried from users, analyze the error patterns, and build a challenging tool-use benchmark called Noisy ToolBench (NoisyToolBench). We find that due to the next-token prediction training objective, LLMs tend to arbitrarily generate the missed argument, which may lead to hallucinations and risks. To address this issue, we propose a novel framework, Ask-when-Needed (AwN), which prompts LLMs to ask questions to users whenever they encounter obstacles due to unclear instructions. Moreover, to reduce the manual labor involved in user-LLM interaction and assess LLMs performance in tool utilization from both accuracy and efficiency perspectives, we design an automated evaluation tool named ToolEvaluator. Our experiments demonstrate that the AwN significantly outperforms existing frameworks for tool learning in the NoisyToolBench. We will release all related code and datasets to support future research.",
            "id": "2409.00557",
            "link": "http://arxiv.org/abs/2409.00557v2",
            "published": "2024-08-31T23:06:12+00:00",
            "updated": "2024-09-04T20:34:27+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.SE"
            ],
            "max_author_hindex": 98
        },
        "2409.00787": {
            "authors": [
                "Bocheng Chen",
                "Hanqing Guo",
                "Guangjing Wang",
                "Yuanda Wang",
                "Qiben Yan"
            ],
            "title": "The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs",
            "abstract": "Large Language Models (LLMs) have demonstrated great capabilities in natural language understanding and generation, largely attributed to the intricate alignment process using human feedback. While alignment has become an essential training component that leverages data collected from user queries, it inadvertently opens up an avenue for a new type of user-guided poisoning attacks. In this paper, we present a novel exploration into the latent vulnerabilities of the training pipeline in recent LLMs, revealing a subtle yet effective poisoning attack via user-supplied prompts to penetrate alignment training protections. Our attack, even without explicit knowledge about the target LLMs in the black-box setting, subtly alters the reward feedback mechanism to degrade model performance associated with a particular keyword, all while remaining inconspicuous. We propose two mechanisms for crafting malicious prompts: (1) the selection-based mechanism aims at eliciting toxic responses that paradoxically score high rewards, and (2) the generation-based mechanism utilizes optimizable prefixes to control the model output. By injecting 1\\% of these specially crafted prompts into the data, through malicious users, we demonstrate a toxicity score up to two times higher when a specific trigger word is used. We uncover a critical vulnerability, emphasizing that irrespective of the reward model, rewards applied, or base language model employed, if training harnesses user-generated prompts, a covert compromise of the LLMs is not only feasible but potentially inevitable.",
            "id": "2409.00787",
            "link": "http://arxiv.org/abs/2409.00787v1",
            "published": "2024-09-01T17:40:04+00:00",
            "updated": "2024-09-01T17:40:04+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CR",
                "cs.LG"
            ],
            "max_author_hindex": 24
        },
        "2409.00861": {
            "authors": [
                "Derian Boer",
                "Fabian Koch",
                "Stefan Kramer"
            ],
            "title": "Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering",
            "abstract": "Large Language Models (LLMs) frequently lack domain-specific knowledge and even fine-tuned models tend to hallucinate. Hence, more reliable models that can include external knowledge are needed. We present a pipeline, 4StepFocus, and specifically a preprocessing step, that can substantially improve the answers of LLMs. This is achieved by providing guided access to external knowledge making use of the model's ability to capture relational context and conduct rudimentary reasoning by themselves. The method narrows down potentially correct answers by triplets-based searches in a semi-structured knowledge base in a direct, traceable fashion, before switching to latent representations for ranking those candidates based on unstructured data. This distinguishes it from related methods that are purely based on latent representations. 4StepFocus consists of the steps: 1) Triplet generation for extraction of relational data by an LLM, 2) substitution of variables in those triplets to narrow down answer candidates employing a knowledge graph, 3) sorting remaining candidates with a vector similarity search involving associated non-structured data, 4) reranking the best candidates by the LLM with background data provided. Experiments on a medical, a product recommendation, and an academic paper search test set demonstrate that this approach is indeed a powerful augmentation. It not only adds relevant traceable background information from information retrieval, but also improves performance considerably in comparison to state-of-the-art methods. This paper presents a novel, largely unexplored direction and therefore provides a wide range of future work opportunities. Used source code is available at https://github.com/kramerlab/4StepFocus.",
            "id": "2409.00861",
            "link": "http://arxiv.org/abs/2409.00861v1",
            "published": "2024-09-01T22:43:27+00:00",
            "updated": "2024-09-01T22:43:27+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "cs.LO"
            ],
            "max_author_hindex": 36
        },
        "2409.00887": {
            "authors": [
                "Atsushi Otsuka",
                "Kazuya Matsuo",
                "Ryo Ishii",
                "Narichika Nomoto",
                "Hiroaki Sugiyama"
            ],
            "title": "User-Specific Dialogue Generation with User Profile-Aware Pre-Training Model and Parameter-Efficient Fine-Tuning",
            "abstract": "This paper addresses user-specific dialogs. In contrast to previous research on personalized dialogue focused on achieving virtual user dialogue as defined by persona descriptions, user-specific dialogue aims to reproduce real-user dialogue beyond persona-based dialogue. Fine-tuning using the target user's dialogue history is an efficient learning method for a user-specific model. However, it is prone to overfitting and model destruction due to the small amount of data. Therefore, we propose a learning method for user-specific models by combining parameter-efficient fine-tuning with a pre-trained dialogue model that includes user profiles. Parameter-efficient fine-tuning adds a small number of parameters to the entire model, so even small amounts of training data can be trained efficiently and are robust to model destruction. In addition, the pre-trained model, which is learned by adding simple prompts for automatically inferred user profiles, can generate speech with enhanced knowledge of the user's profile, even when there is little training data during fine-tuning. In experiments, we compared the proposed model with large-language-model utterance generation using prompts containing users' personal information. Experiments reproducing real users' utterances revealed that the proposed model can generate utterances with higher reproducibility than the compared methods, even with a small model.",
            "id": "2409.00887",
            "link": "http://arxiv.org/abs/2409.00887v1",
            "published": "2024-09-02T01:30:40+00:00",
            "updated": "2024-09-02T01:30:40+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 45
        },
        "2409.01345": {
            "authors": [
                "Jiacan Yu",
                "Hannah An",
                "Lenhart K. Schubert"
            ],
            "title": "Language Models Benefit from Preparation with Elicited Knowledge",
            "abstract": "The zero-shot chain of thought (CoT) approach is often used in question answering (QA) by language models (LMs) for tasks that require multiple reasoning steps, typically enhanced by the prompt \"Let's think step by step.\" However, some QA tasks hinge more on accessing relevant knowledge than on chaining reasoning steps. We introduce a simple general prompting technique, called PREP, that involves using two instances of LMs: the first (LM1) generates relevant information, and the second (LM2) answers the question based on this information. PREP is designed to be general and independent of the user's domain knowledge, making it applicable across various QA tasks without the need for specialized prompt engineering. To evaluate the effectiveness of our prompting method, we create a dataset of 100 binary-choice questions, derived from an extensive schematic dataset on artifact parts and material composition. These questions ask which of two artifacts is less likely to share materials with another artifact. Such questions probe the LM's knowledge of shared materials in the part structure of different artifacts. We test our method on our dataset and three published commonsense reasoning datasets. The average accuracy of our method is consistently higher than that of all the other tested methods across all the tested datasets.",
            "id": "2409.01345",
            "link": "http://arxiv.org/abs/2409.01345v2",
            "published": "2024-09-02T15:58:27+00:00",
            "updated": "2024-09-06T03:35:21+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 6
        },
        "2409.01524": {
            "authors": [
                "Yuchen Yan",
                "Jin Jiang",
                "Yang Liu",
                "Yixin Cao",
                "Xin Xu",
                "Mengdi zhang",
                "Xunliang Cai",
                "Jian Shao"
            ],
            "title": "S$^3$c-Math: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners",
            "abstract": "Self-correction is a novel method that can stimulate the potential reasoning abilities of large language models (LLMs). It involves detecting and correcting errors during the inference process when LLMs solve reasoning problems. However, recent works do not regard self-correction as a spontaneous and intrinsic capability of LLMs. Instead, such correction is achieved through post-hoc generation, external knowledge introduction, multi-model collaboration, and similar techniques. In this paper, we propose a series of mathematical LLMs called S$^3$c-Math, which are able to perform Spontaneous Step-level Self-correction for Mathematical reasoning. This capability helps LLMs to recognize whether their ongoing inference tends to contain errors and simultaneously correct these errors to produce a more reliable response. We proposed a method, which employs a step-level sampling approach to construct step-wise self-correction data for achieving such ability. Additionally, we implement a training strategy that uses above constructed data to equip LLMs with spontaneous step-level self-correction capacities. Our data and methods have been demonstrated to be effective across various foundation LLMs, consistently showing significant progress in evaluations on GSM8K, MATH, and other mathematical benchmarks. To the best of our knowledge, we are the first to introduce the spontaneous step-level self-correction ability of LLMs in mathematical reasoning.",
            "id": "2409.01524",
            "link": "http://arxiv.org/abs/2409.01524v1",
            "published": "2024-09-03T01:40:21+00:00",
            "updated": "2024-09-03T01:40:21+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 55
        },
        "2409.01552": {
            "authors": [
                "Zhuo Li",
                "Yuhao Du",
                "Jinpeng Hu",
                "Xiang Wan",
                "Anningzhe Gao"
            ],
            "title": "Self-Instructed Derived Prompt Generation Meets In-Context Learning: Unlocking New Potential of Black-Box LLMs",
            "abstract": "Large language models (LLMs) have shown success in generating high-quality responses. In order to achieve better alignment with LLMs with human preference, various works are proposed based on specific optimization process, which, however, is not suitable to Black-Box LLMs like GPT-4, due to inaccessible parameters. In Black-Box LLMs case, their performance is highly dependent on the quality of the provided prompts. Existing methods to enhance response quality often involve a prompt refinement model, yet these approaches potentially suffer from semantic inconsistencies between the refined and original prompts, and typically overlook the relationship between them. To address these challenges, we introduce a self-instructed in-context learning framework that empowers LLMs to deliver more effective responses by generating reliable derived prompts to construct informative contextual environments. Our approach incorporates a self-instructed reinforcement learning mechanism, enabling direct interaction with the response model during derived prompt generation for better alignment. We then formulate querying as an in-context learning task, using responses from LLMs combined with the derived prompts to establish a contextual demonstration for the original prompt. This strategy ensures alignment with the original query, reduces discrepancies from refined prompts, and maximizes the LLMs' in-context learning capability. Extensive experiments demonstrate that the proposed method not only generates more reliable derived prompts but also significantly enhances LLMs' ability to deliver more effective responses, including Black-Box models such as GPT-4.",
            "id": "2409.01552",
            "link": "http://arxiv.org/abs/2409.01552v1",
            "published": "2024-09-03T02:42:39+00:00",
            "updated": "2024-09-03T02:42:39+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 32
        },
        "2409.01556": {
            "authors": [
                "Chen-Chi Chang",
                "Ching-Yuan Chen",
                "Hung-Shin Lee",
                "Chih-Cheng Lee"
            ],
            "title": "Benchmarking Cognitive Domains for LLMs: Insights from Taiwanese Hakka Culture",
            "abstract": "This study introduces a comprehensive benchmark designed to evaluate the performance of large language models (LLMs) in understanding and processing cultural knowledge, with a specific focus on Hakka culture as a case study. Leveraging Bloom's Taxonomy, the study develops a multi-dimensional framework that systematically assesses LLMs across six cognitive domains: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating. This benchmark extends beyond traditional single-dimensional evaluations by providing a deeper analysis of LLMs' abilities to handle culturally specific content, ranging from basic recall of facts to higher-order cognitive tasks such as creative synthesis. Additionally, the study integrates Retrieval-Augmented Generation (RAG) technology to address the challenges of minority cultural knowledge representation in LLMs, demonstrating how RAG enhances the models' performance by dynamically incorporating relevant external information. The results highlight the effectiveness of RAG in improving accuracy across all cognitive domains, particularly in tasks requiring precise retrieval and application of cultural knowledge. However, the findings also reveal the limitations of RAG in creative tasks, underscoring the need for further optimization. This benchmark provides a robust tool for evaluating and comparing LLMs in culturally diverse contexts, offering valuable insights for future research and development in AI-driven cultural knowledge preservation and dissemination.",
            "id": "2409.01556",
            "link": "http://arxiv.org/abs/2409.01556v1",
            "published": "2024-09-03T02:50:04+00:00",
            "updated": "2024-09-03T02:50:04+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 27
        },
        "2409.01579": {
            "authors": [
                "Qianchi Zhang",
                "Hainan Zhang",
                "Liang Pang",
                "Hongwei Zheng",
                "Zhiming Zheng"
            ],
            "title": "AdaComp: Extractive Context Compression with Adaptive Predictor for Retrieval-Augmented Large Language Models",
            "abstract": "Retrieved documents containing noise will hinder RAG from detecting answer clues and make the inference process slow and expensive. Therefore, context compression is necessary to enhance its accuracy and efficiency. Existing context compression methods use extractive or generative models to retain the most query-relevant sentences or apply the information bottleneck theory to preserve sufficient information. However, these methods may face issues such as over-compression or high computational costs. We observe that the retriever often ranks relevant documents at the top, but the exact number of documents needed to answer the query is uncertain due to the impact of query complexity and retrieval quality: complex queries like multi-hop questions may require retaining more documents than simpler queries, and a low-quality retrieval may need to rely on more documents to generate accurate outputs. Therefore, determining the minimum number of required documents (compression rate) is still a challenge for RAG. In this paper, we introduce AdaComp, a low-cost extractive context compression method that adaptively determines the compression rate based on both query complexity and retrieval quality. Specifically, we first annotate the minimum top-k documents necessary for the RAG system to answer the current query as the compression rate and then construct triplets of the query, retrieved documents, and its compression rate. Then, we use this triplet dataset to train a compression-rate predictor. Experiments on three QA datasets and one conversational Muiti-doc QA dataset show that AdaComp significantly reduces inference costs while maintaining performance nearly identical to uncompressed models, achieving a balance between efficiency and performance.",
            "id": "2409.01579",
            "link": "http://arxiv.org/abs/2409.01579v1",
            "published": "2024-09-03T03:25:59+00:00",
            "updated": "2024-09-03T03:25:59+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.01808": {
            "authors": [
                "Ike Ebubechukwu",
                "Johane Takeuchi",
                "Antonello Ceravola",
                "Frank Joublin"
            ],
            "title": "Dialogue You Can Trust: Human and AI Perspectives on Generated Conversations",
            "abstract": "As dialogue systems and chatbots increasingly integrate into everyday interactions, the need for efficient and accurate evaluation methods becomes paramount. This study explores the comparative performance of human and AI assessments across a range of dialogue scenarios, focusing on seven key performance indicators (KPIs): Coherence, Innovation, Concreteness, Goal Contribution, Commonsense Contradiction, Incorrect Fact, and Redundancy. Utilizing the GPT-4o API, we generated a diverse dataset of conversations and conducted a two-part experimental analysis. In Experiment 1, we evaluated multi-party conversations on Coherence, Innovation, Concreteness, and Goal Contribution, revealing that GPT models align closely with human judgments. Notably, both human and AI evaluators exhibited a tendency towards binary judgment rather than linear scaling, highlighting a shared challenge in these assessments. Experiment 2 extended the work of Finch et al. (2023) by focusing on dyadic dialogues and assessing Commonsense Contradiction, Incorrect Fact, and Redundancy. The results indicate that while GPT-4o demonstrates strong performance in maintaining factual accuracy and commonsense reasoning, it still struggles with reducing redundancy and self-contradiction. Our findings underscore the potential of GPT models to closely replicate human evaluation in dialogue systems, while also pointing to areas for improvement. This research offers valuable insights for advancing the development and implementation of more refined dialogue evaluation methodologies, contributing to the evolution of more effective and human-like AI communication tools.",
            "id": "2409.01808",
            "link": "http://arxiv.org/abs/2409.01808v2",
            "published": "2024-09-03T11:40:38+00:00",
            "updated": "2024-09-10T13:33:46+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.01893": {
            "authors": [
                "Zhi Chen",
                "Qiguang Chen",
                "Libo Qin",
                "Qipeng Guo",
                "Haijun Lv",
                "Yicheng Zou",
                "Wanxiang Che",
                "Hang Yan",
                "Kai Chen",
                "Dahua Lin"
            ],
            "title": "What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices",
            "abstract": "Recent advancements in large language models (LLMs) with extended context windows have significantly improved tasks such as information extraction, question answering, and complex planning scenarios. In order to achieve success in long context tasks, a large amount of work has been done to enhance the long context capabilities of the model through synthetic data. Existing methods typically utilize the Self-Instruct framework to generate instruction tuning data for better long context capability improvement. However, our preliminary experiments indicate that less than 35% of generated samples are multi-hop, and more than 40% exhibit poor quality, limiting comprehensive understanding and further research. To improve the quality of synthetic data, we propose the Multi-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a Quality Verification Agent, a Single-hop Question Generation Agent, a Multiple Question Sampling Strategy, and a Multi-hop Question Merger Agent. This framework improves the data quality, with the proportion of high-quality, multi-hop, and diverse data exceeding 85%. Furthermore, we systematically investigate strategies for document selection, question merging, and validation techniques through extensive experiments across various models. Our findings show that our synthetic high-quality long-context instruction data significantly enhances model performance, even surpassing models trained on larger amounts of human-annotated data. Our code is available at: https://github.com/WowCZ/LongMIT.",
            "id": "2409.01893",
            "link": "http://arxiv.org/abs/2409.01893v1",
            "published": "2024-09-03T13:30:00+00:00",
            "updated": "2024-09-03T13:30:00+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 74
        },
        "2409.02326": {
            "authors": [
                "Yuxiang Wei",
                "Hojae Han",
                "Rajhans Samdani"
            ],
            "title": "Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining",
            "abstract": "Recent studies have been increasingly demonstrating that high-quality data is crucial for effective pretraining of language models. However, the precise definition of \"high-quality\" remains underexplored. Focusing on the code domain, we introduce Arctic-SnowCoder-1.3B, a data-efficient base code model pretrained on 555B tokens through three phases of progressively refined data: (1) general pretraining with 500B standard-quality code tokens, preprocessed through basic filtering, deduplication, and decontamination, (2) continued pretraining with 50B high-quality tokens, selected from phase one by a BERT-style quality annotator trained to distinguish good code from random data, using positive examples drawn from high-quality code files, along with instruction data from Magicoder and StarCoder2-Instruct, and (3) enhanced pretraining with 5B synthetic data created by Llama-3.1-70B using phase two data as seeds, adapting the Magicoder approach for pretraining. Despite being trained on a limited dataset, Arctic-SnowCoder achieves state-of-the-art performance on BigCodeBench, a coding benchmark focusing on practical and challenging programming tasks, compared to similarly sized models trained on no more than 1T tokens, outperforming Phi-1.5-1.3B by 36%. Across all evaluated benchmarks, Arctic-SnowCoder-1.3B beats StarCoderBase-3B pretrained on 1T tokens. Additionally, it matches the performance of leading small base code models trained on trillions of tokens. For example, Arctic-SnowCoder-1.3B surpasses StarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a benchmark that evaluates function-level code generation, and remains competitive on BigCodeBench. Our evaluation presents a comprehensive analysis justifying various design choices for Arctic-SnowCoder. Most importantly, we find that the key to high-quality data is its alignment with the distribution of downstream applications.",
            "id": "2409.02326",
            "link": "http://arxiv.org/abs/2409.02326v1",
            "published": "2024-09-03T22:36:42+00:00",
            "updated": "2024-09-03T22:36:42+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 24
        },
        "2409.02413": {
            "authors": [
                "Hassan Shakil",
                "Ahmad Farooq",
                "Jugal Kalita"
            ],
            "title": "Abstractive Text Summarization: State of the Art, Challenges, and Improvements",
            "abstract": "Specifically focusing on the landscape of abstractive text summarization, as opposed to extractive techniques, this survey presents a comprehensive overview, delving into state-of-the-art techniques, prevailing challenges, and prospective research directions. We categorize the techniques into traditional sequence-to-sequence models, pre-trained large language models, reinforcement learning, hierarchical methods, and multi-modal summarization. Unlike prior works that did not examine complexities, scalability and comparisons of techniques in detail, this review takes a comprehensive approach encompassing state-of-the-art methods, challenges, solutions, comparisons, limitations and charts out future improvements - providing researchers an extensive overview to advance abstractive summarization research. We provide vital comparison tables across techniques categorized - offering insights into model complexity, scalability and appropriate applications. The paper highlights challenges such as inadequate meaning representation, factual consistency, controllable text summarization, cross-lingual summarization, and evaluation metrics, among others. Solutions leveraging knowledge incorporation and other innovative strategies are proposed to address these challenges. The paper concludes by highlighting emerging research areas like factual inconsistency, domain-specific, cross-lingual, multilingual, and long-document summarization, as well as handling noisy data. Our objective is to provide researchers and practitioners with a structured overview of the domain, enabling them to better understand the current landscape and identify potential areas for further research and improvement.",
            "id": "2409.02413",
            "link": "http://arxiv.org/abs/2409.02413v1",
            "published": "2024-09-04T03:39:23+00:00",
            "updated": "2024-09-04T03:39:23+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 42
        },
        "2409.02569": {
            "authors": [
                "Luca Santagata",
                "Cristiano De Nobili"
            ],
            "title": "More is More: Addition Bias in Large Language Models",
            "abstract": "In this paper, we investigate the presence of additive bias in Large Language Models (LLMs), drawing a parallel to the cognitive bias observed in humans where individuals tend to favor additive over subtractive changes. Using a series of controlled experiments, we tested various LLMs, including GPT-3.5 Turbo, Claude 3.5 Sonnet, Mistral, Math$\\Sigma$tral, and Llama 3.1, on tasks designed to measure their propensity for additive versus subtractive modifications. Our findings demonstrate a significant preference for additive changes across all tested models. For example, in a palindrome creation task, Llama 3.1 favored adding letters 97.85% of the time over removing them. Similarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick 76.38% of the time rather than remove one. In a text summarization task, Mistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked to improve its own or others' writing. These results indicate that, similar to humans, LLMs exhibit a marked additive bias, which might have implications when LLMs are used on a large scale. Addittive bias might increase resource use and environmental impact, leading to higher economic costs due to overconsumption and waste. This bias should be considered in the development and application of LLMs to ensure balanced and efficient problem-solving approaches.",
            "id": "2409.02569",
            "link": "http://arxiv.org/abs/2409.02569v1",
            "published": "2024-09-04T09:39:07+00:00",
            "updated": "2024-09-04T09:39:07+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.HC"
            ],
            "max_author_hindex": 3
        },
        "2409.02686": {
            "authors": [
                "Ruoyu Wang",
                "Xiaoxuan Li",
                "Lina Yao"
            ],
            "title": "Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable efficiency in tackling various tasks based on human instructions, but recent studies reveal that these models often fail to achieve satisfactory results on questions involving reasoning, such as mathematics or physics questions. This phenomenon is usually attributed to the uncertainty regarding whether these models could genuinely comprehend the knowledge embedded in the text or merely learn to replicate the token distribution without a true understanding of the content. In this paper, we delve into this problem and aim to enhance the reasoning capabilities of LLMs. First, we investigate if the model has genuine reasoning capabilities by visualizing the text generation process at the attention and representation level. Then, we formulate the reasoning process of LLMs into a causal framework, which provides a formal explanation of the problems we observe in the visualization. Finally, building upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient fine-tuning (PEFT) method to enhance the model's reasoning capabilities by encouraging the model to extract the general problem-solving skills and apply these skills to different questions. Experiments show that our method outperforms the baseline consistently across multiple benchmarks, and with only 1.2M tunable parameters, we achieve better or comparable results to other fine-tuning methods. This demonstrates the effectiveness and efficiency of our method in improving the overall accuracy and reliability of LLMs.",
            "id": "2409.02686",
            "link": "http://arxiv.org/abs/2409.02686v1",
            "published": "2024-09-04T13:17:09+00:00",
            "updated": "2024-09-04T13:17:09+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 45
        },
        "2409.02840": {
            "authors": [
                "Phuc-Tinh Pham Do",
                "Duy-Ngoc Dinh Cao",
                "Khanh Quoc Tran",
                "Kiet Van Nguyen"
            ],
            "title": "R2GQA: Retriever-Reader-Generator Question Answering System to Support Students Understanding Legal Regulations in Higher Education",
            "abstract": "In this article, we propose the R2GQA system, a Retriever-Reader-Generator Question Answering system, consisting of three main components: Document Retriever, Machine Reader, and Answer Generator. The Retriever module employs advanced information retrieval techniques to extract the context of articles from a dataset of legal regulation documents. The Machine Reader module utilizes state-of-the-art natural language understanding algorithms to comprehend the retrieved documents and extract answers. Finally, the Generator module synthesizes the extracted answers into concise and informative responses to questions of students regarding legal regulations. Furthermore, we built the ViRHE4QA dataset in the domain of university training regulations, comprising 9,758 question-answer pairs with a rigorous construction process. This is the first Vietnamese dataset in the higher regulations domain with various types of answers, both extractive and abstractive. In addition, the R2GQA system is the first system to offer abstractive answers in Vietnamese. This paper discusses the design and implementation of each module within the R2GQA system on the ViRHE4QA dataset, highlighting their functionalities and interactions. Furthermore, we present experimental results demonstrating the effectiveness and utility of the proposed system in supporting the comprehension of students of legal regulations in higher education settings. In general, the R2GQA system and the ViRHE4QA dataset promise to contribute significantly to related research and help students navigate complex legal documents and regulations, empowering them to make informed decisions and adhere to institutional policies effectively. Our dataset is available for research purposes.",
            "id": "2409.02840",
            "link": "http://arxiv.org/abs/2409.02840v1",
            "published": "2024-09-04T16:12:30+00:00",
            "updated": "2024-09-04T16:12:30+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 12
        },
        "2409.03155": {
            "authors": [
                "Jie Ma",
                "Zhitao Gao",
                "Qi Chai",
                "Wangchun Sun",
                "Pinghui Wang",
                "Hongbin Pei",
                "Jing Tao",
                "Lingyun Song",
                "Jun Liu",
                "Chen Zhang",
                "Lizhen Cui"
            ],
            "title": "Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models",
            "abstract": "Large Language Models (LLMs) may suffer from hallucinations in real-world applications due to the lack of relevant knowledge. In contrast, knowledge graphs encompass extensive, multi-relational structures that store a vast array of symbolic facts. Consequently, integrating LLMs with knowledge graphs has been extensively explored, with Knowledge Graph Question Answering (KGQA) serving as a critical touchstone for the integration. This task requires LLMs to answer natural language questions by retrieving relevant triples from knowledge graphs. However, existing methods face two significant challenges: \\textit{excessively long reasoning paths distracting from the answer generation}, and \\textit{false-positive relations hindering the path refinement}. In this paper, we propose an iterative interactive KGQA framework that leverages the interactive learning capabilities of LLMs to perform reasoning and Debating over Graphs (DoG). Specifically, DoG employs a subgraph-focusing mechanism, allowing LLMs to perform answer trying after each reasoning step, thereby mitigating the impact of lengthy reasoning paths. On the other hand, DoG utilizes a multi-role debate team to gradually simplify complex questions, reducing the influence of false-positive relations. This debate mechanism ensures the reliability of the reasoning process. Experimental results on five public datasets demonstrate the effectiveness and superiority of our architecture. Notably, DoG outperforms the state-of-the-art method ToG by 23.7\\% and 9.1\\% in accuracy on WebQuestions and GrailQA, respectively. Furthermore, the integration experiments with various LLMs on the mentioned datasets highlight the flexibility of DoG. Code is available at \\url{https://github.com/reml-group/DoG}.",
            "id": "2409.03155",
            "link": "http://arxiv.org/abs/2409.03155v1",
            "published": "2024-09-05T01:11:58+00:00",
            "updated": "2024-09-05T01:11:58+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "I.2.4"
            ],
            "max_author_hindex": 38
        },
        "2409.03183": {
            "authors": [
                "Zuquan Peng",
                "Yuanyuan He",
                "Jianbing Ni",
                "Ben Niu"
            ],
            "title": "Bypassing DARCY Defense: Indistinguishable Universal Adversarial Triggers",
            "abstract": "Neural networks (NN) classification models for Natural Language Processing (NLP) are vulnerable to the Universal Adversarial Triggers (UAT) attack that triggers a model to produce a specific prediction for any input. DARCY borrows the \"honeypot\" concept to bait multiple trapdoors, effectively detecting the adversarial examples generated by UAT. Unfortunately, we find a new UAT generation method, called IndisUAT, which produces triggers (i.e., tokens) and uses them to craft adversarial examples whose feature distribution is indistinguishable from that of the benign examples in a randomly-chosen category at the detection layer of DARCY. The produced adversarial examples incur the maximal loss of predicting results in the DARCY-protected models. Meanwhile, the produced triggers are effective in black-box models for text generation, text inference, and reading comprehension. Finally, the evaluation results under NN models for NLP tasks indicate that the IndisUAT method can effectively circumvent DARCY and penetrate other defenses. For example, IndisUAT can reduce the true positive rate of DARCY's detection by at least 40.8% and 90.6%, and drop the accuracy by at least 33.3% and 51.6% in the RNN and CNN models, respectively. IndisUAT reduces the accuracy of the BERT's adversarial defense model by at least 34.0%, and makes the GPT-2 language model spew racist outputs even when conditioned on non-racial context.",
            "id": "2409.03183",
            "link": "http://arxiv.org/abs/2409.03183v1",
            "published": "2024-09-05T02:19:34+00:00",
            "updated": "2024-09-05T02:19:34+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ],
            "max_author_hindex": 36
        },
        "2409.03215": {
            "authors": [
                "Jianguo Zhang",
                "Tian Lan",
                "Ming Zhu",
                "Zuxin Liu",
                "Thai Hoang",
                "Shirley Kokane",
                "Weiran Yao",
                "Juntao Tan",
                "Akshara Prabhakar",
                "Haolin Chen",
                "Zhiwei Liu",
                "Yihao Feng",
                "Tulika Awalgaonkar",
                "Rithesh Murthy",
                "Eric Hu",
                "Zeyuan Chen",
                "Ran Xu",
                "Juan Carlos Niebles",
                "Shelby Heinecke",
                "Huan Wang",
                "Silvio Savarese",
                "Caiming Xiong"
            ],
            "title": "xLAM: A Family of Large Action Models to Empower AI Agent Systems",
            "abstract": "Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of high-quality agent datasets and the absence of standard protocols in this area. We introduce and publicly release xLAM, a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture-of-expert architectures, ranging from 1B to 8x22B parameters, trained using a scalable, flexible pipeline that unifies, augments, and synthesizes diverse datasets to enhance AI agents' generalizability and performance across varied environments. Our experimental results demonstrate that xLAM consistently delivers exceptional performance across multiple agent ability benchmarks, notably securing the 1st position on the Berkeley Function-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other models in terms of tool use. By releasing the xLAM series, we aim to advance the performance of open-source LLMs for autonomous AI agents, potentially accelerating progress and democratizing access to high-performance models for agent tasks. Models are available at https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4",
            "id": "2409.03215",
            "link": "http://arxiv.org/abs/2409.03215v1",
            "published": "2024-09-05T03:22:22+00:00",
            "updated": "2024-09-05T03:22:22+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 54
        },
        "2409.03291": {
            "authors": [
                "Henrique Da Silva Gameiro",
                "Andrei Kucharavy",
                "Ljiljana Dolamic"
            ],
            "title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts",
            "abstract": "With the emergence of widely available powerful LLMs, disinformation generated by large Language Models (LLMs) has become a major concern. Historically, LLM detectors have been touted as a solution, but their effectiveness in the real world is still to be proven. In this paper, we focus on an important setting in information operations -- short news-like posts generated by moderately sophisticated attackers.   We demonstrate that existing LLM detectors, whether zero-shot or purpose-trained, are not ready for real-world use in that setting. All tested zero-shot detectors perform inconsistently with prior benchmarks and are highly vulnerable to sampling temperature increase, a trivial attack absent from recent benchmarks. A purpose-trained detector generalizing across LLMs and unseen attacks can be developed, but it fails to generalize to new human-written texts.   We argue that the former indicates domain-specific benchmarking is needed, while the latter suggests a trade-off between the adversarial evasion resilience and overfitting to the reference human text, with both needing evaluation in benchmarks and currently absent. We believe this suggests a re-consideration of current LLM detector benchmarking approaches and provides a dynamically extensible benchmark to allow it (https://github.com/Reliable-Information-Lab-HEVS/dynamic_llm_detector_benchmark).",
            "id": "2409.03291",
            "link": "http://arxiv.org/abs/2409.03291v1",
            "published": "2024-09-05T06:55:13+00:00",
            "updated": "2024-09-05T06:55:13+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CR",
                "cs.LG",
                "I.2.7; K.6.5"
            ],
            "max_author_hindex": 13
        },
        "2409.03375": {
            "authors": [
                "Francisco de Arriba-P\u00e9rez",
                "Silvia Garc\u00eda-M\u00e9ndez"
            ],
            "title": "Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time",
            "abstract": "Based on official estimates, 50 million people worldwide are affected by dementia, and this number increases by 10 million new patients every year. Without a cure, clinical prognostication and early intervention represent the most effective ways to delay its progression. To this end, Artificial Intelligence and computational linguistics can be exploited for natural language analysis, personalized assessment, monitoring, and treatment. However, traditional approaches need more semantic knowledge management and explicability capabilities. Moreover, using Large Language Models (LLMs) for cognitive decline diagnosis is still scarce, even though these models represent the most advanced way for clinical-patient communication using intelligent systems. Consequently, we leverage an LLM using the latest Natural Language Processing (NLP) techniques in a chatbot solution to provide interpretable Machine Learning prediction of cognitive decline in real-time. Linguistic-conceptual features are exploited for appropriate natural language analysis. Through explainability, we aim to fight potential biases of the models and improve their potential to help clinical workers in their diagnosis decisions. More in detail, the proposed pipeline is composed of (i) data extraction employing NLP-based prompt engineering; (ii) stream-based data processing including feature engineering, analysis, and selection; (iii) real-time classification; and (iv) the explainability dashboard to provide visual and natural language descriptions of the prediction outcome. Classification results exceed 80 % in all evaluation metrics, with a recall value for the mental deterioration class about 85 %. To sum up, we contribute with an affordable, flexible, non-invasive, personalized diagnostic system to this work.",
            "id": "2409.03375",
            "link": "http://arxiv.org/abs/2409.03375v1",
            "published": "2024-09-05T09:27:05+00:00",
            "updated": "2024-09-05T09:27:05+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 9
        },
        "2409.03381": {
            "authors": [
                "Yongxin Deng",
                "Xihe Qiu",
                "Xiaoyu Tan",
                "Chao Qu",
                "Jing Pan",
                "Yuan Cheng",
                "Yinghui Xu",
                "Wei Chu"
            ],
            "title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks",
            "abstract": "Cognitive psychology investigates perception, attention, memory, language, problem-solving, decision-making, and reasoning. Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive System 1 and the deliberative, rational System 2. Recent advancements have positioned large language Models (LLMs) as formidable tools nearing human-level proficiency in various cognitive tasks. Nonetheless, the presence of a dual-system framework analogous to human cognition in LLMs remains unexplored. This study introduces the \\textbf{CogniDual Framework for LLMs} (CFLLMs), designed to assess whether LLMs can, through self-training, evolve from deliberate deduction to intuitive responses, thereby emulating the human process of acquiring and mastering new information. Our findings reveal the cognitive mechanisms behind LLMs' response generation, enhancing our understanding of their capabilities in cognitive psychology. Practically, self-trained models can provide faster responses to certain queries, reducing computational demands during inference.",
            "id": "2409.03381",
            "link": "http://arxiv.org/abs/2409.03381v2",
            "published": "2024-09-05T09:33:24+00:00",
            "updated": "2024-09-06T09:37:36+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 20
        },
        "2409.04081": {
            "authors": [
                "Yicheng Fu",
                "Raviteja Anantha",
                "Prabal Vashisht",
                "Jianpeng Cheng",
                "Etai Littwin"
            ],
            "title": "UI-JEPA: Towards Active Perception of User Intent through Onscreen User Activity",
            "abstract": "Generating user intent from a sequence of user interface (UI) actions is a core challenge in comprehensive UI understanding. Recent advancements in multimodal large language models (MLLMs) have led to substantial progress in this area, but their demands for extensive model parameters, computing power, and high latency makes them impractical for scenarios requiring lightweight, on-device solutions with low latency or heightened privacy. Additionally, the lack of high-quality datasets has hindered the development of such lightweight models. To address these challenges, we propose UI-JEPA, a novel framework that employs masking strategies to learn abstract UI embeddings from unlabeled data through self-supervised learning, combined with an LLM decoder fine-tuned for user intent prediction. We also introduce two new UI-grounded multimodal datasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed for few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos across 219 intent categories, while IIT contains 914 videos across 10 categories. We establish the first baselines for these datasets, showing that representations learned using a JEPA-style objective, combined with an LLM decoder, can achieve user intent predictions that match the performance of state-of-the-art large MLLMs, but with significantly reduced annotation and deployment resources. Measured by intent similarity scores, UI-JEPA outperforms GPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged across two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x reduction in computational cost and a 6.6x improvement in latency in the IIW dataset. These results underscore the effectiveness of UI-JEPA, highlighting its potential for lightweight, high-performance UI understanding.",
            "id": "2409.04081",
            "link": "http://arxiv.org/abs/2409.04081v1",
            "published": "2024-09-06T07:44:44+00:00",
            "updated": "2024-09-06T07:44:44+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ],
            "max_author_hindex": 13
        },
        "2409.04109": {
            "authors": [
                "Chenglei Si",
                "Diyi Yang",
                "Tatsunori Hashimoto"
            ],
            "title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
            "abstract": "Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.",
            "id": "2409.04109",
            "link": "http://arxiv.org/abs/2409.04109v1",
            "published": "2024-09-06T08:25:03+00:00",
            "updated": "2024-09-06T08:25:03+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.HC",
                "cs.LG"
            ],
            "max_author_hindex": 38
        },
        "2409.04114": {
            "authors": [
                "Tengfei Xue",
                "Xuefeng Li",
                "Tahir Azim",
                "Roman Smirnov",
                "Jianhui Yu",
                "Arash Sadrieh",
                "Babak Pahlavan"
            ],
            "title": "Multi-Programming Language Ensemble for Code Generation in Large Language Model",
            "abstract": "Large language models (LLMs) have significantly improved code generation, particularly in one-pass code generation. However, most existing approaches focus solely on generating code in a single programming language, overlooking the potential of leveraging the multi-language capabilities of LLMs. LLMs have varying patterns of errors across different languages, suggesting that a more robust approach could be developed by leveraging these multi-language outputs. In this study, we propose Multi-Programming Language Ensemble (MPLE), a novel ensemble-based method that utilizes code generation across multiple programming languages to enhance overall performance. By treating each language-specific code generation process as an individual \"weak expert\" and effectively integrating their outputs, our method mitigates language-specific errors and biases. This multi-language ensemble strategy leverages the complementary strengths of different programming languages, enabling the model to produce more accurate and robust code. Our approach can be seamlessly integrated with commonly used techniques such as the reflection algorithm and Monte Carlo tree search to improve code generation quality further. Experimental results show that our framework consistently enhances baseline performance by up to 17.92% on existing benchmarks (HumanEval and HumanEval-plus), with a standout result of 96.25% accuracy on the HumanEval benchmark, achieving new state-of-the-art results across various LLM models. The code will be released at https://github.com/NinjaTech-AI/MPLE",
            "id": "2409.04114",
            "link": "http://arxiv.org/abs/2409.04114v1",
            "published": "2024-09-06T08:31:18+00:00",
            "updated": "2024-09-06T08:31:18+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.04168": {
            "authors": [
                "Andreas Stephan",
                "Dawei Zhu",
                "Matthias A\u00dfenmacher",
                "Xiaoyu Shen",
                "Benjamin Roth"
            ],
            "title": "From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks",
            "abstract": "To reduce the need for human annotations, large language models (LLMs) have been proposed as judges of the quality of other candidate models. LLM judges are typically evaluated by measuring the correlation with human judgments on generation tasks such as summarization or machine translation. In contrast, we study LLM judges on mathematical reasoning tasks. These tasks require multi-step reasoning, and the correctness of their solutions is verifiable, enabling a more objective evaluation. We perform a detailed performance analysis and find that the used judges are mostly unable to improve task performance but are able to pick the better model. Our analysis uncovers a strong correlation between judgment performance and the candidate model task performance. We observe that judges tend to choose the model of higher quality even if its answer is incorrect. Further, we show that it is possible to use statistics, such as the task performances of the individual models, to predict judgment performance. In an ablation, we either swap or mask the candidate answers and observe that judges often keep the original judgment, providing evidence that judges incorporate writing style in their judgments. In summary, we find that regularities in the judgments are quantifiable using statistical measures and provide various angles on exploiting them.",
            "id": "2409.04168",
            "link": "http://arxiv.org/abs/2409.04168v1",
            "published": "2024-09-06T10:09:41+00:00",
            "updated": "2024-09-06T10:09:41+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 33
        },
        "2409.04183": {
            "authors": [
                "Ziyin Zhang",
                "Hang Yu",
                "Shijie Li",
                "Peng Di",
                "Jianguo Li",
                "Rui Wang"
            ],
            "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding",
            "abstract": "Programming languages possess rich semantic information such as data flow that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with four different baseline LLMs ranging in size from 350M to 8B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3.",
            "id": "2409.04183",
            "link": "http://arxiv.org/abs/2409.04183v1",
            "published": "2024-09-06T10:57:34+00:00",
            "updated": "2024-09-06T10:57:34+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 29
        },
        "2409.04421": {
            "authors": [
                "Jiaxing Wu",
                "Lin Ning",
                "Luyang Liu",
                "Harrison Lee",
                "Neo Wu",
                "Chao Wang",
                "Sushant Prakash",
                "Shawn O'Banion",
                "Bradley Green",
                "Jun Xie"
            ],
            "title": "RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs",
            "abstract": "LLM-powered personalization agent systems employ Large Language Models (LLMs) to predict users' behavior from their past activities. However, their effectiveness often hinges on the ability to effectively leverage extensive, long user historical data due to its inherent noise and length of such data. Existing pretrained LLMs may generate summaries that are concise but lack the necessary context for downstream tasks, hindering their utility in personalization systems. To address these challenges, we introduce Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to generate concise, human-readable user summaries that are optimized for downstream task performance. By maximizing the usefulness of the generated summaries, RLPF effectively distills extensive user history data while preserving essential information for downstream tasks. Our empirical evaluation demonstrates significant improvements in both extrinsic downstream task utility and intrinsic summary quality, surpassing baseline methods by up to 22% on downstream task performance and achieving an up to 84.59% win rate on Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable 74% reduction in context length while improving performance on 16 out of 19 unseen tasks and/or datasets, showcasing its generalizability. This approach offers a promising solution for enhancing LLM personalization by effectively transforming long, noisy user histories into informative and human-readable representations.",
            "id": "2409.04421",
            "link": "http://arxiv.org/abs/2409.04421v1",
            "published": "2024-09-06T17:30:45+00:00",
            "updated": "2024-09-06T17:30:45+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 89
        },
        "2409.04464": {
            "authors": [
                "Teng Wang",
                "Wing-Yin Yu",
                "Ruifeng She",
                "Wenhan Yang",
                "Taijie Chen",
                "Jianping Zhang"
            ],
            "title": "Leveraging Large Language Models for Solving Rare MIP Challenges",
            "abstract": "Mixed Integer Programming (MIP) has been extensively applied in areas requiring mathematical solvers to address complex instances within tight time constraints. However, as the problem scale increases, the complexity of model formulation and finding feasible solutions escalates significantly. In contrast, the model-building cost for end-to-end models, such as large language models (LLMs), remains largely unaffected by problem scale due to their pattern recognition capabilities. While LLMs, like GPT-4, without fine-tuning, can handle some traditional medium-scale MIP problems, they struggle with uncommon or highly specialized MIP scenarios. Fine-tuning LLMs can yield some feasible solutions for medium-scale MIP instances, but these models typically fail to explore diverse solutions when constrained by a low and constant temperature, limiting their performance. In this paper, we propose and evaluate a recursively dynamic temperature method integrated with a chain-of-thought approach. Our findings show that starting with a high temperature and gradually lowering it leads to better feasible solutions compared to other dynamic temperature strategies. Additionally, by comparing results generated by the LLM with those from Gurobi, we demonstrate that the LLM can produce solutions that complement traditional solvers by accelerating the pruning process and improving overall efficiency.",
            "id": "2409.04464",
            "link": "http://arxiv.org/abs/2409.04464v1",
            "published": "2024-09-03T07:25:01+00:00",
            "updated": "2024-09-03T07:25:01+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "math.OC"
            ],
            "max_author_hindex": 16
        },
        "2409.04774": {
            "authors": [
                "Junfeng Tian",
                "Da Zheng",
                "Yang Cheng",
                "Rui Wang",
                "Colin Zhang",
                "Debing Zhang"
            ],
            "title": "Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models",
            "abstract": "Large language models (LLM) have prioritized expanding the context window from which models can incorporate more information. However, training models to handle long contexts presents significant challenges. These include the scarcity of high-quality natural long-context data, the potential for performance degradation on short-context tasks, and the reduced training efficiency associated with attention mechanisms. In this paper, we introduce Untie the Knots (\\textbf{UtK}), a novel data augmentation strategy employed during the continue pre-training phase, designed to efficiently enable LLMs to gain long-context capabilities without the need to modify the existing data mixture. In particular, we chunk the documents, shuffle the chunks, and create a complex and knotted structure of long texts; LLMs are then trained to untie these knots and identify relevant segments within seemingly chaotic token sequences. This approach greatly improves the model's performance by accurately attending to relevant information in long context and the training efficiency is also largely increased. We conduct extensive experiments on models with 7B and 72B parameters, trained on 20 billion tokens, demonstrating that UtK achieves 75\\% and 84.5\\% accurracy on RULER at 128K context length, significantly outperforming other long context strategies. The trained models will open-source for further research.",
            "id": "2409.04774",
            "link": "http://arxiv.org/abs/2409.04774v1",
            "published": "2024-09-07T09:28:55+00:00",
            "updated": "2024-09-07T09:28:55+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 29
        },
        "2409.04787": {
            "authors": [
                "Sonam Gupta",
                "Yatin Nandwani",
                "Asaf Yehudai",
                "Mayank Mishra",
                "Gaurav Pandey",
                "Dinesh Raghu",
                "Sachindra Joshi"
            ],
            "title": "Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models",
            "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-Rehearsal (SSR), a fine-tuning approach that achieves performance comparable to the standard supervised fine-tuning (SFT) while improving generalization. SSR leverages the fact that there can be multiple valid responses to a query. By utilizing the model's correct responses, SSR reduces model specialization during the fine-tuning stage. SSR first identifies the correct model responses from the training set by deploying an appropriate LLM as a judge. Then, it fine-tunes the model using the correct model responses and the gold response for the remaining samples. The effectiveness of SSR is demonstrated through experiments on the task of identifying unanswerable queries across various datasets. The results show that standard SFT can lead to an average performance drop of up to $16.7\\%$ on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, SSR results in close to $2\\%$ drop on average, indicating better generalization capabilities compared to standard SFT.",
            "id": "2409.04787",
            "link": "http://arxiv.org/abs/2409.04787v1",
            "published": "2024-09-07T10:21:03+00:00",
            "updated": "2024-09-07T10:21:03+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 18
        },
        "2409.04833": {
            "authors": [
                "Zhyar Rzgar K Rostam",
                "S\u00e1ndor Sz\u00e9n\u00e1si",
                "G\u00e1bor Kert\u00e9sz"
            ],
            "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
            "abstract": "In recent years, large language models (LLMs) have achieved remarkable success in natural language processing (NLP). LLMs require an extreme amount of parameters to attain high performance. As models grow into the trillion-parameter range, computational and memory costs increase significantly. This makes it difficult for many researchers to access the resources needed to train or apply these models. Optimizing LLM performance involves two main approaches: fine-tuning pre-trained models for specific tasks to achieve state-of-the-art performance, and reducing costs or improving training time while maintaining similar performance. This paper presents a systematic literature review (SLR) following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65 publications out of 983 from 2017 to December 2023, retrieved from 5 databases. The study presents methods to optimize and accelerate LLMs while achieving cutting-edge results without sacrificing accuracy. We begin with an overview of the development of language modeling, followed by a detailed explanation of commonly used frameworks and libraries, and a taxonomy for improving and speeding up LLMs based on three classes: LLM training, LLM inference, and system serving. We then delve into recent optimization and acceleration strategies such as training optimization, hardware optimization, scalability and reliability, accompanied by the taxonomy and categorization of these strategies. Finally, we provide an in-depth comparison of each class and strategy, with two case studies on optimizing model training and enhancing inference efficiency. These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.",
            "id": "2409.04833",
            "link": "http://arxiv.org/abs/2409.04833v1",
            "published": "2024-09-07T13:57:41+00:00",
            "updated": "2024-09-07T13:57:41+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 7
        },
        "2409.04934": {
            "authors": [
                "Anushka Swarup",
                "Avanti Bhandarkar",
                "Olivia P. Dizon-Paradis",
                "Ronald Wilson",
                "Damon L. Woodard"
            ],
            "title": "Maximizing Relation Extraction Potential: A Data-Centric Study to Unveil Challenges and Opportunities",
            "abstract": "Relation extraction is a Natural Language Processing task aiming to extract relationships from textual data. It is a critical step for information extraction. Due to its wide-scale applicability, research in relation extraction has rapidly scaled to using highly advanced neural networks. Despite their computational superiority, modern relation extractors fail to handle complicated extraction scenarios. However, a comprehensive performance analysis of the state-of-the-art relation extractors that compile these challenges has been missing from the literature, and this paper aims to bridge this gap. The goal has been to investigate the possible data-centric characteristics that impede neural relation extraction. Based on extensive experiments conducted using 15 state-of-the-art relation extraction algorithms ranging from recurrent architectures to large language models and seven large-scale datasets, this research suggests that modern relation extractors are not robust to complex data and relation characteristics. It emphasizes pivotal issues, such as contextual ambiguity, correlating relations, long-tail data, and fine-grained relation distributions. In addition, it sets a marker for future directions to alleviate these issues, thereby proving to be a critical resource for novice and advanced researchers. Efficient handling of the challenges described can have significant implications for the field of information extraction, which is a critical part of popular systems such as search engines and chatbots. Data and relevant code can be found at https://github.com/anushkasw/MaxRE.",
            "id": "2409.04934",
            "link": "http://arxiv.org/abs/2409.04934v1",
            "published": "2024-09-07T23:40:47+00:00",
            "updated": "2024-09-07T23:40:47+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 32
        },
        "2409.04964": {
            "authors": [
                "Xuechun Wang",
                "Rodney Beard",
                "Rohitash Chandra"
            ],
            "title": "Evaluation of Google Translate for Mandarin Chinese translation using sentiment and semantic analysis",
            "abstract": "Machine translation using large language models (LLMs) is having a significant global impact, making communication easier. Mandarin Chinese is the official language used for communication by the government, education institutes, and media in China. In this study, we provide an automated assessment of machine translation models with human experts using sentiment and semantic analysis. In order to demonstrate our framework, we select classic early twentieth-century novel 'The True Story of Ah Q' with selected Mandarin Chinese to English translations. We also us Google Translate to generate the given text into English and then conduct a chapter-wise sentiment analysis and semantic analysis to compare the extracted sentiments across the different translations. We utilise LLMs for semantic and sentiment analysis. Our results indicate that the precision of Google Translate differs both in terms of semantic and sentiment analysis when compared to human expert translations. We find that Google Translate is unable to translate some of the specific words or phrases in Chinese, such as Chinese traditional allusions. The mistranslations have to its lack of contextual significance and historical knowledge of China. Thus, this framework brought us some new insights about machine translation for Chinese Mandarin. The future work can explore other languages or types of texts with this framework.",
            "id": "2409.04964",
            "link": "http://arxiv.org/abs/2409.04964v1",
            "published": "2024-09-08T04:03:55+00:00",
            "updated": "2024-09-08T04:03:55+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 23
        },
        "2409.05197": {
            "authors": [
                "Neeladri Bhuiya",
                "Viktor Schlegel",
                "Stefan Winkler"
            ],
            "title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?",
            "abstract": "State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension, over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on their multi-hop reasoning capability: the ability to identify and integrate information from multiple textual sources.   Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate, whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. Motivated by this finding, we propose a challenging multi-hop reasoning benchmark, by generating seemingly plausible multi-hop reasoning chains, which ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs, and find that their performance to perform multi-hop reasoning is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We conduct a deeper analysis and find evidence that while LLMs tend to ignore misleading lexical cues, misleading reasoning paths indeed present a significant challenge.",
            "id": "2409.05197",
            "link": "http://arxiv.org/abs/2409.05197v1",
            "published": "2024-09-08T19:22:58+00:00",
            "updated": "2024-09-08T19:22:58+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ],
            "max_author_hindex": 36
        },
        "2409.01247": {
            "authors": [
                "John Burden",
                "Manuel Cebrian",
                "Jose Hernandez-Orallo"
            ],
            "title": "Conversational Complexity for Assessing Risk in Large Language Models",
            "abstract": "Large Language Models (LLMs) present a dual-use dilemma: they enable beneficial applications while harboring potential for harm, particularly through conversational interactions. Despite various safeguards, advanced LLMs remain vulnerable. A watershed case was Kevin Roose's notable conversation with Bing, which elicited harmful outputs after extended interaction. This contrasts with simpler early jailbreaks that produced similar content more easily, raising the question: How much conversational effort is needed to elicit harmful information from LLMs? We propose two measures: Conversational Length (CL), which quantifies the conversation length used to obtain a specific response, and Conversational Complexity (CC), defined as the Kolmogorov complexity of the user's instruction sequence leading to the response. To address the incomputability of Kolmogorov complexity, we approximate CC using a reference LLM to estimate the compressibility of user instructions. Applying this approach to a large red-teaming dataset, we perform a quantitative analysis examining the statistical distribution of harmful and harmless conversational lengths and complexities. Our empirical findings suggest that this distributional analysis and the minimisation of CC serve as valuable tools for understanding AI safety, offering insights into the accessibility of harmful information. This work establishes a foundation for a new perspective on LLM safety, centered around the algorithmic complexity of pathways to harm.",
            "id": "2409.01247",
            "link": "http://arxiv.org/abs/2409.01247v1",
            "published": "2024-09-02T13:29:44+00:00",
            "updated": "2024-09-02T13:29:44+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.IT",
                "math.IT"
            ],
            "max_author_hindex": 39
        },
        "2409.03271": {
            "authors": [
                "Yu Wang",
                "Shiwan Zhao",
                "Zhihu Wang",
                "Heyuan Huang",
                "Ming Fan",
                "Yubo Zhang",
                "Zhixing Wang",
                "Haijun Wang",
                "Ting Liu"
            ],
            "title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation",
            "abstract": "The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for enhancing the reasoning capabilities of large language models (LLMs). However, despite their widespread adoption and success, CoT methods often exhibit instability due to their inability to consistently ensure the quality of generated reasoning paths, leading to sub-optimal reasoning performance. To address this challenge, we propose the \\textbf{Strategic Chain-of-Thought} (SCoT), a novel methodology designed to refine LLM performance by integrating strategic knowledge prior to generating intermediate reasoning steps. SCoT employs a two-stage approach within a single prompt: first eliciting an effective problem-solving strategy, which is then used to guide the generation of high-quality CoT paths and final answers. Our experiments across eight challenging reasoning datasets demonstrate significant improvements, including a 21.05\\% increase on the GSM8K dataset and 24.13\\% on the Tracking\\_Objects dataset, respectively, using the Llama3-8b model. Additionally, we extend the SCoT framework to develop a few-shot method with automatically matched demonstrations, yielding even stronger results. These findings underscore the efficacy of SCoT, highlighting its potential to substantially enhance LLM performance in complex reasoning tasks.",
            "id": "2409.03271",
            "link": "http://arxiv.org/abs/2409.03271v1",
            "published": "2024-09-05T06:28:05+00:00",
            "updated": "2024-09-05T06:28:05+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.HC"
            ],
            "max_author_hindex": 19
        },
        "2409.03797": {
            "authors": [
                "Kinjal Basu",
                "Ibrahim Abdelaziz",
                "Kelsey Bradford",
                "Maxwell Crouse",
                "Kiran Kate",
                "Sadhana Kumaravel",
                "Saurabh Goyal",
                "Asim Munawar",
                "Yara Rizk",
                "Xin Wang",
                "Luis Lastras",
                "Pavan Kapanipathi"
            ],
            "title": "NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls",
            "abstract": "Autonomous agent applications powered by large language models (LLMs) have recently risen to prominence as effective tools for addressing complex real-world tasks. At their core, agentic workflows rely on LLMs to plan and execute the use of tools and external Application Programming Interfaces (APIs) in sequence to arrive at the answer to a user's request. Various benchmarks and leaderboards have emerged to evaluate an LLM's capabilities for tool and API use; however, most of these evaluations only track single or multiple isolated API calling capabilities. In this paper, we present NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls, i.e., sequences where the output of one API call is passed as input to a subsequent call. NESTFUL has a total of 300 human annotated samples divided into two types - executable and non-executable. The executable samples are curated manually by crawling Rapid-APIs whereas the non-executable samples are hand picked by human annotators from data synthetically generated using an LLM. We evaluate state-of-the-art LLMs with function calling abilities on NESTFUL. Our results show that most models do not perform well on nested APIs in NESTFUL as compared to their performance on the simpler problem settings available in existing benchmarks.",
            "id": "2409.03797",
            "link": "http://arxiv.org/abs/2409.03797v1",
            "published": "2024-09-04T17:53:24+00:00",
            "updated": "2024-09-04T17:53:24+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 22
        },
        "2409.06505": {
            "authors": [
                "Ekaterina Verbitskaia",
                "Joseph P. Near"
            ],
            "title": "Proceedings of the 2024 miniKanren and Relational Programming Workshop",
            "abstract": "The miniKanren and Relational Programming Workshop is a workshop for the miniKanren family of relational (pure constraint logic programming) languages: miniKanren, microKanren, core.logic, OCanren, Guanxi, etc. The workshop solicits papers and talks on the design, implementation, and application of miniKanren-like languages. A major goal of the workshop is to bring together researchers, implementors, and users from the miniKanren community, and to share expertise and techniques for relational programming. Another goal for the workshop is to push the state of the art of relational programming - for example, by developing new techniques for writing interpreters, type inferencers, theorem provers, abstract interpreters, CAD tools, and other interesting programs as relations, which are capable of being \"run backward,\" performing synthesis, etc.",
            "id": "2409.06505",
            "link": "http://arxiv.org/abs/2409.06505v1",
            "published": "2024-09-02T19:15:10+00:00",
            "updated": "2024-09-02T19:15:10+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL"
            ],
            "max_author_hindex": 17
        },
        "2409.00721": {
            "authors": [
                "Michael Haman",
                "Milan \u0160koln\u00edk"
            ],
            "title": "Who Would Chatbots Vote For? Political Preferences of ChatGPT and Gemini in the 2024 European Union Elections",
            "abstract": "This study examines the political bias of chatbots powered by large language models, namely ChatGPT and Gemini, in the context of the 2024 European Parliament elections. The research focused on the evaluation of political parties represented in the European Parliament across 27 EU Member States by these generative artificial intelligence (AI) systems. The methodology involved daily data collection through standardized prompts on both platforms. The results revealed a stark contrast: while Gemini mostly refused to answer political questions, ChatGPT provided consistent ratings. The analysis showed a significant bias in ChatGPT in favor of left-wing and centrist parties, with the highest ratings for the Greens/European Free Alliance. In contrast, right-wing parties, particularly the Identity and Democracy group, received the lowest ratings. The study identified key factors influencing the ratings, including attitudes toward European integration and perceptions of democratic values. The findings highlight the need for a critical approach to information provided by generative AI systems in a political context and call for more transparency and regulation in this area.",
            "id": "2409.00721",
            "link": "http://arxiv.org/abs/2409.00721v1",
            "published": "2024-09-01T13:40:13+00:00",
            "updated": "2024-09-01T13:40:13+00:00",
            "primary_category": "cs.CY",
            "categories": [
                "cs.CY",
                "cs.AI",
                "cs.CL",
                "cs.HC"
            ],
            "max_author_hindex": 6
        },
        "2409.00844": {
            "authors": [
                "Blair Yang",
                "Fuyang Cui",
                "Keiran Paster",
                "Jimmy Ba",
                "Pashootan Vaezipoor",
                "Silviu Pitis",
                "Michael R. Zhang"
            ],
            "title": "Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries",
            "abstract": "The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. We propose report cards, which are human-interpretable, natural language summaries of model behavior for specific skills or topics. We develop a framework to evaluate report cards based on three criteria: specificity (ability to distinguish between models), faithfulness (accurate representation of model capabilities), and interpretability (clarity and relevance to humans). We also propose an iterative algorithm for generating report cards without human supervision and explore its efficacy by ablating various design choices. Through experimentation with popular LLMs, we demonstrate that report cards provide insights beyond traditional benchmarks and can help address the need for a more interpretable and holistic evaluation of LLMs.",
            "id": "2409.00844",
            "link": "http://arxiv.org/abs/2409.00844v1",
            "published": "2024-09-01T21:18:14+00:00",
            "updated": "2024-09-01T21:18:14+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 43
        },
        "2409.00985": {
            "authors": [
                "Jiapeng Yu",
                "Yuqian Wu",
                "Yajing Zhan",
                "Wenhao Guo",
                "Zhou Xu",
                "Raymond Lee"
            ],
            "title": "Co-Learning: Code Learning for Multi-Agent Reinforcement Collaborative Framework with Conversational Natural Language Interfaces",
            "abstract": "Online question-and-answer (Q\\&A) systems based on the Large Language Model (LLM) have progressively diverged from recreational to professional use. This paper proposed a Multi-Agent framework with environmentally reinforcement learning (E-RL) for code correction called Code Learning (Co-Learning) community, assisting beginners to correct code errors independently. It evaluates the performance of multiple LLMs from an original dataset with 702 error codes, uses it as a reward or punishment criterion for E-RL; Analyzes input error codes by the current agent; selects the appropriate LLM-based agent to achieve optimal error correction accuracy and reduce correction time. Experiment results showed that 3\\% improvement in Precision score and 15\\% improvement in time cost as compared with no E-RL method respectively. Our source code is available at: https://github.com/yuqian2003/Co_Learning",
            "id": "2409.00985",
            "link": "http://arxiv.org/abs/2409.00985v1",
            "published": "2024-09-02T07:03:22+00:00",
            "updated": "2024-09-02T07:03:22+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 23
        },
        "2409.01073": {
            "authors": [
                "Yuqi Liu",
                "Wenqian Zhang",
                "Sihan Ren",
                "Chengyu Huang",
                "Jingyi Yu",
                "Lan Xu"
            ],
            "title": "SCOPE: Sign Language Contextual Processing with Embedding from LLMs",
            "abstract": "Sign languages, used by around 70 million Deaf individuals globally, are visual languages that convey visual and contextual information. Current methods in vision-based sign language recognition (SLR) and translation (SLT) struggle with dialogue scenes due to limited dataset diversity and the neglect of contextually relevant information. To address these challenges, we introduce SCOPE (Sign language Contextual Processing with Embedding from LLMs), a novel context-aware vision-based SLR and SLT framework. For SLR, we utilize dialogue contexts through a multi-modal encoder to enhance gloss-level recognition. For subsequent SLT, we further fine-tune a Large Language Model (LLM) by incorporating prior conversational context. We also contribute a new sign language dataset that contains 72 hours of Chinese sign language videos in contextual dialogues across various scenarios. Experimental results demonstrate that our SCOPE framework achieves state-of-the-art performance on multiple datasets, including Phoenix-2014T, CSL-Daily, and our SCOPE dataset. Moreover, surveys conducted with participants from the Deaf community further validate the robustness and effectiveness of our approach in real-world applications. Both our dataset and code will be open-sourced to facilitate further research.",
            "id": "2409.01073",
            "link": "http://arxiv.org/abs/2409.01073v1",
            "published": "2024-09-02T08:56:12+00:00",
            "updated": "2024-09-02T08:56:12+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 45
        },
        "2409.01545": {
            "authors": [
                "Chien-Chun Wang",
                "Li-Wei Chen",
                "Hung-Shin Lee",
                "Berlin Chen",
                "Hsin-Min Wang"
            ],
            "title": "Effective Noise-aware Data Simulation for Domain-adaptive Speech Enhancement Leveraging Dynamic Stochastic Perturbation",
            "abstract": "Cross-domain speech enhancement (SE) is often faced with severe challenges due to the scarcity of noise and background information in an unseen target domain, leading to a mismatch between training and test conditions. This study puts forward a novel data simulation method to address this issue, leveraging noise-extractive techniques and generative adversarial networks (GANs) with only limited target noisy speech data. Notably, our method employs a noise encoder to extract noise embeddings from target-domain data. These embeddings aptly guide the generator to synthesize utterances acoustically fitted to the target domain while authentically preserving the phonetic content of the input clean speech. Furthermore, we introduce the notion of dynamic stochastic perturbation, which can inject controlled perturbations into the noise embeddings during inference, thereby enabling the model to generalize well to unseen noise conditions. Experiments on the VoiceBank-DEMAND benchmark dataset demonstrate that our domain-adaptive SE method outperforms an existing strong baseline based on data simulation.",
            "id": "2409.01545",
            "link": "http://arxiv.org/abs/2409.01545v1",
            "published": "2024-09-03T02:29:01+00:00",
            "updated": "2024-09-03T02:29:01+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 34
        },
        "2409.01548": {
            "authors": [
                "Li-Wei Chen",
                "Hung-Shin Lee",
                "Chen-Chi Chang"
            ],
            "title": "VoxHakka: A Dialectally Diverse Multi-speaker Text-to-Speech System for Taiwanese Hakka",
            "abstract": "This paper introduces VoxHakka, a text-to-speech (TTS) system designed for Taiwanese Hakka, a critically under-resourced language spoken in Taiwan. Leveraging the YourTTS framework, VoxHakka achieves high naturalness and accuracy and low real-time factor in speech synthesis while supporting six distinct Hakka dialects. This is achieved by training the model with dialect-specific data, allowing for the generation of speaker-aware Hakka speech. To address the scarcity of publicly available Hakka speech corpora, we employed a cost-effective approach utilizing a web scraping pipeline coupled with automatic speech recognition (ASR)-based data cleaning techniques. This process ensured the acquisition of a high-quality, multi-speaker, multi-dialect dataset suitable for TTS training. Subjective listening tests conducted using comparative mean opinion scores (CMOS) demonstrate that VoxHakka significantly outperforms existing publicly available Hakka TTS systems in terms of pronunciation accuracy, tone correctness, and overall naturalness. This work represents a significant advancement in Hakka language technology and provides a valuable resource for language preservation and revitalization efforts.",
            "id": "2409.01548",
            "link": "http://arxiv.org/abs/2409.01548v1",
            "published": "2024-09-03T02:37:34+00:00",
            "updated": "2024-09-03T02:37:34+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 28
        },
        "2409.01754": {
            "authors": [
                "Hiromu Yakura",
                "Ezequiel Lopez-Lopez",
                "Levin Brinkmann",
                "Ignacio Serna",
                "Prateek Gupta",
                "Iyad Rahwan"
            ],
            "title": "Empirical evidence of Large Language Model's influence on human spoken communication",
            "abstract": "Artificial Intelligence (AI) agents now interact with billions of humans in natural language, thanks to advances in Large Language Models (LLMs) like ChatGPT. This raises the question of whether AI has the potential to shape a fundamental aspect of human culture: the way we speak. Recent analyses revealed that scientific publications already exhibit evidence of AI-specific language. But this evidence is inconclusive, since scientists may simply be using AI to copy-edit their writing. To explore whether AI has influenced human spoken communication, we transcribed and analyzed about 280,000 English-language videos of presentations, talks, and speeches from more than 20,000 YouTube channels of academic institutions. We find a significant shift in the trend of word usage specific to words distinctively associated with ChatGPT following its release. These findings provide the first empirical evidence that humans increasingly imitate LLMs in their spoken language. Our results raise societal and policy-relevant concerns about the potential of AI to unintentionally reduce linguistic diversity, or to be deliberately misused for mass manipulation. They also highlight the need for further investigation into the feedback loops between machine behavior and human culture.",
            "id": "2409.01754",
            "link": "http://arxiv.org/abs/2409.01754v1",
            "published": "2024-09-03T10:01:51+00:00",
            "updated": "2024-09-03T10:01:51+00:00",
            "primary_category": "cs.CY",
            "categories": [
                "cs.CY",
                "cs.AI",
                "cs.CL",
                "cs.HC"
            ],
            "max_author_hindex": 50
        },
        "2409.01864": {
            "authors": [
                "Pedro Ramoneda",
                "Emilia Parada-Cabaleiro",
                "Benno Weck",
                "Xavier Serra"
            ],
            "title": "The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?",
            "abstract": "In this work, we explore the use and reliability of Large Language Models (LLMs) in musicology. From a discussion with experts and students, we assess the current acceptance and concerns regarding this, nowadays ubiquitous, technology. We aim to go one step further, proposing a semi-automatic method to create an initial benchmark using retrieval-augmented generation models and multiple-choice question generation, validated by human experts. Our evaluation on 400 human-validated questions shows that current vanilla LLMs are less reliable than retrieval augmented generation from music dictionaries. This paper suggests that the potential of LLMs in musicology requires musicology driven research that can specialized LLMs by including accurate and reliable domain knowledge.",
            "id": "2409.01864",
            "link": "http://arxiv.org/abs/2409.01864v1",
            "published": "2024-09-03T13:05:38+00:00",
            "updated": "2024-09-03T13:05:38+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.CL",
                "cs.DL",
                "eess.AS"
            ],
            "max_author_hindex": 13
        },
        "2409.02976": {
            "authors": [
                "Gabriel Y. Arteaga",
                "Thomas B. Sch\u00f6n",
                "Nicolas Pielawski"
            ],
            "title": "Hallucination Detection in LLMs: Fast and Memory-Efficient Finetuned Models",
            "abstract": "Uncertainty estimation is a necessary component when implementing AI in high-risk settings, such as autonomous cars, medicine, or insurances. Large Language Models (LLMs) have seen a surge in popularity in recent years, but they are subject to hallucinations, which may cause serious harm in high-risk settings. Despite their success, LLMs are expensive to train and run: they need a large amount of computations and memory, preventing the use of ensembling methods in practice. In this work, we present a novel method that allows for fast and memory-friendly training of LLM ensembles. We show that the resulting ensembles can detect hallucinations and are a viable approach in practice as only one GPU is needed for training and inference.",
            "id": "2409.02976",
            "link": "http://arxiv.org/abs/2409.02976v1",
            "published": "2024-09-04T13:59:38+00:00",
            "updated": "2024-09-04T13:59:38+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 49
        },
        "2409.03757": {
            "authors": [
                "Yunze Man",
                "Shuhong Zheng",
                "Zhipeng Bao",
                "Martial Hebert",
                "Liang-Yan Gui",
                "Yu-Xiong Wang"
            ],
            "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding",
            "abstract": "Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models. We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks.",
            "id": "2409.03757",
            "link": "http://arxiv.org/abs/2409.03757v1",
            "published": "2024-09-05T17:59:56+00:00",
            "updated": "2024-09-05T17:59:56+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "cs.RO"
            ],
            "max_author_hindex": 105
        },
        "2409.04117": {
            "authors": [
                "Arthur Hemmer",
                "Micka\u00ebl Coustaty",
                "Nicola Bartolo",
                "Jean-Marc Ogier"
            ],
            "title": "Confidence-Aware Document OCR Error Detection",
            "abstract": "Optical Character Recognition (OCR) continues to face accuracy challenges that impact subsequent applications. To address these errors, we explore the utility of OCR confidence scores for enhancing post-OCR error detection. Our study involves analyzing the correlation between confidence scores and error rates across different OCR systems. We develop ConfBERT, a BERT-based model that incorporates OCR confidence scores into token embeddings and offers an optional pre-training phase for noise adjustment. Our experimental results demonstrate that integrating OCR confidence scores can enhance error detection capabilities. This work underscores the importance of OCR confidence scores in improving detection accuracy and reveals substantial disparities in performance between commercial and open-source OCR technologies.",
            "id": "2409.04117",
            "link": "http://arxiv.org/abs/2409.04117v1",
            "published": "2024-09-06T08:35:28+00:00",
            "updated": "2024-09-06T08:35:28+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 90
        },
        "2409.04340": {
            "authors": [
                "Shuirong Cao",
                "Ruoxi Cheng",
                "Zhiqiang Wang"
            ],
            "title": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs",
            "abstract": "LLMs can exhibit age biases, resulting in unequal treatment of individuals across age groups. While much research has addressed racial and gender biases, age bias remains little explored. The scarcity of instruction-tuning and preference datasets for age bias hampers its detection and measurement, and existing fine-tuning methods seldom address age-related fairness. In this paper, we construct age bias preference datasets and instruction-tuning datasets for RLHF. We introduce ARG, an age fairness reward to reduce differences in the response quality of LLMs across different age groups. Extensive experiments demonstrate that this reward significantly improves response accuracy and reduces performance disparities across age groups. Our source code and datasets are available at the anonymous \\href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}.",
            "id": "2409.04340",
            "link": "http://arxiv.org/abs/2409.04340v1",
            "published": "2024-09-06T15:18:12+00:00",
            "updated": "2024-09-06T15:18:12+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 15
        },
        "2409.04831": {
            "authors": [
                "Zeming Wei",
                "Yihao Zhang",
                "Meng Sun"
            ],
            "title": "MILE: A Mutation Testing Framework of In-Context Learning Systems",
            "abstract": "In-context Learning (ICL) has achieved notable success in the applications of large language models (LLMs). By adding only a few input-output pairs that demonstrate a new task, the LLM can efficiently learn the task during inference without modifying the model parameters. Such mysterious ability of LLMs has attracted great research interests in understanding, formatting, and improving the in-context demonstrations, while still suffering from drawbacks like black-box mechanisms and sensitivity against the selection of examples. In this work, inspired by the foundations of adopting testing techniques in machine learning (ML) systems, we propose a mutation testing framework designed to characterize the quality and effectiveness of test data for ICL systems. First, we propose several mutation operators specialized for ICL demonstrations, as well as corresponding mutation scores for ICL test sets. With comprehensive experiments, we showcase the effectiveness of our framework in evaluating the reliability and quality of ICL test suites. Our code is available at https://github.com/weizeming/MILE.",
            "id": "2409.04831",
            "link": "http://arxiv.org/abs/2409.04831v1",
            "published": "2024-09-07T13:51:42+00:00",
            "updated": "2024-09-07T13:51:42+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.CL",
                "cs.CR",
                "cs.LG"
            ],
            "max_author_hindex": 17
        },
        "2409.05907": {
            "authors": [
                "Bruce W. Lee",
                "Inkit Padhi",
                "Karthikeyan Natesan Ramamurthy",
                "Erik Miehling",
                "Pierre Dognin",
                "Manish Nagireddy",
                "Amit Dhurandhar"
            ],
            "title": "Programming Refusal with Conditional Activation Steering",
            "abstract": "LLMs have shown remarkable capabilities, but precisely controlling their response behavior remains challenging. Existing activation steering methods alter LLM behavior indiscriminately, limiting their practical applicability in settings where selective responses are essential, such as content moderation or domain-specific assistants. In this paper, we propose Conditional Activation Steering (CAST), which analyzes LLM activation patterns during inference to selectively apply or withhold activation steering based on the input context. Our method is based on the observation that different categories of prompts activate distinct patterns in the model's hidden states. Using CAST, one can systematically control LLM behavior with rules like \"if input is about hate speech or adult content, then refuse\" or \"if input is not about legal advice, then refuse.\" This allows for selective modification of responses to specific content while maintaining normal responses to other content, all without requiring weight optimization. We release an open-source implementation of our framework.",
            "id": "2409.05907",
            "link": "http://arxiv.org/abs/2409.05907v1",
            "published": "2024-09-06T15:47:40+00:00",
            "updated": "2024-09-06T15:47:40+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 29
        },
        "2409.01648": {
            "authors": [
                "Aziz Amezian El Khalfioui",
                "Jef Wijsen"
            ],
            "title": "Computing Range Consistent Answers to Aggregation Queries via Rewriting",
            "abstract": "We consider the problem of answering conjunctive queries with aggregation on database instances that may violate primary key constraints. In SQL, these queries follow the SELECT-FROM-WHERE-GROUP BY format, where the WHERE-clause involves a conjunction of equalities, and the SELECT-clause can incorporate aggregate operators like MAX, MIN, SUM, AVG, or COUNT. Repairs of a database instance are defined as inclusion-maximal subsets that satisfy all primary keys. For a given query, our primary objective is to identify repairs that yield the lowest aggregated value among all possible repairs. We particularly investigate queries for which this lowest aggregated value can be determined through a rewriting in first-order logic with aggregate operators.",
            "id": "2409.01648",
            "link": "http://arxiv.org/abs/2409.01648v1",
            "published": "2024-09-03T06:36:39+00:00",
            "updated": "2024-09-03T06:36:39+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 24
        },
        "2409.04498": {
            "authors": [
                "Jey Puget Gil",
                "Emmanuel Coquery",
                "John Samuel",
                "Gilles Gesquiere"
            ],
            "title": "Graph versioning for evolving urban data",
            "abstract": "The continuous evolution of cities poses significant challenges in terms of managing and understanding their complex dynamics. With the increasing demand for transparency and the growing availability of open urban data, it has become important to ensure the reproducibility of scientific research and computations in urban planning. To understand past decisions and other possible scenarios, we require solutions that go beyond the management of urban knowledge graphs. In this work, we explore existing solutions and their limits and explain the need and possible approaches for querying across multiple graph versions.",
            "id": "2409.04498",
            "link": "http://arxiv.org/abs/2409.04498v1",
            "published": "2024-09-06T15:41:24+00:00",
            "updated": "2024-09-06T15:41:24+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 40
        },
        "2409.01790": {
            "authors": [
                "Shiwen Ni",
                "Xiangtao Kong",
                "Chengming Li",
                "Xiping Hu",
                "Ruifeng Xu",
                "Jia Zhu",
                "Min Yang"
            ],
            "title": "Training on the Benchmark Is Not All You Need",
            "abstract": "The success of Large Language Models (LLMs) relies heavily on the huge amount of pre-training data learned in the pre-training phase. The opacity of the pre-training process and the training data causes the results of many benchmark tests to become unreliable. If any model has been trained on a benchmark test set, it can seriously hinder the health of the field. In order to automate and efficiently test the capabilities of large language models, numerous mainstream benchmarks adopt a multiple-choice format. As the swapping of the contents of multiple-choice options does not affect the meaning of the question itself, we propose a simple and effective data leakage detection method based on this property. Specifically, we shuffle the contents of the options in the data to generate the corresponding derived data sets, and then detect data leakage based on the model's log probability distribution over the derived data sets. If there is a maximum and outlier in the set of log probabilities, it indicates that the data is leaked. Our method is able to work under black-box conditions without access to model training data or weights, effectively identifying data leakage from benchmark test sets in model pre-training data, including both normal scenarios and complex scenarios where options may have been shuffled intentionally or unintentionally. Through experiments based on two LLMs and benchmark designs, we demonstrate the effectiveness of our method. In addition, we evaluate the degree of data leakage of 31 mainstream open-source LLMs on four benchmark datasets and give a ranking of the leaked LLMs for each benchmark, and we find that the Qwen family of LLMs has the highest degree of data leakage.",
            "id": "2409.01790",
            "link": "http://arxiv.org/abs/2409.01790v1",
            "published": "2024-09-03T11:09:44+00:00",
            "updated": "2024-09-03T11:09:44+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 38
        },
        "2409.02370": {
            "authors": [
                "Yang Liu",
                "Xichou Zhu",
                "Zhou Shen",
                "Yi Liu",
                "Min Li",
                "Yujun Chen",
                "Benzi John",
                "Zhenzhen Ma",
                "Tao Hu",
                "Zhiyang Xu",
                "Wei Luo",
                "Junhui Wang"
            ],
            "title": "Do Large Language Models Possess Sensitive to Sentiment?",
            "abstract": "Large Language Models (LLMs) have recently displayed their extraordinary capabilities in language understanding. However, how to comprehensively assess the sentiment capabilities of LLMs continues to be a challenge. This paper investigates the ability of LLMs to detect and react to sentiment in text modal. As the integration of LLMs into diverse applications is on the rise, it becomes highly critical to comprehend their sensitivity to emotional tone, as it can influence the user experience and the efficacy of sentiment-driven tasks. We conduct a series of experiments to evaluate the performance of several prominent LLMs in identifying and responding appropriately to sentiments like positive, negative, and neutral emotions. The models' outputs are analyzed across various sentiment benchmarks, and their responses are compared with human evaluations. Our discoveries indicate that although LLMs show a basic sensitivity to sentiment, there are substantial variations in their accuracy and consistency, emphasizing the requirement for further enhancements in their training processes to better capture subtle emotional cues. Take an example in our findings, in some cases, the models might wrongly classify a strongly positive sentiment as neutral, or fail to recognize sarcasm or irony in the text. Such misclassifications highlight the complexity of sentiment analysis and the areas where the models need to be refined. Another aspect is that different LLMs might perform differently on the same set of data, depending on their architecture and training datasets. This variance calls for a more in-depth study of the factors that contribute to the performance differences and how they can be optimized.",
            "id": "2409.02370",
            "link": "http://arxiv.org/abs/2409.02370v1",
            "published": "2024-09-04T01:40:20+00:00",
            "updated": "2024-09-04T01:40:20+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 55
        },
        "2409.03444": {
            "authors": [
                "Wei Lu",
                "Rachel K. Luu",
                "Markus J. Buehler"
            ],
            "title": "Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities",
            "abstract": "The advancement of Large Language Models (LLMs) for domain applications in fields such as materials science and engineering depends on the development of fine-tuning strategies that adapt models for specialized, technical capabilities. In this work, we explore the effects of Continued Pretraining (CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization approaches, including Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO), on fine-tuned LLM performance. Our analysis shows how these strategies influence model outcomes and reveals that the merging of multiple fine-tuned models can lead to the emergence of capabilities that surpass the individual contributions of the parent models. We find that model merging leads to new functionalities that neither parent model could achieve alone, leading to improved performance in domain-specific assessments. Experiments with different model architectures are presented, including Llama 3.1 8B and Mistral 7B models, where similar behaviors are observed. Exploring whether the results hold also for much smaller models, we use a tiny LLM with 1.7 billion parameters and show that very small LLMs do not necessarily feature emergent capabilities under model merging, suggesting that model scaling may be a key component. In open-ended yet consistent chat conversations between a human and AI models, our assessment reveals detailed insights into how different model variants perform and show that the smallest model achieves a high intelligence score across key criteria including reasoning depth, creativity, clarity, and quantitative precision. Other experiments include the development of image generation prompts based on disparate biological material design concepts, to create new microstructures, architectural concepts, and urban design based on biological materials-inspired construction principles.",
            "id": "2409.03444",
            "link": "http://arxiv.org/abs/2409.03444v1",
            "published": "2024-09-05T11:49:53+00:00",
            "updated": "2024-09-05T11:49:53+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cond-mat.mtrl-sci",
                "cs.AI"
            ],
            "max_author_hindex": 107
        },
        "2409.03454": {
            "authors": [
                "Inacio Vieira",
                "Will Allred",
                "S\u00e9amus Lankford",
                "Sheila Castilho",
                "Andy Way"
            ],
            "title": "How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House Translation: Performance Evaluation Across Multiple Dataset Sizes",
            "abstract": "Decoder-only LLMs have shown impressive performance in MT due to their ability to learn from extensive datasets and generate high-quality translations. However, LLMs often struggle with the nuances and style required for organisation-specific translation. In this study, we explore the effectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3 8B Instruct, leveraging translation memories (TMs), as a valuable resource to enhance accuracy and efficiency. We investigate the impact of fine-tuning the Llama 3 model using TMs from a specific organisation in the software sector. Our experiments cover five translation directions across languages of varying resource levels (English to Brazilian Portuguese, Czech, German, Finnish, and Korean). We analyse diverse sizes of training datasets (1k to 207k segments) to evaluate their influence on translation quality. We fine-tune separate models for each training set and evaluate their performance based on automatic metrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in translation performance with larger datasets across all metrics. On average, BLEU and COMET scores increase by 13 and 25 points, respectively, on the largest training set against the baseline model. Notably, there is a performance deterioration in comparison with the baseline model when fine-tuning on only 1k and 2k examples; however, we observe a substantial improvement as the training dataset size increases. The study highlights the potential of integrating TMs with LLMs to create bespoke translation models tailored to the specific needs of businesses, thus enhancing translation quality and reducing turn-around times. This approach offers a valuable insight for organisations seeking to leverage TMs and LLMs for optimal translation outcomes, especially in narrower domains.",
            "id": "2409.03454",
            "link": "http://arxiv.org/abs/2409.03454v2",
            "published": "2024-09-05T12:06:38+00:00",
            "updated": "2024-09-10T09:22:26+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "max_author_hindex": 46
        },
        "2409.03563": {
            "authors": [
                "Lorenzo Pacchiardi",
                "Lucy G. Cheke",
                "Jos\u00e9 Hern\u00e1ndez-Orallo"
            ],
            "title": "100 instances is all you need: predicting the success of a new LLM on unseen data by testing on a few instances",
            "abstract": "Predicting the performance of LLMs on individual task instances is essential to ensure their reliability in high-stakes applications. To do so, a possibility is to evaluate the considered LLM on a set of task instances and train an assessor to predict its performance based on features of the instances. However, this approach requires evaluating each new LLM on a sufficiently large set of task instances to train an assessor specific to it. In this work, we leverage the evaluation results of previously tested LLMs to reduce the number of evaluations required to predict the performance of a new LLM. In practice, we propose to test the new LLM on a small set of reference instances and train a generic assessor which predicts the performance of the LLM on an instance based on the performance of the former on the reference set and features of the instance of interest. We conduct empirical studies on HELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets that we introduce, where we evaluate all instruction-fine-tuned OpenAI models until the January 2024 version of GPT4. When predicting performance on instances with the same distribution as those used to train the generic assessor, we find this achieves performance comparable to the LLM-specific assessors trained on the full set of instances. Additionally, we find that randomly selecting the reference instances performs as well as some advanced selection methods we tested. For out of distribution, however, no clear winner emerges and the overall performance is worse, suggesting that the inherent predictability of LLMs is low.",
            "id": "2409.03563",
            "link": "http://arxiv.org/abs/2409.03563v1",
            "published": "2024-09-05T14:19:45+00:00",
            "updated": "2024-09-05T14:19:45+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 39
        },
        "2409.02877": {
            "authors": [
                "Chaojun Xiao",
                "Zhengyan Zhang",
                "Chenyang Song",
                "Dazhi Jiang",
                "Feng Yao",
                "Xu Han",
                "Xiaozhi Wang",
                "Shuo Wang",
                "Yufei Huang",
                "Guanyu Lin",
                "Yingfa Chen",
                "Weilin Zhao",
                "Yuge Tu",
                "Zexuan Zhong",
                "Ao Zhang",
                "Chenglei Si",
                "Khai Hao Moo",
                "Chenyang Zhao",
                "Huimin Chen",
                "Yankai Lin",
                "Zhiyuan Liu",
                "Jingbo Shang",
                "Maosong Sun"
            ],
            "title": "Configurable Foundation Models: Building LLMs from a Modular Perspective",
            "abstract": "Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, we coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. In this paper, we offer a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. We first formalize modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, we further present four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify our perspective, we conduct an empirical analysis on widely-used LLMs. We find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, we highlight several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models.",
            "id": "2409.02877",
            "link": "http://arxiv.org/abs/2409.02877v1",
            "published": "2024-09-04T17:01:02+00:00",
            "updated": "2024-09-04T17:01:02+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 87
        },
        "2409.00920": {
            "authors": [
                "Weiwen Liu",
                "Xu Huang",
                "Xingshan Zeng",
                "Xinlong Hao",
                "Shuai Yu",
                "Dexun Li",
                "Shuai Wang",
                "Weinan Gan",
                "Zhengying Liu",
                "Yuanqing Yu",
                "Zezhong Wang",
                "Yuxian Wang",
                "Wu Ning",
                "Yutai Hou",
                "Bin Wang",
                "Chuhan Wu",
                "Xinzhi Wang",
                "Yong Liu",
                "Yasheng Wang",
                "Duyu Tang",
                "Dandan Tu",
                "Lifeng Shang",
                "Xin Jiang",
                "Ruiming Tang",
                "Defu Lian",
                "Qun Liu",
                "Enhong Chen"
            ],
            "title": "ToolACE: Winning the Points of LLM Function Calling",
            "abstract": "Function calling significantly extends the application boundary of large language models, where high-quality and diverse training data is critical for unlocking this capability. However, real function-calling data is quite challenging to collect and annotate, while synthetic data generated by existing pipelines tends to lack coverage and accuracy. In this paper, we present ToolACE, an automatic agentic pipeline designed to generate accurate, complex, and diverse tool-learning data. ToolACE leverages a novel self-evolution synthesis process to curate a comprehensive API pool of 26,507 diverse APIs. Dialogs are further generated through the interplay among multiple agents, guided by a formalized thinking process. To ensure data accuracy, we implement a dual-layer verification system combining rule-based and model-based checks. We demonstrate that models trained on our synthesized data, even with only 8B parameters, achieve state-of-the-art performance on the Berkeley Function-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a subset of the data are publicly available at https://huggingface.co/Team-ACE.",
            "id": "2409.00920",
            "link": "http://arxiv.org/abs/2409.00920v1",
            "published": "2024-09-02T03:19:56+00:00",
            "updated": "2024-09-02T03:19:56+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 81
        },
        "2409.01369": {
            "authors": [
                "Markus Wulfmeier",
                "Michael Bloesch",
                "Nino Vieillard",
                "Arun Ahuja",
                "Jorg Bornschein",
                "Sandy Huang",
                "Artem Sokolov",
                "Matt Barnes",
                "Guillaume Desjardins",
                "Alex Bewley",
                "Sarah Maria Elisabeth Bechtle",
                "Jost Tobias Springenberg",
                "Nikola Momchev",
                "Olivier Bachem",
                "Matthieu Geist",
                "Martin Riedmiller"
            ],
            "title": "Imitating Language via Scalable Inverse Reinforcement Learning",
            "abstract": "The majority of language model training builds on imitation learning. It covers pretraining, supervised fine-tuning, and affects the starting conditions for reinforcement learning from human feedback (RLHF). The simplicity and scalability of maximum likelihood estimation (MLE) for next token prediction led to its role as predominant paradigm. However, the broader field of imitation learning can more effectively utilize the sequential structure underlying autoregressive generation. We focus on investigating the inverse reinforcement learning (IRL) perspective to imitation, extracting rewards and directly optimizing sequences instead of individual token likelihoods and evaluate its benefits for fine-tuning large language models. We provide a new angle, reformulating inverse soft-Q-learning as a temporal difference regularized extension of MLE. This creates a principled connection between MLE and IRL and allows trading off added complexity with increased performance and diversity of generations in the supervised fine-tuning (SFT) setting. We find clear advantages for IRL-based imitation, in particular for retaining diversity while maximizing task performance, rendering IRL a strong alternative on fixed SFT datasets even without online data generation. Our analysis of IRL-extracted reward functions further indicates benefits for more robust reward functions via tighter integration of supervised and preference-based LLM post-training.",
            "id": "2409.01369",
            "link": "http://arxiv.org/abs/2409.01369v1",
            "published": "2024-09-02T16:48:57+00:00",
            "updated": "2024-09-02T16:48:57+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "stat.ML"
            ],
            "max_author_hindex": 53
        },
        "2409.01901": {
            "authors": [
                "Oline Ranum",
                "Gomer Otterspeer",
                "Jari I. Andersen",
                "Robert G. Belleman",
                "Floris Roelofsen"
            ],
            "title": "3D-LEX v1.0: 3D Lexicons for American Sign Language and Sign Language of the Netherlands",
            "abstract": "In this work, we present an efficient approach for capturing sign language in 3D, introduce the 3D-LEX v1.0 dataset, and detail a method for semi-automatic annotation of phonetic properties. Our procedure integrates three motion capture techniques encompassing high-resolution 3D poses, 3D handshapes, and depth-aware facial features, and attains an average sampling rate of one sign every 10 seconds. This includes the time for presenting a sign example, performing and recording the sign, and archiving the capture. The 3D-LEX dataset includes 1,000 signs from American Sign Language and an additional 1,000 signs from the Sign Language of the Netherlands. We showcase the dataset utility by presenting a simple method for generating handshape annotations directly from 3D-LEX. We produce handshape labels for 1,000 signs from American Sign Language and evaluate the labels in a sign recognition task. The labels enhance gloss recognition accuracy by 5% over using no handshape annotations, and by 1% over expert annotations. Our motion capture data supports in-depth analysis of sign features and facilitates the generation of 2D projections from any viewpoint. The 3D-LEX collection has been aligned with existing sign language benchmarks and linguistic resources, to support studies in 3D-aware sign language processing.",
            "id": "2409.01901",
            "link": "http://arxiv.org/abs/2409.01901v1",
            "published": "2024-09-03T13:44:56+00:00",
            "updated": "2024-09-03T13:44:56+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 23
        },
        "2409.02118": {
            "authors": [
                "Kaihui Chen",
                "Hao Yi",
                "Qingyang Li",
                "Tianyu Qi",
                "Yulan Hu",
                "Fuzheng Zhang",
                "Yong Liu"
            ],
            "title": "TSO: Self-Training with Scaled Preference Optimization",
            "abstract": "Enhancing the conformity of large language models (LLMs) to human preferences remains an ongoing research challenge. Recently, offline approaches such as Direct Preference Optimization (DPO) have gained prominence as attractive options due to offering effective improvement in simple, efficient, and stable without interactions with reward models. However, these offline preference optimization methods highly rely on the quality of pairwise preference samples. Meanwhile, numerous iterative methods require additional training of reward models to select positive and negative samples from the model's own generated responses for preference learning. Furthermore, as LLMs' capabilities advance, it is quite challenging to continuously construct high-quality positive and negative preference instances from the model's outputs due to the lack of diversity. To tackle these challenges, we propose TSO, or Self-Training with Scaled Preference Optimization, a framework for preference optimization that conducts self-training preference learning without training an additional reward model. TSO enhances the diversity of responses by constructing a model matrix and incorporating human preference responses. Furthermore, TSO introduces corrections for model preference errors through human and AI feedback. Finally, TSO adopts iterative and dual clip reward strategies to update the reference model and its responses, adaptively adjusting preference data and balancing the optimization process. Experimental results demonstrate that TSO outperforms existing mainstream methods on various alignment evaluation benchmarks, providing practical insight into preference data construction and model training strategies in the alignment domain.",
            "id": "2409.02118",
            "link": "http://arxiv.org/abs/2409.02118v1",
            "published": "2024-08-31T05:37:01+00:00",
            "updated": "2024-08-31T05:37:01+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 34
        },
        "2409.02119": {
            "authors": [
                "Xiaojun Xiao",
                "Sen Shen",
                "Qiming Bao",
                "Hongfei Rong",
                "Kairui Liu",
                "Zhongsheng Wang",
                "Jiamou Liu"
            ],
            "title": "CoRA: Optimizing Low-Rank Adaptation with Common Subspace of Large Language Models",
            "abstract": "In fine-tuning large language models (LLMs), conserving computational resources while maintaining effectiveness and improving outcomes within the same computational constraints is crucial. The Low-Rank Adaptation (LoRA) strategy balances efficiency and performance in fine-tuning large models by reducing the number of trainable parameters and computational costs. However, current advancements in LoRA might be focused on its fine-tuning methodologies, with not as much exploration as might be expected into further compression of LoRA. Since most of LoRA's parameters might still be superfluous, this may lead to unnecessary wastage of computational resources. In this paper, we propose \\textbf{CoRA}: leveraging shared knowledge to optimize LoRA training by substituting its matrix $B$ with a common subspace from large models. Our two-fold method includes (1) Freezing the substitute matrix $B$ to halve parameters while training matrix $A$ for specific tasks and (2) Using the substitute matrix $B$ as an enhanced initial state for the original matrix $B$, achieving improved results with the same parameters. Our experiments show that the first approach achieves the same efficacy as the original LoRA fine-tuning while being more efficient than halving parameters. At the same time, the second approach has some improvements compared to LoRA's original fine-tuning performance. They generally attest to the effectiveness of our work.",
            "id": "2409.02119",
            "link": "http://arxiv.org/abs/2409.02119v1",
            "published": "2024-08-31T12:48:27+00:00",
            "updated": "2024-08-31T12:48:27+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 22
        },
        "2409.02122": {
            "authors": [
                "Sumit Dalal",
                "Sarika Jain",
                "Mayank Dave"
            ],
            "title": "Deep Knowledge-Infusion For Explainable Depression Detection",
            "abstract": "Discovering individuals depression on social media has become increasingly important. Researchers employed ML/DL or lexicon-based methods for automated depression detection. Lexicon based methods, explainable and easy to implement, match words from user posts in a depression dictionary without considering contexts. While the DL models can leverage contextual information, their black-box nature limits their adoption in the domain. Though surrogate models like LIME and SHAP can produce explanations for DL models, the explanations are suitable for the developer and of limited use to the end user. We propose a Knolwedge-infused Neural Network (KiNN) incorporating domain-specific knowledge from DepressionFeature ontology (DFO) in a neural network to endow the model with user-level explainability regarding concepts and processes the clinician understands. Further, commonsense knowledge from the Commonsense Transformer (COMET) trained on ATOMIC is also infused to consider the generic emotional aspects of user posts in depression detection. The model is evaluated on three expertly curated datasets related to depression. We observed the model to have a statistically significant (p<0.1) boost in performance over the best domain-specific model, MentalBERT, across CLEF e-Risk (25% MCC increase, 12% F1 increase). A similar trend is observed across the PRIMATE dataset, where the proposed model performed better than MentalBERT (2.5% MCC increase, 19% F1 increase). The observations confirm the generated explanations to be informative for MHPs compared to post hoc model explanations. Results demonstrated that the user-level explainability of KiNN also surpasses the performance of baseline models and can provide explanations where other baselines fall short. Infusing the domain and commonsense knowledge in KiNN enhances the ability of models like GPT-3.5 to generate application-relevant explanations.",
            "id": "2409.02122",
            "link": "http://arxiv.org/abs/2409.02122v1",
            "published": "2024-09-01T06:13:55+00:00",
            "updated": "2024-09-01T06:13:55+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 31
        },
        "2409.02141": {
            "authors": [
                "Suhong Moon",
                "Siddharth Jha",
                "Lutfi Eren Erdogan",
                "Sehoon Kim",
                "Woosang Lim",
                "Kurt Keutzer",
                "Amir Gholami"
            ],
            "title": "Efficient and Scalable Estimation of Tool Representations in Vector Space",
            "abstract": "Recent advancements in function calling and tool use have significantly enhanced the capabilities of large language models (LLMs) by enabling them to interact with external information sources and execute complex tasks. However, the limited context window of LLMs presents challenges when a large number of tools are available, necessitating efficient methods to manage prompt length and maintain accuracy. Existing approaches, such as fine-tuning LLMs or leveraging their reasoning capabilities, either require frequent retraining or incur significant latency overhead. A more efficient solution involves training smaller models to retrieve the most relevant tools for a given query, although this requires high quality, domain-specific data. To address those challenges, we present a novel framework for generating synthetic data for tool retrieval applications and an efficient data-driven tool retrieval strategy using small encoder models. Empowered by LLMs, we create ToolBank, a new tool retrieval dataset that reflects real human user usages. For tool retrieval methodologies, we propose novel approaches: (1) Tool2Vec: usage-driven tool embedding generation for tool retrieval, (2) ToolRefiner: a staged retrieval method that iteratively improves the quality of retrieved tools, and (3) MLC: framing tool retrieval as a multi-label classification problem. With these new methods, we achieve improvements of up to 27.28 in Recall@K on the ToolBench dataset and 30.5 in Recall@K on ToolBank. Additionally, we present further experimental results to rigorously validate our methods. Our code is available at \\url{https://github.com/SqueezeAILab/Tool2Vec}",
            "id": "2409.02141",
            "link": "http://arxiv.org/abs/2409.02141v1",
            "published": "2024-09-02T19:39:24+00:00",
            "updated": "2024-09-02T19:39:24+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 91
        },
        "2409.02239": {
            "authors": [
                "Xugang Lu",
                "Peng Shen",
                "Yu Tsao",
                "Hisashi Kawai"
            ],
            "title": "Temporal Order Preserved Optimal Transport-based Cross-modal Knowledge Transfer Learning for ASR",
            "abstract": "Transferring linguistic knowledge from a pretrained language model (PLM) to an acoustic model has been shown to greatly improve the performance of automatic speech recognition (ASR). However, due to the heterogeneous feature distributions in cross-modalities, designing an effective model for feature alignment and knowledge transfer between linguistic and acoustic sequences remains a challenging task. Optimal transport (OT), which efficiently measures probability distribution discrepancies, holds great potential for aligning and transferring knowledge between acoustic and linguistic modalities. Nonetheless, the original OT treats acoustic and linguistic feature sequences as two unordered sets in alignment and neglects temporal order information during OT coupling estimation. Consequently, a time-consuming pretraining stage is required to learn a good alignment between the acoustic and linguistic representations. In this paper, we propose a Temporal Order Preserved OT (TOT)-based Cross-modal Alignment and Knowledge Transfer (CAKT) (TOT-CAKT) for ASR. In the TOT-CAKT, local neighboring frames of acoustic sequences are smoothly mapped to neighboring regions of linguistic sequences, preserving their temporal order relationship in feature alignment and matching. With the TOT-CAKT model framework, we conduct Mandarin ASR experiments with a pretrained Chinese PLM for linguistic knowledge transfer. Our results demonstrate that the proposed TOT-CAKT significantly improves ASR performance compared to several state-of-the-art models employing linguistic knowledge transfer, and addresses the weaknesses of the original OT-based method in sequential feature alignment for ASR.",
            "id": "2409.02239",
            "link": "http://arxiv.org/abs/2409.02239v2",
            "published": "2024-09-03T19:11:15+00:00",
            "updated": "2024-09-05T11:34:00+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 30
        },
        "2409.02428": {
            "authors": [
                "Guanwen Xie",
                "Jingzehua Xu",
                "Yiyuan Yang",
                "Shuai Zhang"
            ],
            "title": "Large Language Models as Efficient Reward Function Searchers for Custom-Environment Multi-Objective Reinforcement Learning",
            "abstract": "Leveraging large language models (LLMs) for designing reward functions demonstrates significant potential. However, achieving effective design and improvement of reward functions in reinforcement learning (RL) tasks with complex custom environments and multiple requirements presents considerable challenges. In this paper, we enable LLMs to be effective white-box searchers, highlighting their advanced semantic understanding capabilities. Specifically, we generate reward components for each explicit user requirement and employ the reward critic to identify the correct code form. Then, LLMs assign weights to the reward components to balance their values and iteratively search and optimize these weights based on the context provided by the training log analyzer, while adaptively determining the search step size. We applied the framework to an underwater information collection RL task without direct human feedback or reward examples (zero-shot). The reward critic successfully correct the reward code with only one feedback for each requirement, effectively preventing irreparable errors that can occur when reward function feedback is provided in aggregate. The effective initialization of weights enables the acquisition of different reward functions within the Pareto solution set without weight search. Even in the case where a weight is 100 times off, fewer than four iterations are needed to obtain solutions that meet user requirements. The framework also works well with most prompts utilizing GPT-3.5 Turbo, since it does not require advanced numerical understanding or calculation.",
            "id": "2409.02428",
            "link": "http://arxiv.org/abs/2409.02428v1",
            "published": "2024-09-04T04:15:14+00:00",
            "updated": "2024-09-04T04:15:14+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.SY",
                "eess.SY"
            ],
            "max_author_hindex": 8
        },
        "2409.02908": {
            "authors": [
                "Kaiwen Zheng",
                "Yongxin Chen",
                "Hanzi Mao",
                "Ming-Yu Liu",
                "Jun Zhu",
                "Qinsheng Zhang"
            ],
            "title": "Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling",
            "abstract": "Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their superior performance over other discrete diffusion models, and are rivaling the auto-regressive models (ARMs) for language modeling tasks. The recent effort in simplifying the masked diffusion framework further leads to alignment with continuous-space diffusion models and more principled training and sampling recipes. In this paper, however, we reveal that both training and sampling of MDMs are theoretically free from the time variable, arguably the key signature of diffusion models, and are instead equivalent to masked models. The connection on the sampling aspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we show that the FHS is theoretically equivalent to MDMs' original generation process while significantly alleviating the time-consuming categorical sampling and achieving a 20$\\times$ speedup. In addition, our investigation challenges previous claims that MDMs can surpass ARMs in generative perplexity. We identify, for the first time, an underlying numerical issue, even with the 32-bit floating-point precision, which results in inaccurate categorical sampling. We show that the numerical issue lowers the effective temperature both theoretically and empirically, leading to unfair assessments of MDMs' generation results in the previous literature.",
            "id": "2409.02908",
            "link": "http://arxiv.org/abs/2409.02908v1",
            "published": "2024-09-04T17:48:19+00:00",
            "updated": "2024-09-04T17:48:19+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 106
        },
        "2409.02920": {
            "authors": [
                "Yao Mu",
                "Tianxing Chen",
                "Shijia Peng",
                "Zanxin Chen",
                "Zeyu Gao",
                "Yude Zou",
                "Lunkai Lin",
                "Zhiqiang Xie",
                "Ping Luo"
            ],
            "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)",
            "abstract": "Effective collaboration of dual-arm robots and their tool use capabilities are increasingly important areas in the advancement of robotics. These skills play a significant role in expanding robots' ability to operate in diverse real-world environments. However, progress is impeded by the scarcity of specialized training data. This paper introduces RoboTwin, a novel benchmark dataset combining real-world teleoperated data with synthetic data from digital twins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform, we have collected diverse data on tool usage and human-robot interaction. We present a innovative approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented toward functionality. Our key contributions are: 1) the RoboTwin benchmark dataset, 2) an efficient real-to-simulation pipeline, and 3) the use of language models for automatic expert-level data generation. These advancements are designed to address the shortage of robotic training data, potentially accelerating the development of more capable and versatile robotic systems for a wide range of real-world applications. The project page is available at https://robotwin-benchmark.github.io/early-version/",
            "id": "2409.02920",
            "link": "http://arxiv.org/abs/2409.02920v1",
            "published": "2024-09-04T17:59:52+00:00",
            "updated": "2024-09-04T17:59:52+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 68
        },
        "2409.03733": {
            "authors": [
                "Evan Wang",
                "Federico Cassano",
                "Catherine Wu",
                "Yunfeng Bai",
                "Will Song",
                "Vaskar Nath",
                "Ziwen Han",
                "Sean Hendryx",
                "Summer Yue",
                "Hugh Zhang"
            ],
            "title": "Planning In Natural Language Improves LLM Search For Code Generation",
            "abstract": "While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PLANSEARCH generates a diverse set of observations about the problem and then uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PLANSEARCH explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PLANSEARCH on top of Claude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on LiveCodeBench, outperforming both the best score achieved without search (pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains due to search as a direct function of the diversity over generated ideas.",
            "id": "2409.03733",
            "link": "http://arxiv.org/abs/2409.03733v1",
            "published": "2024-09-05T17:44:49+00:00",
            "updated": "2024-09-05T17:44:49+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 16
        },
        "2409.03788": {
            "authors": [
                "Cheng Qian",
                "Hainan Zhang",
                "Lei Sha",
                "Zhiming Zheng"
            ],
            "title": "HSF: Defending against Jailbreak Attacks with Hidden State Filtering",
            "abstract": "With the growing deployment of LLMs in daily applications like chatbots and content generation, efforts to ensure outputs align with human values and avoid harmful content have intensified. However, increasingly sophisticated jailbreak attacks threaten this alignment, aiming to induce unsafe outputs. Current defense efforts either focus on prompt rewriting or detection, which are limited in effectiveness due to the various design of jailbreak prompts, or on output control and detection, which are computationally expensive as they require LLM inference. Therefore, designing a pre-inference defense method that resists diverse jailbreak prompts is crucial for preventing LLM jailbreak attacks. We observe that jailbreak attacks, safe queries, and harmful queries exhibit different clustering patterns within the LLM's hidden state representation space. This suggests that by leveraging the LLM's hidden state representational capabilities, we can analyze the LLM's forthcoming behavior and proactively intervene for defense. In this paper, we propose a jailbreak attack defense strategy based on a Hidden State Filter (HSF), a lossless architectural defense mechanism that enables the model to preemptively identify and reject adversarial inputs before the inference process begins. We activate its defensive potential through an additional plugin module, effectively framing the defense task as a classification problem. Experimental results on two benchmark datasets, utilizing three different LLMs, show that HSF significantly enhances resilience against six cutting-edge jailbreak attacks. It significantly reduces the success rate of jailbreak attacks while minimally impacting responses to benign user queries, with negligible inference overhead, and outperforming defense baselines.Our code and data are available at https://anonymous.4open.science/r/Hidden-State-Filtering-8652/",
            "id": "2409.03788",
            "link": "http://arxiv.org/abs/2409.03788v1",
            "published": "2024-08-31T06:50:07+00:00",
            "updated": "2024-08-31T06:50:07+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 19
        },
        "2409.03810": {
            "authors": [
                "Yejie Wang",
                "Keqing He",
                "Dayuan Fu",
                "Zhuoma Gongque",
                "Heyang Xu",
                "Yanxu Chen",
                "Zhexu Wang",
                "Yujia Fu",
                "Guanting Dong",
                "Muxi Diao",
                "Jingang Wang",
                "Mengdi Zhang",
                "Xunliang Cai",
                "Weiran Xu"
            ],
            "title": "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data",
            "abstract": "Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show XCoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs. Our models and dataset are released in https://github.com/banksy23/XCoder",
            "id": "2409.03810",
            "link": "http://arxiv.org/abs/2409.03810v1",
            "published": "2024-09-05T17:46:30+00:00",
            "updated": "2024-09-05T17:46:30+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 23
        },
        "2409.01587": {
            "authors": [
                "Vivian Ding",
                "Co\u015fku Acay",
                "Andrew C. Myers"
            ],
            "title": "An Array Intermediate Language for Mixed Cryptography",
            "abstract": "We introduce AIRduct, a new array-based intermediate representation designed to support generating efficient code for interactive programs employing multiple cryptographic mechanisms. AIRduct is intended as an IR for the Viaduct compiler, which can synthesize secure, distributed programs with an extensible suite of cryptography. Therefore, AIRduct supports an extensible variety of cryptographic mechanisms, including MPC and ZKP.",
            "id": "2409.01587",
            "link": "http://arxiv.org/abs/2409.01587v1",
            "published": "2024-09-03T04:01:25+00:00",
            "updated": "2024-09-03T04:01:25+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.PL"
            ],
            "max_author_hindex": 49
        },
        "2409.01841": {
            "authors": [
                "Ian Smith"
            ],
            "title": "BinSub: The Simple Essence of Polymorphic Type Inference for Machine Code",
            "abstract": "Recovering high-level type information in binaries is a key task in reverse engineering and binary analysis. Binaries contain very little explicit type information. The structure of binary code is incredibly flexible allowing for ad-hoc subtyping and polymorphism. Prior work has shown that precise type inference on binary code requires expressive subtyping and polymorphism.   Implementations of these type system features in a binary type inference algorithm have thus-far been too inefficient to achieve widespread adoption. Recent advances in traditional type inference have achieved simple and efficient principal type inference in an ML like language with subtyping and polymorphism through the framework of algebraic subtyping. BinSub, a new binary type inference algorithm, recognizes the connection between algebraic subtyping and the type system features required to analyze binaries effectively. Using this connection, BinSub achieves simple, precise, and efficient binary type inference. We show that BinSub maintains a similar precision to prior work, while achieving a 63x improvement in average runtime for 1568 functions. We also present a formalization of BinSub and show that BinSub's type system maintains the expressiveness of prior work.",
            "id": "2409.01841",
            "link": "http://arxiv.org/abs/2409.01841v1",
            "published": "2024-09-03T12:40:46+00:00",
            "updated": "2024-09-03T12:40:46+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL",
                "D.3.1; F.3.2"
            ],
            "max_author_hindex": 19
        },
        "2409.02398": {
            "authors": [
                "Lee Naish"
            ],
            "title": "Sharing Analysis in the Pawns Compiler",
            "abstract": "Pawns is a programming language under development that supports algebraic data types, polymorphism, higher order functions and \"pure\" declarative programming. It also supports impure imperative features including destructive update of shared data structures via pointers, allowing significantly increased efficiency for some operations. A novelty of Pawns is that all impure \"effects\" must be made obvious in the source code and they can be safely encapsulated in pure functions in a way that is checked by the compiler. Execution of a pure function can perform destructive updates on data structures that are local to or eventually returned from the function without risking modification of the data structures passed to the function. This paper describes the sharing analysis which allows impurity to be encapsulated. Aspects of the analysis are similar to other published work, but in addition it handles explicit pointers and destructive update, higher order functions including closures and pre- and post-conditions concerning sharing for functions.",
            "id": "2409.02398",
            "link": "http://arxiv.org/abs/2409.02398v1",
            "published": "2024-09-04T02:47:34+00:00",
            "updated": "2024-09-04T02:47:34+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL",
                "D.3.4; D.3.2"
            ],
            "max_author_hindex": 25
        },
        "2409.02771": {
            "authors": [
                "Ethan Chen",
                "Jiwon Chang",
                "Yuhao Zhu"
            ],
            "title": "CoolerSpace: A Language for Physically Correct and Computationally Efficient Color Programming",
            "abstract": "Color programmers manipulate lights, materials, and the resulting colors from light-material interactions. Existing libraries for color programming provide only a thin layer of abstraction around matrix operations. Color programs are, thus, vulnerable to bugs arising from mathematically permissible but physically meaningless matrix computations. Correct implementations are difficult to write and optimize. We introduce CoolerSpace to facilitate physically correct and computationally efficient color programming. CoolerSpace raises the level of abstraction of color programming by allowing programmers to focus on describing the logic of color physics. Correctness and efficiency are handled by CoolerSpace. The type system in CoolerSpace assigns physical meaning and dimensions to user-defined objects. The typing rules permit only legal computations informed by color physics and perception. Along with type checking, CoolerSpace also generates performance-optimized programs using equality saturation. CoolerSpace is implemented as a Python library and compiles to ONNX, a common intermediate representation for tensor computations. CoolerSpace not only prevents common errors in color programming, but also does so without run-time overhead: even unoptimized CoolerSpace programs out-perform existing Python-based color programming systems by up to 5.7 times; our optimizations provide up to an additional 1.4 times speed-up.",
            "id": "2409.02771",
            "link": "http://arxiv.org/abs/2409.02771v1",
            "published": "2024-09-04T14:50:57+00:00",
            "updated": "2024-09-04T14:50:57+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL",
                "cs.GR"
            ],
            "max_author_hindex": 10
        },
        "2409.03152": {
            "authors": [
                "Lee Naish"
            ],
            "title": "A Brief Overview of the Pawns Programming Language",
            "abstract": "Pawns is a programming language under development which supports pure functional programming (including algebraic data types, higher order programming and parametric polymorphism) and imperative programming (including pointers, destructive update of shared data structures and global variables), integrated so each can call the other and with purity checked by the compiler. For pure functional code the programmer need not understand the representation of the data structures. For imperative code the representation must be understood and all effects and dependencies must be documented in the code. For example, if a function may update one of its arguments, this must be declared in the function type signature and noted where the function is called. A single update operation may affect several variables due to sharing of representations (pointer aliasing). Pawns code requires all affected variables to be annotated wherever they may be updated and information about sharing to be declared. Annotations are also required where IO or other global variables are used and this must be declared in type signatures as well. Sharing analysis, performed by the compiler, is the key to many aspects of Pawns. It enables us to check that all effects are made obvious in the source code, effects can be encapsulated inside a pure interface and effects can be used safely in the presence of polymorphism.",
            "id": "2409.03152",
            "link": "http://arxiv.org/abs/2409.03152v1",
            "published": "2024-09-05T00:59:27+00:00",
            "updated": "2024-09-05T00:59:27+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL"
            ],
            "max_author_hindex": 25
        },
        "2409.03864": {
            "authors": [
                "Martin Paul L\u00fccke",
                "Oleksandr Zinenko",
                "William S. Moses",
                "Michel Steuwer",
                "Albert Cohen"
            ],
            "title": "The MLIR Transform Dialect. Your compiler is more powerful than you think",
            "abstract": "To take full advantage of a specific hardware target, performance engineers need to gain control on compilers in order to leverage their domain knowledge about the program and hardware. Yet, modern compilers are poorly controlled, usually by configuring a sequence of coarse-grained monolithic black-box passes, or by means of predefined compiler annotations/pragmas. These can be effective, but often do not let users precisely optimize their varying compute loads. As a consequence, performance engineers have to resort to implementing custom passes for a specific optimization heuristic, requiring compiler engineering expert knowledge.   In this paper, we present a technique that provides fine-grained control of general-purpose compilers by introducing the Transform dialect, a controllable IR-based transformation system implemented in MLIR. The Transform dialect empowers performance engineers to optimize their various compute loads by composing and reusing existing - but currently hidden - compiler features without the need to implement new passes or even rebuilding the compiler.   We demonstrate in five case studies that the Transform dialect enables precise, safe composition of compiler transformations and allows for straightforward integration with state-of-the-art search methods.",
            "id": "2409.03864",
            "link": "http://arxiv.org/abs/2409.03864v2",
            "published": "2024-09-05T19:01:47+00:00",
            "updated": "2024-09-09T09:42:31+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL"
            ],
            "max_author_hindex": 37
        },
        "2409.03918": {
            "authors": [
                "Ingkarat Rak-amnouykit",
                "Ana Milanova",
                "Guillaume Baudart",
                "Martin Hirzel",
                "Julian Dolby"
            ],
            "title": "PoTo: A Hybrid Andersen's Points-to Analysis for Python",
            "abstract": "As Python is increasingly being adopted for large and complex programs, the importance of static analysis for Python (such as type inference) grows. Unfortunately, static analysis for Python remains a challenging task due to its dynamic language features and its abundant external libraries. To help fill this gap, this paper presents PoTo, an Andersen-style context-insensitive and flow-insensitive points-to analysis for Python. PoTo addresses Python-specific challenges and works for large programs via a novel hybrid evaluation, integrating traditional static points-to analysis with concrete evaluation in the Python interpreter for external library calls. Next, this paper presents PoTo+, a static type inference for Python built on the points-to analysis. We evaluate PoTo+ and compare it to two state-of-the-art Python type inference techniques: (1) the static rule-based Pytype and (2) the deep-learning based DLInfer. Our results show that PoTo+ outperforms both Pytype and DLInfer on existing Python packages.",
            "id": "2409.03918",
            "link": "http://arxiv.org/abs/2409.03918v1",
            "published": "2024-09-05T21:26:25+00:00",
            "updated": "2024-09-05T21:26:25+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL",
                "cs.SE"
            ],
            "max_author_hindex": 36
        },
        "2409.01388": {
            "authors": [
                "Haoqiong Bian",
                "Dongyang Geng",
                "Yunpeng Chai",
                "Anastasia Ailamaki"
            ],
            "title": "Serverless Query Processing with Flexible Performance SLAs and Prices",
            "abstract": "Serverless query processing has become increasingly popular due to its auto-scaling, high elasticity, and pay-as-you-go pricing. It allows cloud data warehouse (or lakehouse) users to focus on data analysis without the burden of managing systems and resources. Accordingly, in serverless query services, users become more concerned about cost-efficiency under acceptable performance than performance under fixed resources. This poses new challenges for serverless query engine design in providing flexible performance service-level agreements (SLAs) and cost-efficiency (i.e., prices).   In this paper, we first define the problem of flexible performance SLAs and prices in serverless query processing and discuss its significance. Then, we envision the challenges and solutions for solving this problem and the opportunities it raises for other database research. Finally, we present PixelsDB, an open-source prototype with three service levels supported by dedicated architectural designs. Evaluations show that PixelsDB reduces resource costs by 65.5% for near-real-world workloads generated by Cloud Analytics Benchmark (CAB) while not violating the pending time guarantees.",
            "id": "2409.01388",
            "link": "http://arxiv.org/abs/2409.01388v1",
            "published": "2024-09-02T17:33:43+00:00",
            "updated": "2024-09-02T17:33:43+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 61
        },
        "2409.01517": {
            "authors": [
                "Gavin Chait"
            ],
            "title": "Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data",
            "abstract": "This paper presents an open-source curatorial toolkit intended to produce well-structured and interoperable data. Curation is divided into discrete components, with a schema-centric focus for auditable restructuring of complex and scattered tabular data to conform to a destination schema. Task separation allows development of software and analysis without source data being present. Transformations are captured as high-level sequential scripts describing schema-to-schema mappings, reducing complexity and resource requirements. Ultimately, data are transformed, but the objective is that any data meeting a schema definition can be restructured using a crosswalk. The toolkit is available both as a Python package, and as a 'no-code' visual web application. A visual example is presented, derived from a longitudinal study where scattered source data from hundreds of local councils are integrated into a single database.",
            "id": "2409.01517",
            "link": "http://arxiv.org/abs/2409.01517v1",
            "published": "2024-09-03T01:25:11+00:00",
            "updated": "2024-09-03T01:25:11+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 3
        },
        "2409.01675": {
            "authors": [
                "Tieying Zhang",
                "Anthony Tomasic",
                "Andrew Pavlo"
            ],
            "title": "Intelligent Transaction Scheduling via Conflict Prediction in OLTP DBMS",
            "abstract": "Current architectures for main-memory online transaction processing (OLTP) database management systems (DBMS) typically use random scheduling to assign transactions to threads. This approach achieves uniform load across threads but it ignores the likelihood of conflicts between transactions. If the DBMS could estimate the potential for transaction conflict and then intelligently schedule transactions to avoid conflicts, then the system could improve its performance. Such estimation of transaction conflict, however, is non-trivial for several reasons. First, conflicts occur under complex conditions that are far removed in time from the scheduling decision. Second, transactions must be represented in a compact and efficient manner to allow for fast conflict detection. Third, given some evidence of potential conflict, the DBMS must schedule transactions in such a way that minimizes this conflict. In this paper, we systematically explore the design decisions for solving these problems. We then empirically measure the performance impact of different representations on standard OLTP benchmarks. Our results show that intelligent scheduling using a history increases throughput by $\\sim$40\\% on 20-core machine.",
            "id": "2409.01675",
            "link": "http://arxiv.org/abs/2409.01675v1",
            "published": "2024-09-03T07:39:25+00:00",
            "updated": "2024-09-03T07:39:25+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "H.2.6"
            ],
            "max_author_hindex": 35
        },
        "2409.02968": {
            "authors": [
                "Baochao Chen",
                "Liyuan Ma",
                "Hao Xu",
                "Juncheng Ma",
                "Dengcheng Hu",
                "Xiulong Liu",
                "Jie Wu",
                "Jianrong Wang",
                "Keqiu Li"
            ],
            "title": "A Comprehensive Survey of Blockchain Scalability: Shaping Inner-Chain and Inter-Chain Perspectives",
            "abstract": "Blockchain is widely applied in logistics, finance, and agriculture. As single blockchain users grow, scalability becomes crucial. However, existing works lack a comprehensive summary of blockchain scalability. They focus on single chains or cross-chain technologies. This survey summarizes scalability across the physical and logical layers, as well as inner-chain, inter-chain, and technology dimensions. The physical layer covers data and protocols, while the logical layer represents blockchain architecture. Each component is analyzed from inner-chain and inter-chain perspectives, considering technological factors. The aim is to enhance researchers' understanding of blockchain's architecture, data, and protocols to advance scalability research.",
            "id": "2409.02968",
            "link": "http://arxiv.org/abs/2409.02968v1",
            "published": "2024-09-04T06:47:50+00:00",
            "updated": "2024-09-04T06:47:50+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.CR"
            ],
            "max_author_hindex": 50
        },
        "2409.03478": {
            "authors": [
                "Mohsen Shirali",
                "Mohammadreza Fani Sani",
                "Zahra Ahmadi",
                "Estefania Serral"
            ],
            "title": "LLM-based event abstraction and integration for IoT-sourced logs",
            "abstract": "The continuous flow of data collected by Internet of Things (IoT) devices, has revolutionised our ability to understand and interact with the world across various applications. However, this data must be prepared and transformed into event data before analysis can begin. In this paper, we shed light on the potential of leveraging Large Language Models (LLMs) in event abstraction and integration. Our approach aims to create event records from raw sensor readings and merge the logs from multiple IoT sources into a single event log suitable for further Process Mining applications. We demonstrate the capabilities of LLMs in event abstraction considering a case study for IoT application in elderly care and longitudinal health monitoring. The results, showing on average an accuracy of 90% in detecting high-level activities. These results highlight LLMs' promising potential in addressing event abstraction and integration challenges, effectively bridging the existing gap.",
            "id": "2409.03478",
            "link": "http://arxiv.org/abs/2409.03478v1",
            "published": "2024-09-05T12:38:13+00:00",
            "updated": "2024-09-05T12:38:13+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.ET",
                "cs.LG",
                "68M14",
                "I.2.1; H.4.0"
            ],
            "max_author_hindex": 27
        },
        "2409.04499": {
            "authors": [
                "Jey Puget Gil",
                "Emmanuel Coquery",
                "John Samuel",
                "Gilles Gesquiere"
            ],
            "title": "ConVer-G: Concurrent versioning of knowledge graphs",
            "abstract": "The multiplication of platforms offering open data has facilitated access to information that can be used for research, innovation, and decision-making. Providing transparency and availability, open data is regularly updated, allowing us to observe their evolution over time.   We are particularly interested in the evolution of urban data that allows stakeholders to better understand dynamics and propose solutions to improve the quality of life of citizens. In this context, we are interested in the management of evolving data, especially urban data and the ability to query these data across the available versions. In order to have the ability to understand our urban heritage and propose new scenarios, we must be able to search for knowledge through concurrent versions of urban knowledge graphs.   In this work, we present the ConVer-G (Concurrent Versioning of knowledge Graphs) system for storage and querying through multiple concurrent versions of graphs.",
            "id": "2409.04499",
            "link": "http://arxiv.org/abs/2409.04499v1",
            "published": "2024-09-06T15:43:06+00:00",
            "updated": "2024-09-06T15:43:06+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 40
        },
        "2409.03166": {
            "authors": [
                "Weiwei Gu",
                "Suresh Kondepudi",
                "Lixiao Huang",
                "Nakul Gopalan"
            ],
            "title": "Continual Skill and Task Learning via Dialogue",
            "abstract": "Continual and interactive robot learning is a challenging problem as the robot is present with human users who expect the robot to learn novel skills to solve novel tasks perpetually with sample efficiency. In this work we present a framework for robots to query and learn visuo-motor robot skills and task relevant information via natural language dialog interactions with human users. Previous approaches either focus on improving the performance of instruction following agents, or passively learn novel skills or concepts. Instead, we used dialog combined with a language-skill grounding embedding to query or confirm skills and/or tasks requested by a user. To achieve this goal, we developed and integrated three different components for our agent. Firstly, we propose a novel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA), which enables the existing SoTA ACT model to perform few-shot continual learning. Secondly, we develop an alignment model that projects demonstrations across skill embodiments into a shared embedding allowing us to know when to ask questions and/or demonstrations from users. Finally, we integrated an existing LLM to interact with a human user to perform grounded interactive continual skill learning to solve a task. Our ACT-LoRA model learns novel fine-tuned skills with a 100% accuracy when trained with only five demonstrations for a novel skill while still maintaining a 74.75% accuracy on pre-trained skills in the RLBench dataset where other models fall significantly short. We also performed a human-subjects study with 8 subjects to demonstrate the continual learning capabilities of our combined framework. We achieve a success rate of 75% in the task of sandwich making with the real robot learning from participant data demonstrating that robots can learn novel skills or task knowledge from dialogue with non-expert users using our approach.",
            "id": "2409.03166",
            "link": "http://arxiv.org/abs/2409.03166v2",
            "published": "2024-09-05T01:51:54+00:00",
            "updated": "2024-09-11T21:52:22+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CL"
            ],
            "max_author_hindex": 15
        },
        "2409.00568": {
            "authors": [
                "Mauricio Vargas Sepulveda"
            ],
            "title": "Welding R and C++: A Tale of Two Programming Languages",
            "abstract": "This article compares `cpp11armadillo` and `cpp11eigen`, new R packages that integrate the powerful Armadillo and Eigen C++ libraries for linear algebra into the R programming environment. This article provides a detailed comparison between Armadillo and Eigen speed and syntax. The goal of these packages is to simplify a part of the process of solving bottlenecks by using C++ within R, these offer additional ease of integration for users who require high-performance linear algebra operations in their R workflows. This document aims to discuss the tradeoff between computational efficiency and accessibility.",
            "id": "2409.00568",
            "link": "http://arxiv.org/abs/2409.00568v2",
            "published": "2024-09-01T00:09:00+00:00",
            "updated": "2024-09-07T15:39:21+00:00",
            "primary_category": "cs.MS",
            "categories": [
                "cs.MS",
                "cs.PL",
                "stat.CO"
            ],
            "max_author_hindex": 0
        },
        "2409.04055": {
            "authors": [
                "Tony Garnock-Jones"
            ],
            "title": "Conversational Concurrency",
            "abstract": "Concurrent computations resemble conversations. In a conversation, participants direct utterances at others and, as the conversation evolves, exploit the known common context to advance the conversation. Similarly, collaborating software components share knowledge with each other in order to make progress as a group towards a common goal.   This dissertation studies concurrency from the perspective of cooperative knowledge-sharing, taking the conversational exchange of knowledge as a central concern in the design of concurrent programming languages. In doing so, it makes five contributions: 1. It develops the idea of a common dataspace as a medium for knowledge exchange among concurrent components, enabling a new approach to concurrent programming. While dataspaces loosely resemble both \"fact spaces\" from the world of Linda-style languages and Erlang's collaborative model, they significantly differ in many details. 2. It offers the first crisp formulation of cooperative, conversational knowledge-exchange as a mathematical model. 3. It describes two faithful implementations of the model for two quite different languages. 4. It proposes a completely novel suite of linguistic constructs for organizing the internal structure of individual actors in a conversational setting. The combination of dataspaces with these constructs is dubbed Syndicate. 5. It presents and analyzes evidence suggesting that the proposed techniques and constructs combine to simplify concurrent programming.   The dataspace concept stands alone in its focus on representation and manipulation of conversational frames and conversational state and in its integral use of explicit epistemic knowledge. The design is particularly suited to integration of general-purpose I/O with otherwise-functional languages, but also applies to actor-like settings more generally.",
            "id": "2409.04055",
            "link": "http://arxiv.org/abs/2409.04055v1",
            "published": "2024-09-06T06:52:40+00:00",
            "updated": "2024-09-06T06:52:40+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL",
                "D.3.3; D.3.1; D.1.3; D.1.1; D.4.7; E.1"
            ],
            "max_author_hindex": 4
        },
        "2409.01088": {
            "authors": [
                "Michail Zervas",
                "Alexandros Karakasidis"
            ],
            "title": "Towards Split Learning-based Privacy-Preserving Record Linkage",
            "abstract": "Split Learning has been recently introduced to facilitate applications where user data privacy is a requirement. However, it has not been thoroughly studied in the context of Privacy-Preserving Record Linkage, a problem in which the same real-world entity should be identified among databases from different dataholders, but without disclosing any additional information. In this paper, we investigate the potentials of Split Learning for Privacy-Preserving Record Matching, by introducing a novel training method through the utilization of Reference Sets, which are publicly available data corpora, showcasing minimal matching impact against a traditional centralized SVM-based technique.",
            "id": "2409.01088",
            "link": "http://arxiv.org/abs/2409.01088v1",
            "published": "2024-09-02T09:17:05+00:00",
            "updated": "2024-09-02T09:17:05+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.DB",
                "cs.LG"
            ],
            "max_author_hindex": 11
        },
        "2409.01102": {
            "authors": [
                "Am\u00e9lie Gheerbrant",
                "Leonid Libkin",
                "Liat Peterfreund",
                "Alexandra Rogova"
            ],
            "title": "GQL and SQL/PGQ: Theoretical Models and Expressive Power",
            "abstract": "SQL/PGQ and GQL are very recent international standards for querying property graphs: SQL/PGQ specifies how to query relational representations of property graphs in SQL, while GQL is a standalone language for graph databases. The rapid industrial development of these standards left the academic community trailing in its wake. While digests of the languages have appeared, we do not yet have concise foundational models like relational algebra and calculus for relational databases that enable the formal study of languages, including their expressiveness and limitations. At the same time, work on the next versions of the standards has already begun, to address the perceived limitations of their first versions.   Motivated by this, we initiate a formal study of SQL/PGQ and GQL, concentrating on their concise formal model and expressiveness. For the former, we define simple core languages -- Core GQL and Core PGQ -- that capture the essence of the new standards, are amenable to theoretical analysis, and fully clarify the difference between PGQ's bottom up evaluation versus GQL's linear, or pipelined approach. Equipped with these models, we both confirm the necessity to extend the language to fill in the expressiveness gaps and identify the source of these deficiencies. We complement our theoretical analysis with an experimental study, demonstrating that existing workarounds in full GQL and PGQ are impractical which further underscores the necessity to correct deficiencies in the language design.",
            "id": "2409.01102",
            "link": "http://arxiv.org/abs/2409.01102v2",
            "published": "2024-09-02T09:31:50+00:00",
            "updated": "2024-09-04T05:54:01+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 52
        },
        "2409.02088": {
            "authors": [
                "Ruihong Wang",
                "Jianguo Wang",
                "Walid G. Aref"
            ],
            "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
            "abstract": "Disaggregating memory from compute offers the opportunity to better utilize stranded memory in data centers. It is important to cache data in the compute nodes and maintain cache coherence across multiple compute nodes to save on round-trip communication cost between the disaggregated memory and the compute nodes. However, the limited computing power on the disaggregated memory servers makes it challenging to maintain cache coherence among multiple compute-side caches over disaggregated shared memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache Coherence protocol that maintains cache coherence without imposing any computational burden on the remote memory side. SELCC builds on a one-sided shared-exclusive latch protocol by introducing lazy latch release and invalidation messages among the compute nodes so that it can guarantee both data access atomicity and cache coherence. SELCC minimizes communication round-trips by embedding the current cache copy holder IDs into RDMA latch words and prioritizes local concurrency control over global concurrency control. We instantiate the SELCC protocol onto compute-sided cache, forming an abstraction layer over disaggregated memory. This abstraction layer provides main-memory-like APIs to upper-level applications, and thus enabling existing data structures and algorithms to function over disaggregated memory with minimal code change. To demonstrate the usability of SELCC, we implement a B-tree and three transaction concurrency control algorithms over SELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves better performance compared to RPC-based cache-coherence protocols. Additionally, YCSB and TPC-C benchmarks indicate that applications over SELCC can achieve comparable or superior performance against competitors over disaggregated memory.",
            "id": "2409.02088",
            "link": "http://arxiv.org/abs/2409.02088v2",
            "published": "2024-09-03T17:40:24+00:00",
            "updated": "2024-09-05T01:12:04+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB",
                "cs.DC",
                "cs.ET"
            ],
            "max_author_hindex": 57
        },
        "2409.05042": {
            "authors": [
                "Van Ho Long",
                "Nguyen Ho",
                "Trinh Le Cong",
                "Anh-Vu Dinh-Duc",
                "Tu Nguyen Ngoc"
            ],
            "title": "Efficient Rare Temporal Pattern Mining in Time Series",
            "abstract": "Time series data from various domains are increasing continuously. Extracting and analyzing the temporal patterns in these series can reveal significant insights. Temporal pattern mining (TPM) extends traditional pattern mining by incorporating event time intervals into extracted patterns, enhancing their expressiveness but increasing time and space complexities. One valuable type of temporal pattern is known as rare temporal patterns (RTPs), which occur rarely but with high confidence. There exist several challenges when mining rare temporal patterns. The support measure is set very low, leading to a further combinatorial explosion and potentially producing too many uninteresting patterns. Thus, an efficient approach to rare temporal pattern mining is needed. This paper introduces our Rare Temporal Pattern Mining from Time Series (RTPMfTS) method for discovering rare temporal patterns, featuring the following key contributions: (1) An end-to-end RTPMfTS process that takes time series data as input and yields rare temporal patterns as output. (2) An efficient Rare Temporal Pattern Mining (RTPM) algorithm that uses optimized data structures for quick event and pattern retrieval and utilizes effective pruning techniques for much faster mining. (3) A thorough experimental evaluation of RTPM, showing that RTPM outperforms the baseline in terms of runtime and memory usage.",
            "id": "2409.05042",
            "link": "http://arxiv.org/abs/2409.05042v1",
            "published": "2024-09-08T09:44:49+00:00",
            "updated": "2024-09-08T09:44:49+00:00",
            "primary_category": "cs.DB",
            "categories": [
                "cs.DB"
            ],
            "max_author_hindex": 24
        },
        "2409.00702": {
            "authors": [
                "Hyunsoo Kim",
                "Junyoung Kim",
                "Minjin Choi",
                "Sunkyung Lee",
                "Jongwuk Lee"
            ],
            "title": "MARS: Matching Attribute-aware Representations for Text-based Sequential Recommendation",
            "abstract": "Sequential recommendation aims to predict the next item a user is likely to prefer based on their sequential interaction history. Recently, text-based sequential recommendation has emerged as a promising paradigm that uses pre-trained language models to exploit textual item features to enhance performance and facilitate knowledge transfer to unseen datasets. However, existing text-based recommender models still struggle with two key challenges: (i) representing users and items with multiple attributes, and (ii) matching items with complex user interests. To address these challenges, we propose a novel model, Matching Attribute-aware Representations for Text-based Sequential Recommendation (MARS). MARS extracts detailed user and item representations through attribute-aware text encoding, capturing diverse user intents with multiple attribute-aware representations. It then computes user-item scores via attribute-wise interaction matching, effectively capturing attribute-level user preferences. Our extensive experiments demonstrate that MARS significantly outperforms existing sequential models, achieving improvements of up to 24.43% and 29.26% in Recall@10 and NDCG@10 across five benchmark datasets. Code is available at https://github.com/junieberry/MARS",
            "id": "2409.00702",
            "link": "http://arxiv.org/abs/2409.00702v2",
            "published": "2024-09-01T12:11:48+00:00",
            "updated": "2024-09-04T13:19:42+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 25
        },
        "2409.00720": {
            "authors": [
                "Yoji Tomita",
                "Tomohiki Yokoyama"
            ],
            "title": "Fair Reciprocal Recommendation in Matching Markets",
            "abstract": "Recommender systems play an increasingly crucial role in shaping people's opportunities, particularly in online dating platforms. It is essential from the user's perspective to increase the probability of matching with a suitable partner while ensuring an appropriate level of fairness in the matching opportunities. We investigate reciprocal recommendation in two-sided matching markets between agents divided into two sides. In our model, a match is considered successful only when both individuals express interest in each other. Additionally, we assume that agents prefer to appear prominently in the recommendation lists presented to those on the other side. We define each agent's opportunity to be recommended and introduce its fairness criterion, envy-freeness, from the perspective of fair division theory. The recommendations that approximately maximize the expected number of matches, empirically obtained by heuristic algorithms, are likely to result in significant unfairness of opportunity. Therefore, there can be a trade-off between maximizing the expected matches and ensuring fairness of opportunity. To address this challenge, we propose a method to find a policy that is close to being envy-free by leveraging the Nash social welfare function. Experiments on synthetic and real-world datasets demonstrate the effectiveness of our approach in achieving both relatively high expected matches and fairness for opportunities of both sides in reciprocal recommender systems.",
            "id": "2409.00720",
            "link": "http://arxiv.org/abs/2409.00720v1",
            "published": "2024-09-01T13:33:41+00:00",
            "updated": "2024-09-01T13:33:41+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 2
        },
        "2409.00851": {
            "authors": [
                "Andreea-Maria Oncescu",
                "Jo\u00e3o F. Henriques",
                "A. Sophia Koepke"
            ],
            "title": "Dissecting Temporal Understanding in Text-to-Audio Retrieval",
            "abstract": "Recent advancements in machine learning have fueled research on multimodal tasks, such as for instance text-to-video and text-to-audio retrieval. These tasks require models to understand the semantic content of video and audio data, including objects, and characters. The models also need to learn spatial arrangements and temporal relationships. In this work, we analyse the temporal ordering of sounds, which is an understudied problem in the context of text-to-audio retrieval. In particular, we dissect the temporal understanding capabilities of a state-of-the-art model for text-to-audio retrieval on the AudioCaps and Clotho datasets. Additionally, we introduce a synthetic text-audio dataset that provides a controlled setting for evaluating temporal capabilities of recent models. Lastly, we present a loss function that encourages text-audio models to focus on the temporal ordering of events. Code and data are available at https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/dtu/.",
            "id": "2409.00851",
            "link": "http://arxiv.org/abs/2409.00851v1",
            "published": "2024-09-01T22:01:21+00:00",
            "updated": "2024-09-01T22:01:21+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 28
        },
        "2409.01140": {
            "authors": [
                "Ziyu Li",
                "Wenjie Zhao",
                "Asterios Katsifodimos",
                "Rihan Hai"
            ],
            "title": "LLM-PQA: LLM-enhanced Prediction Query Answering",
            "abstract": "The advent of Large Language Models (LLMs) provides an opportunity to change the way queries are processed, moving beyond the constraints of conventional SQL-based database systems. However, using an LLM to answer a prediction query is still challenging, since an external ML model has to be employed and inference has to be performed in order to provide an answer. This paper introduces LLM-PQA, a novel tool that addresses prediction queries formulated in natural language. LLM-PQA is the first to combine the capabilities of LLMs and retrieval-augmented mechanism for the needs of prediction queries by integrating data lakes and model zoos. This integration provides users with access to a vast spectrum of heterogeneous data and diverse ML models, facilitating dynamic prediction query answering. In addition, LLM-PQA can dynamically train models on demand, based on specific query requirements, ensuring reliable and relevant results even when no pre-trained model in a model zoo, available for the task.",
            "id": "2409.01140",
            "link": "http://arxiv.org/abs/2409.01140v1",
            "published": "2024-09-02T10:20:35+00:00",
            "updated": "2024-09-02T10:20:35+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 21
        },
        "2409.01192": {
            "authors": [
                "Haohao Qu",
                "Yifeng Zhang",
                "Liangbo Ning",
                "Wenqi Fan",
                "Qing Li"
            ],
            "title": "SSD4Rec: A Structured State Space Duality Model for Efficient Sequential Recommendation",
            "abstract": "Sequential recommendation methods are crucial in modern recommender systems for their remarkable capability to understand a user's changing interests based on past interactions. However, a significant challenge faced by current methods (e.g., RNN- or Transformer-based models) is to effectively and efficiently capture users' preferences by modeling long behavior sequences, which impedes their various applications like short video platforms where user interactions are numerous. Recently, an emerging architecture named Mamba, built on state space models (SSM) with efficient hardware-aware designs, has showcased the tremendous potential for sequence modeling, presenting a compelling avenue for addressing the challenge effectively. Inspired by this, we propose a novel generic and efficient sequential recommendation backbone, SSD4Rec, which explores the seamless adaptation of Mamba for sequential recommendations. Specifically, SSD4Rec marks the variable- and long-length item sequences with sequence registers and processes the item representations with bidirectional Structured State Space Duality (SSD) blocks. This not only allows for hardware-aware matrix multiplication but also empowers outstanding capabilities in variable-length and long-range sequence modeling. Extensive evaluations on four benchmark datasets demonstrate that the proposed model achieves state-of-the-art performance while maintaining near-linear scalability with user sequence length. Our code is publicly available at https://github.com/ZhangYifeng1995/SSD4Rec.",
            "id": "2409.01192",
            "link": "http://arxiv.org/abs/2409.01192v1",
            "published": "2024-09-02T11:58:56+00:00",
            "updated": "2024-09-02T11:58:56+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 9
        },
        "2409.02425": {
            "authors": [
                "Shuaishuai Huang",
                "Haowei Yang",
                "You Yao",
                "Xueting Lin",
                "Yuming Tu"
            ],
            "title": "Deep Adaptive Interest Network: Personalized Recommendation with Context-Aware Learning",
            "abstract": "In personalized recommendation systems, accurately capturing users' evolving interests and combining them with contextual information is a critical research area. This paper proposes a novel model called the Deep Adaptive Interest Network (DAIN), which dynamically models users' interests while incorporating context-aware learning mechanisms to achieve precise and adaptive personalized recommendations. DAIN leverages deep learning techniques to build an adaptive interest network structure that can capture users' interest changes in real-time while further optimizing recommendation results by integrating contextual information. Experiments conducted on several public datasets demonstrate that DAIN excels in both recommendation performance and computational efficiency. This research not only provides a new solution for personalized recommendation systems but also offers fresh insights into the application of context-aware learning in recommendation systems.",
            "id": "2409.02425",
            "link": "http://arxiv.org/abs/2409.02425v1",
            "published": "2024-09-04T04:12:22+00:00",
            "updated": "2024-09-04T04:12:22+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 30
        },
        "2409.02599": {
            "authors": [
                "Ryotaro Shimizu",
                "Yu Wang",
                "Masanari Kimura",
                "Yuki Hirakawa",
                "Takashi Wada",
                "Yuki Saito",
                "Julian McAuley"
            ],
            "title": "A Fashion Item Recommendation Model in Hyperbolic Space",
            "abstract": "In this work, we propose a fashion item recommendation model that incorporates hyperbolic geometry into user and item representations. Using hyperbolic space, our model aims to capture implicit hierarchies among items based on their visual data and users' purchase history. During training, we apply a multi-task learning framework that considers both hyperbolic and Euclidean distances in the loss function. Our experiments on three data sets show that our model performs better than previous models trained in Euclidean space only, confirming the effectiveness of our model. Our ablation studies show that multi-task learning plays a key role, and removing the Euclidean loss substantially deteriorates the model performance.",
            "id": "2409.02599",
            "link": "http://arxiv.org/abs/2409.02599v1",
            "published": "2024-09-04T10:30:11+00:00",
            "updated": "2024-09-04T10:30:11+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.CV",
                "cs.LG"
            ],
            "max_author_hindex": 61
        },
        "2409.03928": {
            "authors": [
                "Tanay Dixit",
                "Daniel Lee",
                "Sally Fang",
                "Sai Sree Harsha",
                "Anirudh Sureshan",
                "Akash Maharaj",
                "Yunyao Li"
            ],
            "title": "RETAIN: Interactive Tool for Regression Testing Guided LLM Migration",
            "abstract": "Large Language Models (LLMs) are increasingly integrated into diverse applications. The rapid evolution of LLMs presents opportunities for developers to enhance applications continuously. However, this constant adaptation can also lead to performance regressions during model migrations. While several interactive tools have been proposed to streamline the complexity of prompt engineering, few address the specific requirements of regression testing for LLM Migrations. To bridge this gap, we introduce RETAIN (REgression Testing guided LLM migrAtIoN), a tool designed explicitly for regression testing in LLM Migrations. RETAIN comprises two key components: an interactive interface tailored to regression testing needs during LLM migrations, and an error discovery module that facilitates understanding of differences in model behaviors. The error discovery module generates textual descriptions of various errors or differences between model outputs, providing actionable insights for prompt refinement. Our automatic evaluation and empirical user studies demonstrate that RETAIN, when compared to manual evaluation, enabled participants to identify twice as many errors, facilitated experimentation with 75% more prompts, and achieves 12% higher metric scores in a given time frame.",
            "id": "2409.03928",
            "link": "http://arxiv.org/abs/2409.03928v1",
            "published": "2024-09-05T22:22:57+00:00",
            "updated": "2024-09-05T22:22:57+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 27
        },
        "2409.04540": {
            "authors": [
                "Jiangxia Cao",
                "Shen Wang",
                "Gaode Chen",
                "Rui Huang",
                "Shuang Yang",
                "Zhaojie Liu",
                "Guorui Zhou"
            ],
            "title": "A Unified Framework for Cross-Domain Recommendation",
            "abstract": "In addressing the persistent challenges of data-sparsity and cold-start issues in domain-expert recommender systems, Cross-Domain Recommendation (CDR) emerges as a promising methodology. CDR aims at enhancing prediction performance in the target domain by leveraging interaction knowledge from related source domains, particularly through users or items that span across multiple domains (e.g., Short-Video and Living-Room). For academic research purposes, there are a number of distinct aspects to guide CDR method designing, including the auxiliary domain number, domain-overlapped element, user-item interaction types, and downstream tasks. With so many different CDR combination scenario settings, the proposed scenario-expert approaches are tailored to address a specific vertical CDR scenario, and often lack the capacity to adapt to multiple horizontal scenarios. In an effect to coherently adapt to various scenarios, and drawing inspiration from the concept of domain-invariant transfer learning, we extend the former SOTA model UniCDR in five different aspects, named as UniCDR+. Our work was successfully deployed on the Kuaishou Living-Room RecSys.",
            "id": "2409.04540",
            "link": "http://arxiv.org/abs/2409.04540v1",
            "published": "2024-09-06T18:10:42+00:00",
            "updated": "2024-09-06T18:10:42+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 60
        },
        "2409.04810": {
            "authors": [
                "Chengbing Wang",
                "Wentao Shi",
                "Jizhi Zhang",
                "Wenjie Wang",
                "Hang Pan",
                "Fuli Feng"
            ],
            "title": "Debias Can be Unreliable: Mitigating Bias Issue in Evaluating Debiasing Recommendation",
            "abstract": "Recent work has improved recommendation models remarkably by equipping them with debiasing methods. Due to the unavailability of fully-exposed datasets, most existing approaches resort to randomly-exposed datasets as a proxy for evaluating debiased models, employing traditional evaluation scheme to represent the recommendation performance. However, in this study, we reveal that traditional evaluation scheme is not suitable for randomly-exposed datasets, leading to inconsistency between the Recall performance obtained using randomly-exposed datasets and that obtained using fully-exposed datasets. Such inconsistency indicates the potential unreliability of experiment conclusions on previous debiasing techniques and calls for unbiased Recall evaluation using randomly-exposed datasets. To bridge the gap, we propose the Unbiased Recall Evaluation (URE) scheme, which adjusts the utilization of randomly-exposed datasets to unbiasedly estimate the true Recall performance on fully-exposed datasets. We provide theoretical evidence to demonstrate the rationality of URE and perform extensive experiments on real-world datasets to validate its soundness.",
            "id": "2409.04810",
            "link": "http://arxiv.org/abs/2409.04810v1",
            "published": "2024-09-07T12:42:58+00:00",
            "updated": "2024-09-07T12:42:58+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 52
        },
        "2409.04827": {
            "authors": [
                "Mingze Wang",
                "Shuxian Bi",
                "Wenjie Wang",
                "Chongming Gao",
                "Yangyang Li",
                "Fuli Feng"
            ],
            "title": "Incorporate LLMs with Influential Recommender System",
            "abstract": "Recommender systems have achieved increasing accuracy over the years. However, this precision often leads users to narrow their interests, resulting in issues such as limited diversity and the creation of echo chambers. Current research addresses these challenges through proactive recommender systems by recommending a sequence of items (called influence path) to guide user interest in the target item. However, existing methods struggle to construct a coherent influence path that builds up with items the user is likely to enjoy. In this paper, we leverage the Large Language Model's (LLMs) exceptional ability for path planning and instruction following, introducing a novel approach named LLM-based Influence Path Planning (LLM-IPP). Our approach maintains coherence between consecutive recommendations and enhances user acceptability of the recommended items. To evaluate LLM-IPP, we implement various user simulators and metrics to measure user acceptability and path coherence. Experimental results demonstrate that LLM-IPP significantly outperforms traditional proactive recommender systems. This study pioneers the integration of LLMs into proactive recommender systems, offering a reliable and user-engaging methodology for future recommendation technologies.",
            "id": "2409.04827",
            "link": "http://arxiv.org/abs/2409.04827v1",
            "published": "2024-09-07T13:41:37+00:00",
            "updated": "2024-09-07T13:41:37+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 52
        },
        "2409.02136": {
            "authors": [
                "Mohammadreza Ghaffarzadeh-Esfahani",
                "Mahdi Ghaffarzadeh-Esfahani",
                "Arian Salahi-Niri",
                "Hossein Toreyhi",
                "Zahra Atf",
                "Amirali Mohsenzadeh-Kermani",
                "Mahshad Sarikhani",
                "Zohreh Tajabadi",
                "Fatemeh Shojaeian",
                "Mohammad Hassan Bagheri",
                "Aydin Feyzi",
                "Mohammadamin Tarighatpayma",
                "Narges Gazmeh",
                "Fateme Heydari",
                "Hossein Afshar",
                "Amirreza Allahgholipour",
                "Farid Alimardani",
                "Ameneh Salehi",
                "Naghmeh Asadimanesh",
                "Mohammad Amin Khalafi",
                "Hadis Shabanipour",
                "Ali Moradi",
                "Sajjad Hossein Zadeh",
                "Omid Yazdani",
                "Romina Esbati",
                "Moozhan Maleki",
                "Danial Samiei Nasr",
                "Amirali Soheili",
                "Hossein Majlesi",
                "Saba Shahsavan",
                "Alireza Soheilipour",
                "Nooshin Goudarzi",
                "Erfan Taherifard",
                "Hamidreza Hatamabadi",
                "Jamil S Samaan",
                "Thomas Savage",
                "Ankit Sakhuja",
                "Ali Soroush",
                "Girish Nadkarni",
                "Ilad Alavi Darazam",
                "Mohamad Amin Pourhoseingholi",
                "Seyed Amir Ahmad Safavi-Naini"
            ],
            "title": "Large Language Models versus Classical Machine Learning: Performance in COVID-19 Mortality Prediction Using High-Dimensional Tabular Data",
            "abstract": "Background: This study aimed to evaluate and compare the performance of classical machine learning models (CMLs) and large language models (LLMs) in predicting mortality associated with COVID-19 by utilizing a high-dimensional tabular dataset.   Materials and Methods: We analyzed data from 9,134 COVID-19 patients collected across four hospitals. Seven CML models, including XGBoost and random forest (RF), were trained and evaluated. The structured data was converted into text for zero-shot classification by eight LLMs, including GPT-4 and Mistral-7b. Additionally, Mistral-7b was fine-tuned using the QLoRA approach to enhance its predictive capabilities.   Results: Among the CML models, XGBoost and RF achieved the highest accuracy, with F1 scores of 0.87 for internal validation and 0.83 for external validation. In the LLM category, GPT-4 was the top performer with an F1 score of 0.43. Fine-tuning Mistral-7b significantly improved its recall from 1% to 79%, resulting in an F1 score of 0.74, which was stable during external validation.   Conclusion: While LLMs show moderate performance in zero-shot classification, fine-tuning can significantly enhance their effectiveness, potentially aligning them closer to CML models. However, CMLs still outperform LLMs in high-dimensional tabular data tasks.",
            "id": "2409.02136",
            "link": "http://arxiv.org/abs/2409.02136v1",
            "published": "2024-09-02T14:51:12+00:00",
            "updated": "2024-09-02T14:51:12+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "92C50, 68T50",
                "J.3"
            ],
            "max_author_hindex": 60
        },
        "2409.00708": {
            "authors": [
                "Doehyun Baek",
                "Jakob Getz",
                "Yusung Sim",
                "Daniel Lehmann",
                "Ben L. Titzer",
                "Sukyoung Ryu",
                "Michael Pradel"
            ],
            "title": "Wasm-R3: Record-Reduce-Replay for Realistic and Standalone WebAssembly Benchmarks",
            "abstract": "WebAssembly (Wasm for short) brings a new, powerful capability to the web as well as Edge, IoT, and embedded systems. Wasm is a portable, compact binary code format with high performance and robust sandboxing properties. As Wasm applications grow in size and importance, the complex performance characteristics of diverse Wasm engines demand robust, representative benchmarks for proper tuning. Stopgap benchmark suites, such as PolyBenchC and libsodium, continue to be used in the literature, though they are known to be unrepresentative. Porting of more complex suites remains difficult because Wasm lacks many system APIs and extracting real-world Wasm benchmarks from the web is difficult due to complex host interactions. To address this challenge, we introduce Wasm-R3, the first record and replay technique for Wasm. Wasm-R3 transparently injects instrumentation into Wasm modules to record an execution trace from inside the module, then reduces the execution trace via several optimizations, and finally produces a replay module that is executable sandalone without any host environment - on any engine. The benchmarks created by our approach are (i) realistic, because the approach records real-world web applications, (ii) faithful to the original execution, because the replay benchmark includes the unmodified original code, only adding emulation of host interactions, and (iii) standalone, because the replay benchmarks run on any engine. Applying Wasm-R3 to web-based Wasm applications in the wild demonstrates the correctness of our approach as well as the effectiveness of our optimizations, which reduce the recorded traces by 99.53 percent and the size of the replay benchmark by 9.98 percent. We release the resulting benchmark suite of 27 applications, called Wasm-R3-Bench, to the community, to inspire a new generation of realistic and standalone Wasm benchmarks.",
            "id": "2409.00708",
            "link": "http://arxiv.org/abs/2409.00708v1",
            "published": "2024-09-01T12:30:06+00:00",
            "updated": "2024-09-01T12:30:06+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL"
            ],
            "max_author_hindex": 39
        },
        "2409.01161": {
            "authors": [
                "Luke Geeson",
                "James Brotherston",
                "Wilco Dijkstra",
                "Alastair F. Donaldson",
                "Lee Smith",
                "Tyler Sorensen",
                "John Wickerson"
            ],
            "title": "Mix Testing: Specifying and Testing ABI Compatibility of C/C++ Atomics Implementations",
            "abstract": "The correctness of complex software depends on the correctness of both the source code and the compilers that generate corresponding binary code. Compilers must do more than preserve the semantics of a single source file: they must ensure that generated binaries can be composed with other binaries to form a final executable. The compatibility of composition is ensured using an Application Binary Interface (ABI), which specifies details of calling conventions, exception handling, and so on. Unfortunately, there are no official ABIs for concurrent programs, so different atomics mappings, although correct in isolation, may induce bugs when composed. Indeed, today, mixing binaries generated by different compilers can lead to an erroneous resulting binary.   We present mix testing: a new technique designed to find compiler bugs when the instructions of a C/C++ test are separately compiled for multiple compatible architectures and then mixed together. We define a class of compiler bugs, coined mixing bugs, that arise when parts of a program are compiled separately using different mappings from C/C++ atomic operations to assembly sequences. To demonstrate the generality of mix testing, we have designed and implemented a tool, atomic-mixer, which we have used: (a) to reproduce one existing non-mixing bug that state-of-the-art concurrency testing tools are limited to being able to find (showing that atomic-mixer at least meets the capabilities of these tools), and (b) to find four previously-unknown mixing bugs in LLVM and GCC, and one prospective mixing bug in mappings proposed for the Java Virtual Machine. Lastly, we have worked with engineers at Arm to specify, for the first time, an atomics ABI for Armv8, and have used atomic-mixer to validate the LLVM and GCC compilers against it.",
            "id": "2409.01161",
            "link": "http://arxiv.org/abs/2409.01161v1",
            "published": "2024-09-02T10:47:11+00:00",
            "updated": "2024-09-02T10:47:11+00:00",
            "primary_category": "cs.PL",
            "categories": [
                "cs.PL",
                "D.3.4; D.2.5"
            ],
            "max_author_hindex": 36
        },
        "2409.00529": {
            "authors": [
                "Ryo Wakizaka",
                "Yasunari Suzuki",
                "Atsushi Igarashi"
            ],
            "title": "Type-Based Verification of Connectivity Constraints in Lattice Surgery",
            "abstract": "Fault-tolerant quantum computation using lattice surgery can be abstracted as operations on graphs, wherein each logical qubit corresponds to a vertex of the graph, and multi-qubit measurements are accomplished by connecting the vertices with paths between them. Operations attempting to connect vertices without a valid path will result in abnormal termination. As the permissible paths may evolve during execution, it is necessary to statically verify that the execution of a quantum program can be completed.   This paper introduces a type-based method to statically verify that well-typed programs can be executed without encountering halts induced by surgery operations. Alongside, we present $\\mathcal{Q}_{LS}$, a first-order quantum programming language to formalize the execution model of surgery operations. Furthermore, we provide a type checking algorithm by reducing the type checking problem to the offline dynamic connectivity problem.",
            "id": "2409.00529",
            "link": "http://arxiv.org/abs/2409.00529v1",
            "published": "2024-08-31T19:31:19+00:00",
            "updated": "2024-08-31T19:31:19+00:00",
            "primary_category": "quant-ph",
            "categories": [
                "quant-ph",
                "cs.PL"
            ],
            "max_author_hindex": 21
        },
        "2409.01804": {
            "authors": [
                "Kartik Kaushik",
                "Raju Halder",
                "Samrat Mondal"
            ],
            "title": "Strengthening Solidity Invariant Generation: From Post- to Pre-Deployment",
            "abstract": "Invariants are essential for ensuring the security and correctness of Solidity smart contracts, particularly in the context of blockchain's immutability and decentralized execution. This paper introduces InvSol, a novel framework for pre-deployment invariant generation tailored specifically for Solidity smart contracts. Unlike existing solutions, namely InvCon, InvCon+, and Trace2Inv, that rely on post-deployment transaction histories on Ethereum mainnet, InvSol identifies invariants before deployment and offers comprehensive coverage of Solidity language constructs, including loops. Additionally, InvSol incorporates custom templates to effectively prevent critical issues such as reentrancy, out-of-gas errors, and exceptions during invariant generation. We rigorously evaluate InvSol using a benchmark set of smart contracts and compare its performance with state-of-the-art solutions. Our findings reveal that InvSol significantly outperforms these tools, demonstrating its effectiveness in handling new contracts with limited transaction histories. Notably, InvSol achieves a 15% improvement in identifying common vulnerabilities compared to InvCon+ and is able to address certain crucial vulnerabilities using specific invariant templates, better than Trace2Inv.",
            "id": "2409.01804",
            "link": "http://arxiv.org/abs/2409.01804v1",
            "published": "2024-09-03T11:37:30+00:00",
            "updated": "2024-09-03T11:37:30+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.PL"
            ],
            "max_author_hindex": 10
        },
        "2409.02094": {
            "authors": [
                "Daniel Baier",
                "Dirk Beyer",
                "Po-Chun Chien",
                "Marie-Christine Jakobs",
                "Marek Jankola",
                "Matthias Kettl",
                "Nian-Ze Lee",
                "Thomas Lemberger",
                "Marian Lingsch-Rosenfeld",
                "Henrik Wachowitz",
                "Philipp Wendler"
            ],
            "title": "Software Verification with CPAchecker 3.0: Tutorial and User Guide (Extended Version)",
            "abstract": "This tutorial provides an introduction to CPAchecker for users. CPAchecker is a flexible and configurable framework for software verification and testing. The framework provides many abstract domains, such as BDDs, explicit values, intervals, memory graphs, and predicates, and many program-analysis and model-checking algorithms, such as abstract interpretation, bounded model checking, Impact, interpolation-based model checking, k -induction, PDR, predicate abstraction, and symbolic execution. This tutorial presents basic use cases for CPAchecker in formal software verification, focusing on its main verification techniques with their strengths and weaknesses. It also shows further use cases of CPAchecker for test-case generation and witness-based result validation. The envisioned readers are assumed to possess a background in automatic formal verification and program analysis, but prior knowledge of CPAchecker is not required. This tutorial and user guide is based on CPAchecker in version 3.0. This user guide's latest version and other documentation are available at https://cpachecker.sosy-lab.org/doc.php.",
            "id": "2409.02094",
            "link": "http://arxiv.org/abs/2409.02094v1",
            "published": "2024-09-03T17:50:50+00:00",
            "updated": "2024-09-03T17:50:50+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.PL",
                "D.2.4; F.3.1; D.3.1; F.4.3"
            ],
            "max_author_hindex": 49
        },
        "2409.04643": {
            "authors": [
                "Matthew P. Harrigan",
                "Tanuj Khattar",
                "Charles Yuan",
                "Anurudh Peduri",
                "Noureldin Yosri",
                "Fionn D. Malone",
                "Ryan Babbush",
                "Nicholas C. Rubin"
            ],
            "title": "Expressing and Analyzing Quantum Algorithms with Qualtran",
            "abstract": "Quantum computing's transition from theory to reality has spurred the need for novel software tools to manage the increasing complexity, sophistication, toil, and fallibility of quantum algorithm development. We present Qualtran, an open-source library for representing and analyzing quantum algorithms. Using appropriate abstractions and data structures, we can simulate and test algorithms, automatically generate information-rich diagrams, and tabulate resource requirements. Qualtran offers a standard library of algorithmic building blocks that are essential for modern cost-minimizing compilations. Its capabilities are showcased through the re-analysis of key algorithms in Hamiltonian simulation, chemistry, and cryptography. Architecture-independent resource counts output by Qualtran can be forwarded to our implementation of cost models to estimate physical costs like wall-clock time and number of physical qubits assuming a surface-code architecture. Qualtran provides a foundation for explicit constructions and reproducible analysis, fostering greater collaboration within the growing quantum algorithm development community.",
            "id": "2409.04643",
            "link": "http://arxiv.org/abs/2409.04643v1",
            "published": "2024-09-06T22:24:59+00:00",
            "updated": "2024-09-06T22:24:59+00:00",
            "primary_category": "quant-ph",
            "categories": [
                "quant-ph",
                "cs.PL"
            ],
            "max_author_hindex": 61
        },
        "2409.02393": {
            "authors": [
                "Peter B. Lerner"
            ],
            "title": "Determination of language families using deep learning",
            "abstract": "We use a c-GAN (convolutional generative adversarial) neural network to analyze transliterated text fragments of extant, dead comprehensible, and one dead non-deciphered (Cypro-Minoan) language to establish linguistic affinities. The paper is agnostic with respect to translation and/or deciphering. However, there is hope that the proposed approach can be useful for decipherment with more sophisticated neural network techniques.",
            "id": "2409.02393",
            "link": "http://arxiv.org/abs/2409.02393v1",
            "published": "2024-09-04T02:41:44+00:00",
            "updated": "2024-09-04T02:41:44+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "I.2.7"
            ],
            "max_author_hindex": 11
        },
        "2409.05249": {
            "authors": [
                "Danyu Sun",
                "Joann Qiongna Chen",
                "Chen Gong",
                "Tianhao Wang",
                "Zhou Li"
            ],
            "title": "NetDPSyn: Synthesizing Network Traces under Differential Privacy",
            "abstract": "As the utilization of network traces for the network measurement research becomes increasingly prevalent, concerns regarding privacy leakage from network traces have garnered the public's attention. To safeguard network traces, researchers have proposed the trace synthesis that retains the essential properties of the raw data. However, previous works also show that synthesis traces with generative models are vulnerable under linkage attacks.   This paper introduces NetDPSyn, the first system to synthesize high-fidelity network traces under privacy guarantees. NetDPSyn is built with the Differential Privacy (DP) framework as its core, which is significantly different from prior works that apply DP when training the generative model. The experiments conducted on three flow and two packet datasets indicate that NetDPSyn achieves much better data utility in downstream tasks like anomaly detection. NetDPSyn is also 2.5 times faster than the other methods on average in data synthesis.",
            "id": "2409.05249",
            "link": "http://arxiv.org/abs/2409.05249v1",
            "published": "2024-09-08T23:54:00+00:00",
            "updated": "2024-09-08T23:54:00+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.DB",
                "cs.NI"
            ],
            "max_author_hindex": 27
        },
        "2409.00400": {
            "authors": [
                "Qiang Zhang",
                "Zhipeng Teng",
                "Disheng Wu",
                "Jiayin Wang"
            ],
            "title": "An Enhanced Batch Query Architecture in Real-time Recommendation",
            "abstract": "In industrial recommendation systems on websites and apps, it is essential to recall and predict top-n results relevant to user interests from a content pool of billions within milliseconds. To cope with continuous data growth and improve real-time recommendation performance, we have designed and implemented a high-performance batch query architecture for real-time recommendation systems. Our contributions include optimizing hash structures with a cacheline-aware probing method to enhance coalesced hashing, as well as the implementation of a hybrid storage key-value service built upon it. Our experiments indicate this approach significantly surpasses conventional hash tables in batch query throughput, achieving up to 90% of the query throughput of random memory access when incorporating parallel optimization. The support for NVMe, integrating two-tier storage for hot and cold data, notably reduces resource consumption. Additionally, the system facilitates dynamic updates, automated sharding of attributes and feature embedding tables, and introduces innovative protocols for consistency in batch queries, thereby enhancing the effectiveness of real-time incremental learning updates. This architecture has been deployed and in use in the bilibili recommendation system for over a year, a video content community with hundreds of millions of users, supporting 10x increase in model computation with minimal resource growth, improving outcomes while preserving the system's real-time performance.",
            "id": "2409.00400",
            "link": "http://arxiv.org/abs/2409.00400v1",
            "published": "2024-08-31T09:19:41+00:00",
            "updated": "2024-08-31T09:19:41+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.LG",
                "C.3, H.3.3"
            ],
            "max_author_hindex": 22
        },
        "2409.00636": {
            "authors": [
                "Yunxiao Shi",
                "Min Xu",
                "Haimin Zhang",
                "Xing Zi",
                "Qiang Wu"
            ],
            "title": "A Learnable Agent Collaboration Network Framework for Personalized Multimodal AI Search Engine",
            "abstract": "Large language models (LLMs) and retrieval-augmented generation (RAG) techniques have revolutionized traditional information access, enabling AI agent to search and summarize information on behalf of users during dynamic dialogues. Despite their potential, current AI search engines exhibit considerable room for improvement in several critical areas. These areas include the support for multimodal information, the delivery of personalized responses, the capability to logically answer complex questions, and the facilitation of more flexible interactions. This paper proposes a novel AI Search Engine framework called the Agent Collaboration Network (ACN). The ACN framework consists of multiple specialized agents working collaboratively, each with distinct roles such as Account Manager, Solution Strategist, Information Manager, and Content Creator. This framework integrates mechanisms for picture content understanding, user profile tracking, and online evolution, enhancing the AI search engine's response quality, personalization, and interactivity. A highlight of the ACN is the introduction of a Reflective Forward Optimization method (RFO), which supports the online synergistic adjustment among agents. This feature endows the ACN with online learning capabilities, ensuring that the system has strong interactive flexibility and can promptly adapt to user feedback. This learning method may also serve as an optimization approach for agent-based systems, potentially influencing other domains of agent applications.",
            "id": "2409.00636",
            "link": "http://arxiv.org/abs/2409.00636v1",
            "published": "2024-09-01T07:01:22+00:00",
            "updated": "2024-09-01T07:01:22+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.MA"
            ],
            "max_author_hindex": 51
        },
        "2409.00860": {
            "authors": [
                "Bhavik Chandna",
                "Procheta Sen"
            ],
            "title": "A Counterfactual Explanation Framework for Retrieval Models",
            "abstract": "Explainability has become a crucial concern in today's world, aiming to enhance transparency in machine learning and deep learning models. Information retrieval is no exception to this trend. In existing literature on explainability of information retrieval, the emphasis has predominantly been on illustrating the concept of relevance concerning a retrieval model. The questions addressed include why a document is relevant to a query, why one document exhibits higher relevance than another, or why a specific set of documents is deemed relevant for a query.   However, limited attention has been given to understanding why a particular document is considered non-relevant to a query with respect to a retrieval model. In an effort to address this gap, our work focus on the question of what terms need to be added within a document to improve its ranking. This in turn answers the question of which words played a role in not being favored by a retrieval model for a particular query. We use an optimization framework to solve the above-mentioned research problem. % To the best of our knowledge, we mark the first attempt to tackle this specific counterfactual problem. Our experiments show the effectiveness of our proposed approach in predicting counterfactuals for both statistical (e.g. BM25) and deep-learning-based models (e.g. DRMM, DSSM, ColBERT).",
            "id": "2409.00860",
            "link": "http://arxiv.org/abs/2409.00860v2",
            "published": "2024-09-01T22:33:29+00:00",
            "updated": "2024-09-10T10:52:30+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 14
        },
        "2409.01012": {
            "authors": [
                "Shilong Bao",
                "Qianqian Xu",
                "Zhiyong Yang",
                "Yuan He",
                "Xiaochun Cao",
                "Qingming Huang"
            ],
            "title": "Improved Diversity-Promoting Collaborative Metric Learning for Recommendation",
            "abstract": "Collaborative Metric Learning (CML) has recently emerged as a popular method in recommendation systems (RS), closing the gap between metric learning and collaborative filtering. Following the convention of RS, existing practices exploit unique user representation in their model design. This paper focuses on a challenging scenario where a user has multiple categories of interests. Under this setting, the unique user representation might induce preference bias, especially when the item category distribution is imbalanced. To address this issue, we propose a novel method called \\textit{Diversity-Promoting Collaborative Metric Learning} (DPCML), with the hope of considering the commonly ignored minority interest of the user. The key idea behind DPCML is to introduce a set of multiple representations for each user in the system where users' preference toward an item is aggregated by taking the minimum item-user distance among their embedding set. Specifically, we instantiate two effective assignment strategies to explore a proper quantity of vectors for each user. Meanwhile, a \\textit{Diversity Control Regularization Scheme} (DCRS) is developed to accommodate the multi-vector representation strategy better. Theoretically, we show that DPCML could induce a smaller generalization error than traditional CML. Furthermore, we notice that CML-based approaches usually require \\textit{negative sampling} to reduce the heavy computational burden caused by the pairwise objective therein. In this paper, we reveal the fundamental limitation of the widely adopted hard-aware sampling from the One-Way Partial AUC (OPAUC) perspective and then develop an effective sampling alternative for the CML-based paradigm. Finally, comprehensive experiments over a range of benchmark datasets speak to the efficacy of DPCML. Code are available at \\url{https://github.com/statusrank/LibCML}.",
            "id": "2409.01012",
            "link": "http://arxiv.org/abs/2409.01012v1",
            "published": "2024-09-02T07:44:48+00:00",
            "updated": "2024-09-02T07:44:48+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 62
        },
        "2409.01563": {
            "authors": [
                "Jianhai Chen",
                "Yanlin Wu",
                "Dazhong Rong",
                "Guoyao Yu",
                "Lingqi Jiang",
                "Zhenguang Liu",
                "Peng Zhou",
                "Rui Shen"
            ],
            "title": "Blockchain-based Federated Recommendation with Incentive Mechanism",
            "abstract": "Nowadays, federated recommendation technology is rapidly evolving to help multiple organisations share data and train models while meeting user privacy, data security and government regulatory requirements. However, federated recommendation increases customer system costs such as power, computational and communication resources. Besides, federated recommendation systems are also susceptible to model attacks and data poisoning by participating malicious clients. Therefore, most customers are unwilling to participate in federated recommendation without any incentive. To address these problems, we propose a blockchain-based federated recommendation system with incentive mechanism to promote more trustworthy, secure, and efficient federated recommendation service. First, we construct a federated recommendation system based on NeuMF and FedAvg. Then we introduce a reverse auction mechanism to select optimal clients that can maximize the social surplus. Finally, we employ blockchain for on-chain evidence storage of models to ensure the safety of the federated recommendation system. The experimental results show that our proposed incentive mechanism can attract clients with superior training data to engage in the federal recommendation at a lower cost, which can increase the economic benefit of federal recommendation by 54.9\\% while improve the recommendation performance. Thus our work provides theoretical and technological support for the construction of a harmonious and healthy ecological environment for the application of federal recommendation.",
            "id": "2409.01563",
            "link": "http://arxiv.org/abs/2409.01563v1",
            "published": "2024-09-03T03:00:59+00:00",
            "updated": "2024-09-03T03:00:59+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 25
        },
        "2409.03294": {
            "authors": [
                "Li Wang",
                "Quangui Zhang",
                "Lei Sang",
                "Qiang Wu",
                "Min Xu"
            ],
            "title": "Federated Prototype-based Contrastive Learning for Privacy-Preserving Cross-domain Recommendation",
            "abstract": "Cross-domain recommendation (CDR) aims to improve recommendation accuracy in sparse domains by transferring knowledge from data-rich domains. However, existing CDR methods often assume the availability of user-item interaction data across domains, overlooking user privacy concerns. Furthermore, these methods suffer from performance degradation in scenarios with sparse overlapping users, as they typically depend on a large number of fully shared users for effective knowledge transfer. To address these challenges, we propose a Federated Prototype-based Contrastive Learning (CL) method for Privacy-Preserving CDR, named FedPCL-CDR. This approach utilizes non-overlapping user information and prototypes to improve multi-domain performance while protecting user privacy. FedPCL-CDR comprises two modules: local domain (client) learning and global server aggregation. In the local domain, FedPCL-CDR clusters all user data to learn representative prototypes, effectively utilizing non-overlapping user information and addressing the sparse overlapping user issue. It then facilitates knowledge transfer by employing both local and global prototypes returned from the server in a CL manner. Simultaneously, the global server aggregates representative prototypes from local domains to learn both local and global prototypes. The combination of prototypes and federated learning (FL) ensures that sensitive user data remains decentralized, with only prototypes being shared across domains, thereby protecting user privacy. Extensive experiments on four CDR tasks using two real-world datasets demonstrate that FedPCL-CDR outperforms the state-of-the-art baselines.",
            "id": "2409.03294",
            "link": "http://arxiv.org/abs/2409.03294v1",
            "published": "2024-09-05T06:59:56+00:00",
            "updated": "2024-09-05T06:59:56+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 38
        },
        "2409.03449": {
            "authors": [
                "Miao Fan",
                "Jiacheng Guo",
                "Shuai Zhu",
                "Shuo Miao",
                "Mingming Sun",
                "Ping Li"
            ],
            "title": "MOBIUS: Towards the Next Generation of Query-Ad Matching in Baidu's Sponsored Search",
            "abstract": "Baidu runs the largest commercial web search engine in China, serving hundreds of millions of online users every day in response to a great variety of queries. In order to build a high-efficiency sponsored search engine, we used to adopt a three-layer funnel-shaped structure to screen and sort hundreds of ads from billions of ad candidates subject to the requirement of low response latency and the restraints of computing resources. Given a user query, the top matching layer is responsible for providing semantically relevant ad candidates to the next layer, while the ranking layer at the bottom concerns more about business indicators (e.g., CPM, ROI, etc.) of those ads. The clear separation between the matching and ranking objectives results in a lower commercial return. The Mobius project has been established to address this serious issue. It is our first attempt to train the matching layer to consider CPM as an additional optimization objective besides the query-ad relevance, via directly predicting CTR (click-through rate) from billions of query-ad pairs. Specifically, this paper will elaborate on how we adopt active learning to overcome the insufficiency of click history at the matching layer when training our neural click networks offline, and how we use the SOTA ANN search technique for retrieving ads more efficiently (Here ``ANN'' stands for approximate nearest neighbor search). We contribute the solutions to Mobius-V1 as the first version of our next generation query-ad matching system.",
            "id": "2409.03449",
            "link": "http://arxiv.org/abs/2409.03449v1",
            "published": "2024-09-05T11:56:40+00:00",
            "updated": "2024-09-05T11:56:40+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 18
        },
        "2409.04329": {
            "authors": [
                "Davide Abbattista",
                "Vito Walter Anelli",
                "Tommaso Di Noia",
                "Craig Macdonald",
                "Aleksandr Vladimirovich Petrov"
            ],
            "title": "Enhancing Sequential Music Recommendation with Personalized Popularity Awareness",
            "abstract": "In the realm of music recommendation, sequential recommender systems have shown promise in capturing the dynamic nature of music consumption. Nevertheless, traditional Transformer-based models, such as SASRec and BERT4Rec, while effective, encounter challenges due to the unique characteristics of music listening habits. In fact, existing models struggle to create a coherent listening experience due to rapidly evolving preferences. Moreover, music consumption is characterized by a prevalence of repeated listening, i.e., users frequently return to their favourite tracks, an important signal that could be framed as individual or personalized popularity.   This paper addresses these challenges by introducing a novel approach that incorporates personalized popularity information into sequential recommendation. By combining user-item popularity scores with model-generated scores, our method effectively balances the exploration of new music with the satisfaction of user preferences. Experimental results demonstrate that a Personalized Most Popular recommender, a method solely based on user-specific popularity, outperforms existing state-of-the-art models. Furthermore, augmenting Transformer-based models with personalized popularity awareness yields superior performance, showing improvements ranging from 25.2% to 69.8%. The code for this paper is available at https://github.com/sisinflab/personalized-popularity-awareness.",
            "id": "2409.04329",
            "link": "http://arxiv.org/abs/2409.04329v1",
            "published": "2024-09-06T15:05:12+00:00",
            "updated": "2024-09-06T15:05:12+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 52
        },
        "2409.04339": {
            "authors": [
                "Daniele Malitesta",
                "Giacomo Medda",
                "Erasmo Purificato",
                "Ludovico Boratto",
                "Fragkiskos D. Malliaros",
                "Mirko Marras",
                "Ernesto William De Luca"
            ],
            "title": "How Fair is Your Diffusion Recommender Model?",
            "abstract": "Diffusion-based recommender systems have recently proven to outperform traditional generative recommendation approaches, such as variational autoencoders and generative adversarial networks. Nevertheless, the machine learning literature has raised several concerns regarding the possibility that diffusion models, while learning the distribution of data samples, may inadvertently carry information bias and lead to unfair outcomes. In light of this aspect, and considering the relevance that fairness has held in recommendations over the last few decades, we conduct one of the first fairness investigations in the literature on DiffRec, a pioneer approach in diffusion-based recommendation. First, we propose an experimental setting involving DiffRec (and its variant L-DiffRec) along with nine state-of-the-art recommendation models, two popular recommendation datasets from the fairness-aware literature, and six metrics accounting for accuracy and consumer/provider fairness. Then, we perform a twofold analysis, one assessing models' performance under accuracy and recommendation fairness separately, and the other identifying if and to what extent such metrics can strike a performance trade-off. Experimental results from both studies confirm the initial unfairness warnings but pave the way for how to address them in future research directions.",
            "id": "2409.04339",
            "link": "http://arxiv.org/abs/2409.04339v1",
            "published": "2024-09-06T15:17:40+00:00",
            "updated": "2024-09-06T15:17:40+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 26
        },
        "2409.02137": {
            "authors": [
                "Andrea Borgarelli",
                "Constantin Enea",
                "Rupak Majumdar",
                "Srinidhi Nagendra"
            ],
            "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed Systems",
            "abstract": "Bugs in popular distributed protocol implementations have been the source of many downtimes in popular internet services. We describe a randomized testing approach for distributed protocol implementations based on reinforcement learning. Since the natural reward structure is very sparse, the key to successful exploration in reinforcement learning is reward augmentation. We show two different techniques that build on one another. First, we provide a decaying exploration bonus based on the discovery of new states -- the reward decays as the same state is visited multiple times. The exploration bonus captures the intuition from coverage-guided fuzzing of prioritizing new coverage points; in contrast to other schemes, we show that taking the maximum of the bonus and the Q-value leads to more effective exploration. Second, we provide waypoints to the algorithm as a sequence of predicates that capture interesting semantic scenarios. Waypoints exploit designer insight about the protocol and guide the exploration to ``interesting'' parts of the state space. Our reward structure ensures that new episodes can reliably get to deep interesting states even without execution caching. We have implemented our algorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and RSL) shows that our algorithm can significantly outperform baseline approaches in terms of coverage and bug finding.",
            "id": "2409.02137",
            "link": "http://arxiv.org/abs/2409.02137v1",
            "published": "2024-09-02T15:07:05+00:00",
            "updated": "2024-09-02T15:07:05+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.DC",
                "cs.LG",
                "cs.PL"
            ],
            "max_author_hindex": 59
        },
        "2409.02294": {
            "authors": [
                "James McKevitt",
                "Eduard I. Vorobyov",
                "Igor Kulikov"
            ],
            "title": "Accelerating Fortran Codes: A Method for Integrating Coarray Fortran with CUDA Fortran and OpenMP",
            "abstract": "Fortran's prominence in scientific computing requires strategies to ensure both that legacy codes are efficient on high-performance computing systems, and that the language remains attractive for the development of new high-performance codes. Coarray Fortran (CAF), part of the Fortran 2008 standard introduced for parallel programming, facilitates distributed memory parallelism with a syntax familiar to Fortran programmers, simplifying the transition from single-processor to multi-processor coding. This research focuses on innovating and refining a parallel programming methodology that fuses the strengths of Intel Coarray Fortran, Nvidia CUDA Fortran, and OpenMP for distributed memory parallelism, high-speed GPU acceleration and shared memory parallelism respectively. We consider the management of pageable and pinned memory, CPU-GPU affinity in NUMA multiprocessors, and robust compiler interfacing with speed optimisation. We demonstrate our method through its application to a parallelised Poisson solver and compare the methodology, implementation, and scaling performance to that of the Message Passing Interface (MPI), finding CAF offers similar speeds with easier implementation. For new codes, this approach offers a faster route to optimised parallel computing. For legacy codes, it eases the transition to parallel computing, allowing their transformation into scalable, high-performance computing applications without the need for extensive re-design or additional syntax.",
            "id": "2409.02294",
            "link": "http://arxiv.org/abs/2409.02294v1",
            "published": "2024-09-03T21:15:25+00:00",
            "updated": "2024-09-03T21:15:25+00:00",
            "primary_category": "astro-ph.IM",
            "categories": [
                "astro-ph.IM",
                "astro-ph.SR",
                "cs.DC",
                "cs.PL"
            ],
            "max_author_hindex": 33
        },
        "2409.03119": {
            "authors": [
                "Varun Rao",
                "Zachary D. Sisco"
            ],
            "title": "Register Aggregation for Hardware Decompilation",
            "abstract": "Hardware decompilation reverses logic synthesis, converting a gate-level digital electronic design, or netlist, back up to hardware description language (HDL) code. Existing techniques decompile data-oriented features in netlists, like loops and modules, but struggle with sequential logic. In particular, they cannot decompile memory elements, which pose difficulty due to their deconstruction into individual bits and the feedback loops they form in the netlist. Recovering multi-bit registers and memory blocks from netlists would expand the applications of hardware decompilation, notably towards retargeting technologies (e.g. FPGAs to ASICs) and decompiling processor memories. We devise a method for register aggregation, to identify relationships between the data flip-flops in a netlist and group them into registers and memory blocks, resulting in HDL code that instantiates these memory elements. We aggregate flip-flops by identifying common enable pins, and derive the bit-order of the resulting registers using functional dependencies. This scales similarly to memory blocks, where we repeat the algorithm in the second dimension with special attention to the read, write, and address ports of each memory block. We evaluate our technique over a dataset of 13 gate-level netlists, comprising circuits from binary multipliers to CPUs, and we compare the quantity and widths of recovered registers and memory blocks with the original source code. The technique successfully recovers memory elements in all of the tested circuits, even aggregating beyond the source code expectation. In 10 / 13 circuits, all source code memory elements are accounted for, and we are able to compact up to 2048 disjoint bits into a single memory block.",
            "id": "2409.03119",
            "link": "http://arxiv.org/abs/2409.03119v1",
            "published": "2024-09-04T23:06:13+00:00",
            "updated": "2024-09-04T23:06:13+00:00",
            "primary_category": "cs.AR",
            "categories": [
                "cs.AR",
                "cs.PL"
            ],
            "max_author_hindex": 4
        },
        "2409.04597": {
            "authors": [
                "Sally Junsong Wang",
                "Jianan Yao",
                "Kexin Pei",
                "Hidedaki Takahashi",
                "Junfeng Yang"
            ],
            "title": "Detecting Buggy Contracts via Smart Testing",
            "abstract": "Smart contracts are susceptible to critical vulnerabilities. Hybrid dynamic analyses, such as concolic execution assisted fuzzing and foundation model assisted fuzzing, have emerged as highly effective testing techniques for smart contract bug detection recently. This hybrid approach has shown initial promise in real-world benchmarks, but it still suffers from low scalability to find deep bugs buried in complex code patterns. We observe that performance bottlenecks of existing dynamic analyses and model hallucination are two main factors limiting the scalability of this hybrid approach in finding deep bugs.   To overcome the challenges, we design an interactive, self-deciding foundation model based system, called SmartSys, to support hybrid smart contract dynamic analyses. The key idea is to teach foundation models about performance bottlenecks of different dynamic analysis techniques, making it possible to forecast the right technique and generates effective fuzz targets that can reach deep, hidden bugs. To prune hallucinated, incorrect fuzz targets, SmartSys feeds foundation models with feedback from dynamic analysis during compilation and at runtime.   The interesting results of SmartSys include: i) discovering a smart contract protocol vulnerability that has escaped eleven tools and survived multiple audits for over a year; ii) improving coverage by up to 14.3\\% on real-world benchmarks compared to the baselines.",
            "id": "2409.04597",
            "link": "http://arxiv.org/abs/2409.04597v1",
            "published": "2024-09-06T20:09:01+00:00",
            "updated": "2024-09-06T20:09:01+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.LG",
                "cs.PL"
            ],
            "max_author_hindex": 18
        },
        "2409.04711": {
            "authors": [
                "Stefanos Nikolaidis"
            ],
            "title": "Algorithmic Scenario Generation as Quality Diversity Optimization",
            "abstract": "The increasing complexity of robots and autonomous agents that interact with people highlights the critical need for approaches that systematically test them before deployment. This review paper presents a general framework for solving this problem, describes the insights that we have gained from working on each component of the framework, and shows how integrating these components leads to the discovery of a diverse range of realistic and challenging scenarios that reveal previously unknown failures in deployed robotic systems interacting with people.",
            "id": "2409.04711",
            "link": "http://arxiv.org/abs/2409.04711v1",
            "published": "2024-09-07T05:20:41+00:00",
            "updated": "2024-09-07T05:20:41+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.04887": {
            "authors": [
                "Yiwen Ding",
                "Krishna Manoorkar",
                "Ni Wayan Switrayni",
                "Ruoding Wang"
            ],
            "title": "Defeasible Reasoning on Concepts",
            "abstract": "In this paper, we take first steps toward developing defeasible reasoning on concepts in KLM framework. We define generalizations of cumulative reasoning system C and cumulative reasoning system with loop CL to conceptual setting. We also generalize cumulative models, cumulative ordered models, and preferential models to conceptual setting and show the soundness and completeness results for these models.",
            "id": "2409.04887",
            "link": "http://arxiv.org/abs/2409.04887v1",
            "published": "2024-09-07T19:08:17+00:00",
            "updated": "2024-09-07T19:08:17+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LO"
            ],
            "max_author_hindex": 14
        },
        "2409.00352": {
            "authors": [
                "Hongseok Oh",
                "Wonseok Hwang"
            ],
            "title": "Does Alignment Tuning Really Break LLMs' Internal Confidence?",
            "abstract": "Large Language Models (LLMs) have shown remarkable progress, but their real-world application necessitates reliable calibration. This study conducts a comprehensive analysis of calibration degradation of LLMs across four dimensions: models, calibration metrics, tasks, and confidence extraction methods. Initial analysis showed that the relationship between alignment and calibration is not always a trade-off, but under stricter analysis conditions, we found the alignment process consistently harms calibration. This highlights the need for (1) a careful approach when measuring model confidences and calibration errors and (2) future research into algorithms that can help LLMs to achieve both instruction-following and calibration without sacrificing either.",
            "id": "2409.00352",
            "link": "http://arxiv.org/abs/2409.00352v1",
            "published": "2024-08-31T05:12:36+00:00",
            "updated": "2024-08-31T05:12:36+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 15
        },
        "2409.00872": {
            "authors": [
                "Xuechen Liang",
                "Meiling Tao",
                "Yinghui Xia",
                "Tianyu Shi",
                "Jun Wang",
                "JingSong Yang"
            ],
            "title": "Self-evolving Agents with reflective and memory-augmented abilities",
            "abstract": "Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making. In this research, we propose a novel framework by integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents' capabilities in handling multi-tasking and long-span information.",
            "id": "2409.00872",
            "link": "http://arxiv.org/abs/2409.00872v1",
            "published": "2024-09-01T23:36:34+00:00",
            "updated": "2024-09-01T23:36:34+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 25
        },
        "2409.00965": {
            "authors": [
                "Vincent Wilmet",
                "Johnson Du"
            ],
            "title": "What does it take to get state of the art in simultaneous speech-to-speech translation?",
            "abstract": "This paper presents an in-depth analysis of the latency characteristics observed in simultaneous speech-to-speech model's performance, particularly focusing on hallucination-induced latency spikes. By systematically experimenting with various input parameters and conditions, we propose methods to minimize latency spikes and improve overall performance. The findings suggest that a combination of careful input management and strategic parameter adjustments can significantly enhance speech-to-speech model's latency behavior.",
            "id": "2409.00965",
            "link": "http://arxiv.org/abs/2409.00965v1",
            "published": "2024-09-02T06:04:07+00:00",
            "updated": "2024-09-02T06:04:07+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 2
        },
        "2409.01882": {
            "authors": [
                "Ray Umphrey",
                "Jesse Roberts",
                "Lindsey Roberts"
            ],
            "title": "Investigating Expert-in-the-Loop LLM Discourse Patterns for Ancient Intertextual Analysis",
            "abstract": "This study explores the potential of large language models (LLMs) for identifying and examining intertextual relationships within biblical, Koine Greek texts. By evaluating the performance of LLMs on various intertextuality scenarios the study demonstrates that these models can detect direct quotations, allusions, and echoes between texts. The LLM's ability to generate novel intertextual observations and connections highlights its potential to uncover new insights. However, the model also struggles with long query passages and the inclusion of false intertextual dependences, emphasizing the importance of expert evaluation. The expert-in-the-loop methodology presented offers a scalable approach for intertextual research into the complex web of intertextuality within and beyond the biblical corpus.",
            "id": "2409.01882",
            "link": "http://arxiv.org/abs/2409.01882v1",
            "published": "2024-09-03T13:23:11+00:00",
            "updated": "2024-09-03T13:23:11+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 20
        },
        "2409.02865": {
            "authors": [
                "Leanne Nortje"
            ],
            "title": "Visually Grounded Speech Models for Low-resource Languages and Cognitive Modelling",
            "abstract": "This dissertation examines visually grounded speech (VGS) models that learn from unlabelled speech paired with images. It focuses on applications for low-resource languages and understanding human language acquisition. We introduce a task called visually prompted keyword localisation to detect and localise keywords in speech using images. We demonstrate the effectiveness of VGS models in few-shot learning scenarios for low-resource languages like Yoruba. Additionally, we examine the mutual exclusivity bias in VGS models. Our monolingual VGS model exhibits this bias, but we found that multilingualism does not affect the bias in this VGS model similarly to what is observed in children.",
            "id": "2409.02865",
            "link": "http://arxiv.org/abs/2409.02865v1",
            "published": "2024-09-03T17:59:50+00:00",
            "updated": "2024-09-03T17:59:50+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.CV"
            ],
            "max_author_hindex": 5
        },
        "2409.03046": {
            "authors": [
                "Filip Grali\u0144ski",
                "Ryszard Staruch",
                "Krzysztof Jurkiewicz"
            ],
            "title": "Oddballness: universal anomaly detection with language models",
            "abstract": "We present a new method to detect anomalies in texts (in general: in sequences of any data), using language models, in a totally unsupervised manner. The method considers probabilities (likelihoods) generated by a language model, but instead of focusing on low-likelihood tokens, it considers a new metric introduced in this paper: oddballness. Oddballness measures how ``strange'' a given token is according to the language model. We demonstrate in grammatical error detection tasks (a specific case of text anomaly detection) that oddballness is better than just considering low-likelihood events, if a totally unsupervised setup is assumed.",
            "id": "2409.03046",
            "link": "http://arxiv.org/abs/2409.03046v1",
            "published": "2024-09-04T19:31:20+00:00",
            "updated": "2024-09-04T19:31:20+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 6
        },
        "2409.03238": {
            "authors": [
                "Abdul Rehman",
                "Jian Jun Zhang",
                "Xiaosong Yang"
            ],
            "title": "Preserving Empirical Probabilities in BERT for Small-sample Clinical Entity Recognition",
            "abstract": "Named Entity Recognition (NER) encounters the challenge of unbalanced labels, where certain entity types are overrepresented while others are underrepresented in real-world datasets. This imbalance can lead to biased models that perform poorly on minority entity classes, impeding accurate and equitable entity recognition. This paper explores the effects of unbalanced entity labels of the BERT-based pre-trained model. We analyze the different mechanisms of loss calculation and loss propagation for the task of token classification on randomized datasets. Then we propose ways to improve the token classification for the highly imbalanced task of clinical entity recognition.",
            "id": "2409.03238",
            "link": "http://arxiv.org/abs/2409.03238v1",
            "published": "2024-09-05T04:38:49+00:00",
            "updated": "2024-09-05T04:38:49+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG",
                "68T50",
                "I.2.7"
            ],
            "max_author_hindex": 30
        },
        "2409.03946": {
            "authors": [
                "Banooqa Banday",
                "Kowshik Thopalli",
                "Tanzima Z. Islam",
                "Jayaraman J. Thiagarajan"
            ],
            "title": "On The Role of Prompt Construction In Enhancing Efficacy and Efficiency of LLM-Based Tabular Data Generation",
            "abstract": "LLM-based data generation for real-world tabular data can be challenged by the lack of sufficient semantic context in feature names used to describe columns. We hypothesize that enriching prompts with domain-specific insights can improve both the quality and efficiency of data generation. To test this hypothesis, we explore three prompt construction protocols: Expert-guided, LLM-guided, and Novel-Mapping. Through empirical studies with the recently proposed GReaT framework, we find that context-enriched prompts lead to significantly improved data generation quality and training efficiency.",
            "id": "2409.03946",
            "link": "http://arxiv.org/abs/2409.03946v1",
            "published": "2024-09-06T00:02:09+00:00",
            "updated": "2024-09-06T00:02:09+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 26
        },
        "2409.04269": {
            "authors": [
                "Mukhammadsaid Mamasaidov",
                "Abror Shopulatov"
            ],
            "title": "Open Language Data Initiative: Advancing Low-Resource Machine Translation for Karakalpak",
            "abstract": "This study presents several contributions for the Karakalpak language: a FLORES+ devtest dataset translated to Karakalpak, parallel corpora for Uzbek-Karakalpak, Russian-Karakalpak and English-Karakalpak of 100,000 pairs each and open-sourced fine-tuned neural models for translation across these languages. Our experiments compare different model variants and training approaches, demonstrating improvements over existing baselines. This work, conducted as part of the Open Language Data Initiative (OLDI) shared task, aims to advance machine translation capabilities for Karakalpak and contribute to expanding linguistic diversity in NLP technologies.",
            "id": "2409.04269",
            "link": "http://arxiv.org/abs/2409.04269v1",
            "published": "2024-09-06T13:25:18+00:00",
            "updated": "2024-09-06T13:25:18+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 1
        },
        "2409.04593": {
            "authors": [
                "Guanyu Lin",
                "Tao Feng",
                "Pengrui Han",
                "Ge Liu",
                "Jiaxuan You"
            ],
            "title": "Paper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance",
            "abstract": "As scientific research proliferates, researchers face the daunting task of navigating and reading vast amounts of literature. Existing solutions, such as document QA, fail to provide personalized and up-to-date information efficiently. We present Paper Copilot, a self-evolving, efficient LLM system designed to assist researchers, based on thought-retrieval, user profile and high performance optimization. Specifically, Paper Copilot can offer personalized research services, maintaining a real-time updated database. Quantitative evaluation demonstrates that Paper Copilot saves 69.92\\% of time after efficient deployment. This paper details the design and implementation of Paper Copilot, highlighting its contributions to personalized academic support and its potential to streamline the research process.",
            "id": "2409.04593",
            "link": "http://arxiv.org/abs/2409.04593v1",
            "published": "2024-09-06T20:04:04+00:00",
            "updated": "2024-09-06T20:04:04+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 26
        },
        "2409.04599": {
            "authors": [
                "Pavel Chizhov",
                "Catherine Arnett",
                "Elizaveta Korotkova",
                "Ivan P. Yamshchikov"
            ],
            "title": "BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training",
            "abstract": "Language models can largely benefit from efficient tokenization. However, they still mostly utilize the classical BPE algorithm, a simple and reliable method. This has been shown to cause such issues as under-trained tokens and sub-optimal compression that may affect the downstream performance. We introduce Picky BPE, a modified BPE algorithm that carries out vocabulary refinement during tokenizer training. Our method improves vocabulary efficiency, eliminates under-trained tokens, and does not compromise text compression. Our experiments show that our method does not reduce the downstream performance, and in several cases improves it.",
            "id": "2409.04599",
            "link": "http://arxiv.org/abs/2409.04599v1",
            "published": "2024-09-06T20:12:34+00:00",
            "updated": "2024-09-06T20:12:34+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 9
        },
        "2409.02856": {
            "authors": [
                "Marjan Celikik",
                "Jacek Wasilewski",
                "Ana Peleteiro Ramallo",
                "Alexey Kurennoy",
                "Evgeny Labzin",
                "Danilo Ascione",
                "Tural Gurbanov",
                "G\u00e9raud Le Falher",
                "Andrii Dzhoha",
                "Ian Harris"
            ],
            "title": "Building a Scalable, Effective, and Steerable Search and Ranking Platform",
            "abstract": "Modern e-commerce platforms offer vast product selections, making it difficult for customers to find items that they like and that are relevant to their current session intent. This is why it is key for e-commerce platforms to have near real-time scalable and adaptable personalized ranking and search systems. While numerous methods exist in the scientific literature for building such systems, many are unsuitable for large-scale industrial use due to complexity and performance limitations. Consequently, industrial ranking systems often resort to computationally efficient yet simplistic retrieval or candidate generation approaches, which overlook near real-time and heterogeneous customer signals, which results in a less personalized and relevant experience. Moreover, related customer experiences are served by completely different systems, which increases complexity, maintenance, and inconsistent experiences.   In this paper, we present a personalized, adaptable near real-time ranking platform that is reusable across various use cases, such as browsing and search, and that is able to cater to millions of items and customers under heavy load (thousands of requests per second). We employ transformer-based models through different ranking layers which can learn complex behavior patterns directly from customer action sequences while being able to incorporate temporal (e.g. in-session) and contextual information. We validate our system through a series of comprehensive offline and online real-world experiments at a large online e-commerce platform, and we demonstrate its superiority when compared to existing systems, both in terms of customer experience as well as in net revenue. Finally, we share the lessons learned from building a comprehensive, modern ranking platform for use in a large-scale e-commerce environment.",
            "id": "2409.02856",
            "link": "http://arxiv.org/abs/2409.02856v1",
            "published": "2024-09-04T16:29:25+00:00",
            "updated": "2024-09-04T16:29:25+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 21
        },
        "2409.03504": {
            "authors": [
                "Jizhou Huang",
                "Haifeng Wang",
                "Yibo Sun",
                "Miao Fan",
                "Zhengjie Huang",
                "Chunyuan Yuan",
                "Yawen Li"
            ],
            "title": "HGAMN: Heterogeneous Graph Attention Matching Network for Multilingual POI Retrieval at Baidu Maps",
            "abstract": "The increasing interest in international travel has raised the demand of retrieving point of interests in multiple languages. This is even superior to find local venues such as restaurants and scenic spots in unfamiliar languages when traveling abroad. Multilingual POI retrieval, enabling users to find desired POIs in a demanded language using queries in numerous languages, has become an indispensable feature of today's global map applications such as Baidu Maps. This task is non-trivial because of two key challenges: (1) visiting sparsity and (2) multilingual query-POI matching. To this end, we propose a Heterogeneous Graph Attention Matching Network (HGAMN) to concurrently address both challenges. Specifically, we construct a heterogeneous graph that contains two types of nodes: POI node and query node using the search logs of Baidu Maps. To alleviate challenge \\#1, we construct edges between different POI nodes to link the low-frequency POIs with the high-frequency ones, which enables the transfer of knowledge from the latter to the former. To mitigate challenge \\#2, we construct edges between POI and query nodes based on the co-occurrences between queries and POIs, where queries in different languages and formulations can be aggregated for individual POIs. Moreover, we develop an attention-based network to jointly learn node representations of the heterogeneous graph and further design a cross-attention module to fuse the representations of both types of nodes for query-POI relevance scoring. Extensive experiments conducted on large-scale real-world datasets from Baidu Maps demonstrate the superiority and effectiveness of HGAMN. In addition, HGAMN has already been deployed in production at Baidu Maps, and it successfully keeps serving hundreds of millions of requests every day.",
            "id": "2409.03504",
            "link": "http://arxiv.org/abs/2409.03504v1",
            "published": "2024-09-05T13:18:01+00:00",
            "updated": "2024-09-05T13:18:01+00:00",
            "primary_category": "cs.IR",
            "categories": [
                "cs.IR"
            ],
            "max_author_hindex": 19
        },
        "2409.00890": {
            "authors": [
                "Sachin Pathiyan Cherumanal",
                "Falk Scholer",
                "Johanne R. Trippas",
                "Damiano Spina"
            ],
            "title": "Towards Investigating Biases in Spoken Conversational Search",
            "abstract": "Voice-based systems like Amazon Alexa, Google Assistant, and Apple Siri, along with the growing popularity of OpenAI's ChatGPT and Microsoft's Copilot, serve diverse populations, including visually impaired and low-literacy communities. This reflects a shift in user expectations from traditional search to more interactive question-answering models. However, presenting information effectively in voice-only channels remains challenging due to their linear nature. This limitation can impact the presentation of complex queries involving controversial topics with multiple perspectives. Failing to present diverse viewpoints may perpetuate or introduce biases and affect user attitudes. Balancing information load and addressing biases is crucial in designing a fair and effective voice-based system. To address this, we (i) review how biases and user attitude changes have been studied in screen-based web search, (ii) address challenges in studying these changes in voice-based settings like SCS, (iii) outline research questions, and (iv) propose an experimental setup with variables, data, and instruments to explore biases in a voice-based setting like Spoken Conversational Search.",
            "id": "2409.00890",
            "link": "http://arxiv.org/abs/2409.00890v1",
            "published": "2024-09-02T01:54:33+00:00",
            "updated": "2024-09-02T01:54:33+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.IR"
            ],
            "max_author_hindex": 32
        },
        "2409.01082": {
            "authors": [
                "Danilo Dordevic",
                "Suryansh Kumar"
            ],
            "title": "Evidential Transformers for Improved Image Retrieval",
            "abstract": "We introduce the Evidential Transformer, an uncertainty-driven transformer model for improved and robust image retrieval. In this paper, we make several contributions to content-based image retrieval (CBIR). We incorporate probabilistic methods into image retrieval, achieving robust and reliable results, with evidential classification surpassing traditional training based on multiclass classification as a baseline for deep metric learning. Furthermore, we improve the state-of-the-art retrieval results on several datasets by leveraging the Global Context Vision Transformer (GC ViT) architecture. Our experimental results consistently demonstrate the reliability of our approach, setting a new benchmark in CBIR in all test settings on the Stanford Online Products (SOP) and CUB-200-2011 datasets.",
            "id": "2409.01082",
            "link": "http://arxiv.org/abs/2409.01082v1",
            "published": "2024-09-02T09:10:47+00:00",
            "updated": "2024-09-02T09:10:47+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 17
        },
        "2409.03893": {
            "authors": [
                "Veronica Kecki",
                "Alan Said"
            ],
            "title": "Understanding Fairness in Recommender Systems: A Healthcare Perspective",
            "abstract": "Fairness in AI-driven decision-making systems has become a critical concern, especially when these systems directly affect human lives. This paper explores the public's comprehension of fairness in healthcare recommendations. We conducted a survey where participants selected from four fairness metrics -- Demographic Parity, Equal Accuracy, Equalized Odds, and Positive Predictive Value -- across different healthcare scenarios to assess their understanding of these concepts. Our findings reveal that fairness is a complex and often misunderstood concept, with a generally low level of public understanding regarding fairness metrics in recommender systems. This study highlights the need for enhanced information and education on algorithmic fairness to support informed decision-making in using these systems. Furthermore, the results suggest that a one-size-fits-all approach to fairness may be insufficient, pointing to the importance of context-sensitive designs in developing equitable AI systems.",
            "id": "2409.03893",
            "link": "http://arxiv.org/abs/2409.03893v2",
            "published": "2024-09-05T19:59:42+00:00",
            "updated": "2024-09-09T07:47:58+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.IR"
            ],
            "max_author_hindex": 22
        },
        "2409.00359": {
            "authors": [
                "Carlos Medel-Ram\u00edrez",
                "Hilario Medel-L\u00f3pez"
            ],
            "title": "Predicting Femicide in Veracruz: A Fuzzy Logic Approach with the Expanded MFM-FEM-VER-CP-2024 Model",
            "abstract": "The article focuses on the urgent issue of femicide in Veracruz, Mexico, and the development of the MFM_FEM_VER_CP_2024 model, a mathematical framework designed to predict femicide risk using fuzzy logic. This model addresses the complexity and uncertainty inherent in gender based violence by formalizing risk factors such as coercive control, dehumanization, and the cycle of violence. These factors are mathematically modeled through membership functions that assess the degree of risk associated with various conditions, including personal relationships and specific acts of violence. The study enhances the original model by incorporating new rules and refining existing membership functions, which significantly improve the model predictive accuracy.",
            "id": "2409.00359",
            "link": "http://arxiv.org/abs/2409.00359v1",
            "published": "2024-08-31T06:00:49+00:00",
            "updated": "2024-08-31T06:00:49+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "03E72, 91D10, 62P25, 91B76",
                "G.1; G.3; I.2; I.6"
            ],
            "max_author_hindex": 2
        },
        "2409.01007": {
            "authors": [
                "Edward Y. Chang"
            ],
            "title": "Unlocking the Wisdom of Large Language Models: An Introduction to The Path to Artificial General Intelligence",
            "abstract": "This booklet, \"Unlocking the Wisdom of Large Language Models,\" serves as an introduction to the comprehensive work \"The Path to Artificial General Intelligence.\" Through a series of nine aphorisms, we distill key insights and principles that underpin the larger exploration of AI's future through adversarial LLM dialogue. We propose this approach as a potential path to realizing artificial general intelligence (AGI). This booklet also includes the titles, abstracts, and introductions of the chapters in the main book, and presents the first two chapters in their entirety.",
            "id": "2409.01007",
            "link": "http://arxiv.org/abs/2409.01007v1",
            "published": "2024-09-02T07:29:37+00:00",
            "updated": "2024-09-02T07:29:37+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "I.2.7"
            ],
            "max_author_hindex": 45
        },
        "2409.03671": {
            "authors": [
                "Stylianos Loukas Vasileiou",
                "William Yeoh"
            ],
            "title": "TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course Scheduling Problems",
            "abstract": "We present TRACE-cs, a novel hybrid system that combines symbolic reasoning with large language models (LLMs) to address contrastive queries in scheduling problems. TRACE-cs leverages SAT solving techniques to encode scheduling constraints and generate explanations for user queries, while utilizing an LLM to process the user queries into logical clauses as well as refine the explanations generated by the symbolic solver to natural language sentences. By integrating these components, our approach demonstrates the potential of combining symbolic methods with LLMs to create explainable AI agents with correctness guarantees.",
            "id": "2409.03671",
            "link": "http://arxiv.org/abs/2409.03671v1",
            "published": "2024-09-05T16:24:42+00:00",
            "updated": "2024-09-05T16:24:42+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 31
        },
        "2409.04469": {
            "authors": [
                "Zoran Majkic"
            ],
            "title": "Intensional FOL: Many-Sorted Extension",
            "abstract": "The concepts used in IFOL have associated to them a list of sorted attributes, and the sorts are the intensional concepts as well. The requirement to extend the unsorted IFOL (Intensional FOL) to many-sorted IFOL is mainly based on the fact that a natural language is implicitly many-sorted and that we intend to use IFOL to support applications that use natural languages. Thus, the proposed version of many-sorted IFOL is just the completion of this conceptual feature of the IFOL.",
            "id": "2409.04469",
            "link": "http://arxiv.org/abs/2409.04469v1",
            "published": "2024-09-03T19:50:57+00:00",
            "updated": "2024-09-03T19:50:57+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 11
        },
        "2409.01445": {
            "authors": [
                "Ishan Rajendrakumar Dave",
                "Fabian Caba Heilbron",
                "Mubarak Shah",
                "Simon Jenni"
            ],
            "title": "Sync from the Sea: Retrieving Alignable Videos from Large-Scale Datasets",
            "abstract": "Temporal video alignment aims to synchronize the key events like object interactions or action phase transitions in two videos. Such methods could benefit various video editing, processing, and understanding tasks. However, existing approaches operate under the restrictive assumption that a suitable video pair for alignment is given, significantly limiting their broader applicability. To address this, we re-pose temporal alignment as a search problem and introduce the task of Alignable Video Retrieval (AVR). Given a query video, our approach can identify well-alignable videos from a large collection of clips and temporally synchronize them to the query. To achieve this, we make three key contributions: 1) we introduce DRAQ, a video alignability indicator to identify and re-rank the best alignable video from a set of candidates; 2) we propose an effective and generalizable frame-level video feature design to improve the alignment performance of several off-the-shelf feature representations, and 3) we propose a novel benchmark and evaluation protocol for AVR using cycle-consistency metrics. Our experiments on 3 datasets, including large-scale Kinetics700, demonstrate the effectiveness of our approach in identifying alignable video pairs from diverse datasets. Project Page: https://daveishan.github.io/avr-webpage/.",
            "id": "2409.01445",
            "link": "http://arxiv.org/abs/2409.01445v1",
            "published": "2024-09-02T20:00:49+00:00",
            "updated": "2024-09-02T20:00:49+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 116
        },
        "2409.02455": {
            "authors": [
                "Dildar Ali",
                "Harishchandra Kumar",
                "Suman Banerjee",
                "Yamuna Prasad"
            ],
            "title": "An Effective Tag Assignment Approach for Billboard Advertisement",
            "abstract": "Billboard Advertisement has gained popularity due to its significant outrage in return on investment. To make this advertisement approach more effective, the relevant information about the product needs to be reached to the relevant set of people. This can be achieved if the relevant set of tags can be mapped to the correct slots. Formally, we call this problem the Tag Assignment Problem in Billboard Advertisement. Given trajectory, billboard database, and a set of selected billboard slots and tags, this problem asks to output a mapping of selected tags to the selected slots so that the influence is maximized. We model this as a variant of traditional bipartite matching called One-To-Many Bipartite Matching (OMBM). Unlike traditional bipartite matching, a tag can be assigned to only one slot; in the OMBM, a tag can be assigned to multiple slots while the vice versa can not happen. We propose an iterative solution approach that incrementally allocates the tags to the slots. The proposed methodology has been explained with an illustrated example. A complexity analysis of the proposed solution approach has also been conducted. The experimental results on real-world trajectory and billboard datasets prove our claim on the effectiveness and efficiency of the proposed solution.",
            "id": "2409.02455",
            "link": "http://arxiv.org/abs/2409.02455v1",
            "published": "2024-09-04T05:36:00+00:00",
            "updated": "2024-09-04T05:36:00+00:00",
            "primary_category": "cs.DS",
            "categories": [
                "cs.DS",
                "cs.IR"
            ],
            "max_author_hindex": 57
        },
        "2409.02965": {
            "authors": [
                "Zhicheng Ren",
                "Zhiping Xiao",
                "Yizhou Sun"
            ],
            "title": "Do We Trust What They Say or What They Do? A Multimodal User Embedding Provides Personalized Explanations",
            "abstract": "With the rapid development of social media, the importance of analyzing social network user data has also been put on the agenda. User representation learning in social media is a critical area of research, based on which we can conduct personalized content delivery, or detect malicious actors. Being more complicated than many other types of data, social network user data has inherent multimodal nature. Various multimodal approaches have been proposed to harness both text (i.e. post content) and relation (i.e. inter-user interaction) information to learn user embeddings of higher quality. The advent of Graph Neural Network models enables more end-to-end integration of user text embeddings and user interaction graphs in social networks. However, most of those approaches do not adequately elucidate which aspects of the data - text or graph structure information - are more helpful for predicting each specific user under a particular task, putting some burden on personalized downstream analysis and untrustworthy information filtering. We propose a simple yet effective framework called Contribution-Aware Multimodal User Embedding (CAMUE) for social networks. We have demonstrated with empirical evidence, that our approach can provide personalized explainable predictions, automatically mitigating the impact of unreliable information. We also conducted case studies to show how reasonable our results are. We observe that for most users, graph structure information is more trustworthy than text information, but there are some reasonable cases where text helps more. Our work paves the way for more explainable, reliable, and effective social media user embedding which allows for better personalized content delivery.",
            "id": "2409.02965",
            "link": "http://arxiv.org/abs/2409.02965v1",
            "published": "2024-09-04T02:17:32+00:00",
            "updated": "2024-09-04T02:17:32+00:00",
            "primary_category": "cs.SI",
            "categories": [
                "cs.SI",
                "cs.IR",
                "cs.LG"
            ],
            "max_author_hindex": 57
        },
        "2409.00323": {
            "authors": [
                "Unggi Lee",
                "Jiyeong Bae",
                "Yeonji Jung",
                "Minji Kang",
                "Gyuri Byun",
                "Yeonseo Lee",
                "Dohee Kim",
                "Sookbun Lee",
                "Jaekwon Park",
                "Taekyung Ahn",
                "Gunho Lee",
                "Hyeoncheol Kim"
            ],
            "title": "From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education",
            "abstract": "Knowledge Tracing (KT) is a critical component in online learning, but traditional approaches face limitations in interpretability and cross-domain adaptability. This paper introduces Language Model-based Code Knowledge Tracing (CodeLKT), an innovative application of Language model-based Knowledge Tracing (LKT) to programming education. CodeLKT leverages pre-trained language models to process learning data, demonstrating superior performance over existing KT and Code KT models. We explore Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT), showing enhanced performance in the coding domain and investigating cross-domain transfer between mathematics and coding. Additionally, we present an theoretically-informed integrated system combining CodeLKT with large language models to generate personalized, in-depth feedback to support students' programming learning. This work advances the field of Code Knowledge Tracing by expanding the knowledge base with language model-based approach and offering practical implications for programming education through data-informed feedback.",
            "id": "2409.00323",
            "link": "http://arxiv.org/abs/2409.00323v1",
            "published": "2024-08-31T01:36:38+00:00",
            "updated": "2024-08-31T01:36:38+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.SE"
            ],
            "max_author_hindex": 12
        },
        "2409.00355": {
            "authors": [
                "Dongil Yang",
                "Suyeon Lee",
                "Minjin Kim",
                "Jungsoo Won",
                "Namyoung Kim",
                "Dongha Lee",
                "Jinyoung Yeo"
            ],
            "title": "YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion",
            "abstract": "Engagement between instructors and students plays a crucial role in enhancing students'academic performance. However, instructors often struggle to provide timely and personalized support in large classes. To address this challenge, we propose a novel Virtual Teaching Assistant (VTA) named YA-TA, designed to offer responses to students that are grounded in lectures and are easy to understand. To facilitate YA-TA, we introduce the Dual Retrieval-augmented Knowledge Fusion (DRAKE) framework, which incorporates dual retrieval of instructor and student knowledge and knowledge fusion for tailored response generation. Experiments conducted in real-world classroom settings demonstrate that the DRAKE framework excels in aligning responses with knowledge retrieved from both instructor and student sides. Furthermore, we offer additional extensions of YA-TA, such as a Q&A board and self-practice tools to enhance the overall learning experience. Our video is publicly available.",
            "id": "2409.00355",
            "link": "http://arxiv.org/abs/2409.00355v1",
            "published": "2024-08-31T05:37:51+00:00",
            "updated": "2024-08-31T05:37:51+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 24
        },
        "2409.00399": {
            "authors": [
                "Jun Yan",
                "Wenjie Jacky Mo",
                "Xiang Ren",
                "Robin Jia"
            ],
            "title": "Rethinking Backdoor Detection Evaluation for Language Models",
            "abstract": "Backdoor attacks, in which a model behaves maliciously when given an attacker-specified trigger, pose a major security risk for practitioners who depend on publicly released language models. Backdoor detection methods aim to detect whether a released model contains a backdoor, so that practitioners can avoid such vulnerabilities. While existing backdoor detection methods have high accuracy in detecting backdoored models on standard benchmarks, it is unclear whether they can robustly identify backdoors in the wild. In this paper, we examine the robustness of backdoor detectors by manipulating different factors during backdoor planting. We find that the success of existing methods highly depends on how intensely the model is trained on poisoned data during backdoor planting. Specifically, backdoors planted with either more aggressive or more conservative training are significantly more difficult to detect than the default ones. Our results highlight a lack of robustness of existing backdoor detectors and the limitations in current benchmark construction.",
            "id": "2409.00399",
            "link": "http://arxiv.org/abs/2409.00399v1",
            "published": "2024-08-31T09:19:39+00:00",
            "updated": "2024-08-31T09:19:39+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.CR"
            ],
            "max_author_hindex": 24
        },
        "2409.00414": {
            "authors": [
                "Daniel Varab",
                "Christian Hardmeier"
            ],
            "title": "With Good MT There is No Need For End-to-End: A Case for Translate-then-Summarize Cross-lingual Summarization",
            "abstract": "Recent work has suggested that end-to-end system designs for cross-lingual summarization are competitive solutions that perform on par or even better than traditional pipelined designs. A closer look at the evidence reveals that this intuition is based on the results of only a handful of languages or using underpowered pipeline baselines. In this work, we compare these two paradigms for cross-lingual summarization on 39 source languages into English and show that a simple \\textit{translate-then-summarize} pipeline design consistently outperforms even an end-to-end system with access to enormous amounts of parallel data. For languages where our pipeline model does not perform well, we show that system performance is highly correlated with publicly distributed BLEU scores, allowing practitioners to establish the feasibility of a language pair a priori. Contrary to recent publication trends, our result suggests that the combination of individual progress of monolingual summarization and translation tasks offers better performance than an end-to-end system, suggesting that end-to-end designs should be considered with care.",
            "id": "2409.00414",
            "link": "http://arxiv.org/abs/2409.00414v1",
            "published": "2024-08-31T10:44:16+00:00",
            "updated": "2024-08-31T10:44:16+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 22
        },
        "2409.00598": {
            "authors": [
                "Bang An",
                "Sicheng Zhu",
                "Ruiyi Zhang",
                "Michael-Andrei Panaitescu-Liess",
                "Yuancheng Xu",
                "Furong Huang"
            ],
            "title": "Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models",
            "abstract": "Safety-aligned large language models (LLMs) sometimes falsely refuse pseudo-harmful prompts, like \"how to kill a mosquito,\" which are actually harmless. Frequent false refusals not only frustrate users but also provoke a public backlash against the very values alignment seeks to protect. In this paper, we propose the first method to auto-generate diverse, content-controlled, and model-dependent pseudo-harmful prompts. Using this method, we construct an evaluation dataset called PHTest, which is ten times larger than existing datasets, covers more false refusal patterns, and separately labels controversial prompts. We evaluate 20 LLMs on PHTest, uncovering new insights due to its scale and labeling. Our findings reveal a trade-off between minimizing false refusals and improving safety against jailbreak attacks. Moreover, we show that many jailbreak defenses significantly increase the false refusal rates, thereby undermining usability. Our method and dataset can help developers evaluate and fine-tune safer and more usable LLMs. Our code and dataset are available at https://github.com/umd-huang-lab/FalseRefusal",
            "id": "2409.00598",
            "link": "http://arxiv.org/abs/2409.00598v1",
            "published": "2024-09-01T03:25:59+00:00",
            "updated": "2024-09-01T03:25:59+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.CR",
                "cs.CY",
                "cs.LG"
            ],
            "max_author_hindex": 25
        },
        "2409.00626": {
            "authors": [
                "Idris Abdulmumin",
                "Sthembiso Mkhwanazi",
                "Mahlatse S. Mbooi",
                "Shamsuddeen Hassan Muhammad",
                "Ibrahim Said Ahmad",
                "Neo Putini",
                "Miehleketo Mathebula",
                "Matimba Shingange",
                "Tajuddeen Gwadabe",
                "Vukosi Marivate"
            ],
            "title": "Correcting FLORES Evaluation Dataset for Four African Languages",
            "abstract": "This paper describes the corrections made to the FLORES evaluation (dev and devtest) dataset for four African languages, namely Hausa, Northern Sotho (Sepedi), Xitsonga and isiZulu. The original dataset, though groundbreaking in its coverage of low-resource languages, exhibited various inconsistencies and inaccuracies in the reviewed languages that could potentially hinder the integrity of the evaluation of downstream tasks in natural language processing (NLP), especially machine translation. Through a meticulous review process by native speakers, several corrections were identified and implemented, improving the dataset's overall quality and reliability. For each language, we provide a concise summary of the errors encountered and corrected, and also present some statistical analysis that measure the difference between the existing and corrected datasets. We believe that our corrections enhance the linguistic accuracy and reliability of the data and, thereby, contributing to more effective evaluation of NLP tasks involving the four African languages.",
            "id": "2409.00626",
            "link": "http://arxiv.org/abs/2409.00626v1",
            "published": "2024-09-01T06:13:03+00:00",
            "updated": "2024-09-01T06:13:03+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 14
        },
        "2409.00781": {
            "authors": [
                "Michael Schlichtkrull"
            ],
            "title": "Generating Media Background Checks for Automated Source Critical Reasoning",
            "abstract": "Not everything on the internet is true. This unfortunate fact requires both humans and models to perform complex reasoning about credibility when working with retrieved information. In NLP, this problem has seen little attention. Indeed, retrieval-augmented models are not typically expected to distrust retrieved documents. Human experts overcome the challenge by gathering signals about the context, reliability, and tendency of source documents - that is, they perform source criticism. We propose a novel NLP task focused on finding and summarising such signals. We introduce a new dataset of 6,709 \"media background checks\" derived from Media Bias / Fact Check, a volunteer-run website documenting media bias. We test open-source and closed-source LLM baselines with and without retrieval on this dataset, finding that retrieval greatly improves performance. We furthermore carry out human evaluation, demonstrating that 1) media background checks are helpful for humans, and 2) media background checks are helpful for retrieval-augmented models.",
            "id": "2409.00781",
            "link": "http://arxiv.org/abs/2409.00781v1",
            "published": "2024-09-01T17:06:06+00:00",
            "updated": "2024-09-01T17:06:06+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 14
        },
        "2409.00788": {
            "authors": [
                "Ashish Kumar",
                "Durga Toshniwal"
            ],
            "title": "Modeling Text-Label Alignment for Hierarchical Text Classification",
            "abstract": "Hierarchical Text Classification (HTC) aims to categorize text data based on a structured label hierarchy, resulting in predicted labels forming a sub-hierarchy tree. The semantics of the text should align with the semantics of the labels in this sub-hierarchy. With the sub-hierarchy changing for each sample, the dynamic nature of text-label alignment poses challenges for existing methods, which typically process text and labels independently. To overcome this limitation, we propose a Text-Label Alignment (TLA) loss specifically designed to model the alignment between text and labels. We obtain a set of negative labels for a given text and its positive label set. By leveraging contrastive learning, the TLA loss pulls the text closer to its positive label and pushes it away from its negative label in the embedding space. This process aligns text representations with related labels while distancing them from unrelated ones. Building upon this framework, we introduce the Hierarchical Text-Label Alignment (HTLA) model, which leverages BERT as the text encoder and GPTrans as the graph encoder and integrates text-label embeddings to generate hierarchy-aware representations. Experimental results on benchmark datasets and comparison with existing baselines demonstrate the effectiveness of HTLA for HTC.",
            "id": "2409.00788",
            "link": "http://arxiv.org/abs/2409.00788v1",
            "published": "2024-09-01T17:48:29+00:00",
            "updated": "2024-09-01T17:48:29+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 45
        },
        "2409.00800": {
            "authors": [
                "Yaoxun Xu",
                "Shi-Xiong Zhang",
                "Jianwei Yu",
                "Zhiyong Wu",
                "Dong Yu"
            ],
            "title": "Comparing Discrete and Continuous Space LLMs for Speech Recognition",
            "abstract": "This paper investigates discrete and continuous speech representations in Large Language Model (LLM)-based Automatic Speech Recognition (ASR), organizing them by feature continuity and training approach into four categories: supervised and unsupervised for both discrete and continuous types. We further classify LLMs based on their input and autoregressive feedback into continuous and discrete-space models. Using specialized encoders and comparative analysis with a Joint-Training-From-Scratch Language Model (JTFS LM) and pre-trained LLaMA2-7b, we provide a detailed examination of their effectiveness. Our work marks the first extensive comparison of speech representations in LLM-based ASR and explores various modeling techniques. We present an open-sourced achievement of a state-of-the-art Word Error Rate (WER) of 1.69\\% on LibriSpeech using a HuBERT encoder, offering valuable insights for advancing ASR and natural language processing (NLP) research.",
            "id": "2409.00800",
            "link": "http://arxiv.org/abs/2409.00800v1",
            "published": "2024-09-01T18:29:45+00:00",
            "updated": "2024-09-01T18:29:45+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 29
        },
        "2409.00855": {
            "authors": [
                "Xuechen Liang",
                "Meiling Tao",
                "Yinghui Xia",
                "Tianyu Shi",
                "Jun Wang",
                "JingSong Yang"
            ],
            "title": "LanguaShrink: Reducing Token Overhead with Psycholinguistics",
            "abstract": "As large language models (LLMs) improve their capabilities in handling complex tasks, the issues of computational cost and efficiency due to long prompts are becoming increasingly prominent. To accelerate model inference and reduce costs, we propose an innovative prompt compression framework called LanguaShrink. Inspired by the observation that LLM performance depends on the density and position of key information in the input prompts, LanguaShrink leverages psycholinguistic principles and the Ebbinghaus memory curve to achieve task-agnostic prompt compression. This effectively reduces prompt length while preserving essential information. We referred to the training method of OpenChat.The framework introduces part-of-speech priority compression and data distillation techniques, using smaller models to learn compression targets and employing a KL-regularized reinforcement learning strategy for training.\\cite{wang2023openchat} Additionally, we adopt a chunk-based compression algorithm to achieve adjustable compression rates. We evaluate our method on multiple datasets, including LongBench, ZeroScrolls, Arxiv Articles, and a newly constructed novel test set. Experimental results show that LanguaShrink maintains semantic similarity while achieving up to 26 times compression. Compared to existing prompt compression methods, LanguaShrink improves end-to-end latency by 1.43 times.",
            "id": "2409.00855",
            "link": "http://arxiv.org/abs/2409.00855v1",
            "published": "2024-09-01T22:09:20+00:00",
            "updated": "2024-09-01T22:09:20+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "stat.ML"
            ],
            "max_author_hindex": 25
        },
        "2409.00997": {
            "authors": [
                "Keer Lu",
                "Zheng Liang",
                "Xiaonan Nie",
                "Da Pan",
                "Shusen Zhang",
                "Keshi Zhao",
                "Weipeng Chen",
                "Zenan Zhou",
                "Guosheng Dong",
                "Wentao Zhang",
                "Bin Cui"
            ],
            "title": "DataSculpt: Crafting Data Landscapes for LLM Post-Training through Multi-objective Partitioning",
            "abstract": "The effectiveness of long-context modeling is important for Large Language Models (LLMs) in various applications. Despite their potential, LLMs' efficacy in processing long context does not consistently meet expectations, posing significant challenges for efficient management of prolonged sequences in training. This difficulty is compounded by the scarcity of comprehensive and diverse training datasets suitable for long sequences, which stems from inherent length biases across different data sources, and the logistical complexities associated with massive data management for training in extended contexts. In this work, we introduce DataSculpt, a data construction framework designed to strategically augment the data architecture for extended-context training. Our thorough evaluations demonstrate DataSculpt's remarkable capacity to boost long-context training performance, achieving improvements including an 18.09% increase in retrieval augmentation, 21.23% in summarization, 21.27% in reading comprehension, and a 3.81% rise in code completion, all while preserving the models' overall proficiency with a 4.88% improvement.",
            "id": "2409.00997",
            "link": "http://arxiv.org/abs/2409.00997v1",
            "published": "2024-09-02T07:23:13+00:00",
            "updated": "2024-09-02T07:23:13+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 35
        },
        "2409.01011": {
            "authors": [
                "Yingfa Chen",
                "Chenlong Hu",
                "Cong Feng",
                "Chenyang Song",
                "Shi Yu",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "title": "Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts",
            "abstract": "This study presents a multi-modal multi-granularity tokenizer specifically designed for analyzing ancient Chinese scripts, focusing on the Chu bamboo slip (CBS) script used during the Spring and Autumn and Warring States period (771-256 BCE) in Ancient China. Considering the complex hierarchical structure of ancient Chinese scripts, where a single character may be a combination of multiple sub-characters, our tokenizer first adopts character detection to locate character boundaries, and then conducts character recognition at both the character and sub-character levels. Moreover, to support the academic community, we have also assembled the first large-scale dataset of CBSs with over 100K annotated character image scans. On the part-of-speech tagging task built on our dataset, using our tokenizer gives a 5.5% relative improvement in F1-score compared to mainstream sub-word tokenizers. Our work not only aids in further investigations of the specific script but also has the potential to advance research on other forms of ancient Chinese scripts.",
            "id": "2409.01011",
            "link": "http://arxiv.org/abs/2409.01011v1",
            "published": "2024-09-02T07:42:55+00:00",
            "updated": "2024-09-02T07:42:55+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.CV"
            ],
            "max_author_hindex": 87
        },
        "2409.01035": {
            "authors": [
                "Chongjie Si",
                "Zhiyi Shi",
                "Shifan Zhang",
                "Xiaokang Yang",
                "Hanspeter Pfister",
                "Wei Shen"
            ],
            "title": "Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning",
            "abstract": "Large language models demonstrate impressive performance on downstream tasks, yet requiring extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. In this paper, we delve into the concept of task-specific directions--critical for transitioning large models from pre-trained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties, and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of task-specific directions during the fine-tuning process, thereby enhancing model performance on targeted tasks. Extensive experiments have conclusively demonstrated the effectiveness of LoRA-Dash, and in-depth analyses further reveal the underlying mechanisms of LoRA-Dash. The code is available at https://github.com/Chongjie-Si/Subspace-Tuning.",
            "id": "2409.01035",
            "link": "http://arxiv.org/abs/2409.01035v1",
            "published": "2024-09-02T08:10:51+00:00",
            "updated": "2024-09-02T08:10:51+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.CV",
                "cs.LG"
            ],
            "max_author_hindex": 76
        },
        "2409.01037": {
            "authors": [
                "Ke Chang",
                "Hao Li",
                "Junzhao Zhang",
                "Yunfang Wu"
            ],
            "title": "NYK-MS: A Well-annotated Multi-modal Metaphor and Sarcasm Understanding Benchmark on Cartoon-Caption Dataset",
            "abstract": "Metaphor and sarcasm are common figurative expressions in people's communication, especially on the Internet or the memes popular among teenagers. We create a new benchmark named NYK-MS (NewYorKer for Metaphor and Sarcasm), which contains 1,583 samples for metaphor understanding tasks and 1,578 samples for sarcasm understanding tasks. These tasks include whether it contains metaphor/sarcasm, which word or object contains metaphor/sarcasm, what does it satirize and why does it contains metaphor/sarcasm, all of the 7 tasks are well-annotated by at least 3 annotators. We annotate the dataset for several rounds to improve the consistency and quality, and use GUI and GPT-4V to raise our efficiency. Based on the benchmark, we conduct plenty of experiments. In the zero-shot experiments, we show that Large Language Models (LLM) and Large Multi-modal Models (LMM) can't do classification task well, and as the scale increases, the performance on other 5 tasks improves. In the experiments on traditional pre-train models, we show the enhancement with augment and alignment methods, which prove our benchmark is consistent with previous dataset and requires the model to understand both of the two modalities.",
            "id": "2409.01037",
            "link": "http://arxiv.org/abs/2409.01037v1",
            "published": "2024-09-02T08:14:49+00:00",
            "updated": "2024-09-02T08:14:49+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 37
        },
        "2409.01217": {
            "authors": [
                "Asma Amalas",
                "Mounir Ghogho",
                "Mohamed Chetouani",
                "Rachid Oulad Haj Thami"
            ],
            "title": "A multilingual training strategy for low resource Text to Speech",
            "abstract": "Recent speech technologies have led to produce high quality synthesised speech due to recent advances in neural Text to Speech (TTS). However, such TTS models depend on extensive amounts of data that can be costly to produce and is hardly scalable to all existing languages, especially that seldom attention is given to low resource languages. With techniques such as knowledge transfer, the burden of creating datasets can be alleviated. In this paper, we therefore investigate two aspects; firstly, whether data from social media can be used for a small TTS dataset construction, and secondly whether cross lingual transfer learning (TL) for a low resource language can work with this type of data. In this aspect, we specifically assess to what extent multilingual modeling can be leveraged as an alternative to training on monolingual corporas. To do so, we explore how data from foreign languages may be selected and pooled to train a TTS model for a target low resource language. Our findings show that multilingual pre-training is better than monolingual pre-training at increasing the intelligibility and naturalness of the generated speech.",
            "id": "2409.01217",
            "link": "http://arxiv.org/abs/2409.01217v1",
            "published": "2024-09-02T12:53:01+00:00",
            "updated": "2024-09-02T12:53:01+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 41
        },
        "2409.01389": {
            "authors": [
                "Ivana Be\u0148ov\u00e1",
                "Michal Gregor",
                "Albert Gatt"
            ],
            "title": "CV-Probes: Studying the interplay of lexical and world knowledge in visually grounded verb understanding",
            "abstract": "This study investigates the ability of various vision-language (VL) models to ground context-dependent and non-context-dependent verb phrases. To do that, we introduce the CV-Probes dataset, designed explicitly for studying context understanding, containing image-caption pairs with context-dependent verbs (e.g., \"beg\") and non-context-dependent verbs (e.g., \"sit\"). We employ the MM-SHAP evaluation to assess the contribution of verb tokens towards model predictions. Our results indicate that VL models struggle to ground context-dependent verb phrases effectively. These findings highlight the challenges in training VL models to integrate context accurately, suggesting a need for improved methodologies in VL model training and evaluation.",
            "id": "2409.01389",
            "link": "http://arxiv.org/abs/2409.01389v1",
            "published": "2024-09-02T17:39:26+00:00",
            "updated": "2024-09-02T17:39:26+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 29
        },
        "2409.01482": {
            "authors": [
                "Benjamin L. Badger"
            ],
            "title": "Masked Mixers for Language Generation and Retrieval",
            "abstract": "Attention mechanisms that confer selective focus on a strict subset of input elements are nearly ubiquitous in language models today. We posit there to be downside to the use of attention: most information present in the input is necessarily lost. In support of this idea we observe poor input representation accuracy in transformers, but find more accurate representation in what we term masked mixers which replace self-attention with masked convolutions. Applied to TinyStories the masked mixer learns causal language tasks more efficiently than early transformer implementations and somewhat less efficiently than optimized, current implementations. The most efficient learning algorithm observed for this dataset is a transformer-masked mixer hybrid, suggesting that these models learn in an orthogonal manner. We hypothesized that the information loss exhibited by transformers would be much more detrimental to retrieval than generation, and to test this we introduce an efficient training approach for retrieval models based on existing generative model embeddings. With this method, embeddings from masked mixers are found to result in far better summary-to-story retrieval compared to embeddings from transformers.",
            "id": "2409.01482",
            "link": "http://arxiv.org/abs/2409.01482v1",
            "published": "2024-09-02T22:17:18+00:00",
            "updated": "2024-09-02T22:17:18+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 4
        },
        "2409.01497": {
            "authors": [
                "Rajat Rawat",
                "Hudson McBride",
                "Dhiyaan Nirmal",
                "Rajarshi Ghosh",
                "Jong Moon",
                "Dhruv Alamuri",
                "Sean O'Brien",
                "Kevin Zhu"
            ],
            "title": "DiversityMedQA: Assessing Demographic Biases in Medical Diagnosis using Large Language Models",
            "abstract": "As large language models (LLMs) gain traction in healthcare, concerns about their susceptibility to demographic biases are growing. We introduce {DiversityMedQA}, a novel benchmark designed to assess LLM responses to medical queries across diverse patient demographics, such as gender and ethnicity. By perturbing questions from the MedQA dataset, which comprises medical board exam questions, we created a benchmark that captures the nuanced differences in medical diagnosis across varying patient profiles. Our findings reveal notable discrepancies in model performance when tested against these demographic variations. Furthermore, to ensure the perturbations were accurate, we also propose a filtering strategy that validates each perturbation. By releasing DiversityMedQA, we provide a resource for evaluating and mitigating demographic bias in LLM medical diagnoses.",
            "id": "2409.01497",
            "link": "http://arxiv.org/abs/2409.01497v1",
            "published": "2024-09-02T23:37:20+00:00",
            "updated": "2024-09-02T23:37:20+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 46
        },
        "2409.01575": {
            "authors": [
                "Takehiro Sato",
                "Shintaro Ozaki",
                "Daisaku Yokoyama"
            ],
            "title": "An Implementation of Werewolf Agent That does not Truly Trust LLMs",
            "abstract": "Werewolf is an incomplete information game, which has several challenges when creating a computer agent as a player given the lack of understanding of the situation and individuality of utterance (e.g., computer agents are not capable of characterful utterance or situational lying). We propose a werewolf agent that solves some of those difficulties by combining a Large Language Model (LLM) and a rule-based algorithm. In particular, our agent uses a rule-based algorithm to select an output either from an LLM or a template prepared beforehand based on the results of analyzing conversation history using an LLM. It allows the agent to refute in specific situations, identify when to end the conversation, and behave with persona. This approach mitigated conversational inconsistencies and facilitated logical utterance as a result. We also conducted a qualitative evaluation, which resulted in our agent being perceived as more human-like compared to an unmodified LLM. The agent is freely available for contributing to advance the research in the field of Werewolf game.",
            "id": "2409.01575",
            "link": "http://arxiv.org/abs/2409.01575v1",
            "published": "2024-09-03T03:16:03+00:00",
            "updated": "2024-09-03T03:16:03+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 13
        },
        "2409.01584": {
            "authors": [
                "Shintaro Ozaki",
                "Kazuki Hayashi",
                "Yusuke Sakai",
                "Hidetaka Kamigaito",
                "Katsuhiko Hayashi",
                "Taro Watanabe"
            ],
            "title": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models",
            "abstract": "As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow. However, pre-training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data, leaving it uncertain whether LVLMs can completely handle their potential when generating explanations in languages other than English. In addition, multilingual QA benchmarks that create datasets using machine translation have cultural differences and biases, remaining issues for use as evaluation tasks. To address these challenges, this study created an extended dataset in multiple languages without relying on machine translation. This dataset that takes into account nuances and country-specific phrases was then used to evaluate the generation explanation abilities of LVLMs. Furthermore, this study examined whether Instruction-Tuning in resource-rich English improves performance in other languages. Our findings indicate that LVLMs perform worse in languages other than English compared to English. In addition, it was observed that LVLMs struggle to effectively manage the knowledge learned from English data.",
            "id": "2409.01584",
            "link": "http://arxiv.org/abs/2409.01584v1",
            "published": "2024-09-03T03:42:56+00:00",
            "updated": "2024-09-03T03:42:56+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 39
        },
        "2409.01666": {
            "authors": [
                "Tan Yu",
                "Anbang Xu",
                "Rama Akkiraju"
            ],
            "title": "In Defense of RAG in the Era of Long-Context Language Models",
            "abstract": "Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in long-context applications. Unlike the existing works favoring the long-context LLM over RAG, we argue that the extremely long context in LLMs suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which significantly improves the performance of RAG for long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our OP-RAG.",
            "id": "2409.01666",
            "link": "http://arxiv.org/abs/2409.01666v1",
            "published": "2024-09-03T07:17:41+00:00",
            "updated": "2024-09-03T07:17:41+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 29
        },
        "2409.01780": {
            "authors": [
                "Yihao Wang",
                "Ru Zhang",
                "Yifan Tang",
                "Jianyi Liu"
            ],
            "title": "State-of-the-art Advances of Deep-learning Linguistic Steganalysis Research",
            "abstract": "With the evolution of generative linguistic steganography techniques, conventional steganalysis falls short in robustly quantifying the alterations induced by steganography, thereby complicating detection. Consequently, the research paradigm has pivoted towards deep-learning-based linguistic steganalysis. This study offers a comprehensive review of existing contributions and evaluates prevailing developmental trajectories. Specifically, we first provided a formalized exposition of the general formulas for linguistic steganalysis, while comparing the differences between this field and the domain of text classification. Subsequently, we classified the existing work into two levels based on vector space mapping and feature extraction models, thereby comparing the research motivations, model advantages, and other details. A comparative analysis of the experiments is conducted to assess the performances. Finally, the challenges faced by this field are discussed, and several directions for future development and key issues that urgently need to be addressed are proposed.",
            "id": "2409.01780",
            "link": "http://arxiv.org/abs/2409.01780v1",
            "published": "2024-09-03T10:49:42+00:00",
            "updated": "2024-09-03T10:49:42+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 29
        },
        "2409.01787": {
            "authors": [
                "Yifeng Wang",
                "Zhouhong Gu",
                "Siwei Zhang",
                "Suhang Zheng",
                "Tao Wang",
                "Tianyu Li",
                "Hongwei Feng",
                "Yanghua Xiao"
            ],
            "title": "LLM-GAN: Construct Generative Adversarial Network Through Large Language Models For Explainable Fake News Detection",
            "abstract": "Explainable fake news detection predicts the authenticity of news items with annotated explanations. Today, Large Language Models (LLMs) are known for their powerful natural language understanding and explanation generation abilities. However, presenting LLMs for explainable fake news detection remains two main challenges. Firstly, fake news appears reasonable and could easily mislead LLMs, leaving them unable to understand the complex news-faking process. Secondly, utilizing LLMs for this task would generate both correct and incorrect explanations, which necessitates abundant labor in the loop. In this paper, we propose LLM-GAN, a novel framework that utilizes prompting mechanisms to enable an LLM to become Generator and Detector and for realistic fake news generation and detection. Our results demonstrate LLM-GAN's effectiveness in both prediction performance and explanation quality. We further showcase the integration of LLM-GAN to a cloud-native AI platform to provide better fake news detection service in the cloud.",
            "id": "2409.01787",
            "link": "http://arxiv.org/abs/2409.01787v1",
            "published": "2024-09-03T11:06:45+00:00",
            "updated": "2024-09-03T11:06:45+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 28
        },
        "2409.01854": {
            "authors": [
                "Yuchen Shi",
                "Guochao Jiang",
                "Tian Qiu",
                "Deqing Yang"
            ],
            "title": "AgentRE: An Agent-Based Framework for Navigating Complex Information Landscapes in Relation Extraction",
            "abstract": "The relation extraction (RE) in complex scenarios faces challenges such as diverse relation types and ambiguous relations between entities within a single sentence, leading to the poor performance of pure \"text-in, text-out\" language models (LMs). To address these challenges, in this paper, we propose an agent-based RE framework, namely AgentRE, which fully leverages the potential of large language models (LLMs) including memory, retrieval and reflection, to achieve RE in complex scenarios. Specifically, three major modules are built in AgentRE serving as the tools to help the agent acquire and process various useful information, thereby obtaining improved RE performance. Our extensive experimental results upon two datasets in English and Chinese demonstrate our AgentRE's superior performance, especially in low-resource scenarios. Additionally, the trajectories generated by AgentRE can be refined to construct a high-quality training dataset incorporating different reasoning methods, which can be used to fine-tune smaller models. Code is available at https://github.com/Lightblues/AgentRE.",
            "id": "2409.01854",
            "link": "http://arxiv.org/abs/2409.01854v1",
            "published": "2024-09-03T12:53:05+00:00",
            "updated": "2024-09-03T12:53:05+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 15
        },
        "2409.01941": {
            "authors": [
                "Jack Krolik",
                "Herprit Mahal",
                "Feroz Ahmad",
                "Gaurav Trivedi",
                "Bahador Saket"
            ],
            "title": "Towards Leveraging Large Language Models for Automated Medical Q&A Evaluation",
            "abstract": "This paper explores the potential of using Large Language Models (LLMs) to automate the evaluation of responses in medical Question and Answer (Q\\&A) systems, a crucial form of Natural Language Processing. Traditionally, human evaluation has been indispensable for assessing the quality of these responses. However, manual evaluation by medical professionals is time-consuming and costly. Our study examines whether LLMs can reliably replicate human evaluations by using questions derived from patient data, thereby saving valuable time for medical experts. While the findings suggest promising results, further research is needed to address more specific or complex questions that were beyond the scope of this initial investigation.",
            "id": "2409.01941",
            "link": "http://arxiv.org/abs/2409.01941v1",
            "published": "2024-09-03T14:38:29+00:00",
            "updated": "2024-09-03T14:38:29+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG",
                "I.2.7; J.3"
            ],
            "max_author_hindex": 15
        },
        "2409.02050": {
            "authors": [
                "Hukai Huang",
                "Jiayan Lin",
                "Kaidi Wang",
                "Yishuang Li",
                "Wenhao Guan",
                "Lin Li",
                "Qingyang Hong"
            ],
            "title": "Enhancing Code-Switching Speech Recognition with LID-Based Collaborative Mixture of Experts Model",
            "abstract": "Due to the inherent difficulty in modeling phonetic similarities across different languages, code-switching speech recognition presents a formidable challenge. This study proposes a Collaborative-MoE, a Mixture of Experts (MoE) model that leverages a collaborative mechanism among expert groups. Initially, a preceding routing network explicitly learns Language Identification (LID) tasks and selects experts based on acquired LID weights. This process ensures robust routing information to the MoE layer, mitigating interference from diverse language domains on expert network parameter updates. The LID weights are also employed to facilitate inter-group collaboration, enabling the integration of language-specific representations. Furthermore, within each language expert group, a gating network operates unsupervised to foster collaboration on attributes beyond language. Extensive experiments demonstrate the efficacy of our approach, achieving significant performance enhancements compared to alternative methods. Importantly, our method preserves the efficient inference capabilities characteristic of MoE models without necessitating additional pre-training.",
            "id": "2409.02050",
            "link": "http://arxiv.org/abs/2409.02050v2",
            "published": "2024-09-03T16:53:38+00:00",
            "updated": "2024-09-05T11:54:52+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 12
        },
        "2409.02078": {
            "authors": [
                "Michael Burnham",
                "Kayla Kahn",
                "Ryan Yank Wang",
                "Rachel X. Peng"
            ],
            "title": "Political DEBATE: Efficient Zero-shot and Few-shot Classifiers for Political Text",
            "abstract": "Social scientists quickly adopted large language models due to their ability to annotate documents without supervised training, an ability known as zero-shot learning. However, due to their compute demands, cost, and often proprietary nature, these models are often at odds with replication and open science standards. This paper introduces the Political DEBATE (DeBERTa Algorithm for Textual Entailment) language models for zero-shot and few-shot classification of political documents. These models are not only as good, or better than, state-of-the art large language models at zero and few-shot classification, but are orders of magnitude more efficient and completely open source. By training the models on a simple random sample of 10-25 documents, they can outperform supervised classifiers trained on hundreds or thousands of documents and state-of-the-art generative models with complex, engineered prompts. Additionally, we release the PolNLI dataset used to train these models -- a corpus of over 200,000 political documents with highly accurate labels across over 800 classification tasks.",
            "id": "2409.02078",
            "link": "http://arxiv.org/abs/2409.02078v1",
            "published": "2024-09-03T17:26:17+00:00",
            "updated": "2024-09-03T17:26:17+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 6
        },
        "2409.02257": {
            "authors": [
                "Saeid Asgari Taghanaki",
                "Aliasgahr Khani",
                "Amir Khasahmadi"
            ],
            "title": "MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs",
            "abstract": "Existing benchmarks for large language models (LLMs) increasingly struggle to differentiate between top-performing models, underscoring the need for more challenging evaluation frameworks. We introduce MMLU-Pro+, an enhanced benchmark building upon MMLU-Pro to assess shortcut learning and higher-order reasoning in LLMs. By incorporating questions with multiple correct answers across diverse domains, MMLU-Pro+ tests LLMs' ability to engage in complex reasoning and resist simplistic problem-solving strategies. Our results show that MMLU-Pro+ maintains MMLU-Pro's difficulty while providing a more rigorous test of model discrimination, particularly in multi-correct answer scenarios. We introduce novel metrics like shortcut selection ratio and correct pair identification ratio, offering deeper insights into model behavior and anchoring bias. Evaluations of five state-of-the-art LLMs reveal significant performance gaps, highlighting variations in reasoning abilities and bias susceptibility. We release the dataset and evaluation codes at \\url{https://github.com/asgsaeid/mmlu-pro-plus}.",
            "id": "2409.02257",
            "link": "http://arxiv.org/abs/2409.02257v1",
            "published": "2024-09-03T19:31:03+00:00",
            "updated": "2024-09-03T19:31:03+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 12
        },
        "2409.02361": {
            "authors": [
                "Yeonjun In",
                "Sungchul Kim",
                "Ryan A. Rossi",
                "Md Mehrab Tanjim",
                "Tong Yu",
                "Ritwik Sinha",
                "Chanyoung Park"
            ],
            "title": "Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented Ambiguous Question Answering",
            "abstract": "The retrieval augmented generation (RAG) framework addresses an ambiguity in user queries in QA systems by retrieving passages that cover all plausible interpretations and generating comprehensive responses based on the passages. However, our preliminary studies reveal that a single retrieval process often suffers from low quality results, as the retrieved passages frequently fail to capture all plausible interpretations. Although the iterative RAG approach has been proposed to address this problem, it comes at the cost of significantly reduced efficiency. To address these issues, we propose the diversify-verify-adapt (DIVA) framework. DIVA first diversifies the retrieved passages to encompass diverse interpretations. Subsequently, DIVA verifies the quality of the passages and adapts the most suitable approach tailored to their quality. This approach improves the QA systems accuracy and robustness by handling low quality retrieval issue in ambiguous questions, while enhancing efficiency.",
            "id": "2409.02361",
            "link": "http://arxiv.org/abs/2409.02361v1",
            "published": "2024-09-04T01:14:04+00:00",
            "updated": "2024-09-04T01:14:04+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 35
        },
        "2409.02384": {
            "authors": [
                "Shikhar Vashishth",
                "Harman Singh",
                "Shikhar Bharadwaj",
                "Sriram Ganapathy",
                "Chulayuth Asawaroengchai",
                "Kartik Audhkhasi",
                "Andrew Rosenberg",
                "Ankur Bapna",
                "Bhuvana Ramabhadran"
            ],
            "title": "STAB: Speech Tokenizer Assessment Benchmark",
            "abstract": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
            "id": "2409.02384",
            "link": "http://arxiv.org/abs/2409.02384v1",
            "published": "2024-09-04T02:20:59+00:00",
            "updated": "2024-09-04T02:20:59+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 49
        },
        "2409.02481": {
            "authors": [
                "Junyoung Lee",
                "Ninad Dixit",
                "Kaustav Chakrabarti",
                "S. Supraja"
            ],
            "title": "Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification",
            "abstract": "Effective question classification is crucial for AI-driven educational tools, enabling adaptive learning systems to categorize questions by skill area, difficulty level, and competence. This classification not only supports educational diagnostics and analytics but also enhances complex tasks like information retrieval and question answering by associating questions with relevant categories. Traditional methods, often based on word embeddings and conventional classifiers, struggle to capture the nuanced relationships in natural language, leading to suboptimal performance. To address this, we propose a novel approach leveraging graph convolutional networks (GCNs), named Phrase Question-Graph Convolutional Network (PQ-GCN) to better model the inherent structure of questions. By representing questions as graphs -- where nodes signify words or phrases and edges denote syntactic or semantic relationships -- our method allows GCNs to learn from the interconnected nature of language more effectively. Additionally, we explore the incorporation of phrase-based features to enhance classification accuracy, especially in low-resource settings. Our findings demonstrate that GCNs, augmented with these features, offer a promising solution for more accurate and context-aware question classification, bridging the gap between graph neural network research and practical educational applications.",
            "id": "2409.02481",
            "link": "http://arxiv.org/abs/2409.02481v1",
            "published": "2024-09-04T07:13:30+00:00",
            "updated": "2024-09-04T07:13:30+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 23
        },
        "2409.02519": {
            "authors": [
                "Arianna Muti",
                "Federico Ruggeri",
                "Khalid Al-Khatib",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Tommaso Caselli"
            ],
            "title": "Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic Reasoning with Argumentation Theory-Driven Prompts",
            "abstract": "We propose misogyny detection as an Argumentative Reasoning task and we investigate the capacity of large language models (LLMs) to understand the implicit reasoning used to convey misogyny in both Italian and English. The central aim is to generate the missing reasoning link between a message and the implied meanings encoding the misogyny. Our study uses argumentation theory as a foundation to form a collection of prompts in both zero-shot and few-shot settings. These prompts integrate different techniques, including chain-of-thought reasoning and augmented knowledge. Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning.",
            "id": "2409.02519",
            "link": "http://arxiv.org/abs/2409.02519v1",
            "published": "2024-09-04T08:27:43+00:00",
            "updated": "2024-09-04T08:27:43+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.SI"
            ],
            "max_author_hindex": 41
        },
        "2409.02667": {
            "authors": [
                "Gokhan Dogru"
            ],
            "title": "Creating Domain-Specific Translation Memories for Machine Translation Fine-tuning: The TRENCARD Bilingual Cardiology Corpus",
            "abstract": "This article investigates how translation memories (TM) can be created by translators or other language professionals in order to compile domain-specific parallel corpora , which can then be used in different scenarios, such as machine translation training and fine-tuning, TM leveraging, and/or large language model fine-tuning. The article introduces a semi-automatic TM preparation methodology leveraging primarily translation tools used by translators in favor of data quality and control by the translators. This semi-automatic methodology is then used to build a cardiology-based Turkish -> English corpus from bilingual abstracts of Turkish cardiology journals. The resulting corpus called TRENCARD Corpus has approximately 800,000 source words and 50,000 sentences. Using this methodology, translators can build their custom TMs in a reasonable time and use them in their bilingual data requiring tasks.",
            "id": "2409.02667",
            "link": "http://arxiv.org/abs/2409.02667v1",
            "published": "2024-09-04T12:48:30+00:00",
            "updated": "2024-09-04T12:48:30+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 1
        },
        "2409.02712": {
            "authors": [
                "Nidhi Kowtal",
                "Tejas Deshpande",
                "Raviraj Joshi"
            ],
            "title": "A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations",
            "abstract": "Machine translation in low-resource language pairs faces significant challenges due to the scarcity of parallel corpora and linguistic resources. This study focuses on the case of English-Marathi language pairs, where existing datasets are notably noisy, impeding the performance of machine translation models. To mitigate the impact of data quality issues, we propose a data filtering approach based on cross-lingual sentence representations. Our methodology leverages a multilingual SBERT model to filter out problematic translations in the training data. Specifically, we employ an IndicSBERT similarity model to assess the semantic equivalence between original and translated sentences, allowing us to retain linguistically correct translations while discarding instances with substantial deviations. The results demonstrate a significant improvement in translation quality over the baseline post-filtering with IndicSBERT. This illustrates how cross-lingual sentence representations can reduce errors in machine translation scenarios with limited resources. By integrating multilingual sentence BERT models into the translation pipeline, this research contributes to advancing machine translation techniques in low-resource environments. The proposed method not only addresses the challenges in English-Marathi language pairs but also provides a valuable framework for enhancing translation quality in other low-resource language translation tasks.",
            "id": "2409.02712",
            "link": "http://arxiv.org/abs/2409.02712v1",
            "published": "2024-09-04T13:49:45+00:00",
            "updated": "2024-09-04T13:49:45+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 15
        },
        "2409.02725": {
            "authors": [
                "Mathieu La\u00ef-king",
                "Patrick Paroubek"
            ],
            "title": "Pre-training data selection for biomedical domain adaptation using journal impact metrics",
            "abstract": "Domain adaptation is a widely used method in natural language processing (NLP) to improve the performance of a language model within a specific domain. This method is particularly common in the biomedical domain, which sees regular publication of numerous scientific articles. PubMed, a significant corpus of text, is frequently used in the biomedical domain. The primary objective of this study is to explore whether refining a pre-training dataset using specific quality metrics for scientific papers can enhance the performance of the resulting model. To accomplish this, we employ two straightforward journal impact metrics and conduct experiments by continually pre-training BERT on various subsets of the complete PubMed training set, we then evaluate the resulting models on biomedical language understanding tasks from the BLURB benchmark. Our results show that pruning using journal impact metrics is not efficient. But we also show that pre-training using fewer abstracts (but with the same number of training steps) does not necessarily decrease the resulting model's performance.",
            "id": "2409.02725",
            "link": "http://arxiv.org/abs/2409.02725v1",
            "published": "2024-09-04T13:59:48+00:00",
            "updated": "2024-09-04T13:59:48+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "I.2.7"
            ],
            "max_author_hindex": 19
        },
        "2409.02751": {
            "authors": [
                "Yiheng Wang",
                "Jiayu Lin",
                "Zuoquan Lin"
            ],
            "title": "A Comparative Study of Pre-training and Self-training",
            "abstract": "Pre-training and self-training are two approaches to semi-supervised learning. The comparison between pre-training and self-training has been explored. However, the previous works led to confusing findings: self-training outperforms pre-training experienced on some tasks in computer vision, and contrarily, pre-training outperforms self-training experienced on some tasks in natural language processing, under certain conditions of incomparable settings. We propose, comparatively and exhaustively, an ensemble method to empirical study all feasible training paradigms combining pre-training, self-training, and fine-tuning within consistent foundational settings comparable to data augmentation. We conduct experiments on six datasets, four data augmentation, and imbalanced data for sentiment analysis and natural language inference tasks. Our findings confirm that the pre-training and fine-tuning paradigm yields the best overall performances. Moreover, self-training offers no additional benefits when combined with semi-supervised pre-training.",
            "id": "2409.02751",
            "link": "http://arxiv.org/abs/2409.02751v1",
            "published": "2024-09-04T14:30:13+00:00",
            "updated": "2024-09-04T14:30:13+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 58
        },
        "2409.02813": {
            "authors": [
                "Xiang Yue",
                "Tianyu Zheng",
                "Yuansheng Ni",
                "Yubo Wang",
                "Kai Zhang",
                "Shengbang Tong",
                "Yuxuan Sun",
                "Botao Yu",
                "Ge Zhang",
                "Huan Sun",
                "Yu Su",
                "Wenhu Chen",
                "Graham Neubig"
            ],
            "title": "MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark",
            "abstract": "This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly \"see\" and \"read\" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI.",
            "id": "2409.02813",
            "link": "http://arxiv.org/abs/2409.02813v2",
            "published": "2024-09-04T15:31:26+00:00",
            "updated": "2024-09-10T12:55:31+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.CV"
            ],
            "max_author_hindex": 81
        },
        "2409.02841": {
            "authors": [
                "Anton Ehrmanntraut"
            ],
            "title": "Historical German Text Normalization Using Type- and Token-Based Language Modeling",
            "abstract": "Historic variations of spelling poses a challenge for full-text search or natural language processing on historical digitized texts. To minimize the gap between the historic orthography and contemporary spelling, usually an automatic orthographic normalization of the historical source material is pursued. This report proposes a normalization system for German literary texts from c. 1700-1900, trained on a parallel corpus. The proposed system makes use of a machine learning approach using Transformer language models, combining an encoder-decoder model to normalize individual word types, and a pre-trained causal language model to adjust these normalizations within their context. An extensive evaluation shows that the proposed system provides state-of-the-art accuracy, comparable with a much larger fully end-to-end sentence-based normalization system, fine-tuning a pre-trained Transformer large language model. However, the normalization of historical text remains a challenge due to difficulties for models to generalize, and the lack of extensive high-quality parallel data.",
            "id": "2409.02841",
            "link": "http://arxiv.org/abs/2409.02841v1",
            "published": "2024-09-04T16:14:05+00:00",
            "updated": "2024-09-04T16:14:05+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 2
        },
        "2409.03021": {
            "authors": [
                "Yu-Hsiang Wang",
                "Andrew Bai",
                "Che-Ping Tsai",
                "Cho-Jui Hsieh"
            ],
            "title": "CLUE: Concept-Level Uncertainty Estimation for Large Language Models",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in various natural language generation (NLG) tasks. Previous studies suggest that LLMs' generation process involves uncertainty. However, existing approaches to uncertainty estimation mainly focus on sequence-level uncertainty, overlooking individual pieces of information within sequences. These methods fall short in separately assessing the uncertainty of each component in a sequence. In response, we propose a novel framework for Concept-Level Uncertainty Estimation (CLUE) for LLMs. We leverage LLMs to convert output sequences into concept-level representations, breaking down sequences into individual concepts and measuring the uncertainty of each concept separately. We conduct experiments to demonstrate that CLUE can provide more interpretable uncertainty estimation results compared with sentence-level uncertainty, and could be a useful tool for various tasks such as hallucination detection and story generation.",
            "id": "2409.03021",
            "link": "http://arxiv.org/abs/2409.03021v1",
            "published": "2024-09-04T18:27:12+00:00",
            "updated": "2024-09-04T18:27:12+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 74
        },
        "2409.03059": {
            "authors": [
                "Annika Heuser",
                "Tyler Kendall",
                "Miguel del Rio",
                "Quinten McNamara",
                "Nishchal Bhandari",
                "Corey Miller",
                "Mig\u00fcel Jett\u00e9"
            ],
            "title": "Quantification of stylistic differences in human- and ASR-produced transcripts of African American English",
            "abstract": "Common measures of accuracy used to assess the performance of automatic speech recognition (ASR) systems, as well as human transcribers, conflate multiple sources of error. Stylistic differences, such as verbatim vs non-verbatim, can play a significant role in ASR performance evaluation when differences exist between training and test datasets. The problem is compounded for speech from underrepresented varieties, where the speech to orthography mapping is not as standardized. We categorize the kinds of stylistic differences between 6 transcription versions, 4 human- and 2 ASR-produced, of 10 hours of African American English (AAE) speech. Focusing on verbatim features and AAE morphosyntactic features, we investigate the interactions of these categories with how well transcripts can be compared via word error rate (WER). The results, and overall analysis, help clarify how ASR outputs are a function of the decisions made by the training data's human transcribers.",
            "id": "2409.03059",
            "link": "http://arxiv.org/abs/2409.03059v1",
            "published": "2024-09-04T20:18:59+00:00",
            "updated": "2024-09-04T20:18:59+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 19
        },
        "2409.03115": {
            "authors": [
                "Sai Gopinath",
                "Joselyn Rodriguez"
            ],
            "title": "Probing self-attention in self-supervised speech models for cross-linguistic differences",
            "abstract": "Speech models have gained traction thanks to increase in accuracy from novel transformer architectures. While this impressive increase in performance across automatic speech recognition (ASR) benchmarks is noteworthy, there is still much that is unknown about the use of attention mechanisms for speech-related tasks. For example, while it is assumed that these models are learning language-independent (i.e., universal) speech representations, there has not yet been an in-depth exploration of what it would mean for the models to be language-independent. In the current paper, we explore this question within the realm of self-attention mechanisms of one small self-supervised speech transformer model (TERA). We find that even with a small model, the attention heads learned are diverse ranging from almost entirely diagonal to almost entirely global regardless of the training language. We highlight some notable differences in attention patterns between Turkish and English and demonstrate that the models do learn important phonological information during pretraining. We also present a head ablation study which shows that models across languages primarily rely on diagonal heads to classify phonemes.",
            "id": "2409.03115",
            "link": "http://arxiv.org/abs/2409.03115v1",
            "published": "2024-09-04T22:47:33+00:00",
            "updated": "2024-09-04T22:47:33+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG",
                "68T10"
            ],
            "max_author_hindex": 34
        },
        "2409.03171": {
            "authors": [
                "Mitchell DeHaven"
            ],
            "title": "MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering",
            "abstract": "In this paper we present a multi-adapter retrieval augmented generation system (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP 2024. CRAG is a question answering dataset contains 3 different subtasks aimed at realistic question and answering RAG related tasks, with a diverse set of question topics, question types, time dynamic answers, and questions featuring entities of varying popularity.   Our system follows a standard setup for web based RAG, which uses processed web pages to provide context for an LLM to produce generations, while also querying API endpoints for additional information. MARAGS also utilizes multiple different adapters to solve the various requirements for these tasks with a standard cross-encoder model for ranking candidate passages relevant for answering the question. Our system achieved 2nd place for Task 1 as well as 3rd place on Task 2.",
            "id": "2409.03171",
            "link": "http://arxiv.org/abs/2409.03171v1",
            "published": "2024-09-05T01:58:29+00:00",
            "updated": "2024-09-05T01:58:29+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 4
        },
        "2409.03225": {
            "authors": [
                "Jeremy Qin",
                "Bang Liu",
                "Quoc Dinh Nguyen"
            ],
            "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
            "abstract": "Black-box large language models (LLMs) are increasingly deployed in various environments, making it essential for these models to effectively convey their confidence and uncertainty, especially in high-stakes settings. However, these models often exhibit overconfidence, leading to potential risks and misjudgments. Existing techniques for eliciting and calibrating LLM confidence have primarily focused on general reasoning datasets, yielding only modest improvements. Accurate calibration is crucial for informed decision-making and preventing adverse outcomes but remains challenging due to the complexity and variability of tasks these models perform. In this work, we investigate the miscalibration behavior of black-box LLMs within the healthcare setting. We propose a novel method, \\textit{Atypical Presentations Recalibration}, which leverages atypical presentations to adjust the model's confidence estimates. Our approach significantly improves calibration, reducing calibration errors by approximately 60\\% on three medical question answering datasets and outperforming existing methods such as vanilla verbalized confidence, CoT verbalized confidence and others. Additionally, we provide an in-depth analysis of the role of atypicality within the recalibration framework.",
            "id": "2409.03225",
            "link": "http://arxiv.org/abs/2409.03225v1",
            "published": "2024-09-05T03:45:35+00:00",
            "updated": "2024-09-05T03:45:35+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 28
        },
        "2409.03258": {
            "authors": [
                "Yukun Cao",
                "Shuo Han",
                "Zengyi Gao",
                "Zezhong Ding",
                "Xike Xie",
                "S. Kevin Zhou"
            ],
            "title": "GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding",
            "abstract": "Although Large Language Models (LLMs) have demonstrated potential in processing graphs, they struggle with comprehending graphical structure information through prompts of graph description sequences, especially as the graph size increases. We attribute this challenge to the uneven memory performance of LLMs across different positions in graph description sequences, known as ''positional biases''. To address this, we propose GraphInsight, a novel framework aimed at improving LLMs' comprehension of both macro- and micro-level graphical information. GraphInsight is grounded in two key strategies: 1) placing critical graphical information in positions where LLMs exhibit stronger memory performance, and 2) investigating a lightweight external knowledge base for regions with weaker memory performance, inspired by retrieval-augmented generation (RAG). Moreover, GraphInsight explores integrating these two strategies into LLM agent processes for composite graph tasks that require multi-step reasoning. Extensive empirical studies on benchmarks with a wide range of evaluation tasks show that GraphInsight significantly outperforms all other graph description methods (e.g., prompting techniques and reordering strategies) in understanding graph structures of varying sizes.",
            "id": "2409.03258",
            "link": "http://arxiv.org/abs/2409.03258v1",
            "published": "2024-09-05T05:34:16+00:00",
            "updated": "2024-09-05T05:34:16+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 31
        },
        "2409.03327": {
            "authors": [
                "A. Ram\u00edrez-de-Arellano",
                "F. G. C. Cabarle",
                "D. Orellana-Mart\u00edn",
                "M. J. P\u00e9rez-Jim\u00e9nez"
            ],
            "title": "Normal forms in Virus Machines",
            "abstract": "In the present work, we further study the computational power of virus machines (VMs in short). VMs provide a computing paradigm inspired by the transmission and replication networks of viruses. VMs consist of process units (called hosts) structured by a directed graph whose arcs are called channels and an instruction graph that controls the transmissions of virus objects among hosts. The present work complements our understanding of the computing power of VMs by introducing normal forms; these expressions restrict the features in a given computing model. Some of the features that we restrict in our normal forms include (a) the number of hosts, (b) the number of instructions, and (c) the number of virus objects in each host. After we recall some known results on the computing power of VMs we give our normal forms, such as the size of the loops in the network, proving new characterisations of family of sets, such as the finite sets, semilinear sets, or NRE.",
            "id": "2409.03327",
            "link": "http://arxiv.org/abs/2409.03327v1",
            "published": "2024-09-05T08:03:47+00:00",
            "updated": "2024-09-05T08:03:47+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.FL",
                "68Q07 (Primary) 68Q10, 68R01 (Secondary)",
                "F.0; F.1.1"
            ],
            "max_author_hindex": 16
        },
        "2409.03363": {
            "authors": [
                "Cheng Wang",
                "Yiwei Wang",
                "Bryan Hooi",
                "Yujun Cai",
                "Nanyun Peng",
                "Kai-Wei Chang"
            ],
            "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
            "abstract": "The training data in large language models is key to their success, but it also presents privacy and security risks, as it may contain sensitive information. Detecting pre-training data is crucial for mitigating these concerns. Existing methods typically analyze target text in isolation or solely with non-member contexts, overlooking potential insights from simultaneously considering both member and non-member contexts. While previous work suggested that member contexts provide little information due to the minor distributional shift they induce, our analysis reveals that these subtle shifts can be effectively leveraged when contrasted with non-member contexts. In this paper, we propose Con-ReCall, a novel approach that leverages the asymmetric distributional shifts induced by member and non-member contexts through contrastive decoding, amplifying subtle differences to enhance membership inference. Extensive empirical evaluations demonstrate that Con-ReCall achieves state-of-the-art performance on the WikiMIA benchmark and is robust against various text manipulation techniques.",
            "id": "2409.03363",
            "link": "http://arxiv.org/abs/2409.03363v1",
            "published": "2024-09-05T09:10:38+00:00",
            "updated": "2024-09-05T09:10:38+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 57
        },
        "2409.03440": {
            "authors": [
                "Phuc Phan Van",
                "Dat Nguyen Minh",
                "An Dinh Ngoc",
                "Huy Phan Thanh"
            ],
            "title": "Rx Strategist: Prescription Verification using LLM Agents System",
            "abstract": "To protect patient safety, modern pharmaceutical complexity demands strict prescription verification. We offer a new approach - Rx Strategist - that makes use of knowledge graphs and different search strategies to enhance the power of Large Language Models (LLMs) inside an agentic framework. This multifaceted technique allows for a multi-stage LLM pipeline and reliable information retrieval from a custom-built active ingredient database. Different facets of prescription verification, such as indication, dose, and possible drug interactions, are covered in each stage of the pipeline. We alleviate the drawbacks of monolithic LLM techniques by spreading reasoning over these stages, improving correctness and reliability while reducing memory demands. Our findings demonstrate that Rx Strategist surpasses many current LLMs, achieving performance comparable to that of a highly experienced clinical pharmacist. In the complicated world of modern medications, this combination of LLMs with organized knowledge and sophisticated search methods presents a viable avenue for reducing prescription errors and enhancing patient outcomes.",
            "id": "2409.03440",
            "link": "http://arxiv.org/abs/2409.03440v1",
            "published": "2024-09-05T11:42:26+00:00",
            "updated": "2024-09-05T11:42:26+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 18
        },
        "2409.03701": {
            "authors": [
                "Arnon Turetzky",
                "Yossi Adi"
            ],
            "title": "LAST: Language Model Aware Speech Tokenization",
            "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
            "id": "2409.03701",
            "link": "http://arxiv.org/abs/2409.03701v2",
            "published": "2024-09-05T16:57:39+00:00",
            "updated": "2024-09-10T14:45:15+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 30
        },
        "2409.03843": {
            "authors": [
                "Wenchao Dong",
                "Assem Zhunis",
                "Dongyoung Jeong",
                "Hyojin Chin",
                "Jiyoung Han",
                "Meeyoung Cha"
            ],
            "title": "Persona Setting Pitfall: Persistent Outgroup Biases in Large Language Models Arising from Social Identity Adoption",
            "abstract": "Drawing parallels between human cognition and artificial intelligence, we explored how large language models (LLMs) internalize identities imposed by targeted prompts. Informed by Social Identity Theory, these identity assignments lead LLMs to distinguish between \"we\" (the ingroup) and \"they\" (the outgroup). This self-categorization generates both ingroup favoritism and outgroup bias. Nonetheless, existing literature has predominantly focused on ingroup favoritism, often overlooking outgroup bias, which is a fundamental source of intergroup prejudice and discrimination. Our experiment addresses this gap by demonstrating that outgroup bias manifests as strongly as ingroup favoritism. Furthermore, we successfully mitigated the inherent pro-liberal, anti-conservative bias in LLMs by guiding them to adopt the perspectives of the initially disfavored group. These results were replicated in the context of gender bias. Our findings highlight the potential to develop more equitable and balanced language models.",
            "id": "2409.03843",
            "link": "http://arxiv.org/abs/2409.03843v1",
            "published": "2024-09-05T18:08:47+00:00",
            "updated": "2024-09-05T18:08:47+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 43
        },
        "2409.03856": {
            "authors": [
                "Yang Zhou",
                "Zhuoming Chen",
                "Zhaozhuo Xu",
                "Victoria Lin",
                "Beidi Chen"
            ],
            "title": "Sirius: Contextual Sparsity with Correction for Efficient LLMs",
            "abstract": "With the blossom of large language models (LLMs), inference efficiency becomes increasingly important. Various approximation methods are proposed to reduce the cost at inference time. Contextual Sparsity (CS) is appealing for its training-free nature and its ability to reach a higher compression ratio seemingly without quality degradation. However, after a comprehensive evaluation of contextual sparsity methods on various complex generation tasks, we find that although CS succeeds in prompt-understanding tasks, CS significantly degrades the model performance for reasoning, deduction, and knowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that sparse models often share general problem-solving logic and require only a few token corrections to recover the original model performance. This paper introduces Sirius, an efficient correction mechanism, which significantly recovers CS models quality on reasoning tasks while maintaining its efficiency gain. Sirius is evaluated on 6 models with 8 difficult generation tasks in reasoning, math, and coding and shows consistent effectiveness and efficiency. Also, we carefully develop a system implementation for Sirius and show that Sirius achieves roughly 20% reduction in latency for 8B model on-chip and 35% reduction for 70B model offloading. We open-source our implementation of Sirius at https://github.com/Infini-AI-Lab/Sirius.git.",
            "id": "2409.03856",
            "link": "http://arxiv.org/abs/2409.03856v1",
            "published": "2024-09-05T18:38:07+00:00",
            "updated": "2024-09-05T18:38:07+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 24
        },
        "2409.03939": {
            "authors": [
                "Umut Yildirim",
                "Rohan Dutta",
                "Burak Yildirim",
                "Atharva Vaidya"
            ],
            "title": "Experimentation in Content Moderation using RWKV",
            "abstract": "This paper investigates the RWKV model's efficacy in content moderation through targeted experimentation. We introduce a novel dataset specifically designed for distillation into smaller models, enhancing content moderation practices. This comprehensive dataset encompasses images, videos, sounds, and text data that present societal challenges. Leveraging advanced Large Language Models (LLMs), we generated an extensive set of responses -- 558,958 for text and 83,625 for images -- to train and refine content moderation systems. Our core experimentation involved fine-tuning the RWKV model, capitalizing on its CPU-efficient architecture to address large-scale content moderation tasks. By highlighting the dataset's potential for knowledge distillation, this study not only demonstrates RWKV's capability in improving the accuracy and efficiency of content moderation systems but also paves the way for developing more compact, resource-efficient models in this domain. Datasets and models can be found in HuggingFace: https://huggingface.co/modrwkv",
            "id": "2409.03939",
            "link": "http://arxiv.org/abs/2409.03939v1",
            "published": "2024-09-05T23:17:18+00:00",
            "updated": "2024-09-05T23:17:18+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "68T50",
                "I.2.7"
            ],
            "max_author_hindex": 10
        },
        "2409.04009": {
            "authors": [
                "Miao Fan",
                "Yeqi Bai",
                "Mingming Sun",
                "Ping Li"
            ],
            "title": "Large Margin Prototypical Network for Few-shot Relation Classification with Fine-grained Features",
            "abstract": "Relation classification (RC) plays a pivotal role in both natural language understanding and knowledge graph completion. It is generally formulated as a task to recognize the relationship between two entities of interest appearing in a free-text sentence. Conventional approaches on RC, regardless of feature engineering or deep learning based, can obtain promising performance on categorizing common types of relation leaving a large proportion of unrecognizable long-tail relations due to insufficient labeled instances for training. In this paper, we consider few-shot learning is of great practical significance to RC and thus improve a modern framework of metric learning for few-shot RC. Specifically, we adopt the large-margin ProtoNet with fine-grained features, expecting they can generalize well on long-tail relations. Extensive experiments were conducted by FewRel, a large-scale supervised few-shot RC dataset, to evaluate our framework: LM-ProtoNet (FGF). The results demonstrate that it can achieve substantial improvements over many baseline approaches.",
            "id": "2409.04009",
            "link": "http://arxiv.org/abs/2409.04009v1",
            "published": "2024-09-06T03:28:38+00:00",
            "updated": "2024-09-06T03:28:38+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 18
        },
        "2409.04057": {
            "authors": [
                "Ziqi Jin",
                "Wei Lu"
            ],
            "title": "Self-Harmonized Chain of Thought",
            "abstract": "Chain-of-Thought (CoT) prompting reveals that large language models are capable of performing complex reasoning via intermediate steps. CoT prompting is primarily categorized into three approaches. The first approach utilizes straightforward prompts like ``Let's think step by step'' to generate a sequential thought process before yielding an answer. The second approach makes use of human-crafted, step-by-step demonstrations to guide the model's reasoning process. The third automates the generation of reasoned demonstrations with the 'Let's think step by step'.This approach sometimes leads to reasoning errors, highlighting the need to diversify demonstrations to mitigate its misleading effects. However, diverse demonstrations pose challenges for effective representations. In this work, we propose ECHO, a self-harmonized chain-of-thought prompting method. It consolidates diverse solution paths into a uniform and effective solution pattern.ECHO demonstrates the best overall performance across three reasoning domains.",
            "id": "2409.04057",
            "link": "http://arxiv.org/abs/2409.04057v1",
            "published": "2024-09-06T06:57:04+00:00",
            "updated": "2024-09-06T06:57:04+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 55
        },
        "2409.04150": {
            "authors": [
                "Xiangke Zeng",
                "Zuchao Li",
                "Lefei Zhang",
                "Ping Wang",
                "Hongqiu Wu",
                "Hai Zhao"
            ],
            "title": "A Coin Has Two Sides: A Novel Detector-Corrector Framework for Chinese Spelling Correction",
            "abstract": "Chinese Spelling Correction (CSC) stands as a foundational Natural Language Processing (NLP) task, which primarily focuses on the correction of erroneous characters in Chinese texts. Certain existing methodologies opt to disentangle the error correction process, employing an additional error detector to pinpoint error positions. However, owing to the inherent performance limitations of error detector, precision and recall are like two sides of the coin which can not be both facing up simultaneously. Furthermore, it is also worth investigating how the error position information can be judiciously applied to assist the error correction. In this paper, we introduce a novel approach based on error detector-corrector framework. Our detector is designed to yield two error detection results, each characterized by high precision and recall. Given that the occurrence of errors is context-dependent and detection outcomes may be less precise, we incorporate the error detection results into the CSC task using an innovative feature fusion strategy and a selective masking strategy. Empirical experiments conducted on mainstream CSC datasets substantiate the efficacy of our proposed method.",
            "id": "2409.04150",
            "link": "http://arxiv.org/abs/2409.04150v1",
            "published": "2024-09-06T09:26:45+00:00",
            "updated": "2024-09-06T09:26:45+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 46
        },
        "2409.04512": {
            "authors": [
                "Tejas Deshpande",
                "Nidhi Kowtal",
                "Raviraj Joshi"
            ],
            "title": "Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages",
            "abstract": "This paper introduces Chain of Translation Prompting (CoTR), a novel strategy designed to enhance the performance of language models in low-resource languages. CoTR restructures prompts to first translate the input context from a low-resource language into a higher-resource language, such as English. The specified task like generation, classification, or any other NLP function is then performed on the translated text, with the option to translate the output back to the original language if needed. All these steps are specified in a single prompt. We demonstrate the effectiveness of this method through a case study on the low-resource Indic language Marathi. The CoTR strategy is applied to various tasks, including sentiment analysis, hate speech classification, subject classification and text generation, and its efficacy is showcased by comparing it with regular prompting methods. Our results underscore the potential of translation-based prompting strategies to significantly improve multilingual LLM performance in low-resource languages, offering valuable insights for future research and applications. We specifically see the highest accuracy improvements with the hate speech detection task. The technique also has the potential to enhance the quality of synthetic data generation for underrepresented languages using LLMs.",
            "id": "2409.04512",
            "link": "http://arxiv.org/abs/2409.04512v1",
            "published": "2024-09-06T17:15:17+00:00",
            "updated": "2024-09-06T17:15:17+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 15
        },
        "2409.04556": {
            "authors": [
                "Jackson Petty",
                "Sjoerd van Steenkiste",
                "Tal Linzen"
            ],
            "title": "How Does Code Pretraining Affect Language Model Task Performance?",
            "abstract": "Large language models are increasingly trained on corpora containing both natural language and non-linguistic data like source code. Aside from aiding programming-related tasks, anecdotal evidence suggests that including code in pretraining corpora may improve performance on other, unrelated tasks, yet to date no work has been able to establish a causal connection by controlling between language and code data. Here we do just this. We pretrain language models on datasets which interleave natural language and code in two different settings: additive, in which the total volume of data seen during pretraining is held constant; and competitive, in which the volume of language data is held constant. We study how the pretraining mixture affects performance on (a) a diverse collection of tasks included in the BigBench benchmark, and (b) compositionality, measured by generalization accuracy on semantic parsing and syntactic transformations. We find that pretraining on higher proportions of code improves performance on compositional tasks involving structured output (like semantic parsing), and mathematics. Conversely, increase code mixture can harm performance on other tasks, including on tasks that requires sensitivity to linguistic structure such as syntax or morphology, and tasks measuring real-world knowledge.",
            "id": "2409.04556",
            "link": "http://arxiv.org/abs/2409.04556v1",
            "published": "2024-09-06T18:33:38+00:00",
            "updated": "2024-09-06T18:33:38+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 37
        },
        "2409.04574": {
            "authors": [
                "Xinyue Liu",
                "Harshita Diddee",
                "Daphne Ippolito"
            ],
            "title": "Customizing Large Language Model Generation Style using Parameter-Efficient Finetuning",
            "abstract": "One-size-fits-all large language models (LLMs) are increasingly being used to help people with their writing. However, the style these models are trained to write in may not suit all users or use cases. LLMs would be more useful as writing assistants if their idiolect could be customized to match each user. In this paper, we explore whether parameter-efficient finetuning (PEFT) with Low-Rank Adaptation can effectively guide the style of LLM generations. We use this method to customize LLaMA-2 to ten different authors and show that the generated text has lexical, syntactic, and surface alignment with the target author but struggles with content memorization. Our findings highlight the potential of PEFT to support efficient, user-level customization of LLMs.",
            "id": "2409.04574",
            "link": "http://arxiv.org/abs/2409.04574v1",
            "published": "2024-09-06T19:25:18+00:00",
            "updated": "2024-09-06T19:25:18+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 9
        },
        "2409.04617": {
            "authors": [
                "Barrett Martin Lattimer",
                "Varun Gangal",
                "Ryan McDonald",
                "Yi Yang"
            ],
            "title": "Sparse Rewards Can Self-Train Dialogue Agents",
            "abstract": "Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM) agents, especially in multi-turn dialogue tasks, have been primarily driven by supervised fine-tuning and high-quality human feedback. However, as base LLM models continue to improve, acquiring meaningful human feedback has become increasingly challenging and costly. In certain domains, base LLM agents may eventually exceed human capabilities, making traditional feedback-driven methods impractical. In this paper, we introduce a novel self-improvement paradigm that empowers LLM agents to autonomously enhance their performance without external human feedback. Our method, Juxtaposed Outcomes for Simulation Harvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward simulation environment to extract ideal behaviors and further train the LLM on its own outputs. We present ToolWOZ, a sparse reward tool-calling simulation environment derived from MultiWOZ. We demonstrate that models trained with JOSH, both small and frontier, significantly improve tool-based interactions while preserving general model capabilities across diverse benchmarks. Our code and data are publicly available on GitHub.",
            "id": "2409.04617",
            "link": "http://arxiv.org/abs/2409.04617v1",
            "published": "2024-09-06T21:00:57+00:00",
            "updated": "2024-09-06T21:00:57+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 49
        },
        "2409.04778": {
            "authors": [
                "Runming Yang",
                "Taiqiang Wu",
                "Yujiu Yang"
            ],
            "title": "LoCa: Logit Calibration for Knowledge Distillation",
            "abstract": "Knowledge Distillation (KD), aiming to train a better student model by mimicking the teacher model, plays an important role in model compression. One typical way is to align the output logits. However, we find a common issue named mis-instruction, that the student would be misled when the predictions based on teacher logits do not follow the labels. Meanwhile, there is other useful dark knowledge in the logits such as the class discriminability, which is vital for distillation. In this paper, we propose a simple yet effective Logit Calibration (LoCa) method, which calibrates the logits from the teacher model based on the ground-truth labels. The key insight is to correct the prediction (to address the mis-instruction issue) and maintain useful dark knowledge simultaneously. Our proposed LoCa does not require any additional parameters. Empirical results on image classification and text generation tasks demonstrate that LoCa can effectively improve the performance of baselines.",
            "id": "2409.04778",
            "link": "http://arxiv.org/abs/2409.04778v1",
            "published": "2024-09-07T09:38:36+00:00",
            "updated": "2024-09-07T09:38:36+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 26
        },
        "2409.05021": {
            "authors": [
                "Yanni Xue",
                "Haojie Hao",
                "Jiakai Wang",
                "Qiang Sheng",
                "Renshuai Tao",
                "Yu Liang",
                "Pu Feng",
                "Xianglong Liu"
            ],
            "title": "Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text against Neural Machine Translation",
            "abstract": "While neural machine translation (NMT) models achieve success in our daily lives, they show vulnerability to adversarial attacks. Despite being harmful, these attacks also offer benefits for interpreting and enhancing NMT models, thus drawing increased research attention. However, existing studies on adversarial attacks are insufficient in both attacking ability and human imperceptibility due to their sole focus on the scope of language. This paper proposes a novel vision-fused attack (VFA) framework to acquire powerful adversarial text, i.e., more aggressive and stealthy. Regarding the attacking ability, we design the vision-merged solution space enhancement strategy to enlarge the limited semantic solution space, which enables us to search for adversarial candidates with higher attacking ability. For human imperceptibility, we propose the perception-retained adversarial text selection strategy to align the human text-reading mechanism. Thus, the finally selected adversarial text could be more deceptive. Extensive experiments on various models, including large language models (LLMs) like LLaMA and GPT-3.5, strongly support that VFA outperforms the comparisons by large margins (up to 81%/14% improvements on ASR/SSIM).",
            "id": "2409.05021",
            "link": "http://arxiv.org/abs/2409.05021v1",
            "published": "2024-09-08T08:22:17+00:00",
            "updated": "2024-09-08T08:22:17+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 53
        },
        "2409.05112": {
            "authors": [
                "Leyi Pan",
                "Aiwei Liu",
                "Yijian Lu",
                "Zitian Gao",
                "Yichen Di",
                "Lijie Wen",
                "Irwin King",
                "Philip S. Yu"
            ],
            "title": "WaterSeeker: Efficient Detection of Watermarked Segments in Large Documents",
            "abstract": "Watermarking algorithms for large language models (LLMs) have attained high accuracy in detecting LLM-generated text. However, existing methods primarily focus on distinguishing fully watermarked text from non-watermarked text, overlooking real-world scenarios where LLMs generate only small sections within large documents. In this scenario, balancing time complexity and detection performance poses significant challenges. This paper presents WaterSeeker, a novel approach to efficiently detect and locate watermarked segments amid extensive natural text. It first applies an efficient anomaly extraction method to preliminarily locate suspicious watermarked regions. Following this, it conducts a local traversal and performs full-text detection for more precise verification. Theoretical analysis and experimental results demonstrate that WaterSeeker achieves a superior balance between detection accuracy and computational efficiency. Moreover, WaterSeeker's localization ability supports the development of interpretable AI detection systems. This work pioneers a new direction in watermarked segment detection, facilitating more reliable AI-generated content identification.",
            "id": "2409.05112",
            "link": "http://arxiv.org/abs/2409.05112v1",
            "published": "2024-09-08T14:45:47+00:00",
            "updated": "2024-09-08T14:45:47+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "68T50",
                "I.2.7"
            ],
            "max_author_hindex": 33
        },
        "2409.05134": {
            "authors": [
                "Anusha Chhabra",
                "Dinesh Kumar Vishwakarma"
            ],
            "title": "Hate Content Detection via Novel Pre-Processing Sequencing and Ensemble Methods",
            "abstract": "Social media, particularly Twitter, has seen a significant increase in incidents like trolling and hate speech. Thus, identifying hate speech is the need of the hour. This paper introduces a computational framework to curb the hate content on the web. Specifically, this study presents an exhaustive study of pre-processing approaches by studying the impact of changing the sequence of text pre-processing operations for the identification of hate content. The best-performing pre-processing sequence, when implemented with popular classification approaches like Support Vector Machine, Random Forest, Decision Tree, Logistic Regression and K-Neighbor provides a considerable boost in performance. Additionally, the best pre-processing sequence is used in conjunction with different ensemble methods, such as bagging, boosting and stacking to improve the performance further. Three publicly available benchmark datasets (WZ-LS, DT, and FOUNTA), were used to evaluate the proposed approach for hate speech identification. The proposed approach achieves a maximum accuracy of 95.14% highlighting the effectiveness of the unique pre-processing approach along with an ensemble classifier.",
            "id": "2409.05134",
            "link": "http://arxiv.org/abs/2409.05134v1",
            "published": "2024-09-08T15:32:17+00:00",
            "updated": "2024-09-08T15:32:17+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 31
        },
        "2409.05136": {
            "authors": [
                "Anusha Chhabra",
                "Dinesh Kumar Vishwakarma"
            ],
            "title": "MHS-STMA: Multimodal Hate Speech Detection via Scalable Transformer-Based Multilevel Attention Framework",
            "abstract": "Social media has a significant impact on people's lives. Hate speech on social media has emerged as one of society's most serious issues recently. Text and pictures are two forms of multimodal data distributed within articles. Unimodal analysis has been the primary emphasis of earlier approaches. Additionally, when doing multimodal analysis, researchers neglect to preserve the distinctive qualities associated with each modality. The present article suggests a scalable architecture for multimodal hate content detection called transformer-based multilevel attention (STMA) to address these shortcomings. This architecture consists of three main parts: a combined attention-based deep learning mechanism, a vision attention mechanism encoder, and a caption attention-mechanism encoder. To identify hate content, each component uses various attention processes and uniquely handles multimodal data. Several studies employing multiple assessment criteria on three hate speech datasets: Hateful memes, MultiOff, and MMHS150K, validate the suggested architecture's efficacy. The outcomes demonstrate that on all three datasets, the suggested strategy performs better than the baseline approaches.",
            "id": "2409.05136",
            "link": "http://arxiv.org/abs/2409.05136v1",
            "published": "2024-09-08T15:42:18+00:00",
            "updated": "2024-09-08T15:42:18+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 31
        },
        "2409.05137": {
            "authors": [
                "Zichao Li",
                "Aizier Abulaiti",
                "Yaojie Lu",
                "Xuanang Chen",
                "Jia Zheng",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun"
            ],
            "title": "READoc: A Unified Benchmark for Realistic Document Structured Extraction",
            "abstract": "Document Structured Extraction (DSE) aims to extract structured content from raw documents. Despite the emergence of numerous DSE systems, their unified evaluation remains inadequate, significantly hindering the field's advancement. This problem is largely attributed to existing benchmark paradigms, which exhibit fragmented and localized characteristics. To address these limitations and offer a thorough evaluation of DSE systems, we introduce a novel benchmark named READoc, which defines DSE as a realistic task of converting unstructured PDFs into semantically rich Markdown. The READoc dataset is derived from 2,233 diverse and real-world documents from arXiv and GitHub. In addition, we develop a DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring modules, to conduct a unified evaluation of state-of-the-art DSE approaches. By evaluating a range of pipeline tools, expert visual models, and general VLMs, we identify the gap between current work and the unified, realistic DSE objective for the first time. We aspire that READoc will catalyze future research in DSE, fostering more comprehensive and practical solutions.",
            "id": "2409.05137",
            "link": "http://arxiv.org/abs/2409.05137v1",
            "published": "2024-09-08T15:42:48+00:00",
            "updated": "2024-09-08T15:42:48+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.CV"
            ],
            "max_author_hindex": 36
        },
        "2409.05224": {
            "authors": [
                "Zhe Cao",
                "Zhi Qu",
                "Hidetaka Kamigaito",
                "Taro Watanabe"
            ],
            "title": "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation",
            "abstract": "Multilingual neural machine translation models support fine-tuning hundreds of languages simultaneously. However, fine-tuning on full parameters solely is inefficient potentially leading to negative interactions among languages. In this work, we demonstrate that the fine-tuning for a language occurs in its intrinsic language-specific subspace with a tiny fraction of entire parameters. Thus, we propose language-specific LoRA to isolate intrinsic language-specific subspaces. Furthermore, we propose architecture learning techniques and introduce a gradual pruning schedule during fine-tuning to exhaustively explore the optimal setting and the minimal intrinsic subspaces for each language, resulting in a lightweight yet effective fine-tuning procedure. The experimental results on a 12-language subset and a 30-language subset of FLORES-101 show that our methods not only outperform full-parameter fine-tuning up to 2.25 spBLEU scores but also reduce trainable parameters to $0.4\\%$ for high and medium-resource languages and $1.6\\%$ for low-resource ones.",
            "id": "2409.05224",
            "link": "http://arxiv.org/abs/2409.05224v1",
            "published": "2024-09-08T21:40:44+00:00",
            "updated": "2024-09-08T21:40:44+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 25
        },
        "2409.02448": {
            "authors": [
                "Hoang Khanh Lam",
                "Kahandakanaththage Maduni Pramuditha Perera"
            ],
            "title": "Detecting Korean Food Using Image using Hierarchical Model",
            "abstract": "A solution was made available for Korean Food lovers who have dietary restrictions to identify the Korean food before consuming. Just by uploading a clear photo of the dish, people can get to know what they are eating. Image processing techniques together with machine learning helped to come up with this solution.",
            "id": "2409.02448",
            "link": "http://arxiv.org/abs/2409.02448v1",
            "published": "2024-09-04T05:06:34+00:00",
            "updated": "2024-09-04T05:06:34+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 3
        },
        "2409.02026": {
            "authors": [
                "Sean I. Young"
            ],
            "title": "Foundations of Large Language Model Compression -- Part 1: Weight Quantization",
            "abstract": "In recent years, compression of large language models (LLMs) has emerged as an important problem to allow language model deployment on resource-constrained devices, reduce computational costs, and mitigate the environmental footprint of large-scale AI infrastructure. In this paper, we present the foundations of LLM quantization from a convex optimization perspective and propose a quantization method that builds on these foundations and outperforms previous methods. Our quantization framework, CVXQ, scales to models containing hundreds of billions of weight parameters and provides users with the flexibility to compress models to any specified model size, post-training. A reference implementation of CVXQ can be obtained from https://github.com/seannz/cvxq.",
            "id": "2409.02026",
            "link": "http://arxiv.org/abs/2409.02026v1",
            "published": "2024-09-03T16:20:22+00:00",
            "updated": "2024-09-03T16:20:22+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 8
        },
        "2409.03131": {
            "authors": [
                "Alan Aqrawi",
                "Arian Abbasi"
            ],
            "title": "Well, that escalated quickly: The Single-Turn Crescendo Attack (STCA)",
            "abstract": "This paper introduces a new method for adversarial attacks on large language models (LLMs) called the Single-Turn Crescendo Attack (STCA). Building on the multi-turn crescendo attack method introduced by Russinovich, Salem, and Eldan (2024), which gradually escalates the context to provoke harmful responses, the STCA achieves similar outcomes in a single interaction. By condensing the escalation into a single, well-crafted prompt, the STCA bypasses typical moderation filters that LLMs use to prevent inappropriate outputs. This technique reveals vulnerabilities in current LLMs and emphasizes the importance of stronger safeguards in responsible AI (RAI). The STCA offers a novel method that has not been previously explored.",
            "id": "2409.03131",
            "link": "http://arxiv.org/abs/2409.03131v2",
            "published": "2024-09-04T23:45:10+00:00",
            "updated": "2024-09-10T21:53:46+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.CL"
            ],
            "max_author_hindex": 14
        },
        "2409.04649": {
            "authors": [
                "Liang Wang",
                "Shubham Jain",
                "Yingtong Dou",
                "Junpeng Wang",
                "Chin-Chia Michael Yeh",
                "Yujie Fan",
                "Prince Aboagye",
                "Yan Zheng",
                "Xin Dai",
                "Zhongfang Zhuang",
                "Uday Singh Saini",
                "Wei Zhang"
            ],
            "title": "Preserving Individuality while Following the Crowd: Understanding the Role of User Taste and Crowd Wisdom in Online Product Rating Prediction",
            "abstract": "Numerous algorithms have been developed for online product rating prediction, but the specific influence of user and product information in determining the final prediction score remains largely unexplored. Existing research often relies on narrowly defined data settings, which overlooks real-world challenges such as the cold-start problem, cross-category information utilization, and scalability and deployment issues. To delve deeper into these aspects, and particularly to uncover the roles of individual user taste and collective wisdom, we propose a unique and practical approach that emphasizes historical ratings at both the user and product levels, encapsulated using a continuously updated dynamic tree representation. This representation effectively captures the temporal dynamics of users and products, leverages user information across product categories, and provides a natural solution to the cold-start problem. Furthermore, we have developed an efficient data processing strategy that makes this approach highly scalable and easily deployable. Comprehensive experiments in real industry settings demonstrate the effectiveness of our approach. Notably, our findings reveal that individual taste dominates over collective wisdom in online product rating prediction, a perspective that contrasts with the commonly observed wisdom of the crowd phenomenon in other domains. This dominance of individual user taste is consistent across various model types, including the boosting tree model, recurrent neural network (RNN), and transformer-based architectures. This observation holds true across the overall population, within individual product categories, and in cold-start scenarios. Our findings underscore the significance of individual user tastes in the context of online product rating prediction and the robustness of our approach across different model architectures.",
            "id": "2409.04649",
            "link": "http://arxiv.org/abs/2409.04649v1",
            "published": "2024-09-06T23:16:06+00:00",
            "updated": "2024-09-06T23:16:06+00:00",
            "primary_category": "cs.SI",
            "categories": [
                "cs.SI",
                "cs.IR"
            ],
            "max_author_hindex": 29
        },
        "2409.00369": {
            "authors": [
                "Ridong Han",
                "Chaohao Yang",
                "Tao Peng",
                "Prayag Tiwari",
                "Xiang Wan",
                "Lu Liu",
                "Benyou Wang"
            ],
            "title": "An Empirical Study on Information Extraction using Large Language Models",
            "abstract": "Human-like large language models (LLMs), especially the most powerful and popular ones in OpenAI's GPT family, have proven to be very helpful for many natural language processing (NLP) related tasks. Therefore, various attempts have been made to apply LLMs to information extraction (IE), which is a fundamental NLP task that involves extracting information from unstructured plain text. To demonstrate the latest representative progress in LLMs' information extraction ability, we assess the information extraction ability of GPT-4 (the latest version of GPT at the time of writing this paper) from four perspectives: Performance, Evaluation Criteria, Robustness, and Error Types. Our results suggest a visible performance gap between GPT-4 and state-of-the-art (SOTA) IE methods. To alleviate this problem, considering the LLMs' human-like characteristics, we propose and analyze the effects of a series of simple prompt-based methods, which can be generalized to other LLMs and NLP tasks. Rich experiments show our methods' effectiveness and some of their remaining issues in improving GPT-4's information extraction ability.",
            "id": "2409.00369",
            "link": "http://arxiv.org/abs/2409.00369v3",
            "published": "2024-08-31T07:10:16+00:00",
            "updated": "2024-09-09T13:50:30+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 42
        },
        "2409.00509": {
            "authors": [
                "Zhiyuan Hu",
                "Yuliang Liu",
                "Jinman Zhao",
                "Suyuchen Wang",
                "Yan Wang",
                "Wei Shen",
                "Qing Gu",
                "Anh Tuan Luu",
                "See-Kiong Ng",
                "Zhiwei Jiang",
                "Bryan Hooi"
            ],
            "title": "LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models",
            "abstract": "Large language models (LLMs) face significant challenges in handling long-context tasks because of their limited effective context window size during pretraining, which restricts their ability to generalize over extended sequences. Meanwhile, extending the context window in LLMs through post-pretraining is highly resource-intensive. To address this, we introduce LongRecipe, an efficient training strategy for extending the context window of LLMs, including impactful token analysis, position index transformation, and training optimization strategies. It simulates long-sequence inputs while maintaining training efficiency and significantly improves the model's understanding of long-range dependencies. Experiments on three types of LLMs show that LongRecipe can utilize long sequences while requiring only 30% of the target context window size, and reduces computational training resource over 85% compared to full sequence training. Furthermore, LongRecipe also preserves the original LLM's capabilities in general tasks. Ultimately, we can extend the effective context window of open-source LLMs from 8k to 128k, achieving performance close to GPT-4 with just one day of dedicated training using a single GPU with 80G memory. Our code is released at https://github.com/zhiyuanhubj/LongRecipe.",
            "id": "2409.00509",
            "link": "http://arxiv.org/abs/2409.00509v2",
            "published": "2024-08-31T17:19:30+00:00",
            "updated": "2024-09-04T15:55:22+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 44
        },
        "2409.00527": {
            "authors": [
                "Angel Beshirov",
                "Milena Dobreva",
                "Dimitar Dimitrov",
                "Momchil Hardalov",
                "Ivan Koychev",
                "Preslav Nakov"
            ],
            "title": "Post-OCR Text Correction for Bulgarian Historical Documents",
            "abstract": "The digitization of historical documents is crucial for preserving the cultural heritage of the society. An important step in this process is converting scanned images to text using Optical Character Recognition (OCR), which can enable further search, information extraction, etc. Unfortunately, this is a hard problem as standard OCR tools are not tailored to deal with historical orthography as well as with challenging layouts. Thus, it is standard to apply an additional text correction step on the OCR output when dealing with such documents. In this work, we focus on Bulgarian, and we create the first benchmark dataset for evaluating the OCR text correction for historical Bulgarian documents written in the first standardized Bulgarian orthography: the Drinov orthography from the 19th century. We further develop a method for automatically generating synthetic data in this orthography, as well as in the subsequent Ivanchev orthography, by leveraging vast amounts of contemporary literature Bulgarian texts. We then use state-of-the-art LLMs and encoder-decoder framework which we augment with diagonal attention loss and copy and coverage mechanisms to improve the post-OCR text correction. The proposed method reduces the errors introduced during recognition and improves the quality of the documents by 25\\%, which is an increase of 16\\% compared to the state-of-the-art on the ICDAR 2019 Bulgarian dataset. We release our data and code at \\url{https://github.com/angelbeshirov/post-ocr-text-correction}.}",
            "id": "2409.00527",
            "link": "http://arxiv.org/abs/2409.00527v1",
            "published": "2024-08-31T19:27:46+00:00",
            "updated": "2024-08-31T19:27:46+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.DL",
                "cs.LG"
            ],
            "max_author_hindex": 69
        },
        "2409.00608": {
            "authors": [
                "Lutfi Eren Erdogan",
                "Nicholas Lee",
                "Siddharth Jha",
                "Sehoon Kim",
                "Ryan Tabrizi",
                "Suhong Moon",
                "Coleman Hooper",
                "Gopala Anumanchipalli",
                "Kurt Keutzer",
                "Amir Gholami"
            ],
            "title": "TinyAgent: Function Calling at the Edge",
            "abstract": "Recent large language models (LLMs) have enabled the development of advanced agentic systems that can integrate various tools and APIs to fulfill user queries through function calling. However, the deployment of these LLMs on the edge has not been explored since they typically require cloud-based infrastructure due to their substantial model size and computational demands. To this end, we present TinyAgent, an end-to-end framework for training and deploying task-specific small language model agents capable of function calling for driving agentic systems at the edge. We first show how to enable accurate function calling for open-source models via the LLMCompiler framework. We then systematically curate a high-quality dataset for function calling, which we use to fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient inference, we introduce a novel tool retrieval method to reduce the input prompt length and utilize quantization to further accelerate the inference speed. As a driving application, we demonstrate a local Siri-like system for Apple's MacBook that can execute user commands through text or voice input. Our results show that our models can achieve, and even surpass, the function-calling capabilities of larger models like GPT-4-Turbo, while being fully deployed at the edge. We open-source our dataset, models, and installable package and provide a demo video for our MacBook assistant agent.",
            "id": "2409.00608",
            "link": "http://arxiv.org/abs/2409.00608v1",
            "published": "2024-09-01T04:23:48+00:00",
            "updated": "2024-09-01T04:23:48+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 91
        },
        "2409.00935": {
            "authors": [
                "Hai Ye",
                "Hwee Tou Ng"
            ],
            "title": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
            "abstract": "Pre-trained large language models (LLMs) can be tailored to adhere to human instructions through instruction tuning. However, due to shifts in the distribution of test-time data, they may not always execute instructions accurately, potentially generating factual errors or misaligned content when acting as chat assistants. To enhance the reliability of LLMs in following instructions, we propose the study of selective instruction following, whereby the system declines to execute instructions if the anticipated response quality is low. We train judge models that can predict numerical quality scores for model responses. To address data scarcity, we introduce Self-J, a novel self-training framework for developing judge models without needing human-annotated quality scores. Our method leverages the model's inherent self-evaluation capability to extract information about response quality from labeled instruction-tuning data. It incorporates a gold reference answer to facilitate self-evaluation and recalibrates by assessing the semantic similarity between the response sample and the gold reference. During the training phase, we implement self-distillation as a regularization technique to enhance the capability of reference-free estimation. To validate alignment evaluation on general instruction-following tasks, we collect large-scale high-quality instructions from Hugging Face for model training and evaluation. Extensive experiments on five open-source models show that our method correlates much more with GPT-4 than strong baselines, e.g., supervised models distilled from GPT-4 and GPT-3.5-turbo. Our analysis shows our model's strong generalization across domains. Additionally, our judge models serve as good reward models, e.g., boosting WizardLM-13B-V1.2 from 89.17 to 92.48 and from 12.03 to 15.90 in version v1 and v2 of AlpacaEval respectively using best-of-32 sampling with our judge models.",
            "id": "2409.00935",
            "link": "http://arxiv.org/abs/2409.00935v1",
            "published": "2024-09-02T04:14:13+00:00",
            "updated": "2024-09-02T04:14:13+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 61
        },
        "2409.01227": {
            "authors": [
                "Barys Liskavets",
                "Maxim Ushakov",
                "Shuvendu Roy",
                "Mark Klibanov",
                "Ali Etemad",
                "Shane Luke"
            ],
            "title": "Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference",
            "abstract": "Large language models (LLMs) have triggered a new stream of research focusing on compressing the context length to reduce the computational cost while ensuring the retention of helpful information for LLMs to answer the given question. Token-based removal methods are one of the most prominent approaches in this direction, but risk losing the semantics of the context caused by intermediate token removal, especially under high compression ratios, while also facing challenges in computational efficiency. In this work, we propose context-aware prompt compression (CPC), a sentence-level prompt compression technique where its key innovation is a novel context-aware sentence encoder that provides a relevance score for each sentence for a given question. To train this encoder, we generate a new dataset consisting of questions, positives, and negative pairs where positives are sentences relevant to the question, while negatives are irrelevant context sentences. We train the encoder in a contrastive setup to learn context-aware sentence representations. Our method considerably outperforms prior works on prompt compression on benchmark datasets and is up to 10.93x faster at inference compared to the best token-level compression method. We also find better improvement for shorter length constraints in most benchmarks, showing the effectiveness of our proposed solution in the compression of relevant information in a shorter context. Finally, we release the code and the dataset for quick reproducibility and further development: https://github.com/Workday/cpc.",
            "id": "2409.01227",
            "link": "http://arxiv.org/abs/2409.01227v2",
            "published": "2024-09-02T13:02:51+00:00",
            "updated": "2024-09-04T10:20:59+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 25
        },
        "2409.01232": {
            "authors": [
                "Victor De Marez",
                "Thomas Winters",
                "Ayla Rigouts Terryn"
            ],
            "title": "THInC: A Theory-Driven Framework for Computational Humor Detection",
            "abstract": "Humor is a fundamental aspect of human communication and cognition, as it plays a crucial role in social engagement. Although theories about humor have evolved over centuries, there is still no agreement on a single, comprehensive humor theory. Likewise, computationally recognizing humor remains a significant challenge despite recent advances in large language models. Moreover, most computational approaches to detecting humor are not based on existing humor theories. This paper contributes to bridging this long-standing gap between humor theory research and computational humor detection by creating an interpretable framework for humor classification, grounded in multiple humor theories, called THInC (Theory-driven Humor Interpretation and Classification). THInC ensembles interpretable GA2M classifiers, each representing a different humor theory. We engineered a transparent flow to actively create proxy features that quantitatively reflect different aspects of theories. An implementation of this framework achieves an F1 score of 0.85. The associative interpretability of the framework enables analysis of proxy efficacy, alignment of joke features with theories, and identification of globally contributing features. This paper marks a pioneering effort in creating a humor detection framework that is informed by diverse humor theories and offers a foundation for future advancements in theory-driven humor classification. It also serves as a first step in automatically comparing humor theories in a quantitative manner.",
            "id": "2409.01232",
            "link": "http://arxiv.org/abs/2409.01232v1",
            "published": "2024-09-02T13:09:26+00:00",
            "updated": "2024-09-02T13:09:26+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 18
        },
        "2409.01495": {
            "authors": [
                "Yuan Yang",
                "Siheng Xiong",
                "Ehsan Shareghi",
                "Faramarz Fekri"
            ],
            "title": "The Compressor-Retriever Architecture for Language Model OS",
            "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced their capacity to aggregate and process information across multiple modalities, enabling them to perform a wide range of tasks such as multimodal data querying, tool usage, web interactions, and handling long documents. These capabilities pave the way for transforming LLMs from mere chatbots into general-purpose agents capable of interacting with the real world. This paper explores the concept of using a language model as the core component of an operating system (OS), effectively acting as a CPU that processes data stored in a context window, which functions as RAM. A key challenge in realizing such an LM OS is managing the life-long context and ensuring statefulness across sessions, a feature limited by the current session-based interaction paradigm due to context window size limit. To address this, we introduce compressor-retriever, a model-agnostic architecture designed for life-long context management. Unlike other long-context solutions such as retrieval-augmented generation, our approach exclusively uses the base model's forward function to compress and retrieve context, ensuring end-to-end differentiability. Preliminary experiments demonstrate the effectiveness of this architecture in in-context learning tasks, marking a step towards the development of a fully stateful LLM OS. Project repo available at: https://github.com/gblackout/LM-OS",
            "id": "2409.01495",
            "link": "http://arxiv.org/abs/2409.01495v1",
            "published": "2024-09-02T23:28:15+00:00",
            "updated": "2024-09-02T23:28:15+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 36
        },
        "2409.01658": {
            "authors": [
                "Wei Chen",
                "Zhen Huang",
                "Liang Xie",
                "Binbin Lin",
                "Houqiang Li",
                "Le Lu",
                "Xinmei Tian",
                "Deng Cai",
                "Yonggang Zhang",
                "Wenxiao Wan",
                "Xu Shen",
                "Jieping Ye"
            ],
            "title": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language Models with Pinpoint Tuning",
            "abstract": "Large Language Models (LLMs) tend to prioritize adherence to user prompts over providing veracious responses, leading to the sycophancy issue. When challenged by users, LLMs tend to admit mistakes and provide inaccurate responses even if they initially provided the correct answer. Recent works propose to employ supervised fine-tuning (SFT) to mitigate the sycophancy issue, while it typically leads to the degeneration of LLMs' general capability. To address the challenge, we propose a novel supervised pinpoint tuning (SPT), where the region-of-interest modules are tuned for a given objective. Specifically, SPT first reveals and verifies a small percentage (<5%) of the basic modules, which significantly affect a particular behavior of LLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified modules while freezing the rest. To verify the effectiveness of the proposed SPT, we conduct comprehensive experiments, demonstrating that SPT significantly mitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT introduces limited or even no side effects on the general capability of LLMs. Our results shed light on how to precisely, effectively, and efficiently explain and improve the targeted ability of LLMs.",
            "id": "2409.01658",
            "link": "http://arxiv.org/abs/2409.01658v1",
            "published": "2024-09-03T07:01:37+00:00",
            "updated": "2024-09-03T07:01:37+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 62
        },
        "2409.01659": {
            "authors": [
                "Wei Zhang",
                "Chaoqun Wan",
                "Yonggang Zhang",
                "Yiu-ming Cheung",
                "Xinmei Tian",
                "Xu Shen",
                "Jieping Ye"
            ],
            "title": "Interpreting and Improving Large Language Models in Arithmetic Calculation",
            "abstract": "Large language models (LLMs) have demonstrated remarkable potential across numerous applications and have shown an emergent ability to tackle complex reasoning tasks, such as mathematical computations. However, even for the simplest arithmetic calculations, the intrinsic mechanisms behind LLMs remain mysterious, making it challenging to ensure reliability. In this work, we delve into uncovering a specific mechanism by which LLMs execute calculations. Through comprehensive experiments, we find that LLMs frequently involve a small fraction (< 5%) of attention heads, which play a pivotal role in focusing on operands and operators during calculation processes. Subsequently, the information from these operands is processed through multi-layer perceptrons (MLPs), progressively leading to the final solution. These pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability across different datasets and even distinct tasks. This insight prompted us to investigate the potential benefits of selectively fine-tuning these essential heads/MLPs to boost the LLMs' computational performance. We empirically find that such precise tuning can yield notable enhancements on mathematical prowess, without compromising the performance on non-mathematical tasks. Our work serves as a preliminary exploration into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation to reveal more intricate mathematical tasks.",
            "id": "2409.01659",
            "link": "http://arxiv.org/abs/2409.01659v1",
            "published": "2024-09-03T07:01:46+00:00",
            "updated": "2024-09-03T07:01:46+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 42
        },
        "2409.01944": {
            "authors": [
                "Liqun Yang",
                "Jian Yang",
                "Chaoren Wei",
                "Guanglin Niu",
                "Ge Zhang",
                "Yunli Wang",
                "Linzheng ChaI",
                "Wanxu Xia",
                "Hongcheng Guo",
                "Shun Zhang",
                "Jiaheng Liu",
                "Yuwei Yin",
                "Junran Peng",
                "Jiaxin Ma",
                "Liang Sun",
                "Zhoujun Li"
            ],
            "title": "FuzzCoder: Byte-level Fuzzing Test via Large Language Model",
            "abstract": "Fuzzing is an important dynamic program analysis technique designed for finding vulnerabilities in complex software. Fuzzing involves presenting a target program with crafted malicious input to cause crashes, buffer overflows, memory errors, and exceptions. Crafting malicious inputs in an efficient manner is a difficult open problem and the best approaches often apply uniform random mutations to pre-existing valid inputs. In this work, we propose to adopt fine-tuned large language models (FuzzCoder) to learn patterns in the input files from successful attacks to guide future fuzzing explorations. Specifically, we develop a framework to leverage the code LLMs to guide the mutation process of inputs in fuzzing. The mutation process is formulated as the sequence-to-sequence modeling, where LLM receives a sequence of bytes and then outputs the mutated byte sequence. FuzzCoder is fine-tuned on the created instruction dataset (Fuzz-Instruct), where the successful fuzzing history is collected from the heuristic fuzzing tool. FuzzCoder can predict mutation locations and strategies locations in input files to trigger abnormal behaviors of the program. Experimental results show that FuzzCoder based on AFL (American Fuzzy Lop) gain significant improvements in terms of effective proportion of mutation (EPM) and number of crashes (NC) for various input formats including ELF, JPG, MP3, and XML.",
            "id": "2409.01944",
            "link": "http://arxiv.org/abs/2409.01944v1",
            "published": "2024-09-03T14:40:31+00:00",
            "updated": "2024-09-03T14:40:31+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 23
        },
        "2409.02076": {
            "authors": [
                "Yuhao Wu",
                "Ming Shan Hee",
                "Zhiqing Hu",
                "Roy Ka-Wei Lee"
            ],
            "title": "LongGenbench: Benchmarking Long-Form Generation in Long Context LLMs",
            "abstract": "The abilities of long-context language models (LMs) are often evaluated using the \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to assess a model's ability to identify specific information (\"needle\") within large text sequences (\"haystack\"). While these benchmarks measure how well models understand long-context input sequences, they do not effectively gauge the quality of long-form text generation--a critical aspect for applications such as design proposals and creative writing. To address this gap, we have introduced a new long-form text evaluation benchmark, LongGenbench, which tests models' ability to identify specific events within generated long text sequences. In this benchmark, we prompt long-context LMs to create long-form text that must include particular events or constraints and evaluate their ability to incorporate these elements. We evaluated ten long-context LMs across four distinct scenarios, three types of prompt instructions, and two different generation-length settings (16K and 32K). Although these models perform well on NIAH benchmarks, none demonstrated satisfactory performance on the LongGenbench, raising concerns about their ability to generate coherent long-form text that follows instructions. Additionally, as the length of the generated text increases, all models exhibit a significant drop in performance.",
            "id": "2409.02076",
            "link": "http://arxiv.org/abs/2409.02076v3",
            "published": "2024-09-03T17:25:54+00:00",
            "updated": "2024-09-11T16:35:00+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 33
        },
        "2409.02465": {
            "authors": [
                "Zhe Xu",
                "Jiasheng Ye",
                "Xiangyang Liu",
                "Tianxiang Sun",
                "Xiaoran Liu",
                "Qipeng Guo",
                "Linlin Li",
                "Qun Liu",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "title": "DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels",
            "abstract": "With the rapid advancement of Large Language Models (LLMs), long-context information understanding and processing have become a hot topic in academia and industry. However, benchmarks for evaluating the ability of LLMs to handle long-context information do not seem to have kept pace with the development of LLMs. Despite the emergence of various long-context evaluation benchmarks, the types of capability assessed are still limited, without new capability dimensions. In this paper, we introduce DetectiveQA, a narrative reasoning benchmark featured with an average context length of over 100K tokens. DetectiveQA focuses on evaluating the long-context reasoning ability of LLMs, which not only requires a full understanding of context but also requires extracting important evidences from the context and reasoning according to extracted evidences to answer the given questions. This is a new dimension of capability evaluation, which is more in line with the current intelligence level of LLMs. We use detective novels as data sources, which naturally have various reasoning elements. Finally, we manually annotated 600 questions in Chinese and then also provided an English edition of the context information and questions. We evaluate many long-context LLMs on DetectiveQA, including commercial and open-sourced models, and the results indicate that existing long-context LLMs still require significant advancements to effectively process true long-context dependency questions.",
            "id": "2409.02465",
            "link": "http://arxiv.org/abs/2409.02465v1",
            "published": "2024-09-04T06:28:22+00:00",
            "updated": "2024-09-04T06:28:22+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 61
        },
        "2409.02617": {
            "authors": [
                "Aneta Pawelec",
                "Victoria Sara Weso\u0142owska",
                "Zuzanna B\u0105czek",
                "Piotr Sankowski"
            ],
            "title": "PUB: Plot Understanding Benchmark and Dataset for Evaluating Large Language Models on Synthetic Visual Data Interpretation",
            "abstract": "The ability of large language models (LLMs) to interpret visual representations of data is crucial for advancing their application in data analysis and decision-making processes. This paper presents a novel synthetic dataset designed to evaluate the proficiency of LLMs in interpreting various forms of data visualizations, including plots like time series, histograms, violins, boxplots, and clusters. Our dataset is generated using controlled parameters to ensure comprehensive coverage of potential real-world scenarios. We employ multimodal text prompts with questions related to visual data in images to benchmark several state-of-the-art models like ChatGPT or Gemini, assessing their understanding and interpretative accuracy.   To ensure data integrity, our benchmark dataset is generated automatically, making it entirely new and free from prior exposure to the models being tested. This strategy allows us to evaluate the models' ability to truly interpret and understand the data, eliminating possibility of pre-learned responses, and allowing for an unbiased evaluation of the models' capabilities. We also introduce quantitative metrics to assess the performance of the models, providing a robust and comprehensive evaluation tool.   Benchmarking several state-of-the-art LLMs with this dataset reveals varying degrees of success, highlighting specific strengths and weaknesses in interpreting diverse types of visual data. The results provide valuable insights into the current capabilities of LLMs and identify key areas for improvement. This work establishes a foundational benchmark for future research and development aimed at enhancing the visual interpretative abilities of language models. In the future, improved LLMs with robust visual interpretation skills can significantly aid in automated data analysis, scientific research, educational tools, and business intelligence applications.",
            "id": "2409.02617",
            "link": "http://arxiv.org/abs/2409.02617v1",
            "published": "2024-09-04T11:19:17+00:00",
            "updated": "2024-09-04T11:19:17+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 29
        },
        "2409.02834": {
            "authors": [
                "Wentao Liu",
                "Qianjun Pan",
                "Yi Zhang",
                "Zhuo Liu",
                "Ji Wu",
                "Jie Zhou",
                "Aimin Zhou",
                "Qin Chen",
                "Bo Jiang",
                "Liang He"
            ],
            "title": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models",
            "abstract": "Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets.",
            "id": "2409.02834",
            "link": "http://arxiv.org/abs/2409.02834v2",
            "published": "2024-09-04T16:00:21+00:00",
            "updated": "2024-09-06T05:06:27+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 46
        },
        "2409.02897": {
            "authors": [
                "Jiajie Zhang",
                "Yushi Bai",
                "Xin Lv",
                "Wanjun Gu",
                "Danqing Liu",
                "Minhao Zou",
                "Shulin Cao",
                "Lei Hou",
                "Yuxiao Dong",
                "Ling Feng",
                "Juanzi Li"
            ],
            "title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA",
            "abstract": "Though current long-context large language models (LLMs) have demonstrated impressive capacities in answering user questions based on extensive text, the lack of citations in their responses makes user verification difficult, leading to concerns about their trustworthiness due to their potential hallucinations. In this work, we aim to enable long-context LLMs to generate responses with fine-grained sentence-level citations, improving their faithfulness and verifiability. We first introduce LongBench-Cite, an automated benchmark for assessing current LLMs' performance in Long-Context Question Answering with Citations (LQAC), revealing considerable room for improvement. To this end, we propose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs to automatically generate long-context QA instances with precise sentence-level citations, and leverage this pipeline to construct LongCite-45k, a large-scale SFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the LongCite-45k dataset, successfully enabling their generation of accurate responses and fine-grained sentence-level citations in a single output. The evaluation results on LongBench-Cite show that our trained models achieve state-of-the-art citation quality, surpassing advanced proprietary models including GPT-4o.",
            "id": "2409.02897",
            "link": "http://arxiv.org/abs/2409.02897v3",
            "published": "2024-09-04T17:41:19+00:00",
            "updated": "2024-09-10T07:43:19+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 51
        },
        "2409.03161": {
            "authors": [
                "Michiko Yoshitake",
                "Yuta Suzuki",
                "Ryo Igarashi",
                "Yoshitaka Ushiku",
                "Keisuke Nagato"
            ],
            "title": "MaterialBENCH: Evaluating College-Level Materials Science Problem-Solving Abilities of Large Language Models",
            "abstract": "A college-level benchmark dataset for large language models (LLMs) in the materials science field, MaterialBENCH, is constructed. This dataset consists of problem-answer pairs, based on university textbooks. There are two types of problems: one is the free-response answer type, and the other is the multiple-choice type. Multiple-choice problems are constructed by adding three incorrect answers as choices to a correct answer, so that LLMs can choose one of the four as a response. Most of the problems for free-response answer and multiple-choice types overlap except for the format of the answers. We also conduct experiments using the MaterialBENCH on LLMs, including ChatGPT-3.5, ChatGPT-4, Bard (at the time of the experiments), and GPT-3.5 and GPT-4 with the OpenAI API. The differences and similarities in the performance of LLMs measured by the MaterialBENCH are analyzed and discussed. Performance differences between the free-response type and multiple-choice type in the same models and the influence of using system massages on multiple-choice problems are also studied. We anticipate that MaterialBENCH will encourage further developments of LLMs in reasoning abilities to solve more complicated problems and eventually contribute to materials research and discovery.",
            "id": "2409.03161",
            "link": "http://arxiv.org/abs/2409.03161v1",
            "published": "2024-09-05T01:36:00+00:00",
            "updated": "2024-09-05T01:36:00+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 23
        },
        "2409.03621": {
            "authors": [
                "Amit Ben Artzy",
                "Roy Schwartz"
            ],
            "title": "Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers",
            "abstract": "In decoder-based LLMs, the representation of a given layer serves two purposes: as input to the next layer during the computation of the current token; and as input to the attention mechanism of future tokens. In this work, we show that the importance of the latter role might be overestimated. To show that, we start by manipulating the representations of previous tokens; e.g. by replacing the hidden states at some layer k with random vectors. Our experimenting with four LLMs and four tasks show that this operation often leads to small to negligible drop in performance. Importantly, this happens if the manipulation occurs in the top part of the model-k is in the final 30-50% of the layers. In contrast, doing the same manipulation in earlier layers might lead to chance level performance. We continue by switching the hidden state of certain tokens with hidden states of other tokens from another prompt; e.g., replacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We find that when applying this switch in the top 1/3 of the model, the model ignores it (answering \"Rome\"). However if we apply it before, the model conforms to the switch (\"Paris\"). Our results hint at a two stage process in transformer-based LLMs: the first part gathers input from previous tokens, while the second mainly processes that information internally.",
            "id": "2409.03621",
            "link": "http://arxiv.org/abs/2409.03621v1",
            "published": "2024-09-05T15:33:24+00:00",
            "updated": "2024-09-05T15:33:24+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 28
        },
        "2409.03659": {
            "authors": [
                "Ran Zhang",
                "Steffen Eger"
            ],
            "title": "LLM-based multi-agent poetry generation in non-cooperative environments",
            "abstract": "Despite substantial progress of large language models (LLMs) for automatic poetry generation, the generated poetry lacks diversity while the training process differs greatly from human learning. Under the rationale that the learning process of the poetry generation systems should be more human-like and their output more diverse and novel, we introduce a framework based on social learning where we emphasize non-cooperative interactions besides cooperative interactions to encourage diversity. Our experiments are the first attempt at LLM-based multi-agent systems in non-cooperative environments for poetry generation employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED agents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows that our framework benefits the poetry generation process for TRAINING-BASED agents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity and a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams. The generated poetry from TRAINING-BASED agents also exhibits group divergence in terms of lexicons, styles and semantics. PROMPTING-BASED agents in our framework also benefit from non-cooperative environments and a more diverse ensemble of models with non-homogeneous agents has the potential to further enhance diversity, with an increase of 7.0-17.5 pp according to our experiments. However, PROMPTING-BASED agents show a decrease in lexical diversity over time and do not exhibit the group-based divergence intended in the social network. Our paper argues for a paradigm shift in creative tasks such as automatic poetry generation to include social learning processes (via LLM-based agent modeling) similar to human interaction.",
            "id": "2409.03659",
            "link": "http://arxiv.org/abs/2409.03659v2",
            "published": "2024-09-05T16:12:29+00:00",
            "updated": "2024-09-06T06:50:32+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 25
        },
        "2409.03662": {
            "authors": [
                "Diego Doimo",
                "Alessandro Serra",
                "Alessio Ansuini",
                "Alberto Cazzaniga"
            ],
            "title": "The representation landscape of few-shot learning and fine-tuning in large language models",
            "abstract": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs. We approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while the landscape of ICL representations is characterized by less defined peaks. Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models.",
            "id": "2409.03662",
            "link": "http://arxiv.org/abs/2409.03662v2",
            "published": "2024-09-05T16:15:12+00:00",
            "updated": "2024-09-07T12:47:54+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 17
        },
        "2409.03752": {
            "authors": [
                "Zifan Zheng",
                "Yezhaohui Wang",
                "Yuxin Huang",
                "Shichao Song",
                "Bo Tang",
                "Feiyu Xiong",
                "Zhiyu Li"
            ],
            "title": "Attention Heads of Large Language Models: A Survey",
            "abstract": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in various tasks but remain largely as black-box systems. Consequently, their development relies heavily on data-driven approaches, limiting performance enhancement through changes in internal architecture and reasoning pathways. As a result, many researchers have begun exploring the potential internal mechanisms of LLMs, aiming to identify the essence of their reasoning bottlenecks, with most studies focusing on attention heads. Our survey aims to shed light on the internal reasoning processes of LLMs by concentrating on the interpretability and underlying mechanisms of attention heads. We first distill the human thought process into a four-stage framework: Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation. Using this framework, we systematically review existing research to identify and categorize the functions of specific attention heads. Furthermore, we summarize the experimental methodologies used to discover these special heads, dividing them into two categories: Modeling-Free methods and Modeling-Required methods. Also, we outline relevant evaluation methods and benchmarks. Finally, we discuss the limitations of current research and propose several potential future directions. Our reference list is open-sourced at \\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}.",
            "id": "2409.03752",
            "link": "http://arxiv.org/abs/2409.03752v1",
            "published": "2024-09-05T17:59:12+00:00",
            "updated": "2024-09-05T17:59:12+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 23
        },
        "2409.04043": {
            "authors": [
                "Louis Penafiel",
                "Hsien-Te Kao",
                "Isabel Erickson",
                "David Chu",
                "Robert McCormack",
                "Kristina Lerman",
                "Svitlana Volkova"
            ],
            "title": "Towards Safer Online Spaces: Simulating and Assessing Intervention Strategies for Eating Disorder Discussions",
            "abstract": "Eating disorders are complex mental health conditions that affect millions of people around the world. Effective interventions on social media platforms are crucial, yet testing strategies in situ can be risky. We present a novel LLM-driven experimental testbed for simulating and assessing intervention strategies in ED-related discussions. Our framework generates synthetic conversations across multiple platforms, models, and ED-related topics, allowing for controlled experimentation with diverse intervention approaches. We analyze the impact of various intervention strategies on conversation dynamics across four dimensions: intervention type, generative model, social media platform, and ED-related community/topic. We employ cognitive domain analysis metrics, including sentiment, emotions, etc., to evaluate the effectiveness of interventions. Our findings reveal that civility-focused interventions consistently improve positive sentiment and emotional tone across all dimensions, while insight-resetting approaches tend to increase negative emotions. We also uncover significant biases in LLM-generated conversations, with cognitive metrics varying notably between models (Claude-3 Haiku $>$ Mistral $>$ GPT-3.5-turbo $>$ LLaMA3) and even between versions of the same model. These variations highlight the importance of model selection in simulating realistic discussions related to ED. Our work provides valuable information on the complex dynamics of ED-related discussions and the effectiveness of various intervention strategies.",
            "id": "2409.04043",
            "link": "http://arxiv.org/abs/2409.04043v1",
            "published": "2024-09-06T06:27:35+00:00",
            "updated": "2024-09-06T06:27:35+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 60
        },
        "2409.04122": {
            "authors": [
                "Jan Hofmann",
                "Cornelia Sindermann",
                "Roman Klinger"
            ],
            "title": "Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering",
            "abstract": "Author profiling is the task of inferring characteristics about individuals by analyzing content they share. Supervised machine learning still dominates automatic systems that perform this task, despite the popularity of prompting large language models to address natural language understanding tasks. One reason is that the classification instances consist of large amounts of posts, potentially a whole user profile, which may exceed the input length of Transformers. Even if a model can use a large context window, the entirety of posts makes the application of API-accessed black box systems costly and slow, next to issues which come with such \"needle-in-the-haystack\" tasks. To mitigate this limitation, we propose a new method for author profiling which aims at distinguishing relevant from irrelevant content first, followed by the actual user profiling only with relevant data. To circumvent the need for relevance-annotated data, we optimize this relevance filter via reinforcement learning with a reward function that utilizes the zero-shot capabilities of large language models. We evaluate our method for Big Five personality trait prediction on two Twitter corpora. On publicly available real-world data with a skewed label distribution, our method shows similar efficacy to using all posts in a user profile, but with a substantially shorter context. An evaluation on a version of these data balanced with artificial posts shows that the filtering to relevant posts leads to a significantly improved accuracy of the predictions.",
            "id": "2409.04122",
            "link": "http://arxiv.org/abs/2409.04122v1",
            "published": "2024-09-06T08:43:10+00:00",
            "updated": "2024-09-06T08:43:10+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 40
        },
        "2409.04164": {
            "authors": [
                "Luis Mayer",
                "Christian Heumann",
                "Matthias A\u00dfenmacher"
            ],
            "title": "Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language Models for Text-to-Code Generation",
            "abstract": "In recent years, large language models (LLMs) have emerged as powerful tools with potential applications in various fields, including software engineering. Within the scope of this research, we evaluate five different state-of-the-art LLMs - Bard, BingChat, ChatGPT, Llama2, and Code Llama - concerning their capabilities for text-to-code generation. In an empirical study, we feed prompts with textual descriptions of coding problems sourced from the programming website LeetCode to the models with the task of creating solutions in Python. Subsequently, the quality of the generated outputs is assessed using the testing functionalities of LeetCode. The results indicate large differences in performance between the investigated models. ChatGPT can handle these typical programming challenges by far the most effectively, surpassing even code-specialized models like Code Llama. To gain further insights, we measure the runtime as well as the memory usage of the generated outputs and compared them to the other code submissions on Leetcode. A detailed error analysis, encompassing a comparison of the differences concerning correct indentation and form of the generated code as well as an assignment of the incorrectly solved tasks to certain error categories allows us to obtain a more nuanced picture of the results and potential for improvement. The results also show a clear pattern of increasingly incorrect produced code when the models are facing a lot of context in the form of longer prompts.",
            "id": "2409.04164",
            "link": "http://arxiv.org/abs/2409.04164v1",
            "published": "2024-09-06T10:03:49+00:00",
            "updated": "2024-09-06T10:03:49+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "cs.LG",
                "cs.SE"
            ],
            "max_author_hindex": 23
        },
        "2409.04181": {
            "authors": [
                "Larissa Pusch",
                "Tim O. F. Conrad"
            ],
            "title": "Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering",
            "abstract": "Advancements in natural language processing have revolutionized the way we can interact with digital information systems, such as databases, making them more accessible. However, challenges persist, especially when accuracy is critical, as in the biomedical domain. A key issue is the hallucination problem, where models generate information unsupported by the underlying data, potentially leading to dangerous misinformation. This paper presents a novel approach designed to bridge this gap by combining Large Language Models (LLM) and Knowledge Graphs (KG) to improve the accuracy and reliability of question-answering systems, on the example of a biomedical KG. Built on the LangChain framework, our method incorporates a query checker that ensures the syntactical and semantic validity of LLM-generated queries, which are then used to extract information from a Knowledge Graph, substantially reducing errors like hallucinations. We evaluated the overall performance using a new benchmark dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other models in generating accurate queries, open-source models like llama3:70b show promise with appropriate prompt engineering. To make this approach accessible, a user-friendly web-based interface has been developed, allowing users to input natural language queries, view generated and corrected Cypher queries, and verify the resulting paths for accuracy. Overall, this hybrid approach effectively addresses common issues such as data gaps and hallucinations, offering a reliable and intuitive solution for question answering systems. The source code for generating the results of this paper and for the user-interface can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui",
            "id": "2409.04181",
            "link": "http://arxiv.org/abs/2409.04181v1",
            "published": "2024-09-06T10:49:46+00:00",
            "updated": "2024-09-06T10:49:46+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 10
        },
        "2409.04927": {
            "authors": [
                "Junkai Wu",
                "Xulin Fan",
                "Bo-Ru Lu",
                "Xilin Jiang",
                "Nima Mesgarani",
                "Mark Hasegawa-Johnson",
                "Mari Ostendorf"
            ],
            "title": "Just ASR + LLM? A Study on Speech Large Language Models' Ability to Identify and Understand Speaker in Spoken Dialogue",
            "abstract": "In recent years, we have observed a rapid advancement in speech language models (SpeechLLMs), catching up with humans' listening and reasoning abilities. Remarkably, SpeechLLMs have demonstrated impressive spoken dialogue question-answering (SQA) performance in benchmarks like Gaokao, the English listening test of the college entrance exam in China, which seemingly requires understanding both the spoken content and voice characteristics of speakers in a conversation. However, after carefully examining Gaokao's questions, we find the correct answers to many questions can be inferred from the conversation context alone without identifying the speaker asked in the question. Our evaluation of state-of-the-art models Qwen-Audio and WavLLM in both Gaokao and our proposed \"What Do You Like?\" dataset shows a significantly higher accuracy in these context-based questions than in identity-critical questions, which can only be answered correctly with correct speaker identification. Our results and analysis suggest that when solving SQA, the current SpeechLLMs exhibit limited speaker awareness from the audio and behave similarly to an LLM reasoning from the conversation transcription without sound. We propose that our definitions and automated classification of context-based and identity-critical questions could offer a more accurate evaluation framework of SpeechLLMs in SQA tasks.",
            "id": "2409.04927",
            "link": "http://arxiv.org/abs/2409.04927v1",
            "published": "2024-09-07T22:54:47+00:00",
            "updated": "2024-09-07T22:54:47+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 89
        },
        "2409.05199": {
            "authors": [
                "Giannis Karamanolakis",
                "Daniel Hsu",
                "Luis Gravano"
            ],
            "title": "Interactive Machine Teaching by Labeling Rules and Instances",
            "abstract": "Weakly supervised learning aims to reduce the cost of labeling data by using expert-designed labeling rules. However, existing methods require experts to design effective rules in a single shot, which is difficult in the absence of proper guidance and tooling. Therefore, it is still an open question whether experts should spend their limited time writing rules or instead providing instance labels via active learning. In this paper, we investigate how to exploit an expert's limited time to create effective supervision. First, to develop practical guidelines for rule creation, we conduct an exploratory analysis of diverse collections of existing expert-designed rules and find that rule precision is more important than coverage across datasets. Second, we compare rule creation to individual instance labeling via active learning and demonstrate the importance of both across 6 datasets. Third, we propose an interactive learning framework, INTERVAL, that achieves efficiency by automatically extracting candidate rules based on rich patterns (e.g., by prompting a language model), and effectiveness by soliciting expert feedback on both candidate rules and individual instances. Across 6 datasets, INTERVAL outperforms state-of-the-art weakly supervised approaches by 7% in F1. Furthermore, it requires as few as 10 queries for expert feedback to reach F1 values that existing active learning methods cannot match even with 100 queries.",
            "id": "2409.05199",
            "link": "http://arxiv.org/abs/2409.05199v1",
            "published": "2024-09-08T19:24:14+00:00",
            "updated": "2024-09-08T19:24:14+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 51
        },
        "2409.00447": {
            "authors": [
                "I. de Rodrigo",
                "A. Sanchez-Cuadrado",
                "J. Boal",
                "A. J. Lopez-Lopez"
            ],
            "title": "The MERIT Dataset: Modelling and Efficiently Rendering Interpretable Transcripts",
            "abstract": "This paper introduces the MERIT Dataset, a multimodal (text + image + layout) fully labeled dataset within the context of school reports. Comprising over 400 labels and 33k samples, the MERIT Dataset is a valuable resource for training models in demanding Visually-rich Document Understanding (VrDU) tasks. By its nature (student grade reports), the MERIT Dataset can potentially include biases in a controlled way, making it a valuable tool to benchmark biases induced in Language Models (LLMs). The paper outlines the dataset's generation pipeline and highlights its main features in the textual, visual, layout, and bias domains. To demonstrate the dataset's utility, we present a benchmark with token classification models, showing that the dataset poses a significant challenge even for SOTA models and that these would greatly benefit from including samples from the MERIT Dataset in their pretraining phase.",
            "id": "2409.00447",
            "link": "http://arxiv.org/abs/2409.00447v1",
            "published": "2024-08-31T12:56:38+00:00",
            "updated": "2024-08-31T12:56:38+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 51
        },
        "2409.00706": {
            "authors": [
                "Daniela Schuster"
            ],
            "title": "Abstaining Machine Learning -- Philosophical Considerations",
            "abstract": "This paper establishes a connection between the fields of machine learning (ML) and philosophy concerning the phenomenon of behaving neutrally. It investigates a specific class of ML systems capable of delivering a neutral response to a given task, referred to as abstaining machine learning systems, that has not yet been studied from a philosophical perspective. The paper introduces and explains various abstaining machine learning systems, and categorizes them into distinct types. An examination is conducted on how abstention in the different machine learning system types aligns with the epistemological counterpart of suspended judgment, addressing both the nature of suspension and its normative profile. Additionally, a philosophical analysis is suggested on the autonomy and explainability of the abstaining response. It is argued, specifically, that one of the distinguished types of abstaining systems is preferable as it aligns more closely with our criteria for suspended judgment. Moreover, it is better equipped to autonomously generate abstaining outputs and offer explanations for abstaining outputs when compared to the other type.",
            "id": "2409.00706",
            "link": "http://arxiv.org/abs/2409.00706v1",
            "published": "2024-09-01T12:25:06+00:00",
            "updated": "2024-09-01T12:25:06+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 47
        },
        "2409.00824": {
            "authors": [
                "Philippe J. Giabbanelli",
                "Jack T. Beerman"
            ],
            "title": "Accelerating Hybrid Agent-Based Models and Fuzzy Cognitive Maps: How to Combine Agents who Think Alike?",
            "abstract": "While Agent-Based Models can create detailed artificial societies based on individual differences and local context, they can be computationally intensive. Modelers may offset these costs through a parsimonious use of the model, for example by using smaller population sizes (which limits analyses in sub-populations), running fewer what-if scenarios, or accepting more uncertainty by performing fewer simulations. Alternatively, researchers may accelerate simulations via hardware solutions (e.g., GPU parallelism) or approximation approaches that operate a tradeoff between accuracy and compute time. In this paper, we present an approximation that combines agents who `think alike', thus reducing the population size and the compute time. Our innovation relies on representing agent behaviors as networks of rules (Fuzzy Cognitive Maps) and empirically evaluating different measures of distance between these networks. Then, we form groups of think-alike agents via community detection and simplify them to a representative agent. Case studies show that our simplifications remain accuracy.",
            "id": "2409.00824",
            "link": "http://arxiv.org/abs/2409.00824v1",
            "published": "2024-09-01T19:45:15+00:00",
            "updated": "2024-09-01T19:45:15+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.MA"
            ],
            "max_author_hindex": 22
        },
        "2409.00853": {
            "authors": [
                "Chris Lu",
                "Michael Beukman",
                "Michael Matthews",
                "Jakob Foerster"
            ],
            "title": "JaxLife: An Open-Ended Agentic Simulator",
            "abstract": "Human intelligence emerged through the process of natural selection and evolution on Earth. We investigate what it would take to re-create this process in silico. While past work has often focused on low-level processes (such as simulating physics or chemistry), we instead take a more targeted approach, aiming to evolve agents that can accumulate open-ended culture and technologies across generations. Towards this, we present JaxLife: an artificial life simulator in which embodied agents, parameterized by deep neural networks, must learn to survive in an expressive world containing programmable systems. First, we describe the environment and show that it can facilitate meaningful Turing-complete computation. We then analyze the evolved emergent agents' behavior, such as rudimentary communication protocols, agriculture, and tool use. Finally, we investigate how complexity scales with the amount of compute used. We believe JaxLife takes a step towards studying evolved behavior in more open-ended simulations. Our code is available at https://github.com/luchris429/JaxLife",
            "id": "2409.00853",
            "link": "http://arxiv.org/abs/2409.00853v1",
            "published": "2024-09-01T22:05:02+00:00",
            "updated": "2024-09-01T22:05:02+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.NE"
            ],
            "max_author_hindex": 23
        },
        "2409.01178": {
            "authors": [
                "Gemb Kaljavesi",
                "Xiyan Su",
                "Frank Diermeyer"
            ],
            "title": "Integrating End-to-End and Modular Driving Approaches for Online Corner Case Detection in Autonomous Driving",
            "abstract": "Online corner case detection is crucial for ensuring safety in autonomous driving vehicles. Current autonomous driving approaches can be categorized into modular approaches and end-to-end approaches. To leverage the advantages of both, we propose a method for online corner case detection that integrates an end-to-end approach into a modular system. The modular system takes over the primary driving task and the end-to-end network runs in parallel as a secondary one, the disagreement between the systems is then used for corner case detection. We implement this method on a real vehicle and evaluate it qualitatively. Our results demonstrate that end-to-end networks, known for their superior situational awareness, as secondary driving systems, can effectively contribute to corner case detection. These findings suggest that such an approach holds potential for enhancing the safety of autonomous vehicles.",
            "id": "2409.01178",
            "link": "http://arxiv.org/abs/2409.01178v1",
            "published": "2024-09-02T11:14:41+00:00",
            "updated": "2024-09-02T11:14:41+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 15
        },
        "2409.01612": {
            "authors": [
                "Zhen Zhang",
                "Zhuolin Li",
                "Wenyu Yu"
            ],
            "title": "Lexicographic optimization-based approaches to learning a representative model for multi-criteria sorting with non-monotonic criteria",
            "abstract": "Deriving a representative model using value function-based methods from the perspective of preference disaggregation has emerged as a prominent and growing topic in multi-criteria sorting (MCS) problems. A noteworthy observation is that many existing approaches to learning a representative model for MCS problems traditionally assume the monotonicity of criteria, which may not always align with the complexities found in real-world MCS scenarios. Consequently, this paper proposes some approaches to learning a representative model for MCS problems with non-monotonic criteria through the integration of the threshold-based value-driven sorting procedure. To do so, we first define some transformation functions to map the marginal values and category thresholds into a UTA-like functional space. Subsequently, we construct constraint sets to model non-monotonic criteria in MCS problems and develop optimization models to check and rectify the inconsistency of the decision maker's assignment example preference information. By simultaneously considering the complexity and discriminative power of the models, two distinct lexicographic optimization-based approaches are developed to derive a representative model for MCS problems with non-monotonic criteria. Eventually, we offer an illustrative example and conduct comprehensive simulation experiments to elaborate the feasibility and validity of the proposed approaches.",
            "id": "2409.01612",
            "link": "http://arxiv.org/abs/2409.01612v1",
            "published": "2024-09-03T05:29:05+00:00",
            "updated": "2024-09-03T05:29:05+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 23
        },
        "2409.01815": {
            "authors": [
                "Jonas Stein",
                "Florentin D Hildebrandt",
                "Barrett W Thomas",
                "Marlin W Ulmer"
            ],
            "title": "Learning State-Dependent Policy Parametrizations for Dynamic Technician Routing with Rework",
            "abstract": "Home repair and installation services require technicians to visit customers and resolve tasks of different complexity. Technicians often have heterogeneous skills and working experiences. The geographical spread of customers makes achieving only perfect matches between technician skills and task requirements impractical. Additionally, technicians are regularly absent due to sickness. With non-perfect assignments regarding task requirement and technician skill, some tasks may remain unresolved and require a revisit and rework. Companies seek to minimize customer inconvenience due to delay. We model the problem as a sequential decision process where, over a number of service days, customers request service while heterogeneously skilled technicians are routed to serve customers in the system. Each day, our policy iteratively builds tours by adding \"important\" customers. The importance bases on analytical considerations and is measured by respecting routing efficiency, urgency of service, and risk of rework in an integrated fashion. We propose a state-dependent balance of these factors via reinforcement learning. A comprehensive study shows that taking a few non-perfect assignments can be quite beneficial for the overall service quality. We further demonstrate the value provided by a state-dependent parametrization.",
            "id": "2409.01815",
            "link": "http://arxiv.org/abs/2409.01815v1",
            "published": "2024-09-03T11:56:58+00:00",
            "updated": "2024-09-03T11:56:58+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 34
        },
        "2409.01927": {
            "authors": [
                "Segev Shlomov",
                "Ben wiesel",
                "Aviad Sela",
                "Ido Levy",
                "Liane Galanti",
                "Roy Abitbol"
            ],
            "title": "From Grounding to Planning: Benchmarking Bottlenecks in Web Agents",
            "abstract": "General web-based agents are increasingly essential for interacting with complex web environments, yet their performance in real-world web applications remains poor, yielding extremely low accuracy even with state-of-the-art frontier models. We observe that these agents can be decomposed into two primary components: Planning and Grounding. Yet, most existing research treats these agents as black boxes, focusing on end-to-end evaluations which hinder meaningful improvements. We sharpen the distinction between the planning and grounding components and conduct a novel analysis by refining experiments on the Mind2Web dataset. Our work proposes a new benchmark for each of the components separately, identifying the bottlenecks and pain points that limit agent performance. Contrary to prevalent assumptions, our findings suggest that grounding is not a significant bottleneck and can be effectively addressed with current techniques. Instead, the primary challenge lies in the planning component, which is the main source of performance degradation. Through this analysis, we offer new insights and demonstrate practical suggestions for improving the capabilities of web agents, paving the way for more reliable agents.",
            "id": "2409.01927",
            "link": "http://arxiv.org/abs/2409.01927v1",
            "published": "2024-09-03T14:17:09+00:00",
            "updated": "2024-09-03T14:17:09+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.MA"
            ],
            "max_author_hindex": 6
        },
        "2409.02069": {
            "authors": [
                "Anna L. Trella",
                "Kelly W. Zhang",
                "Hinal Jajal",
                "Inbal Nahum-Shani",
                "Vivek Shetty",
                "Finale Doshi-Velez",
                "Susan A. Murphy"
            ],
            "title": "A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial",
            "abstract": "Dental disease is a prevalent chronic condition associated with substantial financial burden, personal suffering, and increased risk of systemic diseases. Despite widespread recommendations for twice-daily tooth brushing, adherence to recommended oral self-care behaviors remains sub-optimal due to factors such as forgetfulness and disengagement. To address this, we developed Oralytics, a mHealth intervention system designed to complement clinician-delivered preventative care for marginalized individuals at risk for dental disease. Oralytics incorporates an online reinforcement learning algorithm to determine optimal times to deliver intervention prompts that encourage oral self-care behaviors. We have deployed Oralytics in a registered clinical trial. The deployment required careful design to manage challenges specific to the clinical trials setting in the U.S. In this paper, we (1) highlight key design decisions of the RL algorithm that address these challenges and (2) conduct a re-sampling analysis to evaluate algorithm design decisions. A second phase (randomized control trial) of Oralytics is planned to start in spring 2025.",
            "id": "2409.02069",
            "link": "http://arxiv.org/abs/2409.02069v1",
            "published": "2024-09-03T17:16:01+00:00",
            "updated": "2024-09-03T17:16:01+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.HC"
            ],
            "max_author_hindex": 54
        },
        "2409.02100": {
            "authors": [
                "Ralf Otte"
            ],
            "title": "On a heuristic approach to the description of consciousness as a hypercomplex system state and the possibility of machine consciousness (German edition)",
            "abstract": "This article presents a heuristic view that shows that the inner states of consciousness experienced by every human being have a physical but imaginary hypercomplex basis. The hypercomplex description is necessary because certain processes of consciousness cannot be physically measured in principle, but nevertheless exist. Based on theoretical considerations, it could be possible - as a result of mathematical investigations into a so-called bicomplex algebra - to generate and use hypercomplex system states on machines in a targeted manner. The hypothesis of the existence of hypercomplex system states on machines is already supported by the surprising performance of highly complex AI systems. However, this has yet to be proven. In particular, there is a lack of experimental data that distinguishes such systems from other systems, which is why this question will be addressed in later articles. This paper describes the developed bicomplex algebra and possible applications of these findings to generate hypercomplex energy states on machines. In the literature, such system states are often referred to as machine consciousness. The article uses mathematical considerations to explain how artificial consciousness could be generated and what advantages this would have for such AI systems.",
            "id": "2409.02100",
            "link": "http://arxiv.org/abs/2409.02100v1",
            "published": "2024-09-03T17:55:57+00:00",
            "updated": "2024-09-03T17:55:57+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "math.AC",
                "physics.app-ph",
                "08A99",
                "I.2.0"
            ],
            "max_author_hindex": 3
        },
        "2409.02291": {
            "authors": [
                "Jeremy Straub",
                "Zach Johnson"
            ],
            "title": "Initial Development and Evaluation of the Creative Artificial Intelligence through Recurring Developments and Determinations (CAIRDD) System",
            "abstract": "Computer system creativity is a key step on the pathway to artificial general intelligence (AGI). It is elusive, however, due to the fact that human creativity is not fully understood and, thus, it is difficult to develop this capability in software. Large language models (LLMs) provide a facsimile of creativity and the appearance of sentience, while not actually being either creative or sentient. While LLMs have created bona fide new content, in some cases - such as with harmful hallucinations - inadvertently, their deliberate creativity is seen by some to not match that of humans. In response to this challenge, this paper proposes a technique for enhancing LLM output creativity via an iterative process of concept injection and refinement. Initial work on the development of the Creative Artificial Intelligence through Recurring Developments and Determinations (CAIRDD) system is presented and the efficacy of key system components is evaluated.",
            "id": "2409.02291",
            "link": "http://arxiv.org/abs/2409.02291v1",
            "published": "2024-09-03T21:04:07+00:00",
            "updated": "2024-09-03T21:04:07+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.HC"
            ],
            "max_author_hindex": 24
        },
        "2409.02549": {
            "authors": [
                "Ayal Taitler"
            ],
            "title": "A Sequential Decision-Making Model for Perimeter Identification",
            "abstract": "Perimeter identification involves ascertaining the boundaries of a designated area or zone, requiring traffic flow monitoring, control, or optimization. Various methodologies and technologies exist for accurately defining these perimeters; however, they often necessitate specialized equipment, precise mapping, or comprehensive data for effective problem delineation. In this study, we propose a sequential decision-making framework for perimeter search, designed to operate efficiently in real-time and require only publicly accessible information. We conceptualize the perimeter search as a game between a playing agent and an artificial environment, where the agent's objective is to identify the optimal perimeter by sequentially improving the current perimeter. We detail the model for the game and discuss its adaptability in determining the definition of an optimal perimeter. Ultimately, we showcase the model's efficacy through a real-world scenario, highlighting the identification of corresponding optimal perimeters.",
            "id": "2409.02549",
            "link": "http://arxiv.org/abs/2409.02549v2",
            "published": "2024-09-04T09:11:39+00:00",
            "updated": "2024-09-05T06:58:38+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 4
        },
        "2409.02632": {
            "authors": [
                "Bobby Khaleque",
                "Mike Cook",
                "Jeremy Gow"
            ],
            "title": "Evaluating Environments Using Exploratory Agents",
            "abstract": "Exploration is a key part of many video games. We investigate the using an exploratory agent to provide feedback on the design of procedurally generated game levels, 5 engaging levels and 5 unengaging levels. We expand upon a framework introduced in previous research which models motivations for exploration and introduce a fitness function for evaluating an environment's potential for exploration. Our study showed that our exploratory agent can clearly distinguish between engaging and unengaging levels. The findings suggest that our agent has the potential to serve as an effective tool for assessing procedurally generated levels, in terms of exploration. This work contributes to the growing field of AI-driven game design by offering new insights into how game environments can be evaluated and optimised for player exploration.",
            "id": "2409.02632",
            "link": "http://arxiv.org/abs/2409.02632v1",
            "published": "2024-09-04T11:51:26+00:00",
            "updated": "2024-09-04T11:51:26+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.HC"
            ],
            "max_author_hindex": 19
        },
        "2409.02760": {
            "authors": [
                "Zhuolin Li",
                "Zhen Zhang",
                "Witold Pedrycz"
            ],
            "title": "An incremental preference elicitation-based approach to learning potentially non-monotonic preferences in multi-criteria sorting",
            "abstract": "This paper introduces a novel incremental preference elicitation-based approach to learning potentially non-monotonic preferences in multi-criteria sorting (MCS) problems, enabling decision makers to progressively provide assignment example preference information. Specifically, we first construct a max-margin optimization-based model to model potentially non-monotonic preferences and inconsistent assignment example preference information in each iteration of the incremental preference elicitation process. Using the optimal objective function value of the max-margin optimization-based model, we devise information amount measurement methods and question selection strategies to pinpoint the most informative alternative in each iteration within the framework of uncertainty sampling in active learning. Once the termination criterion is satisfied, the sorting result for non-reference alternatives can be determined through the use of two optimization models, i.e., the max-margin optimization-based model and the complexity controlling optimization model. Subsequently, two incremental preference elicitation-based algorithms are developed to learn potentially non-monotonic preferences, considering different termination criteria. Ultimately, we apply the proposed approach to a credit rating problem to elucidate the detailed implementation steps, and perform computational experiments on both artificial and real-world data sets to compare the proposed question selection strategies with several benchmark strategies.",
            "id": "2409.02760",
            "link": "http://arxiv.org/abs/2409.02760v1",
            "published": "2024-09-04T14:36:20+00:00",
            "updated": "2024-09-04T14:36:20+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 119
        },
        "2409.03167": {
            "authors": [
                "Pranay Thangeda",
                "Trevor S. Betz",
                "Michael N. Grussing",
                "Melkior Ornik"
            ],
            "title": "InfraLib: Enabling Reinforcement Learning and Decision Making for Large Scale Infrastructure Management",
            "abstract": "Efficient management of infrastructure systems is crucial for economic stability, sustainability, and public safety. However, infrastructure management is challenging due to the vast scale of systems, stochastic deterioration of components, partial observability, and resource constraints. While data-driven approaches like reinforcement learning (RL) offer a promising avenue for optimizing management policies, their application to infrastructure has been limited by the lack of suitable simulation environments. We introduce InfraLib, a comprehensive framework for modeling and analyzing infrastructure management problems. InfraLib employs a hierarchical, stochastic approach to realistically model infrastructure systems and their deterioration. It supports practical functionality such as modeling component unavailability, cyclical budgets, and catastrophic failures. To facilitate research, InfraLib provides tools for expert data collection, simulation-driven analysis, and visualization. We demonstrate InfraLib's capabilities through case studies on a real-world road network and a synthetic benchmark with 100,000 components.",
            "id": "2409.03167",
            "link": "http://arxiv.org/abs/2409.03167v1",
            "published": "2024-09-05T01:54:29+00:00",
            "updated": "2024-09-05T01:54:29+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ],
            "max_author_hindex": 10
        },
        "2409.03260": {
            "authors": [
                "Emir Demirovi\u0107",
                "Christian Schilling",
                "Anna Lukina"
            ],
            "title": "In Search of Trees: Decision-Tree Policy Synthesis for Black-Box Systems via Search",
            "abstract": "Decision trees, owing to their interpretability, are attractive as control policies for (dynamical) systems. Unfortunately, constructing, or synthesising, such policies is a challenging task. Previous approaches do so by imitating a neural-network policy, approximating a tabular policy obtained via formal synthesis, employing reinforcement learning, or modelling the problem as a mixed-integer linear program. However, these works may require access to a hard-to-obtain accurate policy or a formal model of the environment (within reach of formal synthesis), and may not provide guarantees on the quality or size of the final tree policy. In contrast, we present an approach to synthesise optimal decision-tree policies given a black-box environment and specification, and a discretisation of the tree predicates, where optimality is defined with respect to the number of steps to achieve the goal. Our approach is a specialised search algorithm which systematically explores the (exponentially large) space of decision trees under the given discretisation. The key component is a novel pruning mechanism that significantly reduces the search space. Our approach represents a conceptually novel way of synthesising small decision-tree policies with optimality guarantees even for black-box environments with black-box specifications.",
            "id": "2409.03260",
            "link": "http://arxiv.org/abs/2409.03260v1",
            "published": "2024-09-05T05:51:42+00:00",
            "updated": "2024-09-05T05:51:42+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 21
        },
        "2409.04065": {
            "authors": [
                "Wachara Fungwacharakorn",
                "Kanae Tsushima",
                "Hiroshi Hosobe",
                "Hideaki Takeda",
                "Ken Satoh"
            ],
            "title": "An Argumentative Approach for Explaining Preemption in Soft-Constraint Based Norms",
            "abstract": "Although various aspects of soft-constraint based norms have been explored, it is still challenging to understand preemption. Preemption is a situation where higher-level norms override lower-level norms when new information emerges. To address this, we propose a derivation state argumentation framework (DSA-framework). DSA-framework incorporates derivation states to explain how preemption arises based on evolving situational knowledge. Based on DSA-framework, we present an argumentative approach for explaining preemption. We formally prove that, under local optimality, DSA-framework can provide explanations why one consequence is obligatory or forbidden by soft-constraint based norms represented as logical constraint hierarchies.",
            "id": "2409.04065",
            "link": "http://arxiv.org/abs/2409.04065v1",
            "published": "2024-09-06T07:14:32+00:00",
            "updated": "2024-09-06T07:14:32+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 27
        },
        "2409.04102": {
            "authors": [
                "Alessandro Antonucci",
                "Francesca Mangili",
                "Claudio Bonesana",
                "Giorgia Adorni"
            ],
            "title": "Intelligent tutoring systems by Bayesian nets with noisy gates",
            "abstract": "Directed graphical models such as Bayesian nets are often used to implement intelligent tutoring systems able to interact in real-time with learners in a purely automatic way. When coping with such models, keeping a bound on the number of parameters might be important for multiple reasons. First, as these models are typically based on expert knowledge, a huge number of parameters to elicit might discourage practitioners from adopting them. Moreover, the number of model parameters affects the complexity of the inferences, while a fast computation of the queries is needed for real-time feedback. We advocate logical gates with uncertainty for a compact parametrization of the conditional probability tables in the underlying Bayesian net used by tutoring systems. We discuss the semantics of the model parameters to elicit and the assumptions required to apply such approach in this domain. We also derive a dedicated inference scheme to speed up computations.",
            "id": "2409.04102",
            "link": "http://arxiv.org/abs/2409.04102v2",
            "published": "2024-09-06T08:08:55+00:00",
            "updated": "2024-09-09T08:55:22+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 18
        },
        "2409.04224": {
            "authors": [
                "Daniel J. Tan",
                "Qianyi Xu",
                "Kay Choong See",
                "Dilruk Perera",
                "Mengling Feng"
            ],
            "title": "Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework",
            "abstract": "Multi-organ diseases present significant challenges due to their simultaneous impact on multiple organ systems, necessitating complex and adaptive treatment strategies. Despite recent advancements in AI-powered healthcare decision support systems, existing solutions are limited to individual organ systems. They often ignore the intricate dependencies between organ system and thereby fails to provide holistic treatment recommendations that are useful in practice. We propose a novel hierarchical multi-agent reinforcement learning (HMARL) framework to address these challenges. This framework uses dedicated agents for each organ system, and model dynamic through explicit inter-agent communication channels, enabling coordinated treatment strategies across organs. Furthermore, we introduce a dual-layer state representation technique to contextualize patient conditions at various hierarchical levels, enhancing the treatment accuracy and relevance. Through extensive qualitative and quantitative evaluations in managing sepsis (a complex multi-organ disease), our approach demonstrates its ability to learn effective treatment policies that significantly improve patient survival rates. This framework marks a substantial advancement in clinical decision support systems, pioneering a comprehensive approach for multi-organ treatment recommendations.",
            "id": "2409.04224",
            "link": "http://arxiv.org/abs/2409.04224v1",
            "published": "2024-09-06T12:26:47+00:00",
            "updated": "2024-09-06T12:26:47+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 25
        },
        "2409.04415": {
            "authors": [
                "Tan D. Tran",
                "Canh V. Pham",
                "Dung T. K. Ha",
                "Phuong N. H. Pham"
            ],
            "title": "Improved Parallel Algorithm for Non-Monotone Submodular Maximization under Knapsack Constraint",
            "abstract": "This work proposes an efficient parallel algorithm for non-monotone submodular maximization under a knapsack constraint problem over the ground set of size $n$. Our algorithm improves the best approximation factor of the existing parallel one from $8+\\epsilon$ to $7+\\epsilon$ with $O(\\log n)$ adaptive complexity.   The key idea of our approach is to create a new alternate threshold algorithmic framework. This strategy alternately constructs two disjoint candidate solutions within a constant number of sequence rounds. Then, the algorithm boosts solution quality without sacrificing the adaptive complexity. Extensive experimental studies on three applications, Revenue Maximization, Image Summarization, and Maximum Weighted Cut, show that our algorithm not only significantly increases solution quality but also requires comparative adaptivity to state-of-the-art algorithms.",
            "id": "2409.04415",
            "link": "http://arxiv.org/abs/2409.04415v1",
            "published": "2024-09-06T17:17:52+00:00",
            "updated": "2024-09-06T17:17:52+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 13
        },
        "2409.04572": {
            "authors": [
                "Mehwish Alam",
                "Genet Asefa Gesese",
                "Pierre-Henri Paris"
            ],
            "title": "Neurosymbolic Methods for Dynamic Knowledge Graphs",
            "abstract": "Knowledge graphs (KGs) have recently been used for many tools and applications, making them rich resources in structured format. However, in the real world, KGs grow due to the additions of new knowledge in the form of entities and relations, making these KGs dynamic. This chapter formally defines several types of dynamic KGs and summarizes how these KGs can be represented. Additionally, many neurosymbolic methods have been proposed for learning representations over static KGs for several tasks such as KG completion and entity alignment. This chapter further focuses on neurosymbolic methods for dynamic KGs with or without temporal information. More specifically, it provides an insight into neurosymbolic methods for dynamic (temporal or non-temporal) KG completion and entity alignment tasks. It further discusses the challenges of current approaches and provides some future directions.",
            "id": "2409.04572",
            "link": "http://arxiv.org/abs/2409.04572v1",
            "published": "2024-09-06T19:24:29+00:00",
            "updated": "2024-09-06T19:24:29+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 10
        },
        "2409.04693": {
            "authors": [
                "Ruiting Dai",
                "Yuqiao Tan",
                "Lisi Mo",
                "Tao He",
                "Ke Qin",
                "Shuang Liang"
            ],
            "title": "MuAP: Multi-step Adaptive Prompt Learning for Vision-Language Model with Missing Modality",
            "abstract": "Recently, prompt learning has garnered considerable attention for its success in various Vision-Language (VL) tasks. However, existing prompt-based models are primarily focused on studying prompt generation and prompt strategies with complete modality settings, which does not accurately reflect real-world scenarios where partial modality information may be missing. In this paper, we present the first comprehensive investigation into prompt learning behavior when modalities are incomplete, revealing the high sensitivity of prompt-based models to missing modalities. To this end, we propose a novel Multi-step Adaptive Prompt Learning (MuAP) framework, aiming to generate multimodal prompts and perform multi-step prompt tuning, which adaptively learns knowledge by iteratively aligning modalities. Specifically, we generate multimodal prompts for each modality and devise prompt strategies to integrate them into the Transformer model. Subsequently, we sequentially perform prompt tuning from single-stage and alignment-stage, allowing each modality-prompt to be autonomously and adaptively learned, thereby mitigating the imbalance issue caused by only textual prompts that are learnable in previous works. Extensive experiments demonstrate the effectiveness of our MuAP and this model achieves significant improvements compared to the state-of-the-art on all benchmark datasets",
            "id": "2409.04693",
            "link": "http://arxiv.org/abs/2409.04693v1",
            "published": "2024-09-07T03:33:46+00:00",
            "updated": "2024-09-07T03:33:46+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 20
        },
        "2409.04793": {
            "authors": [
                "Yoshiki Fukada"
            ],
            "title": "Action is the primary key: a categorical framework for episode description and logical reasoning",
            "abstract": "This research presents a computational framework for describing and recognizing episodes and for logical reasoning. This framework, named cognitive-logs, consists of a set of relational and graph databases. Cognitive-logs record knowledge, particularly in episodes that consist of \"actions\" represented by verbs in natural languages and \"participants\" who perform the actions. These objects are connected by arrows (morphisms) that link each action to its participant and link cause to effect. Operations based on category theory enable comparisons between episodes and deductive inferences, including abstractions of stories. One of the goals of this study is to develop a database-driven artificial intelligence. This artificial intelligence thinks like a human but possesses the accuracy and rigour of a machine. The vast capacities of databases (up to petabyte scales in current technologies) enable the artificial intelligence to store a greater volume of knowledge than neural-network based artificial intelligences. Cognitive-logs serve as a model of human cognition and designed with references to cognitive linguistics. Cognitive-logs also have the potential to model various human mind activities.",
            "id": "2409.04793",
            "link": "http://arxiv.org/abs/2409.04793v1",
            "published": "2024-09-07T11:09:47+00:00",
            "updated": "2024-09-07T11:09:47+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 6
        },
        "2409.04926": {
            "authors": [
                "J\u00fanior R. Lima",
                "Vin\u00edicius Gandra M. Santos",
                "Marco Antonio M. Carvalho"
            ],
            "title": "A $\u0394$-evaluation function for column permutation problems",
            "abstract": "In this study, a new $\\Delta$-evaluation method is introduced for solving a column permutation problem defined on a sparse binary matrix with the consecutive ones property. This problem models various $\\mathcal{NP}$-hard problems in graph theory and industrial manufacturing contexts. The computational experiments compare the processing time of the $\\Delta$-evaluation method with two other methods used in well-known local search procedures. The study considers a comprehensive set of instances of well-known problems, such as Gate Matrix Layout and Minimization of Open Stacks. The proposed evaluation method is generally competitive and particularly useful for large and dense instances. It can be easily integrated into local search and metaheuristic algorithms to improve solutions without significantly increasing processing time.",
            "id": "2409.04926",
            "link": "http://arxiv.org/abs/2409.04926v1",
            "published": "2024-09-07T22:50:25+00:00",
            "updated": "2024-09-07T22:50:25+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "math.CO",
                "math.OC",
                "90",
                "J.6"
            ],
            "max_author_hindex": 26
        },
        "2409.01539": {
            "authors": [
                "Yanchen Wang",
                "Lisa Singh"
            ],
            "title": "It is Time to Develop an Auditing Framework to Promote Value Aware Chatbots",
            "abstract": "The launch of ChatGPT in November 2022 marked the beginning of a new era in AI, the availability of generative AI tools for everyone to use. ChatGPT and other similar chatbots boast a wide range of capabilities from answering student homework questions to creating music and art. Given the large amounts of human data chatbots are built on, it is inevitable that they will inherit human errors and biases. These biases have the potential to inflict significant harm or increase inequity on different subpopulations. Because chatbots do not have an inherent understanding of societal values, they may create new content that is contrary to established norms. Examples of concerning generated content includes child pornography, inaccurate facts, and discriminatory posts. In this position paper, we argue that the speed of advancement of this technology requires us, as computer and data scientists, to mobilize and develop a values-based auditing framework containing a community established standard set of measurements to monitor the health of different chatbots and LLMs. To support our argument, we use a simple audit template to share the results of basic audits we conduct that are focused on measuring potential bias in search engine style tasks, code generation, and story generation. We identify responses from GPT 3.5 and GPT 4 that are both consistent and not consistent with values derived from existing law. While the findings come as no surprise, they do underscore the urgency of developing a robust auditing framework for openly sharing results in a consistent way so that mitigation strategies can be developed by the academic community, government agencies, and companies when our values are not being adhered to. We conclude this paper with recommendations for value-based strategies for improving the technologies.",
            "id": "2409.01539",
            "link": "http://arxiv.org/abs/2409.01539v1",
            "published": "2024-09-03T02:15:34+00:00",
            "updated": "2024-09-03T02:15:34+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 19
        },
        "2409.02375": {
            "authors": [
                "Xichou Zhu",
                "Yang Liu",
                "Zhou Shen",
                "Yi Liu",
                "Min Li",
                "Yujun Chen",
                "Benzi John",
                "Zhenzhen Ma",
                "Tao Hu",
                "Bolong Yang",
                "Manman Wang",
                "Zongxing Xie",
                "Peng Liu",
                "Dan Cai",
                "Junhui Wang"
            ],
            "title": "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance and Privacy Technical Review",
            "abstract": "The recent advances in large language models (LLMs) have significantly expanded their applications across various fields such as language generation, summarization, and complex question answering. However, their application to privacy compliance and technical privacy reviews remains under-explored, raising critical concerns about their ability to adhere to global privacy standards and protect sensitive user data. This paper seeks to address this gap by providing a comprehensive case study evaluating LLMs' performance in privacy-related tasks such as privacy information extraction (PIE), legal and regulatory key point detection (KPD), and question answering (QA) with respect to privacy policies and data protection regulations. We introduce a Privacy Technical Review (PTR) framework, highlighting its role in mitigating privacy risks during the software development life-cycle. Through an empirical assessment, we investigate the capacity of several prominent LLMs, including BERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks and technical privacy reviews. Our experiments benchmark the models across multiple dimensions, focusing on their precision, recall, and F1-scores in extracting privacy-sensitive information and detecting key regulatory compliance points. While LLMs show promise in automating privacy reviews and identifying regulatory discrepancies, significant gaps persist in their ability to fully comply with evolving legal standards. We provide actionable recommendations for enhancing LLMs' capabilities in privacy compliance, emphasizing the need for robust model improvements and better integration with legal and regulatory requirements. This study underscores the growing importance of developing privacy-aware LLMs that can both support businesses in compliance efforts and safeguard user privacy rights.",
            "id": "2409.02375",
            "link": "http://arxiv.org/abs/2409.02375v1",
            "published": "2024-09-04T01:51:37+00:00",
            "updated": "2024-09-04T01:51:37+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 55
        },
        "2409.02795": {
            "authors": [
                "Bofei Gao",
                "Feifan Song",
                "Yibo Miao",
                "Zefan Cai",
                "Zhe Yang",
                "Liang Chen",
                "Helan Hu",
                "Runxin Xu",
                "Qingxiu Dong",
                "Ce Zheng",
                "Wen Xiao",
                "Ge Zhang",
                "Daoguang Zan",
                "Keming Lu",
                "Bowen Yu",
                "Dayiheng Liu",
                "Zeyu Cui",
                "Jian Yang",
                "Lei Sha",
                "Houfeng Wang",
                "Zhifang Sui",
                "Peiyi Wang",
                "Tianyu Liu",
                "Baobao Chang"
            ],
            "title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey",
            "abstract": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to efficiently enhance the LLM's performance. While effective, research in this area spans multiple domains, and the methods involved are relatively complex to understand. The relationships between different methods have been under-explored, limiting the development of the preference alignment. In light of this, we break down the existing popular alignment strategies into different components and provide a unified framework to study the current alignment strategies, thereby establishing connections among them. In this survey, we decompose all the strategies in preference learning into four components: model, data, feedback, and algorithm. This unified view offers an in-depth understanding of existing alignment algorithms and also opens up possibilities to synergize the strengths of different strategies. Furthermore, we present detailed working examples of prevalent existing algorithms to facilitate a comprehensive understanding for the readers. Finally, based on our unified perspective, we explore the challenges and future research directions for aligning large language models with human preferences.",
            "id": "2409.02795",
            "link": "http://arxiv.org/abs/2409.02795v3",
            "published": "2024-09-04T15:11:55+00:00",
            "updated": "2024-09-09T09:31:30+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 35
        },
        "2409.03905": {
            "authors": [
                "Yujuan Fu",
                "Giridhar Kaushik Ramachandran",
                "Ahmad Halwani",
                "Bridget T. McInnes",
                "Fei Xia",
                "Kevin Lybarger",
                "Meliha Yetisgen",
                "\u00d6zlem Uzuner"
            ],
            "title": "CACER: Clinical Concept Annotations for Cancer Events and Relations",
            "abstract": "Clinical notes contain unstructured representations of patient histories, including the relationships between medical problems and prescription drugs. To investigate the relationship between cancer drugs and their associated symptom burden, we extract structured, semantic representations of medical problem and drug information from the clinical narratives of oncology notes. We present Clinical Concept Annotations for Cancer Events and Relations (CACER), a novel corpus with fine-grained annotations for over 48,000 medical problems and drug events and 10,000 drug-problem and problem-problem relations. Leveraging CACER, we develop and evaluate transformer-based information extraction (IE) models such as BERT, Flan-T5, Llama3, and GPT-4 using fine-tuning and in-context learning (ICL). In event extraction, the fine-tuned BERT and Llama3 models achieved the highest performance at 88.2-88.0 F1, which is comparable to the inter-annotator agreement (IAA) of 88.4 F1. In relation extraction, the fine-tuned BERT, Flan-T5, and Llama3 achieved the highest performance at 61.8-65.3 F1. GPT-4 with ICL achieved the worst performance across both tasks. The fine-tuned models significantly outperformed GPT-4 in ICL, highlighting the importance of annotated training data and model optimization. Furthermore, the BERT models performed similarly to Llama3. For our task, LLMs offer no performance advantage over the smaller BERT models. The results emphasize the need for annotated training data to optimize models. Multiple fine-tuned transformer models achieved performance comparable to IAA for several extraction tasks.",
            "id": "2409.03905",
            "link": "http://arxiv.org/abs/2409.03905v1",
            "published": "2024-09-05T20:42:35+00:00",
            "updated": "2024-09-05T20:42:35+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL"
            ],
            "max_author_hindex": 39
        },
        "2409.05247": {
            "authors": [
                "Andrew Smart",
                "Ben Hutchinson",
                "Lameck Mbangula Amugongo",
                "Suzanne Dikker",
                "Alex Zito",
                "Amber Ebinama",
                "Zara Wudiri",
                "Ding Wang",
                "Erin van Liemt",
                "Jo\u00e3o Sedoc",
                "Seyi Olojo",
                "Stanley Uwakwe",
                "Edem Wornyo",
                "Sonja Schmer-Galunder",
                "Jamila Smith-Loud"
            ],
            "title": "Socially Responsible Data for Large Multilingual Language Models",
            "abstract": "Large Language Models (LLMs) have rapidly increased in size and apparent capabilities in the last three years, but their training data is largely English text. There is growing interest in multilingual LLMs, and various efforts are striving for models to accommodate languages of communities outside of the Global North, which include many languages that have been historically underrepresented in digital realms. These languages have been coined as \"low resource languages\" or \"long-tail languages\", and LLMs performance on these languages is generally poor. While expanding the use of LLMs to more languages may bring many potential benefits, such as assisting cross-community communication and language preservation, great care must be taken to ensure that data collection on these languages is not extractive and that it does not reproduce exploitative practices of the past. Collecting data from languages spoken by previously colonized people, indigenous people, and non-Western languages raises many complex sociopolitical and ethical questions, e.g., around consent, cultural safety, and data sovereignty. Furthermore, linguistic complexity and cultural nuances are often lost in LLMs. This position paper builds on recent scholarship, and our own work, and outlines several relevant social, cultural, and ethical considerations and potential ways to mitigate them through qualitative research, community partnerships, and participatory design approaches. We provide twelve recommendations for consideration when collecting language data on underrepresented language communities outside of the Global North.",
            "id": "2409.05247",
            "link": "http://arxiv.org/abs/2409.05247v1",
            "published": "2024-09-08T23:51:04+00:00",
            "updated": "2024-09-08T23:51:04+00:00",
            "primary_category": "cs.CL",
            "categories": [
                "cs.CL",
                "I.2.7"
            ],
            "max_author_hindex": 30
        },
        "2409.01074": {
            "authors": [
                "Andreas Christmann",
                "Yunwen Lei"
            ],
            "title": "Bootstrap SGD: Algorithmic Stability and Robustness",
            "abstract": "In this paper some methods to use the empirical bootstrap approach for stochastic gradient descent (SGD) to minimize the empirical risk over a separable Hilbert space are investigated from the view point of algorithmic stability and statistical robustness. The first two types of approaches are based on averages and are investigated from a theoretical point of view. A generalization analysis for bootstrap SGD of Type 1 and Type 2 based on algorithmic stability is done. Another type of bootstrap SGD is proposed to demonstrate that it is possible to construct purely distribution-free pointwise confidence intervals of the median curve using bootstrap SGD.",
            "id": "2409.01074",
            "link": "http://arxiv.org/abs/2409.01074v1",
            "published": "2024-09-02T08:56:39+00:00",
            "updated": "2024-09-02T08:56:39+00:00",
            "primary_category": "stat.ML",
            "categories": [
                "stat.ML",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 23
        },
        "2409.01104": {
            "authors": [
                "Marco Cal\u00ec",
                "Alberto Sinigaglia",
                "Niccol\u00f2 Turcato",
                "Ruggero Carli",
                "Gian Antonio Susto"
            ],
            "title": "AI Olympics challenge with Evolutionary Soft Actor Critic",
            "abstract": "In the following report, we describe the solution we propose for the AI Olympics competition held at IROS 2024. Our solution is based on a Model-free Deep Reinforcement Learning approach combined with an evolutionary strategy. We will briefly describe the algorithms that have been used and then provide details of the approach",
            "id": "2409.01104",
            "link": "http://arxiv.org/abs/2409.01104v1",
            "published": "2024-09-02T09:34:18+00:00",
            "updated": "2024-09-02T09:34:18+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG",
                "cs.NE"
            ],
            "max_author_hindex": 33
        },
        "2409.01201": {
            "authors": [
                "Jaeyeon Kim",
                "Minjeon Jeon",
                "Jaeyoon Jung",
                "Sang Hoon Woo",
                "Jinjoo Lee"
            ],
            "title": "EnCLAP++: Analyzing the EnCLAP Framework for Optimizing Automated Audio Captioning Performance",
            "abstract": "In this work, we aim to analyze and optimize the EnCLAP framework, a state-of-the-art model in automated audio captioning. We investigate the impact of modifying the acoustic encoder components, explore pretraining with different dataset scales, and study the effectiveness of a reranking scheme. Through extensive experimentation and quantitative analysis of generated captions, we develop EnCLAP++, an enhanced version that significantly surpasses the original.",
            "id": "2409.01201",
            "link": "http://arxiv.org/abs/2409.01201v1",
            "published": "2024-09-02T12:23:18+00:00",
            "updated": "2024-09-02T12:23:18+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.AI",
                "cs.SD"
            ],
            "max_author_hindex": 21
        },
        "2409.01532": {
            "authors": [
                "Joel Brogan",
                "Olivera Kotevska",
                "Anibely Torres",
                "Sumit Jha",
                "Mark Adams"
            ],
            "title": "Improving Robustness of Spectrogram Classifiers with Neural Stochastic Differential Equations",
            "abstract": "Signal analysis and classification is fraught with high levels of noise and perturbation. Computer-vision-based deep learning models applied to spectrograms have proven useful in the field of signal classification and detection; however, these methods aren't designed to handle the low signal-to-noise ratios inherent within non-vision signal processing tasks. While they are powerful, they are currently not the method of choice in the inherently noisy and dynamic critical infrastructure domain, such as smart-grid sensing, anomaly detection, and non-intrusive load monitoring.",
            "id": "2409.01532",
            "link": "http://arxiv.org/abs/2409.01532v1",
            "published": "2024-09-03T02:03:50+00:00",
            "updated": "2024-09-03T02:03:50+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 27
        },
        "2409.02152": {
            "authors": [
                "Zixu He",
                "Sirin Botan",
                "J\u00e9r\u00f4me Lang",
                "Abdallah Saffidine",
                "Florian Sikora",
                "Silas Workman"
            ],
            "title": "Fair Railway Network Design",
            "abstract": "When designing a public transportation network in a country, one may want to minimise the sum of travel duration of all inhabitants. This corresponds to a purely utilitarian view and does not involve any fairness consideration, as the resulting network will typically benefit the capital city and/or large central cities while leaving some peripheral cities behind. On the other hand, a more egalitarian view will allow some people to travel between peripheral cities without having to go through a central city. We define a model, propose algorithms for computing solution networks, and report on experiments based on real data.",
            "id": "2409.02152",
            "link": "http://arxiv.org/abs/2409.02152v1",
            "published": "2024-09-03T12:13:05+00:00",
            "updated": "2024-09-03T12:13:05+00:00",
            "primary_category": "cs.SI",
            "categories": [
                "cs.SI",
                "cs.AI"
            ],
            "max_author_hindex": 55
        },
        "2409.03320": {
            "authors": [
                "Jingyu Zhang",
                "Wenqing Zhang",
                "Chaoyi Tan",
                "Xiangtian Li",
                "Qianyi Sun"
            ],
            "title": "YOLO-PPA based Efficient Traffic Sign Detection for Cruise Control in Autonomous Driving",
            "abstract": "It is very important to detect traffic signs efficiently and accurately in autonomous driving systems. However, the farther the distance, the smaller the traffic signs. Existing object detection algorithms can hardly detect these small scaled signs.In addition, the performance of embedded devices on vehicles limits the scale of detection models.To address these challenges, a YOLO PPA based traffic sign detection algorithm is proposed in this paper.The experimental results on the GTSDB dataset show that compared to the original YOLO, the proposed method improves inference efficiency by 11.2%. The mAP 50 is also improved by 93.2%, which demonstrates the effectiveness of the proposed YOLO PPA.",
            "id": "2409.03320",
            "link": "http://arxiv.org/abs/2409.03320v1",
            "published": "2024-09-05T07:49:21+00:00",
            "updated": "2024-09-05T07:49:21+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 24
        },
        "2409.03669": {
            "authors": [
                "Edgar Wolf",
                "Tobias Windisch"
            ],
            "title": "A method to benchmark high-dimensional process drift detection",
            "abstract": "Process curves are multi-variate finite time series data coming from manufacturing processes. This paper studies machine learning methods for drifts of process curves. A theoretic framework to synthetically generate process curves in a controlled way is introduced in order to benchmark machine learning algorithms for process drift detection. A evaluation score, called the temporal area under the curve, is introduced, which allows to quantify how well machine learning models unveil curves belonging to drift segments. Finally, a benchmark study comparing popular machine learning approaches on synthetic data generated with the introduced framework shown.",
            "id": "2409.03669",
            "link": "http://arxiv.org/abs/2409.03669v1",
            "published": "2024-09-05T16:23:07+00:00",
            "updated": "2024-09-05T16:23:07+00:00",
            "primary_category": "stat.ML",
            "categories": [
                "stat.ML",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 6
        },
        "2409.03992": {
            "authors": [
                "Jianwei Zhu",
                "Hang Yin",
                "Shunfan Zhou"
            ],
            "title": "Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study",
            "abstract": "This report evaluates the performance impact of enabling Trusted Execution Environments (TEE) on NVIDIA H100 GPUs for large language model (LLM) inference tasks. We benchmark the overhead introduced by TEE mode across various models and token lengths, focusing on the bottleneck caused by CPU-GPU data transfers via PCIe. Our results show that while there is minimal computational overhead within the GPU, the overall performance penalty is primarily due to data transfer. For most typical LLM queries, the overhead remains below 5%, with larger models and longer sequences experiencing near-zero overhead.",
            "id": "2409.03992",
            "link": "http://arxiv.org/abs/2409.03992v1",
            "published": "2024-09-06T02:44:27+00:00",
            "updated": "2024-09-06T02:44:27+00:00",
            "primary_category": "cs.DC",
            "categories": [
                "cs.DC",
                "cs.AI",
                "cs.PF"
            ],
            "max_author_hindex": 22
        },
        "2409.04775": {
            "authors": [
                "Rodrigo P\u00e9rez-Dattari",
                "Zhaoting Li",
                "Robert Babu\u0161ka",
                "Jens Kober",
                "Cosimo Della Santina"
            ],
            "title": "Leveraging LLMs, Graphs and Object Hierarchies for Task Planning in Large-Scale Environments",
            "abstract": "Planning methods struggle with computational intractability in solving task-level problems in large-scale environments. This work explores leveraging the commonsense knowledge encoded in LLMs to empower planning techniques to deal with these complex scenarios. We achieve this by efficiently using LLMs to prune irrelevant components from the planning problem's state space, substantially simplifying its complexity. We demonstrate the efficacy of this system through extensive experiments within a household simulation environment, alongside real-world validation using a 7-DoF manipulator (video https://youtu.be/6ro2UOtOQS4).",
            "id": "2409.04775",
            "link": "http://arxiv.org/abs/2409.04775v2",
            "published": "2024-09-07T09:30:26+00:00",
            "updated": "2024-09-10T11:43:42+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 58
        },
        "2409.05177": {
            "authors": [
                "Yi Cui"
            ],
            "title": "Insights from Benchmarking Frontier Language Models on Web App Code Generation",
            "abstract": "This paper presents insights from evaluating 16 frontier large language models (LLMs) on the WebApp1K benchmark, a test suite designed to assess the ability of LLMs to generate web application code. The results reveal that while all models possess similar underlying knowledge, their performance is differentiated by the frequency of mistakes they make. By analyzing lines of code (LOC) and failure distributions, we find that writing correct code is more complex than generating incorrect code. Furthermore, prompt engineering shows limited efficacy in reducing errors beyond specific cases. These findings suggest that further advancements in coding LLM should emphasize on model reliability and mistake minimization.",
            "id": "2409.05177",
            "link": "http://arxiv.org/abs/2409.05177v1",
            "published": "2024-09-08T18:24:26+00:00",
            "updated": "2024-09-08T18:24:26+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.05208": {
            "authors": [
                "Chhavi Yadav",
                "Ruihan Wu",
                "Kamalika Chaudhuri"
            ],
            "title": "Influence-based Attributions can be Manipulated",
            "abstract": "Influence Functions are a standard tool for attributing predictions to training data in a principled manner and are widely used in applications such as data valuation and fairness. In this work, we present realistic incentives to manipulate influencebased attributions and investigate whether these attributions can be systematically tampered by an adversary. We show that this is indeed possible and provide efficient attacks with backward-friendly implementations. Our work raises questions on the reliability of influence-based attributions under adversarial circumstances.",
            "id": "2409.05208",
            "link": "http://arxiv.org/abs/2409.05208v2",
            "published": "2024-09-08T19:52:00+00:00",
            "updated": "2024-09-10T02:58:54+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 42
        },
        "2409.00754": {
            "authors": [
                "Jiaming Yin",
                "Weixiong Rao",
                "Yu Xiao",
                "Keshuang Tang"
            ],
            "title": "Cooperative Path Planning with Asynchronous Multiagent Reinforcement Learning",
            "abstract": "In this paper, we study the shortest path problem (SPP) with multiple source-destination pairs (MSD), namely MSD-SPP, to minimize average travel time of all shortest paths. The inherent traffic capacity limits within a road network contributes to the competition among vehicles. Multi-agent reinforcement learning (MARL) model cannot offer effective and efficient path planning cooperation due to the asynchronous decision making setting in MSD-SPP, where vehicles (a.k.a agents) cannot simultaneously complete routing actions in the previous time step. To tackle the efficiency issue, we propose to divide an entire road network into multiple sub-graphs and subsequently execute a two-stage process of inter-region and intra-region route planning. To address the asynchronous issue, in the proposed asyn-MARL framework, we first design a global state, which exploits a low-dimensional vector to implicitly represent the joint observations and actions of multi-agents. Then we develop a novel trajectory collection mechanism to decrease the redundancy in training trajectories. Additionally, we design a novel actor network to facilitate the cooperation among vehicles towards the same or close destinations and a reachability graph aimed at preventing infinite loops in routing paths. On both synthetic and real road networks, our evaluation result demonstrates that our approach outperforms state-of-the-art planning approaches.",
            "id": "2409.00754",
            "link": "http://arxiv.org/abs/2409.00754v1",
            "published": "2024-09-01T15:48:14+00:00",
            "updated": "2024-09-01T15:48:14+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 19
        },
        "2409.00837": {
            "authors": [
                "Jediah Katz",
                "Bahar Bateni",
                "Adam M. Smith"
            ],
            "title": "You-Only-Randomize-Once: Shaping Statistical Properties in Constraint-based PCG",
            "abstract": "In procedural content generation, modeling the generation task as a constraint satisfaction problem lets us define local and global constraints on the generated output. However, a generator's perceived quality often involves statistics rather than just hard constraints. For example, we may desire that generated outputs use design elements with a similar distribution to that of reference designs. However, such statistical properties cannot be expressed directly as a hard constraint on the generation of any one output. In contrast, methods which do not use a general-purpose constraint solver, such as Gumin's implementation of the WaveFunctionCollapse (WFC) algorithm, can control output statistics but have limited constraint propagation ability and cannot express non-local constraints. In this paper, we introduce You-Only-Randomize-Once (YORO) pre-rolling, a method for crafting a decision variable ordering for a constraint solver that encodes desired statistics in a constraint-based generator. Using a solver-based WFC as an example, we show that this technique effectively controls the statistics of tile-grid outputs generated by several off-the-shelf SAT solvers, while still enforcing global constraints on the outputs.1 Our approach is immediately applicable to WFC-like generation problems and it offers a conceptual starting point for controlling the design element statistics in other constraint-based generators.",
            "id": "2409.00837",
            "link": "http://arxiv.org/abs/2409.00837v1",
            "published": "2024-09-01T20:43:55+00:00",
            "updated": "2024-09-01T20:43:55+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LO"
            ],
            "max_author_hindex": 23
        },
        "2409.01066": {
            "authors": [
                "Poppy Collis",
                "Ryan Singh",
                "Paul F Kinghorn",
                "Christopher L Buckley"
            ],
            "title": "Learning in Hybrid Active Inference Models",
            "abstract": "An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work in computational neuroscience has considered this functional integration of discrete and continuous variables during decision-making under the formalism of active inference (Parr, Friston & de Vries, 2017; Parr & Friston, 2018). However, their focus is on the expressive physical implementation of categorical decisions and the hierarchical mixed generative model is assumed to be known. As a consequence, it is unclear how this framework might be extended to learning. We therefore present a novel hierarchical hybrid active inference agent in which a high-level discrete active inference planner sits above a low-level continuous active inference controller. We make use of recent work in recurrent switching linear dynamical systems (rSLDS) which implement end-to-end learning of meaningful discrete representations via the piecewise linear decomposition of complex continuous dynamics (Linderman et al., 2016). The representations learned by the rSLDS inform the structure of the hybrid decision-making agent and allow us to (1) specify temporally-abstracted sub-goals in a method reminiscent of the options framework, (2) lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses and (3) `cache' the approximate solutions to low-level problems in the discrete planner. We apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and successful planning through the delineation of abstract sub-goals.",
            "id": "2409.01066",
            "link": "http://arxiv.org/abs/2409.01066v1",
            "published": "2024-09-02T08:41:45+00:00",
            "updated": "2024-09-02T08:41:45+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ],
            "max_author_hindex": 23
        },
        "2409.01374": {
            "authors": [
                "Solim LeGris",
                "Wai Keen Vong",
                "Brenden M. Lake",
                "Todd M. Gureckis"
            ],
            "title": "H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark",
            "abstract": "The Abstraction and Reasoning Corpus (ARC) is a visual program synthesis benchmark designed to test challenging out-of-distribution generalization in humans and machines. Since 2019, limited progress has been observed on the challenge using existing artificial intelligence methods. Comparing human and machine performance is important for the validity of the benchmark. While previous work explored how well humans can solve tasks from the ARC benchmark, they either did so using only a subset of tasks from the original dataset, or from variants of ARC, and therefore only provided a tentative estimate of human performance. In this work, we obtain a more robust estimate of human performance by evaluating 1729 humans on the full set of 400 training and 400 evaluation tasks from the original ARC problem set. We estimate that average human performance lies between 73.3% and 77.2% correct with a reported empirical average of 76.2% on the training set, and between 55.9% and 68.9% correct with a reported empirical average of 64.2% on the public evaluation set. However, we also find that 790 out of the 800 tasks were solvable by at least one person in three attempts, suggesting that the vast majority of the publicly available ARC tasks are in principle solvable by typical crowd-workers recruited over the internet. Notably, while these numbers are slightly lower than earlier estimates, human performance still greatly exceeds current state-of-the-art approaches for solving ARC. To facilitate research on ARC, we publicly release our dataset, called H-ARC (human-ARC), which includes all of the submissions and action traces from human participants.",
            "id": "2409.01374",
            "link": "http://arxiv.org/abs/2409.01374v1",
            "published": "2024-09-02T17:11:32+00:00",
            "updated": "2024-09-02T17:11:32+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 30
        },
        "2409.02522": {
            "authors": [
                "Zhiyuan Li",
                "Yanfeng Lu",
                "Yao Mu",
                "Hong Qiao"
            ],
            "title": "Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments",
            "abstract": "Vision Language Navigation in Continuous Environments (VLN-CE) represents a frontier in embodied AI, demanding agents to navigate freely in unbounded 3D spaces solely guided by natural language instructions. This task introduces distinct challenges in multimodal comprehension, spatial reasoning, and decision-making. To address these challenges, we introduce Cog-GA, a generative agent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA employs a dual-pronged strategy to emulate human-like cognitive processes. Firstly, it constructs a cognitive map, integrating temporal, spatial, and semantic elements, thereby facilitating the development of spatial memory within LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints, strategically optimizing the exploration trajectory to maximize navigational efficiency. Each waypoint is accompanied by a dual-channel scene description, categorizing environmental cues into 'what' and 'where' streams as the brain. This segregation enhances the agent's attentional focus, enabling it to discern pertinent spatial information for navigation. A reflective mechanism complements these strategies by capturing feedback from prior navigation experiences, facilitating continual learning and adaptive replanning. Extensive evaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art performance and ability to simulate human-like navigation behaviors. This research significantly contributes to the development of strategic and interpretable VLN-CE agents.",
            "id": "2409.02522",
            "link": "http://arxiv.org/abs/2409.02522v1",
            "published": "2024-09-04T08:30:03+00:00",
            "updated": "2024-09-04T08:30:03+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 27
        },
        "2409.02561": {
            "authors": [
                "Zhiyuan Li",
                "Yanfeng Lv",
                "Ziqin Tu",
                "Di Shang",
                "Hong Qiao"
            ],
            "title": "Vision-Language Navigation with Continual Learning",
            "abstract": "Vision-language navigation (VLN) is a critical domain within embedded intelligence, requiring agents to navigate 3D environments based on natural language instructions. Traditional VLN research has focused on improving environmental understanding and decision accuracy. However, these approaches often exhibit a significant performance gap when agents are deployed in novel environments, mainly due to the limited diversity of training data. Expanding datasets to cover a broader range of environments is impractical and costly. We propose the Vision-Language Navigation with Continual Learning (VLNCL) paradigm to address this challenge. In this paradigm, agents incrementally learn new environments while retaining previously acquired knowledge. VLNCL enables agents to maintain an environmental memory and extract relevant knowledge, allowing rapid adaptation to new environments while preserving existing information. We introduce a novel dual-loop scenario replay method (Dual-SR) inspired by brain memory replay mechanisms integrated with VLN agents. This method facilitates consolidating past experiences and enhances generalization across new tasks. By utilizing a multi-scenario memory buffer, the agent efficiently organizes and replays task memories, thereby bolstering its ability to adapt quickly to new environments and mitigating catastrophic forgetting. Our work pioneers continual learning in VLN agents, introducing a novel experimental setup and evaluation metrics. We demonstrate the effectiveness of our approach through extensive evaluations and establish a benchmark for the VLNCL paradigm. Comparative experiments with existing continual learning and VLN methods show significant improvements, achieving state-of-the-art performance in continual learning ability and highlighting the potential of our approach in enabling rapid adaptation while preserving prior knowledge.",
            "id": "2409.02561",
            "link": "http://arxiv.org/abs/2409.02561v1",
            "published": "2024-09-04T09:28:48+00:00",
            "updated": "2024-09-04T09:28:48+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 27
        },
        "2409.02697": {
            "authors": [
                "Constantin Waubert de Puiseau",
                "Fabian Wolz",
                "Merlin Montag",
                "Jannik Peters",
                "Hasan Tercan",
                "Tobias Meisen"
            ],
            "title": "Decision Transformer for Enhancing Neural Local Search on the Job Shop Scheduling Problem",
            "abstract": "The job shop scheduling problem (JSSP) and its solution algorithms have been of enduring interest in both academia and industry for decades. In recent years, machine learning (ML) is playing an increasingly important role in advancing existing and building new heuristic solutions for the JSSP, aiming to find better solutions in shorter computation times. In this paper we build on top of a state-of-the-art deep reinforcement learning (DRL) agent, called Neural Local Search (NLS), which can efficiently and effectively control a large local neighborhood search on the JSSP. In particular, we develop a method for training the decision transformer (DT) algorithm on search trajectories taken by a trained NLS agent to further improve upon the learned decision-making sequences. Our experiments show that the DT successfully learns local search strategies that are different and, in many cases, more effective than those of the NLS agent itself. In terms of the tradeoff between solution quality and acceptable computational time needed for the search, the DT is particularly superior in application scenarios where longer computational times are acceptable. In this case, it makes up for the longer inference times required per search step, which are caused by the larger neural network architecture, through better quality decisions per step. Thereby, the DT achieves state-of-the-art results for solving the JSSP with ML-enhanced search.",
            "id": "2409.02697",
            "link": "http://arxiv.org/abs/2409.02697v1",
            "published": "2024-09-04T13:33:38+00:00",
            "updated": "2024-09-04T13:33:38+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 20
        },
        "2409.02711": {
            "authors": [
                "Mohammad Reshadati"
            ],
            "title": "Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for PostNL",
            "abstract": "The developments in the field of generative AI has brought a lot of opportunities for companies, for instance to improve efficiency in customer service and automating tasks. PostNL, the biggest parcel and E-commerce corporation of the Netherlands wants to use generative AI to enhance the communication around track and trace of parcels. During the internship a Minimal Viable Product (MVP) is created to showcase the value of using generative AI technologies, to enhance parcel tracking, analyzing the parcel's journey and being able to communicate about it in an easy to understand manner. The primary goal was to develop an in-house LLM-based system, reducing dependency on external platforms and establishing the feasibility of a dedicated generative AI team within the company. This multi-agent LLM based system aimed to construct parcel journey stories and identify logistical disruptions with heightened efficiency and accuracy. The research involved deploying a sophisticated AI-driven communication system, employing Retrieval-Augmented Generation (RAG) for enhanced response precision, and optimizing large language models (LLMs) tailored to domain specific tasks.   The MVP successfully implemented a multi-agent open-source LLM system, called SuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of user inquiries and improving internal knowledge handling. Results and evaluation demonstrated technological innovation and feasibility, notably in communication about the track and trace of a parcel, which exceeded initial expectations. These advancements highlight the potential of AI-driven solutions in logistics, suggesting many opportunities for further refinement and broader implementation within PostNL operational framework.",
            "id": "2409.02711",
            "link": "http://arxiv.org/abs/2409.02711v1",
            "published": "2024-09-04T13:49:19+00:00",
            "updated": "2024-09-04T13:49:19+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 0
        },
        "2409.03402": {
            "authors": [
                "Jingwei Zhang",
                "Thomas Lampe",
                "Abbas Abdolmaleki",
                "Jost Tobias Springenberg",
                "Martin Riedmiller"
            ],
            "title": "Game On: Towards Language Models as RL Experimenters",
            "abstract": "We propose an agent architecture that automates parts of the common reinforcement learning experiment workflow, to enable automated mastery of control domains for embodied agents. To do so, it leverages a VLM to perform some of the capabilities normally required of a human experimenter, including the monitoring and analysis of experiment progress, the proposition of new tasks based on past successes and failures of the agent, decomposing tasks into a sequence of subtasks (skills), and retrieval of the skill to execute - enabling our system to build automated curricula for learning. We believe this is one of the first proposals for a system that leverages a VLM throughout the full experiment cycle of reinforcement learning. We provide a first prototype of this system, and examine the feasibility of current models and techniques for the desired level of automation. For this, we use a standard Gemini model, without additional fine-tuning, to provide a curriculum of skills to a language-conditioned Actor-Critic algorithm, in order to steer data collection so as to aid learning new skills. Data collected in this way is shown to be useful for learning and iteratively improving control policies in a robotics domain. Additional examination of the ability of the system to build a growing library of skills, and to judge the progress of the training of those skills, also shows promising results, suggesting that the proposed architecture provides a potential recipe for fully automated mastery of tasks and domains for embodied agents.",
            "id": "2409.03402",
            "link": "http://arxiv.org/abs/2409.03402v1",
            "published": "2024-09-05T10:38:16+00:00",
            "updated": "2024-09-05T10:38:16+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 53
        },
        "2409.03937": {
            "authors": [
                "Chenyang Yu",
                "Xinpeng Xie",
                "Yan Huang",
                "Chenxi Qiu"
            ],
            "title": "Harnessing LLMs for Cross-City OD Flow Prediction",
            "abstract": "Understanding and predicting Origin-Destination (OD) flows is crucial for urban planning and transportation management. Traditional OD prediction models, while effective within single cities, often face limitations when applied across different cities due to varied traffic conditions, urban layouts, and socio-economic factors. In this paper, by employing Large Language Models (LLMs), we introduce a new method for cross-city OD flow prediction. Our approach leverages the advanced semantic understanding and contextual learning capabilities of LLMs to bridge the gap between cities with different characteristics, providing a robust and adaptable solution for accurate OD flow prediction that can be transferred from one city to another. Our novel framework involves four major components: collecting OD training datasets from a source city, instruction-tuning the LLMs, predicting destination POIs in a target city, and identifying the locations that best match the predicted destination POIs. We introduce a new loss function that integrates POI semantics and trip distance during training. By extracting high-quality semantic features from human mobility and POI data, the model understands spatial and functional relationships within urban spaces and captures interactions between individuals and various POIs. Extensive experimental results demonstrate the superiority of our approach over the state-of-the-art learning-based methods in cross-city OD flow prediction.",
            "id": "2409.03937",
            "link": "http://arxiv.org/abs/2409.03937v1",
            "published": "2024-09-05T23:04:28+00:00",
            "updated": "2024-09-05T23:04:28+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 40
        },
        "2409.04465": {
            "authors": [
                "Jesse Wright"
            ],
            "title": "Here's Charlie! Realising the Semantic Web vision of Agents in the age of LLMs",
            "abstract": "This paper presents our research towards a near-term future in which legal entities, such as individuals and organisations can entrust semi-autonomous AI-driven agents to carry out online interactions on their behalf. The author's research concerns the development of semi-autonomous Web agents, which consult users if and only if the system does not have sufficient context or confidence to proceed working autonomously. This creates a user-agent dialogue that allows the user to teach the agent about the information sources they trust, their data-sharing preferences, and their decision-making preferences. Ultimately, this enables the user to maximise control over their data and decisions while retaining the convenience of using agents, including those driven by LLMs.   In view of developing near-term solutions, the research seeks to answer the question: \"How do we build a trustworthy and reliable network of semi-autonomous agents which represent individuals and organisations on the Web?\". After identifying key requirements, the paper presents a demo for a sample use case of a generic personal assistant. This is implemented using (Notation3) rules to enforce safety guarantees around belief, data sharing and data usage and LLMs to allow natural language interaction with users and serendipitous dialogues between software agents.",
            "id": "2409.04465",
            "link": "http://arxiv.org/abs/2409.04465v1",
            "published": "2024-09-03T10:32:47+00:00",
            "updated": "2024-09-03T10:32:47+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 20
        },
        "2409.04808": {
            "authors": [
                "Prathamesh Dinesh Joshi",
                "Sahil Pocker",
                "Raj Abhijit Dandekar",
                "Rajat Dandekar",
                "Sreedath Panat"
            ],
            "title": "HULLMI: Human vs LLM identification with explainability",
            "abstract": "As LLMs become increasingly proficient at producing human-like responses, there has been a rise of academic and industrial pursuits dedicated to flagging a given piece of text as \"human\" or \"AI\". Most of these pursuits involve modern NLP detectors like T5-Sentinel and RoBERTa-Sentinel, without paying too much attention to issues of interpretability and explainability of these models. In our study, we provide a comprehensive analysis that shows that traditional ML models (Naive-Bayes,MLP, Random Forests, XGBoost) perform as well as modern NLP detectors, in human vs AI text detection. We achieve this by implementing a robust testing procedure on diverse datasets, including curated corpora and real-world samples. Subsequently, by employing the explainable AI technique LIME, we uncover parts of the input that contribute most to the prediction of each model, providing insights into the detection process. Our study contributes to the growing need for developing production-level LLM detection tools, which can leverage a wide range of traditional as well as modern NLP detectors we propose. Finally, the LIME techniques we demonstrate also have the potential to equip these detection tools with interpretability analysis features, making them more reliable and trustworthy in various domains like education, healthcare, and media.",
            "id": "2409.04808",
            "link": "http://arxiv.org/abs/2409.04808v1",
            "published": "2024-09-07T12:27:25+00:00",
            "updated": "2024-09-07T12:27:25+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 6
        },
        "2409.05061": {
            "authors": [
                "Daniela Sailer",
                "Robert Klein",
                "Claudius Steinhardt"
            ],
            "title": "Dynamic Demand Management for Parcel Lockers",
            "abstract": "In pursuit of a more sustainable and cost-efficient last mile, parcel lockers have gained a firm foothold in the parcel delivery landscape. To fully exploit their potential and simultaneously ensure customer satisfaction, successful management of the locker's limited capacity is crucial. This is challenging as future delivery requests and pickup times are stochastic from the provider's perspective. In response, we propose to dynamically control whether the locker is presented as an available delivery option to each incoming customer with the goal of maximizing the number of served requests weighted by their priority. Additionally, we take different compartment sizes into account, which entails a second type of decision as parcels scheduled for delivery must be allocated. We formalize the problem as an infinite-horizon sequential decision problem and find that exact methods are intractable due to the curses of dimensionality. In light of this, we develop a solution framework that orchestrates multiple algorithmic techniques rooted in Sequential Decision Analytics and Reinforcement Learning, namely cost function approximation and an offline trained parametric value function approximation together with a truncated online rollout. Our innovative approach to combine these techniques enables us to address the strong interrelations between the two decision types. As a general methodological contribution, we enhance the training of our value function approximation with a modified version of experience replay that enforces structure in the value function. Our computational study shows that our method outperforms a myopic benchmark by 13.7% and an industry-inspired policy by 12.6%.",
            "id": "2409.05061",
            "link": "http://arxiv.org/abs/2409.05061v2",
            "published": "2024-09-08T11:38:48+00:00",
            "updated": "2024-09-12T08:19:32+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.00347": {
            "authors": [
                "Paulo Soares",
                "Sean McCurdy",
                "Andrew J. Gerber",
                "Peter Fonagy"
            ],
            "title": "Chatting Up Attachment: Using LLMs to Predict Adult Bonds",
            "abstract": "Obtaining data in the medical field is challenging, making the adoption of AI technology within the space slow and high-risk. We evaluate whether we can overcome this obstacle with synthetic data generated by large language models (LLMs). In particular, we use GPT-4 and Claude 3 Opus to create agents that simulate adults with varying profiles, childhood memories, and attachment styles. These agents participate in simulated Adult Attachment Interviews (AAI), and we use their responses to train models for predicting their underlying attachment styles. We evaluate our models using a transcript dataset from 9 humans who underwent the same interview protocol, analyzed and labeled by mental health professionals. Our findings indicate that training the models using only synthetic data achieves performance comparable to training the models on human data. Additionally, while the raw embeddings from synthetic answers occupy a distinct space compared to those from real human responses, the introduction of unlabeled human data and a simple standardization allows for a closer alignment of these representations. This adjustment is supported by qualitative analyses and is reflected in the enhanced predictive accuracy of the standardized embeddings.",
            "id": "2409.00347",
            "link": "http://arxiv.org/abs/2409.00347v1",
            "published": "2024-08-31T04:29:19+00:00",
            "updated": "2024-08-31T04:29:19+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 126
        },
        "2409.00543": {
            "authors": [
                "Sicheng Wang",
                "Che Liu",
                "Rossella Arcucci"
            ],
            "title": "How Does Diverse Interpretability of Textual Prompts Impact Medical Vision-Language Zero-Shot Tasks?",
            "abstract": "Recent advancements in medical vision-language pre-training (MedVLP) have significantly enhanced zero-shot medical vision tasks such as image classification by leveraging large-scale medical image-text pair pre-training. However, the performance of these tasks can be heavily influenced by the variability in textual prompts describing the categories, necessitating robustness in MedVLP models to diverse prompt styles. Yet, this sensitivity remains underexplored. In this work, we are the first to systematically assess the sensitivity of three widely-used MedVLP methods to a variety of prompts across 15 different diseases. To achieve this, we designed six unique prompt styles to mirror real clinical scenarios, which were subsequently ranked by interpretability. Our findings indicate that all MedVLP models evaluated show unstable performance across different prompt styles, suggesting a lack of robustness. Additionally, the models' performance varied with increasing prompt interpretability, revealing difficulties in comprehending complex medical concepts. This study underscores the need for further development in MedVLP methodologies to enhance their robustness to diverse zero-shot prompts.",
            "id": "2409.00543",
            "link": "http://arxiv.org/abs/2409.00543v1",
            "published": "2024-08-31T20:43:06+00:00",
            "updated": "2024-08-31T20:43:06+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.CL",
                "cs.LG",
                "eess.IV"
            ],
            "max_author_hindex": 21
        },
        "2409.00597": {
            "authors": [
                "Fuqiang Niu",
                "Zebang Cheng",
                "Xianghua Fu",
                "Xiaojiang Peng",
                "Genan Dai",
                "Yin Chen",
                "Hu Huang",
                "Bowen Zhang"
            ],
            "title": "Multimodal Multi-turn Conversation Stance Detection: A Challenge Dataset and Effective Model",
            "abstract": "Stance detection, which aims to identify public opinion towards specific targets using social media data, is an important yet challenging task. With the proliferation of diverse multimodal social media content including text, and images multimodal stance detection (MSD) has become a crucial research area. However, existing MSD studies have focused on modeling stance within individual text-image pairs, overlooking the multi-party conversational contexts that naturally occur on social media. This limitation stems from a lack of datasets that authentically capture such conversational scenarios, hindering progress in conversational MSD. To address this, we introduce a new multimodal multi-turn conversational stance detection dataset (called MmMtCSD). To derive stances from this challenging dataset, we propose a novel multimodal large language model stance detection framework (MLLM-SD), that learns joint stance representations from textual and visual modalities. Experiments on MmMtCSD show state-of-the-art performance of our proposed MLLM-SD approach for multimodal stance detection. We believe that MmMtCSD will contribute to advancing real-world applications of stance detection research.",
            "id": "2409.00597",
            "link": "http://arxiv.org/abs/2409.00597v1",
            "published": "2024-09-01T03:16:30+00:00",
            "updated": "2024-09-01T03:16:30+00:00",
            "primary_category": "cs.MM",
            "categories": [
                "cs.MM",
                "cs.CL"
            ],
            "max_author_hindex": 25
        },
        "2409.00729": {
            "authors": [
                "Benjamin Cohen-Wang",
                "Harshay Shah",
                "Kristian Georgiev",
                "Aleksander Madry"
            ],
            "title": "ContextCite: Attributing Model Generation to Context",
            "abstract": "How do language models use information provided as context when generating a response? Can we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated? To help answer these questions, we introduce the problem of context attribution: pinpointing the parts of the context (if any) that led a model to generate a particular statement. We then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model. Finally, we showcase the utility of ContextCite through three applications: (1) helping verify generated statements (2) improving response quality by pruning the context and (3) detecting poisoning attacks. We provide code for ContextCite at https://github.com/MadryLab/context-cite.",
            "id": "2409.00729",
            "link": "http://arxiv.org/abs/2409.00729v1",
            "published": "2024-09-01T14:36:36+00:00",
            "updated": "2024-09-01T14:36:36+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 51
        },
        "2409.00819": {
            "authors": [
                "Zengrui Jin",
                "Yifan Yang",
                "Mohan Shi",
                "Wei Kang",
                "Xiaoyu Yang",
                "Zengwei Yao",
                "Fangjun Kuang",
                "Liyong Guo",
                "Lingwei Meng",
                "Long Lin",
                "Yong Xu",
                "Shi-Xiong Zhang",
                "Daniel Povey"
            ],
            "title": "LibriheavyMix: A 20,000-Hour Dataset for Single-Channel Reverberant Multi-Talker Speech Separation, ASR and Speaker Diarization",
            "abstract": "The evolving speech processing landscape is increasingly focused on complex scenarios like meetings or cocktail parties with multiple simultaneous speakers and far-field conditions. Existing methodologies for addressing these challenges fall into two categories: multi-channel and single-channel solutions. Single-channel approaches, notable for their generality and convenience, do not require specific information about microphone arrays.   This paper presents a large-scale far-field overlapping speech dataset, crafted to advance research in speech separation, recognition, and speaker diarization. This dataset is a critical resource for decoding ``Who said What and When'' in multi-talker, reverberant environments, a daunting challenge in the field. Additionally, we introduce a pipeline system encompassing speech separation, recognition, and diarization as a foundational benchmark. Evaluations on the WHAMR! dataset validate the broad applicability of the proposed data.",
            "id": "2409.00819",
            "link": "http://arxiv.org/abs/2409.00819v1",
            "published": "2024-09-01T19:23:08+00:00",
            "updated": "2024-09-01T19:23:08+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.CL",
                "eess.AS"
            ],
            "max_author_hindex": 61
        },
        "2409.01380": {
            "authors": [
                "Rui Wen",
                "Zheng Li",
                "Michael Backes",
                "Yang Zhang"
            ],
            "title": "Membership Inference Attacks Against In-Context Learning",
            "abstract": "Adapting Large Language Models (LLMs) to specific tasks introduces concerns about computational efficiency, prompting an exploration of efficient methods such as In-Context Learning (ICL). However, the vulnerability of ICL to privacy attacks under realistic assumptions remains largely unexplored. In this work, we present the first membership inference attack tailored for ICL, relying solely on generated texts without their associated probabilities. We propose four attack strategies tailored to various constrained scenarios and conduct extensive experiments on four popular large language models. Empirical results show that our attacks can accurately determine membership status in most cases, e.g., 95\\% accuracy advantage against LLaMA, indicating that the associated risks are much higher than those shown by existing probability-based attacks. Additionally, we propose a hybrid attack that synthesizes the strengths of the aforementioned strategies, achieving an accuracy advantage of over 95\\% in most cases. Furthermore, we investigate three potential defenses targeting data, instruction, and output. Results demonstrate combining defenses from orthogonal dimensions significantly reduces privacy leakage and offers enhanced privacy assurances.",
            "id": "2409.01380",
            "link": "http://arxiv.org/abs/2409.01380v1",
            "published": "2024-09-02T17:23:23+00:00",
            "updated": "2024-09-02T17:23:23+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.CL"
            ],
            "max_author_hindex": 67
        },
        "2409.01763": {
            "authors": [
                "Hoang-Thang Ta",
                "Duy-Quy Thai",
                "Abu Bakar Siddiqur Rahman",
                "Grigori Sidorov",
                "Alexander Gelbukh"
            ],
            "title": "FC-KAN: Function Combinations in Kolmogorov-Arnold Networks",
            "abstract": "In this paper, we introduce FC-KAN, a Kolmogorov-Arnold Network (KAN) that leverages combinations of popular mathematical functions such as B-splines, wavelets, and radial basis functions on low-dimensional data through element-wise operations. We explore several methods for combining the outputs of these functions, including sum, element-wise product, the addition of sum and element-wise product, quadratic function representation, and concatenation. In our experiments, we compare FC-KAN with multi-layer perceptron network (MLP) and other existing KANs, such as BSRBF-KAN, EfficientKAN, FastKAN, and FasterKAN, on the MNIST and Fashion-MNIST datasets. A variant of FC-KAN, which uses a combination of outputs from B-splines and Difference of Gaussians (DoG) in the form of a quadratic function, outperformed all other models on the average of 5 independent training runs. We expect that FC-KAN can leverage function combinations to design future KANs. Our repository is publicly available at: https://github.com/hoangthangta/FC_KAN.",
            "id": "2409.01763",
            "link": "http://arxiv.org/abs/2409.01763v1",
            "published": "2024-09-03T10:16:43+00:00",
            "updated": "2024-09-03T10:16:43+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 49
        },
        "2409.01835": {
            "authors": [
                "Soumitri Chattopadhyay",
                "Sanket Biswas",
                "Emanuele Vivoli",
                "Josep Llad\u00f3s"
            ],
            "title": "Towards Generative Class Prompt Learning for Fine-grained Visual Recognition",
            "abstract": "Although foundational vision-language models (VLMs) have proven to be very successful for various semantic discrimination tasks, they still struggle to perform faithfully for fine-grained categorization. Moreover, foundational models trained on one domain do not generalize well on a different domain without fine-tuning. We attribute these to the limitations of the VLM's semantic representations and attempt to improve their fine-grained visual awareness using generative modeling. Specifically, we propose two novel methods: Generative Class Prompt Learning (GCPL) and Contrastive Multi-class Prompt Learning (CoMPLe). Utilizing text-to-image diffusion models, GCPL significantly improves the visio-linguistic synergy in class embeddings by conditioning on few-shot exemplars with learnable class prompts. CoMPLe builds on this foundation by introducing a contrastive learning component that encourages inter-class separation during the generative optimization process. Our empirical results demonstrate that such a generative class prompt learning approach substantially outperform existing methods, offering a better alternative to few shot image recognition challenges. The source code will be made available at: https://github.com/soumitri2001/GCPL.",
            "id": "2409.01835",
            "link": "http://arxiv.org/abs/2409.01835v2",
            "published": "2024-09-03T12:34:21+00:00",
            "updated": "2024-09-07T22:51:50+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.CL"
            ],
            "max_author_hindex": 35
        },
        "2409.02259": {
            "authors": [
                "Ali Lotfi",
                "Ian McQuillan"
            ],
            "title": "Optimal L-Systems for Stochastic L-system Inference Problems",
            "abstract": "This paper presents two novel theorems that address two open problems in stochastic Lindenmayer-system (L-system) inference, specifically focusing on the construction of an optimal stochastic L-system capable of generating a given sequence of strings. The first theorem delineates a method for crafting a stochastic L-system that maximizes the likelihood of producing a given sequence of words through a singular derivation. Furthermore, the second theorem determines the stochastic L-systems with the highest probability of producing a given sequence of words with multiple possible derivations. From these, we introduce an algorithm to infer an optimal stochastic L-system from a given sequence. This algorithm incorporates sophisticated optimization techniques, such as interior point methods, ensuring production of a stochastically optimal stochastic L-system suitable for generating the given sequence. This allows for the use of using stochastic L-systems as model for machine learning using only positive data for training.",
            "id": "2409.02259",
            "link": "http://arxiv.org/abs/2409.02259v1",
            "published": "2024-09-03T19:34:25+00:00",
            "updated": "2024-09-03T19:34:25+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL",
                "cs.CV",
                "cs.DS",
                "cs.FL"
            ],
            "max_author_hindex": 13
        },
        "2409.02596": {
            "authors": [
                "Ryan Whetten",
                "Titouan Parcollet",
                "Adel Moumen",
                "Marco Dinarelli",
                "Yannick Est\u00e8ve"
            ],
            "title": "An Analysis of Linear Complexity Attention Substitutes with BEST-RQ",
            "abstract": "Self-Supervised Learning (SSL) has proven to be effective in various domains, including speech processing. However, SSL is computationally and memory expensive. This is in part due the quadratic complexity of multi-head self-attention (MHSA). Alternatives for MHSA have been proposed and used in the speech domain, but have yet to be investigated properly in an SSL setting. In this work, we study the effects of replacing MHSA with recent state-of-the-art alternatives that have linear complexity, namely, HyperMixing, Fastformer, SummaryMixing, and Mamba. We evaluate these methods by looking at the speed, the amount of VRAM consumed, and the performance on the SSL MP3S benchmark. Results show that these linear alternatives maintain competitive performance compared to MHSA while, on average, decreasing VRAM consumption by around 20% to 60% and increasing speed from 7% to 65% for input sequences ranging from 20 to 80 seconds.",
            "id": "2409.02596",
            "link": "http://arxiv.org/abs/2409.02596v1",
            "published": "2024-09-04T10:27:07+00:00",
            "updated": "2024-09-04T10:27:07+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 30
        },
        "2409.02690": {
            "authors": [
                "Michael Achmann-Denkler",
                "Jakob Fehle",
                "Mario Haim",
                "Christian Wolff"
            ],
            "title": "Detecting Calls to Action in Multimodal Content: Analysis of the 2021 German Federal Election Campaign on Instagram",
            "abstract": "This study investigates the automated classification of Calls to Action (CTAs) within the 2021 German Instagram election campaign to advance the understanding of mobilization in social media contexts. We analyzed over 2,208 Instagram stories and 712 posts using fine-tuned BERT models and OpenAI's GPT-4 models. The fine-tuned BERT model incorporating synthetic training data achieved a macro F1 score of 0.93, demonstrating a robust classification performance. Our analysis revealed that 49.58% of Instagram posts and 10.64% of stories contained CTAs, highlighting significant differences in mobilization strategies between these content types. Additionally, we found that FDP and the Greens had the highest prevalence of CTAs in posts, whereas CDU and CSU led in story CTAs.",
            "id": "2409.02690",
            "link": "http://arxiv.org/abs/2409.02690v1",
            "published": "2024-09-04T13:23:50+00:00",
            "updated": "2024-09-04T13:23:50+00:00",
            "primary_category": "cs.SI",
            "categories": [
                "cs.SI",
                "cs.CL"
            ],
            "max_author_hindex": 32
        },
        "2409.02718": {
            "authors": [
                "Zi Liang",
                "Qingqing Ye",
                "Yanyun Wang",
                "Sen Zhang",
                "Yaxin Xiao",
                "Ronghua Li",
                "Jianliang Xu",
                "Haibo Hu"
            ],
            "title": "Alignment-Aware Model Extraction Attacks on Large Language Models",
            "abstract": "Model extraction attacks (MEAs) on large language models (LLMs) have received increasing research attention lately. Existing attack methods on LLMs inherit the extraction strategies from those designed for deep neural networks (DNNs) yet neglect the inconsistency of training tasks between MEA and LLMs' alignments. As such, they result in poor attack performances. To tackle this issue, we present Locality Reinforced Distillation (LoRD), a novel model extraction attack algorithm specifically for LLMs. In particular, we design a policy-gradient-style training task, which utilizes victim models' responses as a signal to guide the crafting of preference for the local model. Theoretical analysis has shown that i) LoRD's convergence procedure in MEAs is consistent with the alignments of LLMs, and ii) LoRD can reduce query complexity while mitigating watermark protection through exploration-based stealing. Extensive experiments on domain-specific extractions demonstrate the superiority of our method by examining the extraction of various state-of-the-art commercial LLMs.",
            "id": "2409.02718",
            "link": "http://arxiv.org/abs/2409.02718v1",
            "published": "2024-09-04T13:54:38+00:00",
            "updated": "2024-09-04T13:54:38+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.CL"
            ],
            "max_author_hindex": 59
        },
        "2409.03668": {
            "authors": [
                "Abdurahman Maarouf",
                "Stefan Feuerriegel",
                "Nicolas Pr\u00f6llochs"
            ],
            "title": "A Fused Large Language Model for Predicting Startup Success",
            "abstract": "Investors are continuously seeking profitable investment opportunities in startups and, hence, for effective decision-making, need to predict a startup's probability of success. Nowadays, investors can use not only various fundamental information about a startup (e.g., the age of the startup, the number of founders, and the business sector) but also textual description of a startup's innovation and business model, which is widely available through online venture capital (VC) platforms such as Crunchbase. To support the decision-making of investors, we develop a machine learning approach with the aim of locating successful startups on VC platforms. Specifically, we develop, train, and evaluate a tailored, fused large language model to predict startup success. Thereby, we assess to what extent self-descriptions on VC platforms are predictive of startup success. Using 20,172 online profiles from Crunchbase, we find that our fused large language model can predict startup success, with textual self-descriptions being responsible for a significant part of the predictive power. Our work provides a decision support tool for investors to find profitable investment opportunities.",
            "id": "2409.03668",
            "link": "http://arxiv.org/abs/2409.03668v1",
            "published": "2024-09-05T16:22:31+00:00",
            "updated": "2024-09-05T16:22:31+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 37
        },
        "2409.04185": {
            "authors": [
                "Tim Lawson",
                "Lucy Farnik",
                "Conor Houghton",
                "Laurence Aitchison"
            ],
            "title": "Residual Stream Analysis with Multi-Layer SAEs",
            "abstract": "Sparse autoencoders (SAEs) are a promising approach to interpreting the internal representations of transformer language models. However, standard SAEs are trained separately on each transformer layer, making it difficult to use them to study how information flows across layers. To solve this problem, we introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual stream activation vectors from every transformer layer simultaneously. The residual stream is usually understood as preserving information across layers, so we expected to, and did, find individual SAE features that are active at multiple layers. Interestingly, while a single SAE feature is active at different layers for different prompts, for a single prompt, we find that a single feature is far more likely to be active at a single layer. For larger underlying models, we find that the cosine similarities between adjacent layers in the residual stream are higher, so we expect more features to be active at multiple layers. These results show that MLSAEs are a promising method to study information flow in transformers. We release our code to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.",
            "id": "2409.04185",
            "link": "http://arxiv.org/abs/2409.04185v1",
            "published": "2024-09-06T11:01:55+00:00",
            "updated": "2024-09-06T11:01:55+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 21
        },
        "2409.04206": {
            "authors": [
                "Adir Rahamim",
                "Naomi Saphra",
                "Sara Kangaslahti",
                "Yonatan Belinkov"
            ],
            "title": "Fast Forwarding Low-Rank Training",
            "abstract": "Parameter efficient finetuning methods like low-rank adaptation (LoRA) aim to reduce the computational costs of finetuning pretrained Language Models (LMs). Enabled by these low-rank settings, we propose an even more efficient optimization strategy: Fast Forward, a simple and effective approach to accelerate large segments of training. In a Fast Forward stage, we repeat the most recent optimizer step until the loss stops improving on a tiny validation set. By alternating between regular optimization steps and Fast Forward stages, Fast Forward provides up to an 87\\% reduction in FLOPs and up to an 81\\% reduction in train time over standard SGD with Adam. We validate Fast Forward by finetuning various models on different tasks and demonstrate that it speeds up training without compromising model performance. Additionally, we analyze when and how to apply Fast Forward.",
            "id": "2409.04206",
            "link": "http://arxiv.org/abs/2409.04206v1",
            "published": "2024-09-06T11:53:37+00:00",
            "updated": "2024-09-06T11:53:37+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 47
        },
        "2409.04384": {
            "authors": [
                "Charlesquin Kemajou Mbakam",
                "Jean-Francois Giovannelli",
                "Marcelo Pereyra"
            ],
            "title": "Empirical Bayesian image restoration by Langevin sampling with a denoising diffusion implicit prior",
            "abstract": "Score-based diffusion methods provide a powerful strategy to solve image restoration tasks by flexibly combining a pre-trained foundational prior model with a likelihood function specified during test time. Such methods are predominantly derived from two stochastic processes: reversing Ornstein-Uhlenbeck, which underpins the celebrated denoising diffusion probabilistic models (DDPM) and denoising diffusion implicit models (DDIM), and the Langevin diffusion process. The solutions delivered by DDPM and DDIM are often remarkably realistic, but they are not always consistent with measurements because of likelihood intractability issues and the associated required approximations. Alternatively, using a Langevin process circumvents the intractable likelihood issue, but usually leads to restoration results of inferior quality and longer computing times. This paper presents a novel and highly computationally efficient image restoration method that carefully embeds a foundational DDPM denoiser within an empirical Bayesian Langevin algorithm, which jointly calibrates key model hyper-parameters as it estimates the model's posterior mean. Extensive experimental results on three canonical tasks (image deblurring, super-resolution, and inpainting) demonstrate that the proposed approach improves on state-of-the-art strategies both in image estimation accuracy and computing time.",
            "id": "2409.04384",
            "link": "http://arxiv.org/abs/2409.04384v1",
            "published": "2024-09-06T16:20:24+00:00",
            "updated": "2024-09-06T16:20:24+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.CL",
                "eess.IV"
            ],
            "max_author_hindex": 24
        },
        "2409.04507": {
            "authors": [
                "Nicola Amico",
                "Achille Felicetti"
            ],
            "title": "3D Data Long-Term Preservation in Cultural Heritage",
            "abstract": "The report explores the challenges and strategies for preserving 3D digital data in cultural heritage. It discusses the issue of technological obsolescence, emphasising the need for ustainable storage solutions and ongoing data management strategies. Key topics include understanding technological obsolescence, the lifecycle of digital content, digital continuity, data management plans (DMP), FAIR principles, and the use of public repositories. The report also covers the importance of metadata in long-term digital preservation, including types of metadata and strategies for building valuable metadata. It examines the evolving standards and interoperability in 3D format preservation and the importance of managing metadata and paradata. The document provides a comprehensive overview of the challenges and solutions for preserving 3D cultural heritage data in the long term.",
            "id": "2409.04507",
            "link": "http://arxiv.org/abs/2409.04507v1",
            "published": "2024-09-06T16:32:46+00:00",
            "updated": "2024-09-06T16:32:46+00:00",
            "primary_category": "cs.IT",
            "categories": [
                "cs.IT",
                "cs.CG",
                "cs.CL",
                "cs.DL",
                "cs.GR",
                "math.IT",
                "E.1; I.4; H.1.1; H.3.2"
            ],
            "max_author_hindex": 11
        },
        "2409.05005": {
            "authors": [
                "Hongbo Wang",
                "Junyu Lu",
                "Yan Han",
                "Kai Ma",
                "Liang Yang",
                "Hongfei Lin"
            ],
            "title": "Towards Patronizing and Condescending Language in Chinese Videos: A Multimodal Dataset and Detector",
            "abstract": "Patronizing and Condescending Language (PCL) is a form of discriminatory toxic speech targeting vulnerable groups, threatening both online and offline safety. While toxic speech research has mainly focused on overt toxicity, such as hate speech, microaggressions in the form of PCL remain underexplored. Additionally, dominant groups' discriminatory facial expressions and attitudes toward vulnerable communities can be more impactful than verbal cues, yet these frame features are often overlooked. In this paper, we introduce the PCLMM dataset, the first Chinese multimodal dataset for PCL, consisting of 715 annotated videos from Bilibili, with high-quality PCL facial frame spans. We also propose the MultiPCL detector, featuring a facial expression detection module for PCL recognition, demonstrating the effectiveness of modality complementarity in this challenging task. Our work makes an important contribution to advancing microaggression detection within the domain of toxic speech.",
            "id": "2409.05005",
            "link": "http://arxiv.org/abs/2409.05005v2",
            "published": "2024-09-08T07:26:13+00:00",
            "updated": "2024-09-10T02:50:54+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.CL"
            ],
            "max_author_hindex": 29
        },
        "2409.05902": {
            "authors": [
                "Jahyun Koo",
                "Dahoon Park",
                "Sangwoo Jung",
                "Jaeha Kung"
            ],
            "title": "OPAL: Outlier-Preserved Microscaling Quantization A ccelerator for Generative Large Language Models",
            "abstract": "To overcome the burden on the memory size and bandwidth due to ever-increasing size of large language models (LLMs), aggressive weight quantization has been recently studied, while lacking research on quantizing activations. In this paper, we present a hardware-software co-design method that results in an energy-efficient LLM accelerator, named OPAL, for generation tasks. First of all, a novel activation quantization method that leverages the microscaling data format while preserving several outliers per sub-tensor block (e.g., four out of 128 elements) is proposed. Second, on top of preserving outliers, mixed precision is utilized that sets 5-bit for inputs to sensitive layers in the decoder block of an LLM, while keeping inputs to less sensitive layers to 3-bit. Finally, we present the OPAL hardware architecture that consists of FP units for handling outliers and vectorized INT multipliers for dominant non-outlier related operations. In addition, OPAL uses log2-based approximation on softmax operations that only requires shift and subtraction to maximize power efficiency. As a result, we are able to improve the energy efficiency by 1.6~2.2x, and reduce the area by 2.4~3.1x with negligible accuracy loss, i.e., <1 perplexity increase.",
            "id": "2409.05902",
            "link": "http://arxiv.org/abs/2409.05902v1",
            "published": "2024-09-06T02:33:20+00:00",
            "updated": "2024-09-06T02:33:20+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AR",
                "cs.CL"
            ],
            "max_author_hindex": 14
        },
        "2409.05910": {
            "authors": [
                "Tzu-Quan Lin",
                "Guan-Ting Lin",
                "Hung-yi Lee",
                "Hao Tang"
            ],
            "title": "Property Neurons in Self-Supervised Speech Transformers",
            "abstract": "There have been many studies on analyzing self-supervised speech Transformers, in particular, with layer-wise analysis. It is, however, desirable to have an approach that can pinpoint exactly a subset of neurons that is responsible for a particular property of speech, being amenable to model pruning and model editing. In this work, we identify a set of property neurons in the feedforward layers of Transformers to study how speech-related properties, such as phones, gender, and pitch, are stored. When removing neurons of a particular property (a simple form of model editing), the respective downstream performance significantly degrades, showing the importance of the property neurons. We apply this approach to pruning the feedforward layers in Transformers, where most of the model parameters are. We show that protecting property neurons during pruning is significantly more effective than norm-based pruning.",
            "id": "2409.05910",
            "link": "http://arxiv.org/abs/2409.05910v1",
            "published": "2024-09-07T05:59:19+00:00",
            "updated": "2024-09-07T05:59:19+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.CL",
                "cs.LG",
                "cs.SD"
            ],
            "max_author_hindex": 47
        },
        "2409.00494": {
            "authors": [
                "Haowen Xu",
                "Jinghui Yuan",
                "Anye Zhou",
                "Guanhao Xu",
                "Wan Li",
                "Xuegang Ban",
                "Xinyue Ye"
            ],
            "title": "GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility: Opportunities and Challenges for Integrating Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems",
            "abstract": "Leveraging recent advances in generative AI, multi-agent systems are increasingly being developed to enhance the functionality and efficiency of smart city applications. This paper explores the transformative potential of large language models (LLMs) and emerging Retrieval-Augmented Generation (RAG) technologies in Intelligent Transportation Systems (ITS), paving the way for innovative solutions to address critical challenges in urban mobility. We begin by providing a comprehensive overview of the current state-of-the-art in mobility data, ITS, and Connected Vehicles (CV) applications. Building on this review, we discuss the rationale behind RAG and examine the opportunities for integrating these Generative AI (GenAI) technologies into the smart mobility sector. We propose a conceptual framework aimed at developing multi-agent systems capable of intelligently and conversationally delivering smart mobility services to urban commuters, transportation operators, and decision-makers. Our approach seeks to foster an autonomous and intelligent approach that (a) promotes science-based advisory to reduce traffic congestion, accidents, and carbon emissions at multiple scales, (b) facilitates public education and engagement in participatory mobility management, and (c) automates specialized transportation management tasks and the development of critical ITS platforms, such as data analytics and interpretation, knowledge representation, and traffic simulations. By integrating LLM and RAG, our approach seeks to overcome the limitations of traditional rule-based multi-agent systems, which rely on fixed knowledge bases and limited reasoning capabilities. This integration paves the way for a more scalable, intuitive, and automated multi-agent paradigm, driving advancements in ITS and urban mobility.",
            "id": "2409.00494",
            "link": "http://arxiv.org/abs/2409.00494v2",
            "published": "2024-08-31T16:14:42+00:00",
            "updated": "2024-09-04T18:00:53+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.SE"
            ],
            "max_author_hindex": 46
        },
        "2409.00735": {
            "authors": [
                "Mahsa Khosravi",
                "Matthew Carroll",
                "Kai Liang Tan",
                "Liza Van der Laan",
                "Joscif Raigne",
                "Daren S. Mueller",
                "Arti Singh",
                "Aditya Balu",
                "Baskar Ganapathysubramanian",
                "Asheesh Kumar Singh",
                "Soumik Sarkar"
            ],
            "title": "AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning",
            "abstract": "Agricultural production requires careful management of inputs such as fungicides, insecticides, and herbicides to ensure a successful crop that is high-yielding, profitable, and of superior seed quality. Current state-of-the-art field crop management relies on coarse-scale crop management strategies, where entire fields are sprayed with pest and disease-controlling chemicals, leading to increased cost and sub-optimal soil and crop management. To overcome these challenges and optimize crop production, we utilize machine learning tools within a virtual field environment to generate localized management plans for farmers to manage biotic threats while maximizing profits. Specifically, we present AgGym, a modular, crop and stress agnostic simulation framework to model the spread of biotic stresses in a field and estimate yield losses with and without chemical treatments. Our validation with real data shows that AgGym can be customized with limited data to simulate yield outcomes under various biotic stress conditions. We further demonstrate that deep reinforcement learning (RL) policies can be trained using AgGym for designing ultra-precise biotic stress mitigation strategies with potential to increase yield recovery with less chemicals and lower cost. Our proposed framework enables personalized decision support that can transform biotic stress management from being schedule based and reactive to opportunistic and prescriptive. We also release the AgGym software implementation as a community resource and invite experts to contribute to this open-sourced publicly available modular environment framework. The source code can be accessed at: https://github.com/SCSLabISU/AgGym.",
            "id": "2409.00735",
            "link": "http://arxiv.org/abs/2409.00735v1",
            "published": "2024-09-01T14:55:45+00:00",
            "updated": "2024-09-01T14:55:45+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 44
        },
        "2409.01903": {
            "authors": [
                "Abdelmalek Mouazer",
                "Sophie Dubois",
                "Romain L\u00e9guillon",
                "Nada Boudegzdame",
                "Thibaud Levrard",
                "Yoann Le Bars",
                "Christian Simon",
                "Brigitte S\u00e9roussi",
                "Julien Grosjean",
                "Romain Lelong",
                "Catherine Letord",
                "St\u00e9fan Darmoni",
                "Karima Sedki",
                "Pierre Meneton",
                "Rosy Tsopra",
                "Hector Falcoff",
                "Jean-Baptiste Lamy"
            ],
            "title": "A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management",
            "abstract": "Background: Medication review is a structured interview of the patient, performed by the pharmacist and aimed at optimizing drug treatments. In practice, medication review is a long and cognitively-demanding task that requires specific knowledge. Clinical practice guidelines have been proposed, but their application is tedious. Methods: We designed ABiMed, a clinical decision support system for medication reviews, based on the implementation of the STOPP/START v2 guidelines and on the visual presentation of aggregated drug knowledge using tables, graphs and flower glyphs. We evaluated ABiMed with 39 community pharmacists during a randomized simulation trial, each pharmacist performing a medication review for two fictitious patients without ABiMed, and two others with ABiMed. We recorded the problems identified by the pharmacists, the interventions proposed, the response time, the perceived usability and the comments. Pharmacists' medication reviews were compared to an expert-designed gold standard. Results: With ABiMed, pharmacists found 1.6 times more relevant drug-related problems during the medication review (p=1.1e-12) and proposed better interventions (p=9.8e-9), without needing more time (p=0.56). The System Usability Scale score is 82.7, which is ranked \"excellent\". In their comments, pharmacists appreciated the visual aspect of ABiMed and its ability to compare the current treatment with the proposed one. A multifactor analysis showed no difference in the support offered by ABiMed according to the pharmacist's age or sex, in terms of percentage of problems identified or quality of the proposed interventions. Conclusions: The use of an intelligent and visual clinical decision support system can help pharmacists when they perform medication reviews. Our main perspective is the validation of the system in clinical conditions.",
            "id": "2409.01903",
            "link": "http://arxiv.org/abs/2409.01903v1",
            "published": "2024-09-03T13:50:59+00:00",
            "updated": "2024-09-03T13:50:59+00:00",
            "primary_category": "cs.AI",
            "categories": [
                "cs.AI",
                "cs.HC",
                "J.3"
            ],
            "max_author_hindex": 49
        },
        "2409.00483": {
            "authors": [
                "Tomasz Stanisz",
                "Stanis\u0142aw Dro\u017cd\u017c",
                "Jaros\u0142aw Kwapie\u0144"
            ],
            "title": "Statistics of punctuation in experimental literature -- the remarkable case of \"Finnegans Wake\" by James Joyce",
            "abstract": "As the recent studies indicate, the structure imposed onto written texts by the presence of punctuation develops patterns which reveal certain characteristics of universality. In particular, based on a large collection of classic literary works, it has been evidenced that the distances between consecutive punctuation marks, measured in terms of the number of words, obey the discrete Weibull distribution - a discrete variant of a distribution often used in survival analysis. The present work extends the analysis of punctuation usage patterns to more experimental pieces of world literature. It turns out that the compliance of the the distances between punctuation marks with the discrete Weibull distribution typically applies here as well. However, some of the works by James Joyce are distinct in this regard - in the sense that the tails of the relevant distributions are significantly thicker and, consequently, the corresponding hazard functions are decreasing functions not observed in typical literary texts in prose. \"Finnegans Wake\" - the same one to which science owes the word \"quarks\" for the most fundamental constituents of matter - is particularly striking in this context. At the same time, in all the studied texts, the sentence lengths - representing the distances between sentence-ending punctuation marks - reveal more freedom and are not constrained by the discrete Weibull distribution. This freedom in some cases translates into long-range nonlinear correlations, which manifest themselves in multifractality. Again, a text particularly spectacular in terms of multifractality is \"Finnegans Wake\".",
            "id": "2409.00483",
            "link": "http://arxiv.org/abs/2409.00483v1",
            "published": "2024-08-31T15:30:51+00:00",
            "updated": "2024-08-31T15:30:51+00:00",
            "primary_category": "physics.soc-ph",
            "categories": [
                "physics.soc-ph",
                "cs.CL",
                "stat.AP"
            ],
            "max_author_hindex": 36
        },
        "2409.01071": {
            "authors": [
                "Yuxuan Wang",
                "Cihang Xie",
                "Yang Liu",
                "Zilong Zheng"
            ],
            "title": "VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges",
            "abstract": "Recent advancements in large-scale video-language models have shown significant potential for real-time planning and detailed interactions. However, their high computational demands and the scarcity of annotated datasets limit their practicality for academic researchers. In this work, we introduce VideoLLaMB, a novel framework that utilizes temporal memory tokens within bridge layers to allow for the encoding of entire video sequences alongside historical visual data, effectively preserving semantic continuity and enhancing model performance across various tasks. This approach includes recurrent memory tokens and a SceneTilling algorithm, which segments videos into independent semantic units to preserve semantic integrity. Empirically, VideoLLaMB significantly outstrips existing video-language models, demonstrating a 5.5 points improvement over its competitors across three VideoQA benchmarks, and 2.06 points on egocentric planning. Comprehensive results on the MVBench show that VideoLLaMB-7B achieves markedly better results than previous 7B models of same LLM. Remarkably, it maintains robust performance as PLLaVA even as video length increases up to 8 times. Besides, the frame retrieval results on our specialized Needle in a Video Haystack (NIAVH) benchmark, further validate VideoLLaMB's prowess in accurately identifying specific frames within lengthy videos. Our SceneTilling algorithm also enables the generation of streaming video captions directly, without necessitating additional training. In terms of efficiency, VideoLLaMB, trained on 16 frames, supports up to 320 frames on a single Nvidia A100 GPU with linear GPU memory scaling, ensuring both high performance and cost-effectiveness, thereby setting a new foundation for long-form video-language models in both academic and practical applications.",
            "id": "2409.01071",
            "link": "http://arxiv.org/abs/2409.01071v1",
            "published": "2024-09-02T08:52:58+00:00",
            "updated": "2024-09-02T08:52:58+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.CL"
            ],
            "max_author_hindex": 55
        },
        "2409.01483": {
            "authors": [
                "Soumajyoti Sarkar",
                "Leonard Lausen",
                "Volkan Cevher",
                "Sheng Zha",
                "Thomas Brox",
                "George Karypis"
            ],
            "title": "Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning",
            "abstract": "Sparse Mixture of Expert (SMoE) models have emerged as a scalable alternative to dense models in language modeling. These models use conditionally activated feedforward subnetworks in transformer blocks, allowing for a separation between total model parameters and per-example computation. However, large token-routed SMoE models face a significant challenge: during inference, the entire model must be used for a sequence or a batch, resulting in high latencies in a distributed setting that offsets the advantages of per-token sparse activation. Our research explores task-specific model pruning to inform decisions about designing SMoE architectures, mainly modulating the choice of expert counts in pretraining. We investigate whether such pruned models offer advantages over smaller SMoE models trained from scratch, when evaluating and comparing them individually on tasks. To that end, we introduce an adaptive task-aware pruning technique UNCURL to reduce the number of experts per MoE layer in an offline manner post-training. Our findings reveal a threshold pruning factor for the reduction that depends on the number of experts used in pretraining, above which, the reduction starts to degrade model performance. These insights contribute to our understanding of model design choices when pretraining with SMoE architectures, particularly useful when considering task-specific inference optimization for later stages.",
            "id": "2409.01483",
            "link": "http://arxiv.org/abs/2409.01483v1",
            "published": "2024-09-02T22:35:03+00:00",
            "updated": "2024-09-02T22:35:03+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 95
        },
        "2409.01628": {
            "authors": [
                "Riya Samanta",
                "Bidyut Saha",
                "Soumya K. Ghosh",
                "Sajal K. Das"
            ],
            "title": "CTG-KrEW: Generating Synthetic Structured Contextually Correlated Content by Conditional Tabular GAN with K-Means Clustering and Efficient Word Embedding",
            "abstract": "Conditional Tabular Generative Adversarial Networks (CTGAN) and their various derivatives are attractive for their ability to efficiently and flexibly create synthetic tabular data, showcasing strong performance and adaptability. However, there are certain critical limitations to such models. The first is their inability to preserve the semantic integrity of contextually correlated words or phrases. For instance, skillset in freelancer profiles is one such attribute where individual skills are semantically interconnected and indicative of specific domain interests or qualifications. The second challenge of traditional approaches is that, when applied to generate contextually correlated tabular content, besides generating semantically shallow content, they consume huge memory resources and CPU time during the training stage. To address these problems, we introduce a novel framework, CTGKrEW (Conditional Tabular GAN with KMeans Clustering and Word Embedding), which is adept at generating realistic synthetic tabular data where attributes are collections of semantically and contextually coherent words. CTGKrEW is trained and evaluated using a dataset from Upwork, a realworld freelancing platform. Comprehensive experiments were conducted to analyze the variability, contextual similarity, frequency distribution, and associativity of the generated data, along with testing the framework's system feasibility. CTGKrEW also takes around 99\\% less CPU time and 33\\% less memory footprints than the conventional approach. Furthermore, we developed KrEW, a web application to facilitate the generation of realistic data containing skill-related information. This application, available at https://riyasamanta.github.io/krew.html, is freely accessible to both the general public and the research community.",
            "id": "2409.01628",
            "link": "http://arxiv.org/abs/2409.01628v1",
            "published": "2024-09-03T05:53:57+00:00",
            "updated": "2024-09-03T05:53:57+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 88
        },
        "2409.01690": {
            "authors": [
                "Ada-Astrid Balauca",
                "Danda Pani Paudel",
                "Kristina Toutanova",
                "Luc Van Gool"
            ],
            "title": "Taming CLIP for Fine-grained and Structured Visual Understanding of Museum Exhibits",
            "abstract": "CLIP is a powerful and widely used tool for understanding images in the context of natural language descriptions to perform nuanced tasks. However, it does not offer application-specific fine-grained and structured understanding, due to its generic nature. In this work, we aim to adapt CLIP for fine-grained and structured -- in the form of tabular data -- visual understanding of museum exhibits. To facilitate such understanding we (a) collect, curate, and benchmark a dataset of 200K+ image-table pairs, and (b) develop a method that allows predicting tabular outputs for input images. Our dataset is the first of its kind in the public domain. At the same time, the proposed method is novel in leveraging CLIP's powerful representations for fine-grained and tabular understanding. The proposed method (MUZE) learns to map CLIP's image embeddings to the tabular structure by means of a proposed transformer-based parsing network (parseNet). More specifically, parseNet enables prediction of missing attribute values while integrating context from known attribute-value pairs for an input image. We show that this leads to significant improvement in accuracy. Through exhaustive experiments, we show the effectiveness of the proposed method on fine-grained and structured understanding of museum exhibits, by achieving encouraging results in a newly established benchmark. Our dataset and source-code can be found at: https://github.com/insait-institute/MUZE",
            "id": "2409.01690",
            "link": "http://arxiv.org/abs/2409.01690v1",
            "published": "2024-09-03T08:13:06+00:00",
            "updated": "2024-09-03T08:13:06+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.CL"
            ],
            "max_author_hindex": 171
        },
        "2409.02228": {
            "authors": [
                "Eric Zhang",
                "Leshem Chosen",
                "Jacob Andreas"
            ],
            "title": "Unforgettable Generalization in Language Models",
            "abstract": "When language models (LMs) are trained to forget (or \"unlearn'') a skill, how precisely does their behavior change? We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels. Such LMs learn to generate near-random predictions for individual examples in the \"training'' set used for forgetting. Across tasks, however, LMs exhibit extreme variability in whether LM predictions change on examples outside the training set. In some tasks (like entailment classification), forgetting generalizes robustly, and causes models to produce uninformative predictions on new task instances; in other tasks (like physical commonsense reasoning and scientific question answering) forgetting affects only the training examples, and models continue to perform the \"forgotten'' task accurately even for examples very similar to those that appeared in the training set. Dataset difficulty is not predictive of whether a behavior can be forgotten; instead, generalization in forgetting is (weakly) predicted by the confidence of LMs' initial task predictions and the variability of LM representations of training data, with low confidence and low variability both associated with greater generalization. Perhaps most surprisingly, random-label forgetting appears to be somewhat insensitive to the contents of the training set: for example, models trained on science questions with random labels continue to answer other science questions accurately, but begin to produce random labels on entailment classification tasks. Finally, we show that even generalizable forgetting is shallow: linear probes trained on LMs' representations can still perform tasks reliably after forgetting. Our results highlight the difficulty and unpredictability of performing targeted skill removal from models via fine-tuning.",
            "id": "2409.02228",
            "link": "http://arxiv.org/abs/2409.02228v1",
            "published": "2024-09-03T18:55:54+00:00",
            "updated": "2024-09-03T18:55:54+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 10
        },
        "2409.02244": {
            "authors": [
                "Zainab Iftikhar",
                "Sean Ransom",
                "Amy Xiao",
                "Jeff Huang"
            ],
            "title": "Therapy as an NLP Task: Psychologists' Comparison of LLMs and Human Peers in CBT",
            "abstract": "Wider access to therapeutic care is one of the biggest challenges in mental health treatment. Due to institutional barriers, some people seeking mental health support have turned to large language models (LLMs) for personalized therapy, even though these models are largely unsanctioned and untested. We investigate the potential and limitations of using LLMs as providers of evidence-based therapy by using mixed methods clinical metrics. Using HELPERT, a prompt run on a large language model using the same process and training as a comparative group of peer counselors, we replicated publicly accessible mental health conversations rooted in Cognitive Behavioral Therapy (CBT) to compare session dynamics and counselor's CBT-based behaviors between original peer support sessions and their reconstructed HELPERT sessions. Two licensed, CBT-trained clinical psychologists evaluated the sessions using the Cognitive Therapy Rating Scale and provided qualitative feedback. Our findings show that the peer sessions are characterized by empathy, small talk, therapeutic alliance, and shared experiences but often exhibit therapist drift. Conversely, HELPERT reconstructed sessions exhibit minimal therapist drift and higher adherence to CBT methods but display a lack of collaboration, empathy, and cultural understanding. Through CTRS ratings and psychologists' feedback, we highlight the importance of human-AI collaboration for scalable mental health. Our work outlines the ethical implication of imparting human-like subjective qualities to LLMs in therapeutic settings, particularly the risk of deceptive empathy, which may lead to unrealistic patient expectations and potential harm.",
            "id": "2409.02244",
            "link": "http://arxiv.org/abs/2409.02244v1",
            "published": "2024-09-03T19:19:13+00:00",
            "updated": "2024-09-03T19:19:13+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.CL",
                "I.2.7; J.4"
            ],
            "max_author_hindex": 17
        },
        "2409.02474": {
            "authors": [
                "Merve Astekin",
                "Max Hort",
                "Leon Moonen"
            ],
            "title": "A Comparative Study on Large Language Models for Log Parsing",
            "abstract": "Background: Log messages provide valuable information about the status of software systems. This information is provided in an unstructured fashion and automated approaches are applied to extract relevant parameters. To ease this process, log parsing can be applied, which transforms log messages into structured log templates. Recent advances in language models have led to several studies that apply ChatGPT to the task of log parsing with promising results. However, the performance of other state-of-the-art large language models (LLMs) on the log parsing task remains unclear.   Aims: In this study, we investigate the current capability of state-of-the-art LLMs to perform log parsing.   Method: We select six recent LLMs, including both paid proprietary (GPT-3.5, Claude 2.1) and four free-to-use open models, and compare their performance on system logs obtained from a selection of mature open-source projects. We design two different prompting approaches and apply the LLMs on 1, 354 log templates across 16 different projects. We evaluate their effectiveness, in the number of correctly identified templates, and the syntactic similarity between the generated templates and the ground truth.   Results: We found that free-to-use models are able to compete with paid models, with CodeLlama extracting 10% more log templates correctly than GPT-3.5. Moreover, we provide qualitative insights into the usability of language models (e.g., how easy it is to use their responses).   Conclusions: Our results reveal that some of the smaller, free-to-use LLMs can considerably assist log parsing compared to their paid proprietary competitors, especially code-specialized models.",
            "id": "2409.02474",
            "link": "http://arxiv.org/abs/2409.02474v1",
            "published": "2024-09-04T06:46:31+00:00",
            "updated": "2024-09-04T06:46:31+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.CL"
            ],
            "max_author_hindex": 35
        },
        "2409.02645": {
            "authors": [
                "Jannik Peters",
                "Constantin Waubert de Puiseau",
                "Hasan Tercan",
                "Arya Gopikrishnan",
                "Gustavo Adolpho Lucas De Carvalho",
                "Christian Bitter",
                "Tobias Meisen"
            ],
            "title": "A Survey on Emergent Language",
            "abstract": "The field of emergent language represents a novel area of research within the domain of artificial intelligence, particularly within the context of multi-agent reinforcement learning. Although the concept of studying language emergence is not new, early approaches were primarily concerned with explaining human language formation, with little consideration given to its potential utility for artificial agents. In contrast, studies based on reinforcement learning aim to develop communicative capabilities in agents that are comparable to or even superior to human language. Thus, they extend beyond the learned statistical representations that are common in natural language processing research. This gives rise to a number of fundamental questions, from the prerequisites for language emergence to the criteria for measuring its success. This paper addresses these questions by providing a comprehensive review of 181 scientific publications on emergent language in artificial intelligence. Its objective is to serve as a reference for researchers interested in or proficient in the field. Consequently, the main contributions are the definition and overview of the prevailing terminology, the analysis of existing evaluation methods and metrics, and the description of the identified research gaps.",
            "id": "2409.02645",
            "link": "http://arxiv.org/abs/2409.02645v1",
            "published": "2024-09-04T12:22:05+00:00",
            "updated": "2024-09-04T12:22:05+00:00",
            "primary_category": "cs.MA",
            "categories": [
                "cs.MA",
                "cs.CL"
            ],
            "max_author_hindex": 20
        },
        "2409.03643": {
            "authors": [
                "Bin Wang",
                "Fan Wu",
                "Linke Ouyang",
                "Zhuangcheng Gu",
                "Rui Zhang",
                "Renqiu Xia",
                "Bo Zhang",
                "Conghui He"
            ],
            "title": "CDM: A Reliable Metric for Fair and Accurate Formula Recognition Evaluation",
            "abstract": "Formula recognition presents significant challenges due to the complicated structure and varied notation of mathematical expressions. Despite continuous advancements in formula recognition models, the evaluation metrics employed by these models, such as BLEU and Edit Distance, still exhibit notable limitations. They overlook the fact that the same formula has diverse representations and is highly sensitive to the distribution of training data, thereby causing the unfairness in formula recognition evaluation. To this end, we propose a Character Detection Matching (CDM) metric, ensuring the evaluation objectivity by designing a image-level rather than LaTex-level metric score. Specifically, CDM renders both the model-predicted LaTeX and the ground-truth LaTeX formulas into image-formatted formulas, then employs visual feature extraction and localization techniques for precise character-level matching, incorporating spatial position information. Such a spatially-aware and character-matching method offers a more accurate and equitable evaluation compared with previous BLEU and Edit Distance metrics that rely solely on text-based character matching. Experimentally, we evaluated various formula recognition models using CDM, BLEU, and ExpRate metrics. Their results demonstrate that the CDM aligns more closely with human evaluation standards and provides a fairer comparison across different models by eliminating discrepancies caused by diverse formula representations.",
            "id": "2409.03643",
            "link": "http://arxiv.org/abs/2409.03643v1",
            "published": "2024-09-05T16:01:21+00:00",
            "updated": "2024-09-05T16:01:21+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.CL"
            ],
            "max_author_hindex": 32
        },
        "2409.03650": {
            "authors": [
                "Yong Lin",
                "Skyler Seto",
                "Maartje ter Hoeve",
                "Katherine Metcalf",
                "Barry-John Theobald",
                "Xuan Wang",
                "Yizhe Zhang",
                "Chen Huang",
                "Tong Zhang"
            ],
            "title": "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization",
            "abstract": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach for aligning language models to human preferences. Central to RLHF is learning a reward function for scoring human preferences. Two main approaches for learning a reward model are 1) training an EXplicit Reward Model (EXRM) as in RLHF, and 2) using an implicit reward learned from preference data through methods such as Direct Preference Optimization (DPO). Prior work has shown that the implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in the limit. DPORM's effectiveness directly implies the optimality of the learned policy, and also has practical implication for LLM alignment methods including iterative DPO. However, it is unclear how well DPORM empirically matches the performance of EXRM. This work studies the accuracy at distinguishing preferred and rejected answers for both DPORM and EXRM. Our findings indicate that even though DPORM fits the training dataset comparably, it generalizes less effectively than EXRM, especially when the validation datasets contain distribution shifts. Across five out-of-distribution settings, DPORM has a mean drop in accuracy of 3% and a maximum drop of 7%. These findings highlight that DPORM has limited generalization ability and substantiates the integration of an explicit reward model in iterative DPO approaches.",
            "id": "2409.03650",
            "link": "http://arxiv.org/abs/2409.03650v1",
            "published": "2024-09-05T16:08:19+00:00",
            "updated": "2024-09-05T16:08:19+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "max_author_hindex": 21
        },
        "2409.04085": {
            "authors": [
                "Diletta Goglia",
                "Davide Vega"
            ],
            "title": "Structure and dynamics of growing networks of Reddit threads",
            "abstract": "Millions of people use online social networks to reinforce their sense of belonging, for example by giving and asking for feedback as a form of social validation and self-recognition. It is common to observe disagreement among people beliefs and points of view when expressing this feedback. Modeling and analyzing such interactions is crucial to understand social phenomena that happen when people face different opinions while expressing and discussing their values. In this work, we study a Reddit community in which people participate to judge or be judged with respect to some behavior, as it represents a valuable source to study how users express judgments online. We model threads of this community as complex networks of user interactions growing in time, and we analyze the evolution of their structural properties. We show that the evolution of Reddit networks differ from other real social networks, despite falling in the same category. This happens because their global clustering coefficient is extremely small and the average shortest path length increases over time. Such properties reveal how users discuss in threads, i.e. with mostly one other user and often by a single message. We strengthen such result by analyzing the role that disagreement and reciprocity play in such conversations. We also show that Reddit thread's evolution over time is governed by two subgraphs growing at different speeds. We discover that, in the studied community, the difference of such speed is higher than in other communities because of the user guidelines enforcing specific user interactions. Finally, we interpret the obtained results on user behavior drawing back to Social Judgment Theory.",
            "id": "2409.04085",
            "link": "http://arxiv.org/abs/2409.04085v1",
            "published": "2024-09-06T07:53:33+00:00",
            "updated": "2024-09-06T07:53:33+00:00",
            "primary_category": "cs.SI",
            "categories": [
                "cs.SI",
                "cs.CL",
                "cs.CY"
            ],
            "max_author_hindex": 11
        },
        "2409.04992": {
            "authors": [
                "Xiurui Pan",
                "Endian Li",
                "Qiao Li",
                "Shengwen Liang",
                "Yizhou Shan",
                "Ke Zhou",
                "Yingwei Luo",
                "Xiaolin Wang",
                "Jie Zhang"
            ],
            "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference",
            "abstract": "The widespread of Large Language Models (LLMs) marks a significant milestone in generative AI. Nevertheless, the increasing context length and batch size in offline LLM inference escalate the memory requirement of the key-value (KV) cache, which imposes a huge burden on the GPU VRAM, especially for resource-constraint scenarios (e.g., edge computing and personal devices). Several cost-effective solutions leverage host memory or SSDs to reduce storage costs for offline inference scenarios and improve the throughput. Nevertheless, they suffer from significant performance penalties imposed by intensive KV cache accesses due to limited PCIe bandwidth. To address these issues, we propose InstInfer, a novel LLM inference system that offloads the most performance-critical computation (i.e., attention in decoding phase) and data (i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize the enormous KV transfer overheads. InstInfer designs a dedicated flash-aware in-storage attention engine with KV cache management mechanisms to exploit the high internal bandwidths of CSDs instead of being limited by the PCIe bandwidth. The optimized P2P transmission between GPU and CSDs further reduces data migration overheads. Experimental results demonstrate that for a 13B model using an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence inference by up to 11.1$\\times$, compared to existing SSD-based solutions such as FlexGen.",
            "id": "2409.04992",
            "link": "http://arxiv.org/abs/2409.04992v1",
            "published": "2024-09-08T06:06:44+00:00",
            "updated": "2024-09-08T06:06:44+00:00",
            "primary_category": "cs.AR",
            "categories": [
                "cs.AR",
                "cs.CL"
            ],
            "max_author_hindex": 36
        },
        "2409.05028": {
            "authors": [
                "Yakun Zhang",
                "Chen Liu",
                "Xiaofei Xie",
                "Yun Lin",
                "Jin Song Dong",
                "Dan Hao",
                "Lu Zhang"
            ],
            "title": "LLM-based Abstraction and Concretization for GUI Test Migration",
            "abstract": "GUI test migration aims to produce test cases with events and assertions to test specific functionalities of a target app. Existing migration approaches typically focus on the widget-mapping paradigm that maps widgets from source apps to target apps. However, since different apps may implement the same functionality in different ways, direct mapping may result in incomplete or buggy test cases, thus significantly impacting the effectiveness of testing target functionality and the practical applicability.   In this paper, we propose a new migration paradigm (i.e., abstraction-concretization paradigm) that first abstracts the test logic for the target functionality and then utilizes this logic to generate the concrete GUI test case. Furthermore, we introduce MACdroid, the first approach that migrates GUI test cases based on this paradigm. Specifically, we propose an abstraction technique that utilizes source test cases from source apps targeting the same functionality to extract a general test logic for that functionality. Then, we propose a concretization technique that utilizes the general test logic to guide an LLM in generating the corresponding GUI test case (including events and assertions) for the target app. We evaluate MACdroid on two widely-used datasets (including 31 apps, 34 functionalities, and 123 test cases). On the FrUITeR dataset, the test cases generated by MACdroid successfully test 64% of the target functionalities, improving the baselines by 191%. On the Lin dataset, MACdroid successfully tests 75% of the target functionalities, outperforming the baselines by 42%. These results underscore the effectiveness of MACdroid in GUI test migration.",
            "id": "2409.05028",
            "link": "http://arxiv.org/abs/2409.05028v1",
            "published": "2024-09-08T08:46:05+00:00",
            "updated": "2024-09-08T08:46:05+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.CL"
            ],
            "max_author_hindex": 35
        },
        "2409.05148": {
            "authors": [
                "Elena Ortega-Beltr\u00e1n",
                "Josep Cabacas-Maso",
                "Ismael Benito-Altamirano",
                "Carles Ventura"
            ],
            "title": "Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis",
            "abstract": "Within the context of creating new Socially Assistive Robots, emotion recognition has become a key development factor, as it allows the robot to adapt to the user's emotional state in the wild. In this work, we focused on the analysis of two voice recording Spanish datasets: ELRA-S0329 and EmoMatchSpanishDB. Specifically, we centered our work in the paralanguage, e.~g. the vocal characteristics that go along with the message and clarifies the meaning. We proposed the use of the DeepSpectrum method, which consists of extracting a visual representation of the audio tracks and feeding them to a pretrained CNN model. For the classification task, DeepSpectrum is often paired with a Support Vector Classifier --DS-SVC--, or a Fully-Connected deep-learning classifier --DS-FC--. We compared the results of the DS-SVC and DS-FC architectures with the state-of-the-art (SOTA) for ELRA-S0329 and EmoMatchSpanishDB. Moreover, we proposed our own classifier based upon Attention Mechanisms, namely DS-AM. We trained all models against both datasets, and we found that our DS-AM model outperforms the SOTA models for the datasets and the SOTA DeepSpectrum architectures. Finally, we trained our DS-AM model in one dataset and tested it in the other, to simulate real-world conditions on how biased is the model to the dataset.",
            "id": "2409.05148",
            "link": "http://arxiv.org/abs/2409.05148v1",
            "published": "2024-09-08T16:25:38+00:00",
            "updated": "2024-09-08T16:25:38+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.CL",
                "cs.CV",
                "eess.AS"
            ],
            "max_author_hindex": 4
        },
        "2409.00316": {
            "authors": [
                "Guang Yang",
                "Muru Zhang",
                "Lin Qiu",
                "Yanming Wan",
                "Noah A. Smith"
            ],
            "title": "Toward a More Complete OMR Solution",
            "abstract": "Optical music recognition (OMR) aims to convert music notation into digital formats. One approach to tackle OMR is through a multi-stage pipeline, where the system first detects visual music notation elements in the image (object detection) and then assembles them into a music notation (notation assembly). Most previous work on notation assembly unrealistically assumes perfect object detection. In this study, we focus on the MUSCIMA++ v2.0 dataset, which represents musical notation as a graph with pairwise relationships among detected music objects, and we consider both stages together. First, we introduce a music object detector based on YOLOv8, which improves detection performance. Second, we introduce a supervised training pipeline that completes the notation assembly stage based on detection output. We find that this model is able to outperform existing models trained on perfect detection output, showing the benefit of considering the detection and assembly stages in a more holistic way. These findings, together with our novel evaluation metric, are important steps toward a more complete OMR solution.",
            "id": "2409.00316",
            "link": "http://arxiv.org/abs/2409.00316v1",
            "published": "2024-08-31T01:09:12+00:00",
            "updated": "2024-08-31T01:09:12+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 107
        },
        "2409.00327": {
            "authors": [
                "Jiaxiang Geng",
                "Beilong Tang",
                "Boyan Zhang",
                "Jiaqi Shao",
                "Bing Luo"
            ],
            "title": "Demo: FedCampus: A Real-world Privacy-preserving Mobile Application for Smart Campus via Federated Learning & Analytics",
            "abstract": "In this demo, we introduce FedCampus, a privacy-preserving mobile application for smart \\underline{campus} with \\underline{fed}erated learning (FL) and federated analytics (FA). FedCampus enables cross-platform on-device FL/FA for both iOS and Android, supporting continuously models and algorithms deployment (MLOps). Our app integrates privacy-preserving processed data via differential privacy (DP) from smartwatches, where the processed parameters are used for FL/FA through the FedCampus backend platform. We distributed 100 smartwatches to volunteers at Duke Kunshan University and have successfully completed a series of smart campus tasks featuring capabilities such as sleep tracking, physical activity monitoring, personalized recommendations, and heavy hitters. Our project is opensourced at https://github.com/FedCampus/FedCampus_Flutter. See the FedCampus video at https://youtu.be/k5iu46IjA38.",
            "id": "2409.00327",
            "link": "http://arxiv.org/abs/2409.00327v1",
            "published": "2024-08-31T01:58:36+00:00",
            "updated": "2024-08-31T01:58:36+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.DC"
            ],
            "max_author_hindex": 7
        },
        "2409.00391": {
            "authors": [
                "Georgios Ioannides",
                "Adrian Kieback",
                "Aman Chadha",
                "Aaron Elkins"
            ],
            "title": "Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders",
            "abstract": "Speech-based depression detection poses significant challenges for automated detection due to its unique manifestation across individuals and data scarcity. Addressing these challenges, we introduce DAAMAudioCNNLSTM and DAAMAudioTransformer, two parameter efficient and explainable models for audio feature extraction and depression detection. DAAMAudioCNNLSTM features a novel CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM), focusing dynamically on informative speech segments. DAAMAudioTransformer, leveraging a transformer encoder in place of the CNN-LSTM architecture, incorporates the same DAAM module for enhanced attention and interpretability. These approaches not only enhance detection robustness and interpretability but also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the DAIC-WOZ dataset, without reliance on supplementary information such as vowel positions and speaker information during training/validation as in previous approaches. Both models' significant explainability and efficiency in leveraging speech signals for depression detection represent a leap towards more reliable, clinically useful diagnostic tools, promising advancements in speech and mental health care. To foster further research in this domain, we make our code publicly available.",
            "id": "2409.00391",
            "link": "http://arxiv.org/abs/2409.00391v1",
            "published": "2024-08-31T08:50:28+00:00",
            "updated": "2024-08-31T08:50:28+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 16
        },
        "2409.00418": {
            "authors": [
                "Kosuke Nakanishi",
                "Akihiro Kubo",
                "Yuji Yasui",
                "Shin Ishii"
            ],
            "title": "Robust off-policy Reinforcement Learning via Soft Constrained Adversary",
            "abstract": "Recently, robust reinforcement learning (RL) methods against input observation have garnered significant attention and undergone rapid evolution due to RL's potential vulnerability. Although these advanced methods have achieved reasonable success, there have been two limitations when considering adversary in terms of long-term horizons. First, the mutual dependency between the policy and its corresponding optimal adversary limits the development of off-policy RL algorithms; although obtaining optimal adversary should depend on the current policy, this has restricted applications to off-policy RL. Second, these methods generally assume perturbations based only on the $L_p$-norm, even when prior knowledge of the perturbation distribution in the environment is available. We here introduce another perspective on adversarial RL: an f-divergence constrained problem with the prior knowledge distribution. From this, we derive two typical attacks and their corresponding robust learning frameworks. The evaluation of robustness is conducted and the results demonstrate that our proposed methods achieve excellent performance in sample-efficient off-policy RL.",
            "id": "2409.00418",
            "link": "http://arxiv.org/abs/2409.00418v1",
            "published": "2024-08-31T11:13:33+00:00",
            "updated": "2024-08-31T11:13:33+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 35
        },
        "2409.00488": {
            "authors": [
                "Yair Stolero",
                "Itzik Klein"
            ],
            "title": "Rapid Gyroscope Calibration: A Deep Learning Approach",
            "abstract": "Low-cost gyroscope calibration is essential for ensuring the accuracy and reliability of gyroscope measurements. Stationary calibration estimates the deterministic parts of measurement errors. To this end, a common practice is to average the gyroscope readings during a predefined period and estimate the gyroscope bias. Calibration duration plays a crucial role in performance, therefore, longer periods are preferred. However, some applications require quick startup times and calibration is therefore allowed only for a short time. In this work, we focus on reducing low-cost gyroscope calibration time using deep learning methods. We propose a deep-learning framework and explore the possibilities of using multiple real and virtual gyroscopes to improve the calibration performance of single gyroscopes. To train and validate our approach, we recorded a dataset consisting of 169 hours of gyroscope readings, using 24 gyroscopes of two different brands. We also created a virtual dataset consisting of simulated gyroscope readings. The two datasets were used to evaluate our proposed approach. One of our key achievements in this work is reducing gyroscope calibration time by up to 89% using three low-cost gyroscopes.",
            "id": "2409.00488",
            "link": "http://arxiv.org/abs/2409.00488v1",
            "published": "2024-08-31T15:47:31+00:00",
            "updated": "2024-08-31T15:47:31+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.RO",
                "eess.SP"
            ],
            "max_author_hindex": 17
        },
        "2409.00489": {
            "authors": [
                "Chia-Yu Hsu",
                "Wenwen Li",
                "Sizhe Wang"
            ],
            "title": "Geospatial foundation models for image analysis: evaluating and enhancing NASA-IBM Prithvi's domain adaptability",
            "abstract": "Research on geospatial foundation models (GFMs) has become a trending topic in geospatial artificial intelligence (AI) research due to their potential for achieving high generalizability and domain adaptability, reducing model training costs for individual researchers. Unlike large language models, such as ChatGPT, constructing visual foundation models for image analysis, particularly in remote sensing, encountered significant challenges such as formulating diverse vision tasks into a general problem framework. This paper evaluates the recently released NASA-IBM GFM Prithvi for its predictive performance on high-level image analysis tasks across multiple benchmark datasets. Prithvi was selected because it is one of the first open-source GFMs trained on time-series of high-resolution remote sensing imagery. A series of experiments were designed to assess Prithvi's performance as compared to other pre-trained task-specific AI models in geospatial image analysis. New strategies, including band adaptation, multi-scale feature generation, and fine-tuning techniques, are introduced and integrated into an image analysis pipeline to enhance Prithvi's domain adaptation capability and improve model performance. In-depth analyses reveal Prithvi's strengths and weaknesses, offering insights for both improving Prithvi and developing future visual foundation models for geospatial tasks.",
            "id": "2409.00489",
            "link": "http://arxiv.org/abs/2409.00489v1",
            "published": "2024-08-31T15:51:23+00:00",
            "updated": "2024-08-31T15:51:23+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 28
        },
        "2409.00510": {
            "authors": [
                "Lemeng Zhao",
                "Junjie Hu",
                "Jianchao Bi",
                "Yanbing Bai",
                "Erick Mas",
                "Shunichi Koshimura"
            ],
            "title": "Streamlining Forest Wildfire Surveillance: AI-Enhanced UAVs Utilizing the FLAME Aerial Video Dataset for Lightweight and Efficient Monitoring",
            "abstract": "In recent years, unmanned aerial vehicles (UAVs) have played an increasingly crucial role in supporting disaster emergency response efforts by analyzing aerial images. While current deep-learning models focus on improving accuracy, they often overlook the limited computing resources of UAVs. This study recognizes the imperative for real-time data processing in disaster response scenarios and introduces a lightweight and efficient approach for aerial video understanding. Our methodology identifies redundant portions within the video through policy networks and eliminates this excess information using frame compression techniques. Additionally, we introduced the concept of a `station point,' which leverages future information in the sequential policy network, thereby enhancing accuracy. To validate our method, we employed the wildfire FLAME dataset. Compared to the baseline, our approach reduces computation costs by more than 13 times while boosting accuracy by 3$\\%$. Moreover, our method can intelligently select salient frames from the video, refining the dataset. This feature enables sophisticated models to be effectively trained on a smaller dataset, significantly reducing the time spent during the training process.",
            "id": "2409.00510",
            "link": "http://arxiv.org/abs/2409.00510v1",
            "published": "2024-08-31T17:26:53+00:00",
            "updated": "2024-08-31T17:26:53+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 39
        },
        "2409.00513": {
            "authors": [
                "Shivam Pande",
                "Baki Uzun",
                "Florent Guiotte",
                "Thomas Corpetti",
                "Florian Delerue",
                "S\u00e9bastien Lef\u00e8vre"
            ],
            "title": "Plant detection from ultra high resolution remote sensing images: A Semantic Segmentation approach based on fuzzy loss",
            "abstract": "In this study, we tackle the challenge of identifying plant species from ultra high resolution (UHR) remote sensing images. Our approach involves introducing an RGB remote sensing dataset, characterized by millimeter-level spatial resolution, meticulously curated through several field expeditions across a mountainous region in France covering various landscapes. The task of plant species identification is framed as a semantic segmentation problem for its practical and efficient implementation across vast geographical areas. However, when dealing with segmentation masks, we confront instances where distinguishing boundaries between plant species and their background is challenging. We tackle this issue by introducing a fuzzy loss within the segmentation model. Instead of utilizing one-hot encoded ground truth (GT), our model incorporates Gaussian filter refined GT, introducing stochasticity during training. First experimental results obtained on both our UHR dataset and a public dataset are presented, showing the relevance of the proposed methodology, as well as the need for future improvement.",
            "id": "2409.00513",
            "link": "http://arxiv.org/abs/2409.00513v1",
            "published": "2024-08-31T17:40:17+00:00",
            "updated": "2024-08-31T17:40:17+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.00518": {
            "authors": [
                "Baki Uzun",
                "Shivam Pande",
                "Gwendal Cachin-Bernard",
                "Minh-Tan Pham",
                "S\u00e9bastien Lef\u00e8vre",
                "Rumais Blatrix",
                "Doyle McKey"
            ],
            "title": "Mapping earth mounds from space",
            "abstract": "Regular patterns of vegetation are considered widespread landscapes, although their global extent has never been estimated. Among them, spotted landscapes are of particular interest in the context of climate change. Indeed, regularly spaced vegetation spots in semi-arid shrublands result from extreme resource depletion and prefigure catastrophic shift of the ecosystem to a homogeneous desert, while termite mounds also producing spotted landscapes were shown to increase robustness to climate change. Yet, their identification at large scale calls for automatic methods, for instance using the popular deep learning framework, able to cope with a vast amount of remote sensing data, e.g., optical satellite imagery. In this paper, we tackle this problem and benchmark some state-of-the-art deep networks on several landscapes and geographical areas. Despite the promising results we obtained, we found that more research is needed to be able to map automatically these earth mounds from space.",
            "id": "2409.00518",
            "link": "http://arxiv.org/abs/2409.00518v1",
            "published": "2024-08-31T18:08:37+00:00",
            "updated": "2024-08-31T18:08:37+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 56
        },
        "2409.00553": {
            "authors": [
                "Gang Li",
                "Qihang Lin",
                "Ayush Ghosh",
                "Tianbao Yang"
            ],
            "title": "Multi-Output Distributional Fairness via Post-Processing",
            "abstract": "The post-processing approaches are becoming prominent techniques to enhance machine learning models' fairness because of their intuitiveness, low computational cost, and excellent scalability. However, most existing post-processing methods are designed for task-specific fairness measures and are limited to single-output models. In this paper, we introduce a post-processing method for multi-output models, such as the ones used for multi-task/multi-class classification and representation learning, to enhance a model's distributional parity, a task-agnostic fairness measure. Existing techniques to achieve distributional parity are based on the (inverse) cumulative density function of a model's output, which is limited to single-output models. Extending previous works, our method employs an optimal transport mapping to move a model's outputs across different groups towards their empirical Wasserstein barycenter. An approximation technique is applied to reduce the complexity of computing the exact barycenter and a kernel regression method is proposed for extending this process to out-of-sample data. Our empirical studies, which compare our method to current existing post-processing baselines on multi-task/multi-class classification and representation learning tasks, demonstrate the effectiveness of the proposed approach.",
            "id": "2409.00553",
            "link": "http://arxiv.org/abs/2409.00553v1",
            "published": "2024-08-31T22:41:26+00:00",
            "updated": "2024-08-31T22:41:26+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CY",
                "stat.ML"
            ],
            "max_author_hindex": 46
        },
        "2409.00564": {
            "authors": [
                "P. Curvo",
                "D. R. Ferreira",
                "R. Jorge"
            ],
            "title": "Using Deep Learning to Design High Aspect Ratio Fusion Devices",
            "abstract": "The design of fusion devices is typically based on computationally expensive simulations. This can be alleviated using high aspect ratio models that employ a reduced number of free parameters, especially in the case of stellarator optimization where non-axisymmetric magnetic fields with a large parameter space are optimized to satisfy certain performance criteria. However, optimization is still required to find configurations with properties such as low elongation, high rotational transform, finite plasma beta, and good fast particle confinement. In this work, we train a machine learning model to construct configurations with favorable confinement properties by finding a solution to the inverse design problem, that is, obtaining a set of model input parameters for given desired properties. Since the solution of the inverse problem is non-unique, a probabilistic approach, based on mixture density networks, is used. It is shown that optimized configurations can be generated reliably using this method.",
            "id": "2409.00564",
            "link": "http://arxiv.org/abs/2409.00564v1",
            "published": "2024-08-31T23:28:10+00:00",
            "updated": "2024-08-31T23:28:10+00:00",
            "primary_category": "physics.plasm-ph",
            "categories": [
                "physics.plasm-ph",
                "cs.AI"
            ],
            "max_author_hindex": 49
        },
        "2409.00584": {
            "authors": [
                "Jiantong Jiang",
                "Ajmal Mian"
            ],
            "title": "FastBO: Fast HPO and NAS with Adaptive Fidelity Identification",
            "abstract": "Hyperparameter optimization (HPO) and neural architecture search (NAS) are powerful in attaining state-of-the-art machine learning models, with Bayesian optimization (BO) standing out as a mainstream method. Extending BO into the multi-fidelity setting has been an emerging research topic, but faces the challenge of determining an appropriate fidelity for each hyperparameter configuration to fit the surrogate model. To tackle the challenge, we propose a multi-fidelity BO method named FastBO, which adaptively decides the fidelity for each configuration and efficiently offers strong performance. The advantages are achieved based on the novel concepts of efficient point and saturation point for each configuration.We also show that our adaptive fidelity identification strategy provides a way to extend any single-fidelity method to the multi-fidelity setting, highlighting its generality and applicability.",
            "id": "2409.00584",
            "link": "http://arxiv.org/abs/2409.00584v1",
            "published": "2024-09-01T02:40:04+00:00",
            "updated": "2024-09-01T02:40:04+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 60
        },
        "2409.00592": {
            "authors": [
                "Fenglei Fan",
                "Juntong Fan",
                "Dayang Wang",
                "Jingbo Zhang",
                "Zelin Dong",
                "Shijun Zhang",
                "Ge Wang",
                "Tieyong Zeng"
            ],
            "title": "Hyper-Compression: Model Compression via Hyperfunction",
            "abstract": "The rapid growth of large models' size has far outpaced that of GPU memory. To bridge this gap, inspired by the succinct relationship between genotype and phenotype, we turn the model compression problem into the issue of parameter representation to propose the so-called hyper-compression. The hyper-compression uses a hyperfunction to represent the parameters of the target network, and notably, here the hyperfunction is designed per ergodic theory that relates to a problem: if a low-dimensional dynamic system can fill the high-dimensional space eventually. Empirically, the proposed hyper-compression enjoys the following merits: 1) \\textbf{P}referable compression ratio; 2) \\textbf{N}o post-hoc retraining; 3) \\textbf{A}ffordable inference time; and 4) \\textbf{S}hort compression time. It compresses LLaMA2-7B in an hour and achieves close-to-int4-quantization performance, without retraining and with a performance drop of less than 1\\%. Our work has the potential to invigorate the field of model compression, towards a harmony between the scaling law and the stagnation of hardware upgradation.",
            "id": "2409.00592",
            "link": "http://arxiv.org/abs/2409.00592v1",
            "published": "2024-09-01T02:57:41+00:00",
            "updated": "2024-09-01T02:57:41+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.ET"
            ],
            "max_author_hindex": 37
        },
        "2409.00622": {
            "authors": [
                "Manthan Chelenahalli Satish",
                "Duo Lu",
                "Bharatesh Chakravarthi",
                "Mohammad Farhadi",
                "Yezhou Yang"
            ],
            "title": "Roundabout Dilemma Zone Data Mining and Forecasting with Trajectory Prediction and Graph Neural Networks",
            "abstract": "Traffic roundabouts, as complex and critical road scenarios, pose significant safety challenges for autonomous vehicles. In particular, the encounter of a vehicle with a dilemma zone (DZ) at a roundabout intersection is a pivotal concern. This paper presents an automated system that leverages trajectory forecasting to predict DZ events, specifically at traffic roundabouts. Our system aims to enhance safety standards in both autonomous and manual transportation. The core of our approach is a modular, graph-structured recurrent model that forecasts the trajectories of diverse agents, taking into account agent dynamics and integrating heterogeneous data, such as semantic maps. This model, based on graph neural networks, aids in predicting DZ events and enhances traffic management decision-making. We evaluated our system using a real-world dataset of traffic roundabout intersections. Our experimental results demonstrate that our dilemma forecasting system achieves a high precision with a low false positive rate of 0.1. This research represents an advancement in roundabout DZ data mining and forecasting, contributing to the assurance of intersection safety in the era of autonomous vehicles.",
            "id": "2409.00622",
            "link": "http://arxiv.org/abs/2409.00622v1",
            "published": "2024-09-01T05:47:58+00:00",
            "updated": "2024-09-01T05:47:58+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 57
        },
        "2409.00687": {
            "authors": [
                "Zhixiang Shen",
                "Zhao Kang"
            ],
            "title": "When Heterophily Meets Heterogeneous Graphs: Latent Graphs Guided Unsupervised Representation Learning",
            "abstract": "Unsupervised heterogeneous graph representation learning (UHGRL) has gained increasing attention due to its significance in handling practical graphs without labels. However, heterophily has been largely ignored, despite its ubiquitous presence in real-world heterogeneous graphs. In this paper, we define semantic heterophily and propose an innovative framework called Latent Graphs Guided Unsupervised Representation Learning (LatGRL) to handle this problem. First, we develop a similarity mining method that couples global structures and attributes, enabling the construction of fine-grained homophilic and heterophilic latent graphs to guide the representation learning. Moreover, we propose an adaptive dual-frequency semantic fusion mechanism to address the problem of node-level semantic heterophily. To cope with the massive scale of real-world data, we further design a scalable implementation. Extensive experiments on benchmark datasets validate the effectiveness and efficiency of our proposed framework. The source code and datasets have been made available at https://github.com/zxlearningdeep/LatGRL.",
            "id": "2409.00687",
            "link": "http://arxiv.org/abs/2409.00687v1",
            "published": "2024-09-01T10:25:06+00:00",
            "updated": "2024-09-01T10:25:06+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ],
            "max_author_hindex": 18
        },
        "2409.00695": {
            "authors": [
                "Xiuqi Zheng",
                "Yuhang Zhang",
                "Haoran Zhang",
                "Hongrui Liang",
                "Xueqi Bao",
                "Zhuqing Jiang",
                "Qicheng Lao"
            ],
            "title": "Curriculum Prompting Foundation Models for Medical Image Segmentation",
            "abstract": "Adapting large pre-trained foundation models, e.g., SAM, for medical image segmentation remains a significant challenge. A crucial step involves the formulation of a series of specialized prompts that incorporate specific clinical instructions. Past works have been heavily reliant on a singular type of prompt for each instance, necessitating manual input of an ideally correct prompt, which is less efficient. To tackle this issue, we propose to utilize prompts of different granularity, which are sourced from original images to provide a broader scope of clinical insights. However, combining prompts of varying types can pose a challenge due to potential conflicts. In response, we have designed a coarse-to-fine mechanism, referred to as curriculum prompting, that progressively integrates prompts of different types. Through extensive experiments on three public medical datasets across various modalities, we demonstrate the effectiveness of our proposed approach, which not only automates the prompt generation process but also yields superior performance compared to other SAM-based medical image segmentation methods. Code is available at: https://github.com/AnnaZzz-zxq/Curriculum-Prompting.",
            "id": "2409.00695",
            "link": "http://arxiv.org/abs/2409.00695v1",
            "published": "2024-09-01T11:00:18+00:00",
            "updated": "2024-09-01T11:00:18+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.00700": {
            "authors": [
                "Yan Rong",
                "Li Liu"
            ],
            "title": "Seeing Your Speech Style: A Novel Zero-Shot Identity-Disentanglement Face-based Voice Conversion",
            "abstract": "Face-based Voice Conversion (FVC) is a novel task that leverages facial images to generate the target speaker's voice style. Previous work has two shortcomings: (1) suffering from obtaining facial embeddings that are well-aligned with the speaker's voice identity information, and (2) inadequacy in decoupling content and speaker identity information from the audio input. To address these issues, we present a novel FVC method, Identity-Disentanglement Face-based Voice Conversion (ID-FaceVC), which overcomes the above two limitations. More precisely, we propose an Identity-Aware Query-based Contrastive Learning (IAQ-CL) module to extract speaker-specific facial features, and a Mutual Information-based Dual Decoupling (MIDD) module to purify content features from audio, ensuring clear and high-quality voice conversion. Besides, unlike prior works, our method can accept either audio or text inputs, offering controllable speech generation with adjustable emotional tone and speed. Extensive experiments demonstrate that ID-FaceVC achieves state-of-the-art performance across various metrics, with qualitative and user study results confirming its effectiveness in naturalness, similarity, and diversity. Project website with audio samples and code can be found at https://id-facevc.github.io.",
            "id": "2409.00700",
            "link": "http://arxiv.org/abs/2409.00700v1",
            "published": "2024-09-01T11:51:18+00:00",
            "updated": "2024-09-01T11:51:18+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.CV",
                "eess.AS"
            ],
            "max_author_hindex": 8
        },
        "2409.00707": {
            "authors": [
                "Aditya Chandrasekar",
                "Goirik Chakrabarty",
                "Jai Bardhan",
                "Ramya Hebbalaguppe",
                "Prathosh AP"
            ],
            "title": "ReMOVE: A Reference-free Metric for Object Erasure",
            "abstract": "We introduce $\\texttt{ReMOVE}$, a novel reference-free metric for assessing object erasure efficacy in diffusion-based image editing models post-generation. Unlike existing measures such as LPIPS and CLIPScore, $\\texttt{ReMOVE}$ addresses the challenge of evaluating inpainting without a reference image, common in practical scenarios. It effectively distinguishes between object removal and replacement. This is a key issue in diffusion models due to stochastic nature of image generation. Traditional metrics fail to align with the intuitive definition of inpainting, which aims for (1) seamless object removal within masked regions (2) while preserving the background continuity. $\\texttt{ReMOVE}$ not only correlates with state-of-the-art metrics and aligns with human perception but also captures the nuanced aspects of the inpainting process, providing a finer-grained evaluation of the generated outputs.",
            "id": "2409.00707",
            "link": "http://arxiv.org/abs/2409.00707v1",
            "published": "2024-09-01T12:26:14+00:00",
            "updated": "2024-09-01T12:26:14+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 13
        },
        "2409.00717": {
            "authors": [
                "Natalia Zhang",
                "Xinqi Wang",
                "Qiwen Cui",
                "Runlong Zhou",
                "Sham M. Kakade",
                "Simon S. Du"
            ],
            "title": "Multi-Agent Reinforcement Learning from Human Feedback: Data Coverage and Algorithmic Techniques",
            "abstract": "We initiate the study of Multi-Agent Reinforcement Learning from Human Feedback (MARLHF), exploring both theoretical foundations and empirical validations. We define the task as identifying Nash equilibrium from a preference-only offline dataset in general-sum games, a problem marked by the challenge of sparse feedback signals. Our theory establishes the upper complexity bounds for Nash Equilibrium in effective MARLHF, demonstrating that single-policy coverage is inadequate and highlighting the importance of unilateral dataset coverage. These theoretical insights are verified through comprehensive experiments. To enhance the practical performance, we further introduce two algorithmic techniques. (1) We propose a Mean Squared Error (MSE) regularization along the time axis to achieve a more uniform reward distribution and improve reward learning outcomes. (2) We utilize imitation learning to approximate the reference policy, ensuring stability and effectiveness in training. Our findings underscore the multifaceted approach required for MARLHF, paving the way for effective preference-based multi-agent systems.",
            "id": "2409.00717",
            "link": "http://arxiv.org/abs/2409.00717v2",
            "published": "2024-09-01T13:14:41+00:00",
            "updated": "2024-09-04T15:50:40+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.GT",
                "cs.MA"
            ],
            "max_author_hindex": 87
        },
        "2409.00742": {
            "authors": [
                "Gonzalo Bohorquez",
                "John Cartlidge"
            ],
            "title": "Simulation of Social Media-Driven Bubble Formation in Financial Markets using an Agent-Based Model with Hierarchical Influence Network",
            "abstract": "We propose that a tree-like hierarchical structure represents a simple and effective way to model the emergent behaviour of financial markets, especially markets where there exists a pronounced intersection between social media influences and investor behaviour. To explore this hypothesis, we introduce an agent-based model of financial markets, where trading agents are embedded in a hierarchical network of communities, and communities influence the strategies and opinions of traders. Empirical analysis of the model shows that its behaviour conforms to several stylized facts observed in real financial markets; and the model is able to realistically simulate the effects that social media-driven phenomena, such as echo chambers and pump-and-dump schemes, have on financial markets.",
            "id": "2409.00742",
            "link": "http://arxiv.org/abs/2409.00742v1",
            "published": "2024-09-01T15:09:35+00:00",
            "updated": "2024-09-01T15:09:35+00:00",
            "primary_category": "cs.MA",
            "categories": [
                "cs.MA",
                "cs.AI",
                "q-fin.TR",
                "I.2.11"
            ],
            "max_author_hindex": 14
        },
        "2409.00743": {
            "authors": [
                "Lianyu Hu",
                "Mudi Jiang",
                "Junjie Dong",
                "Xinying Liu",
                "Zengyou He"
            ],
            "title": "Interpretable Clustering: A Survey",
            "abstract": "In recent years, much of the research on clustering algorithms has primarily focused on enhancing their accuracy and efficiency, frequently at the expense of interpretability. However, as these methods are increasingly being applied in high-stakes domains such as healthcare, finance, and autonomous systems, the need for transparent and interpretable clustering outcomes has become a critical concern. This is not only necessary for gaining user trust but also for satisfying the growing ethical and regulatory demands in these fields. Ensuring that decisions derived from clustering algorithms can be clearly understood and justified is now a fundamental requirement. To address this need, this paper provides a comprehensive and structured review of the current state of explainable clustering algorithms, identifying key criteria to distinguish between various methods. These insights can effectively assist researchers in making informed decisions about the most suitable explainable clustering methods for specific application contexts, while also promoting the development and adoption of clustering algorithms that are both efficient and transparent.",
            "id": "2409.00743",
            "link": "http://arxiv.org/abs/2409.00743v1",
            "published": "2024-09-01T15:09:51+00:00",
            "updated": "2024-09-01T15:09:51+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 31
        },
        "2409.00815": {
            "authors": [
                "Hao Shi",
                "Yuan Gao",
                "Zhaoheng Ni",
                "Tatsuya Kawahara"
            ],
            "title": "Serialized Speech Information Guidance with Overlapped Encoding Separation for Multi-Speaker Automatic Speech Recognition",
            "abstract": "Serialized output training (SOT) attracts increasing attention due to its convenience and flexibility for multi-speaker automatic speech recognition (ASR). However, it is not easy to train with attention loss only. In this paper, we propose the overlapped encoding separation (EncSep) to fully utilize the benefits of the connectionist temporal classification (CTC) and attention hybrid loss. This additional separator is inserted after the encoder to extract the multi-speaker information with CTC losses. Furthermore, we propose the serialized speech information guidance SOT (GEncSep) to further utilize the separated encodings. The separated streams are concatenated to provide single-speaker information to guide attention during decoding. The experimental results on LibriMix show that the single-speaker encoding can be separated from the overlapped encoding. The CTC loss helps to improve the encoder representation under complex scenarios. GEncSep further improved performance.",
            "id": "2409.00815",
            "link": "http://arxiv.org/abs/2409.00815v3",
            "published": "2024-09-01T19:07:34+00:00",
            "updated": "2024-09-11T02:33:17+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 41
        },
        "2409.00899": {
            "authors": [
                "Yizhou Liu",
                "Pengfei Gao",
                "Xinchen Wang",
                "Jie Liu",
                "Yexuan Shi",
                "Zhao Zhang",
                "Chao Peng"
            ],
            "title": "MarsCode Agent: AI-native Automated Bug Fixing",
            "abstract": "Recent advances in large language models (LLMs) have shown significant potential to automate various software development tasks, including code completion, test generation, and bug fixing. However, the application of LLMs for automated bug fixing remains challenging due to the complexity and diversity of real-world software systems. In this paper, we introduce MarsCode Agent, a novel framework that leverages LLMs to automatically identify and repair bugs in software code. MarsCode Agent combines the power of LLMs with advanced code analysis techniques to accurately localize faults and generate patches. Our approach follows a systematic process of planning, bug reproduction, fault localization, candidate patch generation, and validation to ensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a comprehensive benchmark of real-world software projects, and our results show that MarsCode Agent achieves a high success rate in bug fixing compared to most of the existing automated approaches.",
            "id": "2409.00899",
            "link": "http://arxiv.org/abs/2409.00899v2",
            "published": "2024-09-02T02:24:38+00:00",
            "updated": "2024-09-04T06:19:08+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 49
        },
        "2409.00909": {
            "authors": [
                "Chao Gu",
                "Ke Lin",
                "Yiyang Luo",
                "Jiahui Hou",
                "Xiang-Yang Li"
            ],
            "title": "ViRED: Prediction of Visual Relations in Engineering Drawings",
            "abstract": "To accurately understand engineering drawings, it is essential to establish the correspondence between images and their description tables within the drawings. Existing document understanding methods predominantly focus on text as the main modality, which is not suitable for documents containing substantial image information. In the field of visual relation detection, the structure of the task inherently limits its capacity to assess relationships among all entity pairs in the drawings. To address this issue, we propose a vision-based relation detection model, named ViRED, to identify the associations between tables and circuits in electrical engineering drawings. Our model mainly consists of three parts: a vision encoder, an object encoder, and a relation decoder. We implement ViRED using PyTorch to evaluate its performance. To validate the efficacy of ViRED, we conduct a series of experiments. The experimental results indicate that, within the engineering drawing dataset, our approach attained an accuracy of 96\\% in the task of relation prediction, marking a substantial improvement over existing methodologies. The results also show that ViRED can inference at a fast speed even when there are numerous objects in a single engineering drawing.",
            "id": "2409.00909",
            "link": "http://arxiv.org/abs/2409.00909v1",
            "published": "2024-09-02T02:42:34+00:00",
            "updated": "2024-09-02T02:42:34+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 24
        },
        "2409.00919": {
            "authors": [
                "Jinlong Zhu",
                "Keigo Sakurai",
                "Ren Togo",
                "Takahiro Ogawa",
                "Miki Haseyama"
            ],
            "title": "MMT-BERT: Chord-aware Symbolic Music Generation Based on Multitrack Music Transformer and MusicBERT",
            "abstract": "We propose a novel symbolic music representation and Generative Adversarial Network (GAN) framework specially designed for symbolic multitrack music generation. The main theme of symbolic music generation primarily encompasses the preprocessing of music data and the implementation of a deep learning framework. Current techniques dedicated to symbolic music generation generally encounter two significant challenges: training data's lack of information about chords and scales and the requirement of specially designed model architecture adapted to the unique format of symbolic music representation. In this paper, we solve the above problems by introducing new symbolic music representation with MusicLang chord analysis model. We propose our MMT-BERT architecture adapting to the representation. To build a robust multitrack music generator, we fine-tune a pre-trained MusicBERT model to serve as the discriminator, and incorporate relativistic standard loss. This approach, supported by the in-depth understanding of symbolic music encoded within MusicBERT, fortifies the consonance and humanity of music generated by our method. Experimental results demonstrate the effectiveness of our approach which strictly follows the state-of-the-art methods.",
            "id": "2409.00919",
            "link": "http://arxiv.org/abs/2409.00919v1",
            "published": "2024-09-02T03:18:56+00:00",
            "updated": "2024-09-02T03:18:56+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 30
        },
        "2409.00923": {
            "authors": [
                "Shijie Wang"
            ],
            "title": "Development of Occupancy Prediction Algorithm for Underground Parking Lots",
            "abstract": "The core objective of this study is to address the perception challenges faced by autonomous driving in adverse environments like basements. Initially, this paper commences with data collection in an underground garage. A simulated underground garage model is established within the CARLA simulation environment, and SemanticKITTI format occupancy ground truth data is collected in this simulated setting. Subsequently, the study integrates a Transformer-based Occupancy Network model to complete the occupancy grid prediction task within this scenario. A comprehensive BEV perception framework is designed to enhance the accuracy of neural network models in dimly lit, challenging autonomous driving environments. Finally, experiments validate the accuracy of the proposed solution's perception performance in basement scenarios. The proposed solution is tested on our self-constructed underground garage dataset, SUSTech-COE-ParkingLot, yielding satisfactory results.",
            "id": "2409.00923",
            "link": "http://arxiv.org/abs/2409.00923v1",
            "published": "2024-09-02T03:31:49+00:00",
            "updated": "2024-09-02T03:31:49+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 13
        },
        "2409.00946": {
            "authors": [
                "Kaung Myat Kyaw",
                "Jonathan Hoyin Chan"
            ],
            "title": "A Framework for Synthetic Audio Conversations Generation using Large Language Models",
            "abstract": "In this paper, we introduce ConversaSynth, a framework designed to generate synthetic conversation audio using large language models (LLMs) with multiple persona settings. The framework first creates diverse and coherent text-based dialogues across various topics, which are then converted into audio using text-to-speech (TTS) systems. Our experiments demonstrate that ConversaSynth effectively generates highquality synthetic audio datasets, which can significantly enhance the training and evaluation of models for audio tagging, audio classification, and multi-speaker speech recognition. The results indicate that the synthetic datasets generated by ConversaSynth exhibit substantial diversity and realism, making them suitable for developing robust, adaptable audio-based AI systems.",
            "id": "2409.00946",
            "link": "http://arxiv.org/abs/2409.00946v1",
            "published": "2024-09-02T05:09:46+00:00",
            "updated": "2024-09-02T05:09:46+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 5
        },
        "2409.00947": {
            "authors": [
                "Yanfeng Zhou",
                "Lingrui Li",
                "Zichen Wang",
                "Guole Liu",
                "Ziwen Liu",
                "Ge Yang"
            ],
            "title": "XNet v2: Fewer Limitations, Better Results and Greater Universality",
            "abstract": "XNet introduces a wavelet-based X-shaped unified architecture for fully- and semi-supervised biomedical segmentation. So far, however, XNet still faces the limitations, including performance degradation when images lack high-frequency (HF) information, underutilization of raw images and insufficient fusion. To address these issues, we propose XNet v2, a low- and high-frequency complementary model. XNet v2 performs wavelet-based image-level complementary fusion, using fusion results along with raw images inputs three different sub-networks to construct consistency loss. Furthermore, we introduce a feature-level fusion module to enhance the transfer of low-frequency (LF) information and HF information. XNet v2 achieves state-of-the-art in semi-supervised segmentation while maintaining competitve results in fully-supervised learning. More importantly, XNet v2 excels in scenarios where XNet fails. Compared to XNet, XNet v2 exhibits fewer limitations, better results and greater universality. Extensive experiments on three 2D and two 3D datasets demonstrate the effectiveness of XNet v2. Code is available at https://github.com/Yanfeng-Zhou/XNetv2 .",
            "id": "2409.00947",
            "link": "http://arxiv.org/abs/2409.00947v1",
            "published": "2024-09-02T05:20:18+00:00",
            "updated": "2024-09-02T05:20:18+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.00968": {
            "authors": [
                "Hongpei Li",
                "Han Zhang",
                "Ziyan He",
                "Yunkai Jia",
                "Bo Jiang",
                "Xiang Huang",
                "Dongdong Ge"
            ],
            "title": "Solving Integrated Process Planning and Scheduling Problem via Graph Neural Network Based Deep Reinforcement Learning",
            "abstract": "The Integrated Process Planning and Scheduling (IPPS) problem combines process route planning and shop scheduling to achieve high efficiency in manufacturing and maximize resource utilization, which is crucial for modern manufacturing systems. Traditional methods using Mixed Integer Linear Programming (MILP) and heuristic algorithms can not well balance solution quality and speed when solving IPPS. In this paper, we propose a novel end-to-end Deep Reinforcement Learning (DRL) method. We model the IPPS problem as a Markov Decision Process (MDP) and employ a Heterogeneous Graph Neural Network (GNN) to capture the complex relationships among operations, machines, and jobs. To optimize the scheduling strategy, we use Proximal Policy Optimization (PPO). Experimental results show that, compared to traditional methods, our approach significantly improves solution efficiency and quality in large-scale IPPS instances, providing superior scheduling strategies for modern intelligent manufacturing systems.",
            "id": "2409.00968",
            "link": "http://arxiv.org/abs/2409.00968v1",
            "published": "2024-09-02T06:18:30+00:00",
            "updated": "2024-09-02T06:18:30+00:00",
            "primary_category": "math.OC",
            "categories": [
                "math.OC",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 31
        },
        "2409.00980": {
            "authors": [
                "Priyanka Chudasama",
                "Anil Surisetty",
                "Aakarsh Malhotra",
                "Alok Singh"
            ],
            "title": "DNN-GDITD: Out-of-distribution detection via Deep Neural Network based Gaussian Descriptor for Imbalanced Tabular Data",
            "abstract": "Classification tasks present challenges due to class imbalances and evolving data distributions. Addressing these issues requires a robust method to handle imbalances while effectively detecting out-of-distribution (OOD) samples not encountered during training. This study introduces a novel OOD detection algorithm designed for tabular datasets, titled Deep Neural Network-based Gaussian Descriptor for Imbalanced Tabular Data (DNN-GDITD). The DNN-GDITD algorithm can be placed on top of any DNN to facilitate better classification of imbalanced data and OOD detection using spherical decision boundaries. Using a combination of Push, Score-based, and focal losses, DNN-GDITD assigns confidence scores to test data points, categorizing them as known classes or as an OOD sample. Extensive experimentation on tabular datasets demonstrates the effectiveness of DNN-GDITD compared to three OOD algorithms. Evaluation encompasses imbalanced and balanced scenarios on diverse tabular datasets, including a synthetic financial dispute dataset and publicly available tabular datasets like Gas Sensor, Drive Diagnosis, and MNIST, showcasing DNN-GDITD's versatility.",
            "id": "2409.00980",
            "link": "http://arxiv.org/abs/2409.00980v2",
            "published": "2024-09-02T06:52:01+00:00",
            "updated": "2024-09-04T12:25:28+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 19
        },
        "2409.01013": {
            "authors": [
                "Mevan Ekanayake",
                "Zhifeng Chen",
                "Gary Egan",
                "Mehrtash Harandi",
                "Zhaolin Chen"
            ],
            "title": "SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution",
            "abstract": "Implicit Neural Representations (INRs) have recently advanced the field of deep learning due to their ability to learn continuous representations of signals without the need for large training datasets. Although INR methods have been studied for medical image super-resolution, their adaptability to localized priors in medical images has not been extensively explored. Medical images contain rich anatomical divisions that could provide valuable local prior information to enhance the accuracy and robustness of INRs. In this work, we propose a novel framework, referred to as the Semantically Conditioned INR (SeCo-INR), that conditions an INR using local priors from a medical image, enabling accurate model fitting and interpolation capabilities to achieve super-resolution. Our framework learns a continuous representation of the semantic segmentation features of a medical image and utilizes it to derive the optimal INR for each semantic region of the image. We tested our framework using several medical imaging modalities and achieved higher quantitative scores and more realistic super-resolution outputs compared to state-of-the-art methods.",
            "id": "2409.01013",
            "link": "http://arxiv.org/abs/2409.01013v1",
            "published": "2024-09-02T07:45:06+00:00",
            "updated": "2024-09-02T07:45:06+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 38
        },
        "2409.01046": {
            "authors": [
                "Varun Prakash Rajamohan",
                "Senthil Kumar Jagatheesaperumal"
            ],
            "title": "Accelerated Multi-objective Task Learning using Modified Q-learning Algorithm",
            "abstract": "Robots find extensive applications in industry. In recent years, the influence of robots has also increased rapidly in domestic scenarios. The Q-learning algorithm aims to maximise the reward for reaching the goal. This paper proposes a modified version of the Q-learning algorithm, known as Q-learning with scaled distance metric (Q-SD). This algorithm enhances task learning and makes task completion more meaningful. A robotic manipulator (agent) applies the Q-SD algorithm to the task of table cleaning. Using Q-SD, the agent acquires the sequence of steps necessary to accomplish the task while minimising the manipulator's movement distance. We partition the table into grids of different dimensions. The first has a grid count of 3 times 3, and the second has a grid count of 4 times 4. Using the Q-SD algorithm, the maximum success obtained in these two environments was 86% and 59% respectively. Moreover, Compared to the conventional Q-learning algorithm, the drop in average distance moved by the agent in these two environments using the Q-SD algorithm was 8.61% and 6.7% respectively.",
            "id": "2409.01046",
            "link": "http://arxiv.org/abs/2409.01046v1",
            "published": "2024-09-02T08:20:41+00:00",
            "updated": "2024-09-02T08:20:41+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "68T05, 93C85, 93B40, 90C29",
                "I.2.6; I.2.9; I.2.8; F.1.1; F.2.1; H.1.2; G.1.6"
            ],
            "max_author_hindex": 9
        },
        "2409.01083": {
            "authors": [
                "Fan Zhang",
                "Michael Gienger"
            ],
            "title": "Affordance-based Robot Manipulation with Flow Matching",
            "abstract": "We present a framework for assistive robot manipulation, which focuses on two fundamental challenges: first, efficiently adapting large-scale models to downstream scene affordance understanding tasks, especially in daily living scenarios where gathering multi-task data involving humans requires strenuous effort; second, effectively learning robot trajectories by grounding the visual affordance model. We tackle the first challenge by employing a parameter-efficient prompt tuning method that prepends learnable text prompts to the frozen vision model to predict manipulation affordances in multi-task scenarios. Then we propose to learn robot trajectories guided by affordances in a supervised Flow Matching method. Flow matching represents a robot visuomotor policy as a conditional process of flowing random waypoints to desired robot trajectories. Finally, we introduce a real-world dataset with 10 tasks across Activities of Daily Living to test our framework. Our extensive evaluation highlights that the proposed prompt tuning method for learning manipulation affordance with language prompter achieves competitive performance and even outperforms other finetuning protocols across data scales, while satisfying parameter efficiency. Learning multi-task robot trajectories with a single flow matching policy also leads to consistently better performance than alternative behavior cloning methods, especially given multimodal robot action distributions. Our framework seamlessly unifies affordance model learning and trajectory generation with flow matching for robot manipulation.",
            "id": "2409.01083",
            "link": "http://arxiv.org/abs/2409.01083v1",
            "published": "2024-09-02T09:11:28+00:00",
            "updated": "2024-09-02T09:11:28+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 37
        },
        "2409.01093": {
            "authors": [
                "Yang Li",
                "Jianli Xiao"
            ],
            "title": "DS MYOLO: A Reliable Object Detector Based on SSMs for Driving Scenarios",
            "abstract": "Accurate real-time object detection enhances the safety of advanced driver-assistance systems, making it an essential component in driving scenarios. With the rapid development of deep learning technology, CNN-based YOLO real-time object detectors have gained significant attention. However, the local focus of CNNs results in performance bottlenecks. To further enhance detector performance, researchers have introduced Transformer-based self-attention mechanisms to leverage global receptive fields, but their quadratic complexity incurs substantial computational costs. Recently, Mamba, with its linear complexity, has made significant progress through global selective scanning. Inspired by Mamba's outstanding performance, we propose a novel object detector: DS MYOLO. This detector captures global feature information through a simplified selective scanning fusion block (SimVSS Block) and effectively integrates the network's deep features. Additionally, we introduce an efficient channel attention convolution (ECAConv) that enhances cross-channel feature interaction while maintaining low computational complexity. Extensive experiments on the CCTSDB 2021 and VLD-45 driving scenarios datasets demonstrate that DS MYOLO exhibits significant potential and competitive advantage among similarly scaled YOLO series real-time object detectors.",
            "id": "2409.01093",
            "link": "http://arxiv.org/abs/2409.01093v1",
            "published": "2024-09-02T09:22:33+00:00",
            "updated": "2024-09-02T09:22:33+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.01133": {
            "authors": [
                "Zhongyi Xia",
                "Tianzhao Wu"
            ],
            "title": "Large Language Models Can Understanding Depth from Monocular Images",
            "abstract": "Monocular depth estimation is a critical function in computer vision applications. This paper shows that large language models (LLMs) can effectively interpret depth with minimal supervision, using efficient resource utilization and a consistent neural network architecture. We introduce LLM-MDE, a multimodal framework that deciphers depth through language comprehension. Specifically, LLM-MDE employs two main strategies to enhance the pretrained LLM's capability for depth estimation: cross-modal reprogramming and an adaptive prompt estimation module. These strategies align vision representations with text prototypes and automatically generate prompts based on monocular images, respectively. Comprehensive experiments on real-world MDE datasets confirm the effectiveness and superiority of LLM-MDE, which excels in few-/zero-shot tasks while minimizing resource use. The source code is available.",
            "id": "2409.01133",
            "link": "http://arxiv.org/abs/2409.01133v1",
            "published": "2024-09-02T10:11:52+00:00",
            "updated": "2024-09-02T10:11:52+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 5
        },
        "2409.01160": {
            "authors": [
                "Jaeyeon Kim",
                "Jaeyoon Jung",
                "Minjeong Jeon",
                "Sang Hoon Woo",
                "Jinjoo Lee"
            ],
            "title": "Expanding on EnCLAP with Auxiliary Retrieval Model for Automated Audio Captioning",
            "abstract": "In this technical report, we describe our submission to DCASE2024 Challenge Task6 (Automated Audio Captioning) and Task8 (Language-based Audio Retrieval). We develop our approach building upon the EnCLAP audio captioning framework and optimizing it for Task6 of the challenge. Notably, we outline the changes in the underlying components and the incorporation of the reranking process. Additionally, we submit a supplementary retriever model, a byproduct of our modified framework, to Task8. Our proposed systems achieve FENSE score of 0.542 on Task6 and mAP@10 score of 0.386 on Task8, significantly outperforming the baseline models.",
            "id": "2409.01160",
            "link": "http://arxiv.org/abs/2409.01160v1",
            "published": "2024-09-02T10:47:07+00:00",
            "updated": "2024-09-02T10:47:07+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.AI",
                "cs.SD"
            ],
            "max_author_hindex": 21
        },
        "2409.01175": {
            "authors": [
                "Andrija Djurisic",
                "Rosanne Liu",
                "Mladen Nikolic"
            ],
            "title": "Logit Scaling for Out-of-Distribution Detection",
            "abstract": "The safe deployment of machine learning and AI models in open-world settings hinges critically on the ability to detect out-of-distribution (OOD) data accurately, data samples that contrast vastly from what the model was trained with. Current approaches to OOD detection often require further training the model, and/or statistics about the training data which may no longer be accessible. Additionally, many existing OOD detection methods struggle to maintain performance when transferred across different architectures. Our research tackles these issues by proposing a simple, post-hoc method that does not require access to the training data distribution, keeps a trained network intact, and holds strong performance across a variety of architectures. Our method, Logit Scaling (LTS), as the name suggests, simply scales the logits in a manner that effectively distinguishes between in-distribution (ID) and OOD samples. We tested our method on benchmarks across various scales, including CIFAR-10, CIFAR-100, ImageNet and OpenOOD. The experiments cover 3 ID and 14 OOD datasets, as well as 9 model architectures. Overall, we demonstrate state-of-the-art performance, robustness and adaptability across different architectures, paving the way towards a universally applicable solution for advanced OOD detection.",
            "id": "2409.01175",
            "link": "http://arxiv.org/abs/2409.01175v1",
            "published": "2024-09-02T11:10:44+00:00",
            "updated": "2024-09-02T11:10:44+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 17
        },
        "2409.01245": {
            "authors": [
                "David Eckel",
                "Baohe Zhang",
                "Joschka B\u00f6decker"
            ],
            "title": "Revisiting Safe Exploration in Safe Reinforcement learning",
            "abstract": "Safe reinforcement learning (SafeRL) extends standard reinforcement learning with the idea of safety, where safety is typically defined through the constraint of the expected cost return of a trajectory being below a set limit. However, this metric fails to distinguish how costs accrue, treating infrequent severe cost events as equal to frequent mild ones, which can lead to riskier behaviors and result in unsafe exploration. We introduce a new metric, expected maximum consecutive cost steps (EMCC), which addresses safety during training by assessing the severity of unsafe steps based on their consecutive occurrence. This metric is particularly effective for distinguishing between prolonged and occasional safety violations. We apply EMMC in both on- and off-policy algorithm for benchmarking their safe exploration capability. Finally, we validate our metric through a set of benchmarks and propose a new lightweight benchmark task, which allows fast evaluation for algorithm design.",
            "id": "2409.01245",
            "link": "http://arxiv.org/abs/2409.01245v1",
            "published": "2024-09-02T13:29:29+00:00",
            "updated": "2024-09-02T13:29:29+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 8
        },
        "2409.01256": {
            "authors": [
                "Haicheng Liao",
                "Yongkang Li",
                "Chengyue Wang",
                "Songning Lai",
                "Zhenning Li",
                "Zilin Bian",
                "Jaeyoung Lee",
                "Zhiyong Cui",
                "Guohui Zhang",
                "Chengzhong Xu"
            ],
            "title": "Real-time Accident Anticipation for Autonomous Driving Through Monocular Depth-Enhanced 3D Modeling",
            "abstract": "The primary goal of traffic accident anticipation is to foresee potential accidents in real time using dashcam videos, a task that is pivotal for enhancing the safety and reliability of autonomous driving technologies. In this study, we introduce an innovative framework, AccNet, which significantly advances the prediction capabilities beyond the current state-of-the-art (SOTA) 2D-based methods by incorporating monocular depth cues for sophisticated 3D scene modeling. Addressing the prevalent challenge of skewed data distribution in traffic accident datasets, we propose the Binary Adaptive Loss for Early Anticipation (BA-LEA). This novel loss function, together with a multi-task learning strategy, shifts the focus of the predictive model towards the critical moments preceding an accident. {We rigorously evaluate the performance of our framework on three benchmark datasets--Dashcam Accident Dataset (DAD), Car Crash Dataset (CCD), and AnAn Accident Detection (A3D), and DADA-2000 Dataset--demonstrating its superior predictive accuracy through key metrics such as Average Precision (AP) and mean Time-To-Accident (mTTA).",
            "id": "2409.01256",
            "link": "http://arxiv.org/abs/2409.01256v1",
            "published": "2024-09-02T13:46:25+00:00",
            "updated": "2024-09-02T13:46:25+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 25
        },
        "2409.01303": {
            "authors": [
                "Mahefa Ratsisetraina Ravelonanosy",
                "Vlado Menkovski",
                "Jacobus W. Portegies"
            ],
            "title": "Topological degree as a discrete diagnostic for disentanglement, with applications to the $\u0394$VAE",
            "abstract": "We investigate the ability of Diffusion Variational Autoencoder ($\\Delta$VAE) with unit sphere $\\mathcal{S}^2$ as latent space to capture topological and geometrical structure and disentangle latent factors in datasets. For this, we introduce a new diagnostic of disentanglement: namely the topological degree of the encoder, which is a map from the data manifold to the latent space. By using tools from homology theory, we derive and implement an algorithm that computes this degree. We use the algorithm to compute the degree of the encoder of models that result from the training procedure. Our experimental results show that the $\\Delta$VAE achieves relatively small LSBD scores, and that regardless of the degree after initialization, the degree of the encoder after training becomes $-1$ or $+1$, which implies that the resulting encoder is at least homotopic to a homeomorphism.",
            "id": "2409.01303",
            "link": "http://arxiv.org/abs/2409.01303v1",
            "published": "2024-09-02T14:51:31+00:00",
            "updated": "2024-09-02T14:51:31+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "math.AT",
                "51H20 55N35 68T09 68T07"
            ],
            "max_author_hindex": 19
        },
        "2409.01315": {
            "authors": [
                "Daoqi Liu",
                "Tao Shan",
                "Maokun Li",
                "Fan Yang",
                "Shenheng Xu"
            ],
            "title": "Multi-frequency Neural Born Iterative Method for Solving 2-D Inverse Scattering Problems",
            "abstract": "In this work, we propose a deep learning-based imaging method for addressing the multi-frequency electromagnetic (EM) inverse scattering problem (ISP). By combining deep learning technology with EM physical laws, we have successfully developed a multi-frequency neural Born iterative method (NeuralBIM), guided by the principles of the single-frequency NeuralBIM. This method integrates multitask learning techniques with NeuralBIM's efficient iterative inversion process to construct a robust multi-frequency Born iterative inversion model. During training, the model employs a multitask learning approach guided by homoscedastic uncertainty to adaptively allocate the weights of each frequency's data. Additionally, an unsupervised learning method, constrained by the physical laws of ISP, is used to train the multi-frequency NeuralBIM model, eliminating the need for contrast and total field data. The effectiveness of the multi-frequency NeuralBIM is validated through synthetic and experimental data, demonstrating improvements in accuracy and computational efficiency for solving ISP. Moreover, this method exhibits strong generalization capabilities and noise resistance. The multi-frequency NeuralBIM method explores a novel inversion method for multi-frequency EM data and provides an effective solution for the electromagnetic ISP of multi-frequency data.",
            "id": "2409.01315",
            "link": "http://arxiv.org/abs/2409.01315v1",
            "published": "2024-09-02T15:16:07+00:00",
            "updated": "2024-09-02T15:16:07+00:00",
            "primary_category": "physics.comp-ph",
            "categories": [
                "physics.comp-ph",
                "cs.AI",
                "cs.LG",
                "35Q61",
                "I.2.6; G.1.8; G.1.3"
            ],
            "max_author_hindex": 39
        },
        "2409.01326": {
            "authors": [
                "Jin Wang",
                "Nikos Tsagarakis"
            ],
            "title": "Grounding Language Models in Autonomous Loco-manipulation Tasks",
            "abstract": "Humanoid robots with behavioral autonomy have consistently been regarded as ideal collaborators in our daily lives and promising representations of embodied intelligence. Compared to fixed-based robotic arms, humanoid robots offer a larger operational space while significantly increasing the difficulty of control and planning. Despite the rapid progress towards general-purpose humanoid robots, most studies remain focused on locomotion ability with few investigations into whole-body coordination and tasks planning, thus limiting the potential to demonstrate long-horizon tasks involving both mobility and manipulation under open-ended verbal instructions. In this work, we propose a novel framework that learns, selects, and plans behaviors based on tasks in different scenarios. We combine reinforcement learning (RL) with whole-body optimization to generate robot motions and store them into a motion library. We further leverage the planning and reasoning features of the large language model (LLM), constructing a hierarchical task graph that comprises a series of motion primitives to bridge lower-level execution with higher-level planning. Experiments in simulation and real-world using the CENTAURO robot show that the language model based planner can efficiently adapt to new loco-manipulation tasks, demonstrating high autonomy from free-text commands in unstructured scenes.",
            "id": "2409.01326",
            "link": "http://arxiv.org/abs/2409.01326v1",
            "published": "2024-09-02T15:27:48+00:00",
            "updated": "2024-09-02T15:27:48+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 31
        },
        "2409.01387": {
            "authors": [
                "Muhammad Hadir Khan",
                "Bugra Onal",
                "Eren Dogan",
                "Matthew R. Guthaus"
            ],
            "title": "VLSI Hypergraph Partitioning with Deep Learning",
            "abstract": "Partitioning is a known problem in computer science and is critical in chip design workflows, as advancements in this area can significantly influence design quality and efficiency. Deep Learning (DL) techniques, particularly those involving Graph Neural Networks (GNNs), have demonstrated strong performance in various node, edge, and graph prediction tasks using both inductive and transductive learning methods. A notable area of recent interest within GNNs are pooling layers and their application to graph partitioning. While these methods have yielded promising results across social, computational, and other random graphs, their effectiveness has not yet been explored in the context of VLSI hypergraph netlists. In this study, we introduce a new set of synthetic partitioning benchmarks that emulate real-world netlist characteristics and possess a known upper bound for solution cut quality. We distinguish these benchmarks with the prior work and evaluate existing state-of-the-art partitioning algorithms alongside GNN-based approaches, highlighting their respective advantages and disadvantages.",
            "id": "2409.01387",
            "link": "http://arxiv.org/abs/2409.01387v1",
            "published": "2024-09-02T17:32:01+00:00",
            "updated": "2024-09-02T17:32:01+00:00",
            "primary_category": "cs.AR",
            "categories": [
                "cs.AR",
                "cs.AI",
                "cs.DC",
                "cs.LG"
            ],
            "max_author_hindex": 13
        },
        "2409.01437": {
            "authors": [
                "Sushant Gautam",
                "Andrea Stor\u00e5s",
                "Cise Midoglu",
                "Steven A. Hicks",
                "Vajira Thambawita",
                "P\u00e5l Halvorsen",
                "Michael A. Riegler"
            ],
            "title": "Kvasir-VQA: A Text-Image Pair GI Tract Dataset",
            "abstract": "We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and Kvasir-Instrument datasets, augmented with question-and-answer annotations to facilitate advanced machine learning tasks in Gastrointestinal (GI) diagnostics. This dataset comprises 6,500 annotated images spanning various GI tract conditions and surgical instruments, and it supports multiple question types including yes/no, choice, location, and numerical count. The dataset is intended for applications such as image captioning, Visual Question Answering (VQA), text-based generation of synthetic medical images, object detection, and classification. Our experiments demonstrate the dataset's effectiveness in training models for three selected tasks, showcasing significant applications in medical image analysis and diagnostics. We also present evaluation metrics for each task, highlighting the usability and versatility of our dataset. The dataset and supporting artifacts are available at https://datasets.simula.no/kvasir-vqa.",
            "id": "2409.01437",
            "link": "http://arxiv.org/abs/2409.01437v1",
            "published": "2024-09-02T19:41:59+00:00",
            "updated": "2024-09-02T19:41:59+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 44
        },
        "2409.01449": {
            "authors": [
                "Esraa Elelimy",
                "Adam White",
                "Michael Bowling",
                "Martha White"
            ],
            "title": "Real-Time Recurrent Learning using Trace Units in Reinforcement Learning",
            "abstract": "Recurrent Neural Networks (RNNs) are used to learn representations in partially observable environments. For agents that learn online and continually interact with the environment, it is desirable to train RNNs with real-time recurrent learning (RTRL); unfortunately, RTRL is prohibitively expensive for standard RNNs. A promising direction is to use linear recurrent architectures (LRUs), where dense recurrent weights are replaced with a complex-valued diagonal, making RTRL efficient. In this work, we build on these insights to provide a lightweight but effective approach for training RNNs in online RL. We introduce Recurrent Trace Units (RTUs), a small modification on LRUs that we nonetheless find to have significant performance benefits over LRUs when trained with RTRL. We find RTUs significantly outperform other recurrent architectures across several partially observable environments while using significantly less computation.",
            "id": "2409.01449",
            "link": "http://arxiv.org/abs/2409.01449v1",
            "published": "2024-09-02T20:08:23+00:00",
            "updated": "2024-09-02T20:08:23+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 42
        },
        "2409.01491": {
            "authors": [
                "Ansh Sharma",
                "Albert Xiao",
                "Praneet Rathi",
                "Rohit Kundu",
                "Albert Zhai",
                "Yuan Shen",
                "Shenlong Wang"
            ],
            "title": "EarthGen: Generating the World from Top-Down Views",
            "abstract": "In this work, we present a novel method for extensive multi-scale generative terrain modeling. At the core of our model is a cascade of superresolution diffusion models that can be combined to produce consistent images across multiple resolutions. Pairing this concept with a tiled generation method yields a scalable system that can generate thousands of square kilometers of realistic Earth surfaces at high resolution. We evaluate our method on a dataset collected from Bing Maps and show that it outperforms super-resolution baselines on the extreme super-resolution task of 1024x zoom. We also demonstrate its ability to create diverse and coherent scenes via an interactive gigapixel-scale generated map. Finally, we demonstrate how our system can be extended to enable novel content creation applications including controllable world generation and 3D scene generation.",
            "id": "2409.01491",
            "link": "http://arxiv.org/abs/2409.01491v2",
            "published": "2024-09-02T23:17:56+00:00",
            "updated": "2024-09-07T21:49:56+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "J.2; I.4.8"
            ],
            "max_author_hindex": 32
        },
        "2409.01502": {
            "authors": [
                "Zhangsihao Yang",
                "Mengyi Shan",
                "Mohammad Farazi",
                "Wenhui Zhu",
                "Yanxi Chen",
                "Xuanzhao Dong",
                "Yalin Wang"
            ],
            "title": "AMG: Avatar Motion Guided Video Generation",
            "abstract": "Human video generation task has gained significant attention with the advancement of deep generative models. Generating realistic videos with human movements is challenging in nature, due to the intricacies of human body topology and sensitivity to visual artifacts. The extensively studied 2D media generation methods take advantage of massive human media datasets, but struggle with 3D-aware control; whereas 3D avatar-based approaches, while offering more freedom in control, lack photorealism and cannot be harmonized seamlessly with background scene. We propose AMG, a method that combines the 2D photorealism and 3D controllability by conditioning video diffusion models on controlled rendering of 3D avatars. We additionally introduce a novel data processing pipeline that reconstructs and renders human avatar movements from dynamic camera videos. AMG is the first method that enables multi-person diffusion video generation with precise control over camera positions, human motions, and background style. We also demonstrate through extensive evaluation that it outperforms existing human video generation methods conditioned on pose sequences or driving videos in terms of realism and adaptability.",
            "id": "2409.01502",
            "link": "http://arxiv.org/abs/2409.01502v1",
            "published": "2024-09-02T23:59:01+00:00",
            "updated": "2024-09-02T23:59:01+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ],
            "max_author_hindex": 75
        },
        "2409.01514": {
            "authors": [
                "David S. Bolme",
                "Deniz Aykac",
                "Ryan Shivers",
                "Joel Brogan",
                "Nell Barber",
                "Bob Zhang",
                "Laura Davies",
                "David Cornett III"
            ],
            "title": "From Data to Insights: A Covariate Analysis of the IARPA BRIAR Dataset for Multimodal Biometric Recognition Algorithms at Altitude and Range",
            "abstract": "This paper examines covariate effects on fused whole body biometrics performance in the IARPA BRIAR dataset, specifically focusing on UAV platforms, elevated positions, and distances up to 1000 meters. The dataset includes outdoor videos compared with indoor images and controlled gait recordings. Normalized raw fusion scores relate directly to predicted false accept rates (FAR), offering an intuitive means for interpreting model results. A linear model is developed to predict biometric algorithm scores, analyzing their performance to identify the most influential covariates on accuracy at altitude and range. Weather factors like temperature, wind speed, solar loading, and turbulence are also investigated in this analysis. The study found that resolution and camera distance best predicted accuracy and findings can guide future research and development efforts in long-range/elevated/UAV biometrics and support the creation of more reliable and robust systems for national security and other critical domains.",
            "id": "2409.01514",
            "link": "http://arxiv.org/abs/2409.01514v1",
            "published": "2024-09-03T00:58:50+00:00",
            "updated": "2024-09-03T00:58:50+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 24
        },
        "2409.01531": {
            "authors": [
                "Jishnu Ray Chowdhury",
                "Cornelia Caragea"
            ],
            "title": "On the Design Space Between Transformers and Recursive Neural Nets",
            "abstract": "In this paper, we study two classes of models, Recursive Neural Networks (RvNNs) and Transformers, and show that a tight connection between them emerges from the recent development of two recent models - Continuous Recursive Neural Networks (CRvNN) and Neural Data Routers (NDR). On one hand, CRvNN pushes the boundaries of traditional RvNN, relaxing its discrete structure-wise composition and ends up with a Transformer-like structure. On the other hand, NDR constrains the original Transformer to induce better structural inductive bias, ending up with a model that is close to CRvNN. Both models, CRvNN and NDR, show strong performance in algorithmic tasks and generalization in which simpler forms of RvNNs and Transformers fail. We explore these \"bridge\" models in the design space between RvNNs and Transformers, formalize their tight connections, discuss their limitations, and propose ideas for future research.",
            "id": "2409.01531",
            "link": "http://arxiv.org/abs/2409.01531v1",
            "published": "2024-09-03T02:03:35+00:00",
            "updated": "2024-09-03T02:03:35+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 37
        },
        "2409.01588": {
            "authors": [
                "Hongyuan Su",
                "Yu Zheng",
                "Jingtao Ding",
                "Depeng Jin",
                "Yong Li"
            ],
            "title": "Large-scale Urban Facility Location Selection with Knowledge-informed Reinforcement Learning",
            "abstract": "The facility location problem (FLP) is a classical combinatorial optimization challenge aimed at strategically laying out facilities to maximize their accessibility. In this paper, we propose a reinforcement learning method tailored to solve large-scale urban FLP, capable of producing near-optimal solutions at superfast inference speed. We distill the essential swap operation from local search, and simulate it by intelligently selecting edges on a graph of urban regions, guided by a knowledge-informed graph neural network, thus sidestepping the need for heavy computation of local search. Extensive experiments on four US cities with different geospatial conditions demonstrate that our approach can achieve comparable performance to commercial solvers with less than 5\\% accessibility loss, while displaying up to 1000 times speedup. We deploy our model as an online geospatial application at https://huggingface.co/spaces/randommmm/MFLP.",
            "id": "2409.01588",
            "link": "http://arxiv.org/abs/2409.01588v2",
            "published": "2024-09-03T04:04:40+00:00",
            "updated": "2024-09-06T08:16:02+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CY",
                "68T20"
            ],
            "max_author_hindex": 54
        },
        "2409.01610": {
            "authors": [
                "Yearim Kim",
                "Sangyu Han",
                "Sangbum Han",
                "Nojun Kwak"
            ],
            "title": "Decompose the model: Mechanistic interpretability in image models with Generalized Integrated Gradients (GIG)",
            "abstract": "In the field of eXplainable AI (XAI) in language models, the progression from local explanations of individual decisions to global explanations with high-level concepts has laid the groundwork for mechanistic interpretability, which aims to decode the exact operations. However, this paradigm has not been adequately explored in image models, where existing methods have primarily focused on class-specific interpretations. This paper introduces a novel approach to systematically trace the entire pathway from input through all intermediate layers to the final output within the whole dataset. We utilize Pointwise Feature Vectors (PFVs) and Effective Receptive Fields (ERFs) to decompose model embeddings into interpretable Concept Vectors. Then, we calculate the relevance between concept vectors with our Generalized Integrated Gradients (GIG), enabling a comprehensive, dataset-wide analysis of model behavior. We validate our method of concept extraction and concept attribution in both qualitative and quantitative evaluations. Our approach advances the understanding of semantic significance within image models, offering a holistic view of their operational mechanics.",
            "id": "2409.01610",
            "link": "http://arxiv.org/abs/2409.01610v1",
            "published": "2024-09-03T05:19:35+00:00",
            "updated": "2024-09-03T05:19:35+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 36
        },
        "2409.01630": {
            "authors": [
                "Wenxiao Zhang",
                "Xiangrui Kong",
                "Thomas Braunl",
                "Jin B. Hong"
            ],
            "title": "SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems",
            "abstract": "Embodied AI systems, including AI-powered robots that autonomously interact with the physical world, stand to be significantly advanced by Large Language Models (LLMs), which enable robots to better understand complex language commands and perform advanced tasks with enhanced comprehension and adaptability, highlighting their potential to improve embodied AI capabilities. However, this advancement also introduces safety challenges, particularly in robotic navigation tasks. Improper safety management can lead to failures in complex environments and make the system vulnerable to malicious command injections, resulting in unsafe behaviours such as detours or collisions. To address these issues, we propose \\textit{SafeEmbodAI}, a safety framework for integrating mobile robots into embodied AI systems. \\textit{SafeEmbodAI} incorporates secure prompting, state management, and safety validation mechanisms to secure and assist LLMs in reasoning through multi-modal data and validating responses. We designed a metric to evaluate mission-oriented exploration, and evaluations in simulated environments demonstrate that our framework effectively mitigates threats from malicious commands and improves performance in various environment settings, ensuring the safety of embodied AI systems. Notably, In complex environments with mixed obstacles, our method demonstrates a significant performance increase of 267\\% compared to the baseline in attack scenarios, highlighting its robustness in challenging conditions.",
            "id": "2409.01630",
            "link": "http://arxiv.org/abs/2409.01630v1",
            "published": "2024-09-03T05:56:50+00:00",
            "updated": "2024-09-03T05:56:50+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.ET"
            ],
            "max_author_hindex": 21
        },
        "2409.01633": {
            "authors": [
                "Mingze Ni",
                "Wei Liu"
            ],
            "title": "Dreaming is All You Need",
            "abstract": "In classification tasks, achieving a harmonious balance between exploration and precision is of paramount importance. To this end, this research introduces two novel deep learning models, SleepNet and DreamNet, to strike this balance. SleepNet seamlessly integrates supervised learning with unsupervised ``sleep\" stages using pre-trained encoder models. Dedicated neurons within SleepNet are embedded in these unsupervised features, forming intermittent ``sleep\" blocks that facilitate exploratory learning. Building upon the foundation of SleepNet, DreamNet employs full encoder-decoder frameworks to reconstruct the hidden states, mimicking the human \"dreaming\" process. This reconstruction process enables further exploration and refinement of the learned representations. Moreover, the principle ideas of our SleepNet and DreamNet are generic and can be applied to both computer vision and natural language processing downstream tasks. Through extensive empirical evaluations on diverse image and text datasets, SleepNet and DreanNet have demonstrated superior performance compared to state-of-the-art models, showcasing the strengths of unsupervised exploration and supervised precision afforded by our innovative approaches.",
            "id": "2409.01633",
            "link": "http://arxiv.org/abs/2409.01633v2",
            "published": "2024-09-03T06:04:39+00:00",
            "updated": "2024-09-10T02:39:25+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 29
        },
        "2409.01635": {
            "authors": [
                "Ricardo Knauer",
                "Marvin Grimm",
                "Erik Rodner"
            ],
            "title": "PMLBmini: A Tabular Classification Benchmark Suite for Data-Scarce Applications",
            "abstract": "In practice, we are often faced with small-sized tabular data. However, current tabular benchmarks are not geared towards data-scarce applications, making it very difficult to derive meaningful conclusions from empirical comparisons. We introduce PMLBmini, a tabular benchmark suite of 44 binary classification datasets with sample sizes $\\leq$ 500. We use our suite to thoroughly evaluate current automated machine learning (AutoML) frameworks, off-the-shelf tabular deep neural networks, as well as classical linear models in the low-data regime. Our analysis reveals that state-of-the-art AutoML and deep learning approaches often fail to appreciably outperform even a simple logistic regression baseline, but we also identify scenarios where AutoML and deep learning methods are indeed reasonable to apply. Our benchmark suite, available on https://github.com/RicardoKnauer/TabMini , allows researchers and practitioners to analyze their own methods and challenge their data efficiency.",
            "id": "2409.01635",
            "link": "http://arxiv.org/abs/2409.01635v1",
            "published": "2024-09-03T06:13:03+00:00",
            "updated": "2024-09-03T06:13:03+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 31
        },
        "2409.01668": {
            "authors": [
                "Wenhan Yao",
                "Zedong Xing",
                "Xiarun Chen",
                "Jia Liu",
                "Yongqiang He",
                "Weiping Wen"
            ],
            "title": "Pureformer-VC: Non-parallel One-Shot Voice Conversion with Pure Transformer Blocks and Triplet Discriminative Training",
            "abstract": "One-shot voice conversion(VC) aims to change the timbre of any source speech to match that of the target speaker with only one speech sample. Existing style transfer-based VC methods relied on speech representation disentanglement and suffered from accurately and independently encoding each speech component and recomposing back to converted speech effectively. To tackle this, we proposed Pureformer-VC, which utilizes Conformer blocks to build a disentangled encoder, and Zipformer blocks to build a style transfer decoder as the generator. In the decoder, we used effective styleformer blocks to integrate speaker characteristics effectively into the generated speech. The models used the generative VAE loss for encoding components and triplet loss for unsupervised discriminative training. We applied the styleformer method to Zipformer's shared weights for style transfer. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
            "id": "2409.01668",
            "link": "http://arxiv.org/abs/2409.01668v2",
            "published": "2024-09-03T07:21:19+00:00",
            "updated": "2024-09-06T08:24:19+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 28
        },
        "2409.01676": {
            "authors": [
                "Wenyang Hu",
                "Gaetan Frusque",
                "Tianyang Wang",
                "Fulei Chu",
                "Olga Fink"
            ],
            "title": "Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring",
            "abstract": "Deriving health indicators of rotating machines is crucial for their maintenance. However, this process is challenging for the prevalent adopted intelligent methods since they may take the whole data distributions, not only introducing noise interference but also lacking the explainability. To address these issues, we propose a diffusion-based weakly-supervised approach for deriving health indicators of rotating machines, enabling early fault detection and continuous monitoring of condition evolution. This approach relies on a classifier-free diffusion model trained using healthy samples and a few anomalies. This model generates healthy samples. and by comparing the differences between the original samples and the generated ones in the envelope spectrum, we construct an anomaly map that clearly identifies faults. Health indicators are then derived, which can explain the fault types and mitigate noise interference. Comparative studies on two cases demonstrate that the proposed method offers superior health monitoring effectiveness and robustness compared to baseline models.",
            "id": "2409.01676",
            "link": "http://arxiv.org/abs/2409.01676v1",
            "published": "2024-09-03T07:41:55+00:00",
            "updated": "2024-09-03T07:41:55+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "eess.SP"
            ],
            "max_author_hindex": 63
        },
        "2409.01679": {
            "authors": [
                "Hyungkeun Park",
                "Jong-Seok Lee"
            ],
            "title": "Adaptive Explicit Knowledge Transfer for Knowledge Distillation",
            "abstract": "Logit-based knowledge distillation (KD) for classification is cost-efficient compared to feature-based KD but often subject to inferior performance. Recently, it was shown that the performance of logit-based KD can be improved by effectively delivering the probability distribution for the non-target classes from the teacher model, which is known as `implicit (dark) knowledge', to the student model. Through gradient analysis, we first show that this actually has an effect of adaptively controlling the learning of implicit knowledge. Then, we propose a new loss that enables the student to learn explicit knowledge (i.e., the teacher's confidence about the target class) along with implicit knowledge in an adaptive manner. Furthermore, we propose to separate the classification and distillation tasks for effective distillation and inter-class relationship modeling. Experimental results demonstrate that the proposed method, called adaptive explicit knowledge transfer (AEKT) method, achieves improved performance compared to the state-of-the-art KD methods on the CIFAR-100 and ImageNet datasets.",
            "id": "2409.01679",
            "link": "http://arxiv.org/abs/2409.01679v2",
            "published": "2024-09-03T07:42:59+00:00",
            "updated": "2024-09-05T07:44:14+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.01713": {
            "authors": [
                "Patrick Knab",
                "Sascha Marton",
                "Christian Bartelt",
                "Robert Fuder"
            ],
            "title": "Interpreting Outliers in Time Series Data through Decoding Autoencoder",
            "abstract": "Outlier detection is a crucial analytical tool in various fields. In critical systems like manufacturing, malfunctioning outlier detection can be costly and safety-critical. Therefore, there is a significant need for explainable artificial intelligence (XAI) when deploying opaque models in such environments. This study focuses on manufacturing time series data from a German automotive supply industry. We utilize autoencoders to compress the entire time series and then apply anomaly detection techniques to its latent features. For outlier interpretation, we (i) adopt widely used XAI techniques to the autoencoder's encoder. Additionally, (ii) we propose AEE, Aggregated Explanatory Ensemble, a novel approach that fuses explanations of multiple XAI techniques into a single, more expressive interpretation. For evaluation of explanations, (iii) we propose a technique to measure the quality of encoder explanations quantitatively. Furthermore, we qualitatively assess the effectiveness of outlier explanations with domain expertise.",
            "id": "2409.01713",
            "link": "http://arxiv.org/abs/2409.01713v1",
            "published": "2024-09-03T08:52:21+00:00",
            "updated": "2024-09-03T08:52:21+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 11
        },
        "2409.01871": {
            "authors": [
                "Salah Eddine Laidoudi",
                "Madjid Maidi",
                "Samir Otmane"
            ],
            "title": "Real-Time Indoor Object Detection based on hybrid CNN-Transformer Approach",
            "abstract": "Real-time object detection in indoor settings is a challenging area of computer vision, faced with unique obstacles such as variable lighting and complex backgrounds. This field holds significant potential to revolutionize applications like augmented and mixed realities by enabling more seamless interactions between digital content and the physical world. However, the scarcity of research specifically fitted to the intricacies of indoor environments has highlighted a clear gap in the literature. To address this, our study delves into the evaluation of existing datasets and computational models, leading to the creation of a refined dataset. This new dataset is derived from OpenImages v7, focusing exclusively on 32 indoor categories selected for their relevance to real-world applications. Alongside this, we present an adaptation of a CNN detection model, incorporating an attention mechanism to enhance the model's ability to discern and prioritize critical features within cluttered indoor scenes. Our findings demonstrate that this approach is not just competitive with existing state-of-the-art models in accuracy and speed but also opens new avenues for research and application in the field of real-time indoor object detection.",
            "id": "2409.01871",
            "link": "http://arxiv.org/abs/2409.01871v1",
            "published": "2024-09-03T13:14:08+00:00",
            "updated": "2024-09-03T13:14:08+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 16
        },
        "2409.01872": {
            "authors": [
                "Francesco Pasti",
                "Marina Ceccon",
                "Davide Dalle Pezze",
                "Francesco Paissan",
                "Elisabetta Farella",
                "Gian Antonio Susto",
                "Nicola Bellotto"
            ],
            "title": "Latent Distillation for Continual Object Detection at the Edge",
            "abstract": "While numerous methods achieving remarkable performance exist in the Object Detection literature, addressing data distribution shifts remains challenging. Continual Learning (CL) offers solutions to this issue, enabling models to adapt to new data while maintaining performance on previous data. This is particularly pertinent for edge devices, common in dynamic environments like automotive and robotics. In this work, we address the memory and computation constraints of edge devices in the Continual Learning for Object Detection (CLOD) scenario. Specifically, (i) we investigate the suitability of an open-source, lightweight, and fast detector, namely NanoDet, for CLOD on edge devices, improving upon larger architectures used in the literature. Moreover, (ii) we propose a novel CL method, called Latent Distillation~(LD), that reduces the number of operations and the memory required by state-of-the-art CL approaches without significantly compromising detection performance. Our approach is validated using the well-known VOC and COCO benchmarks, reducing the distillation parameter overhead by 74\\% and the Floating Points Operations~(FLOPs) by 56\\% per model update compared to other distillation methods.",
            "id": "2409.01872",
            "link": "http://arxiv.org/abs/2409.01872v1",
            "published": "2024-09-03T13:14:13+00:00",
            "updated": "2024-09-03T13:14:13+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 34
        },
        "2409.01876": {
            "authors": [
                "Gaojie Lin",
                "Jianwen Jiang",
                "Chao Liang",
                "Tianyun Zhong",
                "Jiaqi Yang",
                "Yanbo Zheng"
            ],
            "title": "CyberHost: Taming Audio-driven Avatar Diffusion Model with Region Codebook Attention",
            "abstract": "Diffusion-based video generation technology has advanced significantly, catalyzing a proliferation of research in human animation. However, the majority of these studies are confined to same-modality driving settings, with cross-modality human body animation remaining relatively underexplored. In this paper, we introduce, an end-to-end audio-driven human animation framework that ensures hand integrity, identity consistency, and natural motion. The key design of CyberHost is the Region Codebook Attention mechanism, which improves the generation quality of facial and hand animations by integrating fine-grained local features with learned motion pattern priors. Furthermore, we have developed a suite of human-prior-guided training strategies, including body movement map, hand clarity score, pose-aligned reference feature, and local enhancement supervision, to improve synthesis results. To our knowledge, CyberHost is the first end-to-end audio-driven human diffusion model capable of facilitating zero-shot video generation within the scope of human body. Extensive experiments demonstrate that CyberHost surpasses previous works in both quantitative and qualitative aspects.",
            "id": "2409.01876",
            "link": "http://arxiv.org/abs/2409.01876v2",
            "published": "2024-09-03T13:19:31+00:00",
            "updated": "2024-09-05T03:31:28+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 19
        },
        "2409.01914": {
            "authors": [
                "Filippo Aglietti",
                "Francesco Della Santa",
                "Andrea Piano",
                "Virginia Aglietti"
            ],
            "title": "GradINN: Gradient Informed Neural Network",
            "abstract": "We propose Gradient Informed Neural Networks (GradINNs), a methodology inspired by Physics Informed Neural Networks (PINNs) that can be used to efficiently approximate a wide range of physical systems for which the underlying governing equations are completely unknown or cannot be defined, a condition that is often met in complex engineering problems. GradINNs leverage prior beliefs about a system's gradient to constrain the predicted function's gradient across all input dimensions. This is achieved using two neural networks: one modeling the target function and an auxiliary network expressing prior beliefs, e.g., smoothness. A customized loss function enables training the first network while enforcing gradient constraints derived from the auxiliary network. We demonstrate the advantages of GradINNs, particularly in low-data regimes, on diverse problems spanning non time-dependent systems (Friedman function, Stokes Flow) and time-dependent systems (Lotka-Volterra, Burger's equation). Experimental results showcase strong performance compared to standard neural networks and PINN-like approaches across all tested scenarios.",
            "id": "2409.01914",
            "link": "http://arxiv.org/abs/2409.01914v1",
            "published": "2024-09-03T14:03:29+00:00",
            "updated": "2024-09-03T14:03:29+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 12
        },
        "2409.01974": {
            "authors": [
                "Wouter M. Kouw"
            ],
            "title": "Planning to avoid ambiguous states through Gaussian approximations to non-linear sensors in active inference agents",
            "abstract": "In nature, active inference agents must learn how observations of the world represent the state of the agent. In engineering, the physics behind sensors is often known reasonably accurately and measurement functions can be incorporated into generative models. When a measurement function is non-linear, the transformed variable is typically approximated with a Gaussian distribution to ensure tractable inference. We show that Gaussian approximations that are sensitive to the curvature of the measurement function, such as a second-order Taylor approximation, produce a state-dependent ambiguity term. This induces a preference over states, based on how accurately the state can be inferred from the observation. We demonstrate this preference with a robot navigation experiment where agents plan trajectories.",
            "id": "2409.01974",
            "link": "http://arxiv.org/abs/2409.01974v1",
            "published": "2024-09-03T15:17:16+00:00",
            "updated": "2024-09-03T15:17:16+00:00",
            "primary_category": "eess.SY",
            "categories": [
                "eess.SY",
                "cs.AI",
                "cs.RO",
                "cs.SY",
                "stat.ML"
            ],
            "max_author_hindex": 8
        },
        "2409.01992": {
            "authors": [
                "Bozhidar Stevanoski",
                "Ana-Maria Cretu",
                "Yves-Alexandre de Montjoye"
            ],
            "title": "QueryCheetah: Fast Automated Discovery of Attribute Inference Attacks Against Query-Based Systems",
            "abstract": "Query-based systems (QBSs) are one of the key approaches for sharing data. QBSs allow analysts to request aggregate information from a private protected dataset. Attacks are a crucial part of ensuring QBSs are truly privacy-preserving. The development and testing of attacks is however very labor-intensive and unable to cope with the increasing complexity of systems. Automated approaches have been shown to be promising but are currently extremely computationally intensive, limiting their applicability in practice. We here propose QueryCheetah, a fast and effective method for automated discovery of privacy attacks against QBSs. We instantiate QueryCheetah on attribute inference attacks and show it to discover stronger attacks than previous methods while being 18 times faster than the state-of-the-art automated approach. We then show how QueryCheetah allows system developers to thoroughly evaluate the privacy risk, including for various attacker strengths and target individuals. We finally show how QueryCheetah can be used out-of-the-box to find attacks in larger syntaxes and workarounds around ad-hoc defenses.",
            "id": "2409.01992",
            "link": "http://arxiv.org/abs/2409.01992v1",
            "published": "2024-09-03T15:37:05+00:00",
            "updated": "2024-09-03T15:37:05+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 19
        },
        "2409.02008": {
            "authors": [
                "Wenshuai Liu",
                "Yaru Fu",
                "Zheng Shi",
                "Hong Wang"
            ],
            "title": "When Digital Twin Meets 6G: Concepts, Obstacles, and Research Prospects",
            "abstract": "The convergence of digital twin technology and the emerging 6G network presents both challenges and numerous research opportunities. This article explores the potential synergies between digital twin and 6G, highlighting the key challenges and proposing fundamental principles for their integration. We discuss the unique requirements and capabilities of digital twin in the context of 6G networks, such as sustainable deployment, real-time synchronization, seamless migration, predictive analytic, and closed-loop control. Furthermore, we identify research opportunities for leveraging digital twin and artificial intelligence to enhance various aspects of 6G, including network optimization, resource allocation, security, and intelligent service provisioning. This article aims to stimulate further research and innovation at the intersection of digital twin and 6G, paving the way for transformative applications and services in the future.",
            "id": "2409.02008",
            "link": "http://arxiv.org/abs/2409.02008v1",
            "published": "2024-09-03T15:57:05+00:00",
            "updated": "2024-09-03T15:57:05+00:00",
            "primary_category": "cs.NI",
            "categories": [
                "cs.NI",
                "cs.AI",
                "cs.DC"
            ],
            "max_author_hindex": 31
        },
        "2409.02017": {
            "authors": [
                "Chuhao Wu",
                "He Zhang",
                "John M. Carroll"
            ],
            "title": "AI Governance in Higher Education: Case Studies of Guidance at Big Ten Universities",
            "abstract": "Generative AI has drawn significant attention from stakeholders in higher education. As it introduces new opportunities for personalized learning and tutoring support, it simultaneously poses challenges to academic integrity and leads to ethical issues. Consequently, governing responsible AI usage within higher education institutions (HEIs) becomes increasingly important. Leading universities have already published guidelines on Generative AI, with most attempting to embrace this technology responsibly. This study provides a new perspective by focusing on strategies for responsible AI governance as demonstrated in these guidelines. Through a case study of 14 prestigious universities in the United States, we identified the multi-unit governance of AI, the role-specific governance of AI, and the academic characteristics of AI governance from their AI guidelines. The strengths and potential limitations of these strategies and characteristics are discussed. The findings offer practical implications for guiding responsible AI usage in HEIs and beyond.",
            "id": "2409.02017",
            "link": "http://arxiv.org/abs/2409.02017v1",
            "published": "2024-09-03T16:06:45+00:00",
            "updated": "2024-09-03T16:06:45+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "max_author_hindex": 82
        },
        "2409.02018": {
            "authors": [
                "Bobby Azad",
                "Pourya Adibfar",
                "Kaiqun Fu"
            ],
            "title": "TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation",
            "abstract": "In healthcare, medical image segmentation is crucial for accurate disease diagnosis and the development of effective treatment strategies. Early detection can significantly aid in managing diseases and potentially prevent their progression. Machine learning, particularly deep convolutional neural networks, has emerged as a promising approach to addressing segmentation challenges. Traditional methods like U-Net use encoding blocks for local representation modeling and decoding blocks to uncover semantic relationships. However, these models often struggle with multi-scale objects exhibiting significant variations in texture and shape, and they frequently fail to capture long-range dependencies in the input data. Transformers designed for sequence-to-sequence predictions have been proposed as alternatives, utilizing global self-attention mechanisms. Yet, they can sometimes lack precise localization due to insufficient granular details. To overcome these limitations, we introduce TransDAE: a novel approach that reimagines the self-attention mechanism to include both spatial and channel-wise associations across the entire feature space, while maintaining computational efficiency. Additionally, TransDAE enhances the skip connection pathway with an inter-scale interaction module, promoting feature reuse and improving localization accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on the Synaps multi-organ dataset, even without relying on pre-trained weights.",
            "id": "2409.02018",
            "link": "http://arxiv.org/abs/2409.02018v1",
            "published": "2024-09-03T16:08:48+00:00",
            "updated": "2024-09-03T16:08:48+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "68T07"
            ],
            "max_author_hindex": 10
        },
        "2409.02049": {
            "authors": [
                "Ruixin Shi",
                "Weijia Guo",
                "Shiming Ge"
            ],
            "title": "Low-Resolution Face Recognition via Adaptable Instance-Relation Distillation",
            "abstract": "Low-resolution face recognition is a challenging task due to the missing of informative details. Recent approaches based on knowledge distillation have proven that high-resolution clues can well guide low-resolution face recognition via proper knowledge transfer. However, due to the distribution difference between training and testing faces, the learned models often suffer from poor adaptability. To address that, we split the knowledge transfer process into distillation and adaptation steps, and propose an adaptable instance-relation distillation approach to facilitate low-resolution face recognition. In the approach, the student distills knowledge from high-resolution teacher in both instance level and relation level, providing sufficient cross-resolution knowledge transfer. Then, the learned student can be adaptable to recognize low-resolution faces with adaptive batch normalization in inference. In this manner, the capability of recovering missing details of familiar low-resolution faces can be effectively enhanced, leading to a better knowledge transfer. Extensive experiments on low-resolution face recognition clearly demonstrate the effectiveness and adaptability of our approach.",
            "id": "2409.02049",
            "link": "http://arxiv.org/abs/2409.02049v1",
            "published": "2024-09-03T16:53:34+00:00",
            "updated": "2024-09-03T16:53:34+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ],
            "max_author_hindex": 10
        },
        "2409.02130": {
            "authors": [
                "Muhammad Arbab Arshad",
                "Pallavi Kandanur",
                "Saurabh Sonawani"
            ],
            "title": "From Predictive Importance to Causality: Which Machine Learning Model Reflects Reality?",
            "abstract": "This study analyzes the Ames Housing Dataset using CatBoost and LightGBM models to explore feature importance and causal relationships in housing price prediction. We examine the correlation between SHAP values and EconML predictions, achieving high accuracy in price forecasting. Our analysis reveals a moderate Spearman rank correlation of 0.48 between SHAP-based feature importance and causally significant features, highlighting the complexity of aligning predictive modeling with causal understanding in housing market analysis. Through extensive causal analysis, including heterogeneity exploration and policy tree interpretation, we provide insights into how specific features like porches impact housing prices across various scenarios. This work underscores the need for integrated approaches that combine predictive power with causal insights in real estate valuation, offering valuable guidance for stakeholders in the industry.",
            "id": "2409.02130",
            "link": "http://arxiv.org/abs/2409.02130v1",
            "published": "2024-09-01T22:37:47+00:00",
            "updated": "2024-09-01T22:37:47+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 3
        },
        "2409.02134": {
            "authors": [
                "Samer Francy",
                "Raghubir Singh"
            ],
            "title": "Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks",
            "abstract": "This work evaluates the compression techniques on ConvNeXt models in image classification tasks using the CIFAR-10 dataset. Structured pruning, unstructured pruning, and dynamic quantization methods are evaluated to reduce model size and computational complexity while maintaining accuracy. The experiments, conducted on cloud-based platforms and edge device, assess the performance of these techniques. Results show significant reductions in model size, with up to 75% reduction achieved using structured pruning techniques. Additionally, dynamic quantization achieves a reduction of up to 95% in the number of parameters. Fine-tuned models exhibit improved compression performance, indicating the benefits of pre-training in conjunction with compression techniques. Unstructured pruning methods reveal trends in accuracy and compression, with limited reductions in computational complexity. The combination of OTOV3 pruning and dynamic quantization further enhances compression performance, resulting 89.7% reduction in size, 95% reduction with number of parameters and MACs, and 3.8% increase with accuracy. The deployment of the final compressed model on edge device demonstrates high accuracy 92.5% and low inference time 20 ms, validating the effectiveness of compression techniques for real-world edge computing applications.",
            "id": "2409.02134",
            "link": "http://arxiv.org/abs/2409.02134v1",
            "published": "2024-09-02T11:48:19+00:00",
            "updated": "2024-09-02T11:48:19+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 15
        },
        "2409.02138": {
            "authors": [
                "Zhuohan Wang",
                "Carmine Ventre"
            ],
            "title": "A Financial Time Series Denoiser Based on Diffusion Model",
            "abstract": "Financial time series often exhibit low signal-to-noise ratio, posing significant challenges for accurate data interpretation and prediction and ultimately decision making. Generative models have gained attention as powerful tools for simulating and predicting intricate data patterns, with the diffusion model emerging as a particularly effective method. This paper introduces a novel approach utilizing the diffusion model as a denoiser for financial time series in order to improve data predictability and trading performance. By leveraging the forward and reverse processes of the conditional diffusion model to add and remove noise progressively, we reconstruct original data from noisy inputs. Our extensive experiments demonstrate that diffusion model-based denoised time series significantly enhance the performance on downstream future return classification tasks. Moreover, trading signals derived from the denoised data yield more profitable trades with fewer transactions, thereby minimizing transaction costs and increasing overall trading efficiency. Finally, we show that by using classifiers trained on denoised time series, we can recognize the noising state of the market and obtain excess return.",
            "id": "2409.02138",
            "link": "http://arxiv.org/abs/2409.02138v1",
            "published": "2024-09-02T15:55:36+00:00",
            "updated": "2024-09-02T15:55:36+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "q-fin.CP",
                "q-fin.TR"
            ],
            "max_author_hindex": 17
        },
        "2409.02140": {
            "authors": [
                "Daniel Otero",
                "Rafael Mateus"
            ],
            "title": "Self-Supervised Learning for Identifying Defects in Sewer Footage",
            "abstract": "Sewerage infrastructure is among the most expensive modern investments requiring time-intensive manual inspections by qualified personnel. Our study addresses the need for automated solutions without relying on large amounts of labeled data. We propose a novel application of Self-Supervised Learning (SSL) for sewer inspection that offers a scalable and cost-effective solution for defect detection. We achieve competitive results with a model that is at least 5 times smaller than other approaches found in the literature and obtain competitive performance with 10\\% of the available data when training with a larger architecture. Our findings highlight the potential of SSL to revolutionize sewer maintenance in resource-limited settings.",
            "id": "2409.02140",
            "link": "http://arxiv.org/abs/2409.02140v1",
            "published": "2024-09-02T19:28:48+00:00",
            "updated": "2024-09-02T19:28:48+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 13
        },
        "2409.02145": {
            "authors": [
                "Zekang Yang",
                "Hong Liu",
                "Xiangdong Wang"
            ],
            "title": "A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction",
            "abstract": "Computer-aided cancer survival risk prediction plays an important role in the timely treatment of patients. This is a challenging weakly supervised ordinal regression task associated with multiple clinical factors involved such as pathological images, genomic data and etc. In this paper, we propose a new training method, multimodal object-level contrast learning, for cancer survival risk prediction. First, we construct contrast learning pairs based on the survival risk relationship among the samples in the training sample set. Then we introduce the object-level contrast learning method to train the survival risk predictor. We further extend it to the multimodal scenario by applying cross-modal constrast. Considering the heterogeneity of pathological images and genomics data, we construct a multimodal survival risk predictor employing attention-based and self-normalizing based nerural network respectively. Finally, the survival risk predictor trained by our proposed method outperforms state-of-the-art methods on two public multimodal cancer datasets for survival risk prediction.",
            "id": "2409.02145",
            "link": "http://arxiv.org/abs/2409.02145v1",
            "published": "2024-09-03T07:36:34+00:00",
            "updated": "2024-09-03T07:36:34+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 16
        },
        "2409.02148": {
            "authors": [
                "Alban Puech",
                "Jonas Weiss",
                "Thomas Brunschwiler",
                "Hendrik F. Hamann"
            ],
            "title": "Optimal Power Grid Operations with Foundation Models",
            "abstract": "The energy transition, crucial for tackling the climate crisis, demands integrating numerous distributed, renewable energy sources into existing grids. Along with climate change and consumer behavioral changes, this leads to changes and variability in generation and load patterns, introducing significant complexity and uncertainty into grid planning and operations. While the industry has already started to exploit AI to overcome computational challenges of established grid simulation tools, we propose the use of AI Foundation Models (FMs) and advances in Graph Neural Networks to efficiently exploit poorly available grid data for different downstream tasks, enhancing grid operations. For capturing the grid's underlying physics, we believe that building a self-supervised model learning the power flow dynamics is a critical first step towards developing an FM for the power grid. We show how this approach may close the gap between the industry needs and current grid analysis capabilities, to bring the industry closer to optimal grid operation and planning.",
            "id": "2409.02148",
            "link": "http://arxiv.org/abs/2409.02148v1",
            "published": "2024-09-03T09:06:13+00:00",
            "updated": "2024-09-03T09:06:13+00:00",
            "primary_category": "eess.SY",
            "categories": [
                "eess.SY",
                "cs.AI",
                "cs.LG",
                "cs.SY",
                "math.OC"
            ],
            "max_author_hindex": 35
        },
        "2409.02245": {
            "authors": [
                "Takuhiro Kaneko",
                "Hirokazu Kameoka",
                "Kou Tanaka",
                "Yuto Kondo"
            ],
            "title": "FastVoiceGrad: One-step Diffusion-Based Voice Conversion with Adversarial Conditional Diffusion Distillation",
            "abstract": "Diffusion-based voice conversion (VC) techniques such as VoiceGrad have attracted interest because of their high VC performance in terms of speech quality and speaker similarity. However, a notable limitation is the slow inference caused by the multi-step reverse diffusion. Therefore, we propose FastVoiceGrad, a novel one-step diffusion-based VC that reduces the number of iterations from dozens to one while inheriting the high VC performance of the multi-step diffusion-based VC. We obtain the model using adversarial conditional diffusion distillation (ACDD), leveraging the ability of generative adversarial networks and diffusion models while reconsidering the initial states in sampling. Evaluations of one-shot any-to-any VC demonstrate that FastVoiceGrad achieves VC performance superior to or comparable to that of previous multi-step diffusion-based VC while enhancing the inference speed. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/fastvoicegrad/.",
            "id": "2409.02245",
            "link": "http://arxiv.org/abs/2409.02245v1",
            "published": "2024-09-03T19:19:48+00:00",
            "updated": "2024-09-03T19:19:48+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.LG",
                "eess.AS",
                "stat.ML"
            ],
            "max_author_hindex": 41
        },
        "2409.02261": {
            "authors": [
                "Yichun Li",
                "Yuxing Yang",
                "Syed Nohsen Naqvi"
            ],
            "title": "Action-Based ADHD Diagnosis in Video",
            "abstract": "Attention Deficit Hyperactivity Disorder (ADHD) causes significant impairment in various domains. Early diagnosis of ADHD and treatment could significantly improve the quality of life and functioning. Recently, machine learning methods have improved the accuracy and efficiency of the ADHD diagnosis process. However, the cost of the equipment and trained staff required by the existing methods are generally huge. Therefore, we introduce the video-based frame-level action recognition network to ADHD diagnosis for the first time. We also record a real multi-modal ADHD dataset and extract three action classes from the video modality for ADHD diagnosis. The whole process data have been reported to CNTW-NHS Foundation Trust, which would be reviewed by medical consultants/professionals and will be made public in due course.",
            "id": "2409.02261",
            "link": "http://arxiv.org/abs/2409.02261v1",
            "published": "2024-09-03T19:38:23+00:00",
            "updated": "2024-09-03T19:38:23+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 13
        },
        "2409.02270": {
            "authors": [
                "Hassan El Alami",
                "Danda B. Rawat"
            ],
            "title": "Reinforcement Learning-enabled Satellite Constellation Reconfiguration and Retasking for Mission-Critical Applications",
            "abstract": "The development of satellite constellation applications is rapidly advancing due to increasing user demands, reduced operational costs, and technological advancements. However, a significant gap in the existing literature concerns reconfiguration and retasking issues within satellite constellations, which is the primary focus of our research. In this work, we critically assess the impact of satellite failures on constellation performance and the associated task requirements. To facilitate this analysis, we introduce a system modeling approach for GPS satellite constellations, enabling an investigation into performance dynamics and task distribution strategies, particularly in scenarios where satellite failures occur during mission-critical operations. Additionally, we introduce reinforcement learning (RL) techniques, specifically Q-learning, Policy Gradient, Deep Q-Network (DQN), and Proximal Policy Optimization (PPO), for managing satellite constellations, addressing the challenges posed by reconfiguration and retasking following satellite failures. Our results demonstrate that DQN and PPO achieve effective outcomes in terms of average rewards, task completion rates, and response times.",
            "id": "2409.02270",
            "link": "http://arxiv.org/abs/2409.02270v1",
            "published": "2024-09-03T20:01:56+00:00",
            "updated": "2024-09-03T20:01:56+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ],
            "max_author_hindex": 0
        },
        "2409.02284": {
            "authors": [
                "Suhang You",
                "Sanyukta Adap",
                "Siddhesh Thakur",
                "Bhakti Baheti",
                "Spyridon Bakas"
            ],
            "title": "Biochemical Prostate Cancer Recurrence Prediction: Thinking Fast & Slow",
            "abstract": "Time to biochemical recurrence in prostate cancer is essential for prognostic monitoring of the progression of patients after prostatectomy, which assesses the efficacy of the surgery. In this work, we proposed to leverage multiple instance learning through a two-stage ``thinking fast \\& slow'' strategy for the time to recurrence (TTR) prediction. The first (``thinking fast'') stage finds the most relevant WSI area for biochemical recurrence and the second (``thinking slow'') stage leverages higher resolution patches to predict TTR. Our approach reveals a mean C-index ($Ci$) of 0.733 ($\\theta=0.059$) on our internal validation and $Ci=0.603$ on the LEOPARD challenge validation set. Post hoc attention visualization shows that the most attentive area contributes to the TTR prediction.",
            "id": "2409.02284",
            "link": "http://arxiv.org/abs/2409.02284v1",
            "published": "2024-09-03T20:37:43+00:00",
            "updated": "2024-09-03T20:37:43+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "68T10",
                "I.5.4"
            ],
            "max_author_hindex": 40
        },
        "2409.02302": {
            "authors": [
                "Anmol Guragain",
                "Tianchi Liu",
                "Zihan Pan",
                "Hardik B. Sailor",
                "Qiongqiong Wang"
            ],
            "title": "Speech Foundation Model Ensembles for the Controlled Singing Voice Deepfake Detection (CtrSVDD) Challenge 2024",
            "abstract": "This work details our approach to achieving a leading system with a 1.79% pooled equal error rate (EER) on the evaluation set of the Controlled Singing Voice Deepfake Detection (CtrSVDD). The rapid advancement of generative AI models presents significant challenges for detecting AI-generated deepfake singing voices, attracting increased research attention. The Singing Voice Deepfake Detection (SVDD) Challenge 2024 aims to address this complex task. In this work, we explore the ensemble methods, utilizing speech foundation models to develop robust singing voice anti-spoofing systems. We also introduce a novel Squeeze-and-Excitation Aggregation (SEA) method, which efficiently and effectively integrates representation features from the speech foundation models, surpassing the performance of our other individual systems. Evaluation results confirm the efficacy of our approach in detecting deepfake singing voices. The codes can be accessed at https://github.com/Anmol2059/SVDD2024.",
            "id": "2409.02302",
            "link": "http://arxiv.org/abs/2409.02302v1",
            "published": "2024-09-03T21:28:45+00:00",
            "updated": "2024-09-03T21:28:45+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.AI",
                "cs.SD"
            ],
            "max_author_hindex": 14
        },
        "2409.02391": {
            "authors": [
                "Ali Merali"
            ],
            "title": "Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Translation",
            "abstract": "This paper derives 'scaling laws' -- empirical relationships between the amount of training compute used for a Large Language Model (LLM) and its performance -- for economic outcomes. In a preregistered experiment, 300 professional translators completed 1800 tasks with access to one of thirteen LLMs with differing model training compute sizes (or a control). Our results show that model scaling substantially raises productivity: for every 10x increase in model compute, translators completed tasks 12.3% quicker, received 0.18 s.d. higher grades, and earned 16.1% more per minute (including bonus payments). Further, the gains from model scaling are much higher for lower-skilled workers who gain a 4x larger improvement in task completion speed. These results imply further frontier model scaling -- which is currently estimated at 4x increase per year -- may have significant economic implications.",
            "id": "2409.02391",
            "link": "http://arxiv.org/abs/2409.02391v1",
            "published": "2024-09-04T02:39:31+00:00",
            "updated": "2024-09-04T02:39:31+00:00",
            "primary_category": "econ.GN",
            "categories": [
                "econ.GN",
                "cs.AI",
                "q-fin.EC"
            ],
            "max_author_hindex": 1
        },
        "2409.02451": {
            "authors": [
                "Yisi Liu",
                "Bohan Yu",
                "Drake Lin",
                "Peter Wu",
                "Cheol Jun Cho",
                "Gopala Krishna Anumanchipalli"
            ],
            "title": "Fast, High-Quality and Parameter-Efficient Articulatory Synthesis using Differentiable DSP",
            "abstract": "Articulatory trajectories like electromagnetic articulography (EMA) provide a low-dimensional representation of the vocal tract filter and have been used as natural, grounded features for speech synthesis. Differentiable digital signal processing (DDSP) is a parameter-efficient framework for audio synthesis. Therefore, integrating low-dimensional EMA features with DDSP can significantly enhance the computational efficiency of speech synthesis. In this paper, we propose a fast, high-quality, and parameter-efficient DDSP articulatory vocoder that can synthesize speech from EMA, F0, and loudness. We incorporate several techniques to solve the harmonics / noise imbalance problem, and add a multi-resolution adversarial loss for better synthesis quality. Our model achieves a transcription word error rate (WER) of 6.67% and a mean opinion score (MOS) of 3.74, with an improvement of 1.63% and 0.16 compared to the state-of-the-art (SOTA) baseline. Our DDSP vocoder is 4.9x faster than the baseline on CPU during inference, and can generate speech of comparable quality with only 0.4M parameters, in contrast to the 9M parameters required by the SOTA.",
            "id": "2409.02451",
            "link": "http://arxiv.org/abs/2409.02451v1",
            "published": "2024-09-04T05:12:15+00:00",
            "updated": "2024-09-04T05:12:15+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.AI",
                "cs.SD"
            ],
            "max_author_hindex": 23
        },
        "2409.02489": {
            "authors": [
                "Dashanka De Silva",
                "Siqi Cai",
                "Saurav Pahuja",
                "Tanja Schultz",
                "Haizhou Li"
            ],
            "title": "NeuroSpex: Neuro-Guided Speaker Extraction with Cross-Modal Attention",
            "abstract": "In the study of auditory attention, it has been revealed that there exists a robust correlation between attended speech and elicited neural responses, measurable through electroencephalography (EEG). Therefore, it is possible to use the attention information available within EEG signals to guide the extraction of the target speaker in a cocktail party computationally. In this paper, we present a neuro-guided speaker extraction model, i.e. NeuroSpex, using the EEG response of the listener as the sole auxiliary reference cue to extract attended speech from monaural speech mixtures. We propose a novel EEG signal encoder that captures the attention information. Additionally, we propose a cross-attention (CA) mechanism to enhance the speech feature representations, generating a speaker extraction mask. Experimental results on a publicly available dataset demonstrate that our proposed model outperforms two baseline models across various evaluation metrics.",
            "id": "2409.02489",
            "link": "http://arxiv.org/abs/2409.02489v1",
            "published": "2024-09-04T07:33:01+00:00",
            "updated": "2024-09-04T07:33:01+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 12
        },
        "2409.02512": {
            "authors": [
                "Jifeng Hu",
                "Li Shen",
                "Sili Huang",
                "Zhejian Yang",
                "Hechang Chen",
                "Lichao Sun",
                "Yi Chang",
                "Dacheng Tao"
            ],
            "title": "Continual Diffuser (CoD): Mastering Continual Offline Reinforcement Learning with Experience Rehearsal",
            "abstract": "Artificial neural networks, especially recent diffusion-based models, have shown remarkable superiority in gaming, control, and QA systems, where the training tasks' datasets are usually static. However, in real-world applications, such as robotic control of reinforcement learning (RL), the tasks are changing, and new tasks arise in a sequential order. This situation poses the new challenge of plasticity-stability trade-off for training an agent who can adapt to task changes and retain acquired knowledge. In view of this, we propose a rehearsal-based continual diffusion model, called Continual Diffuser (CoD), to endow the diffuser with the capabilities of quick adaptation (plasticity) and lasting retention (stability). Specifically, we first construct an offline benchmark that contains 90 tasks from multiple domains. Then, we train the CoD on each task with sequential modeling and conditional generation for making decisions. Next, we preserve a small portion of previous datasets as the rehearsal buffer and replay it to retain the acquired knowledge. Extensive experiments on a series of tasks show CoD can achieve a promising plasticity-stability trade-off and outperform existing diffusion-based methods and other representative baselines on most tasks.",
            "id": "2409.02512",
            "link": "http://arxiv.org/abs/2409.02512v1",
            "published": "2024-09-04T08:21:47+00:00",
            "updated": "2024-09-04T08:21:47+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 153
        },
        "2409.02530": {
            "authors": [
                "Chih-Yuan Li",
                "Jun-Ting Wu",
                "Chan Hsu",
                "Ming-Yen Lin",
                "Yihuang Kang"
            ],
            "title": "Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models",
            "abstract": "The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.",
            "id": "2409.02530",
            "link": "http://arxiv.org/abs/2409.02530v1",
            "published": "2024-09-04T08:44:36+00:00",
            "updated": "2024-09-04T08:44:36+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 80
        },
        "2409.02629": {
            "authors": [
                "Melih Catal",
                "Manuel G\u00fcnther"
            ],
            "title": "AdvSecureNet: A Python Toolkit for Adversarial Machine Learning",
            "abstract": "Machine learning models are vulnerable to adversarial attacks. Several tools have been developed to research these vulnerabilities, but they often lack comprehensive features and flexibility. We introduce AdvSecureNet, a PyTorch based toolkit for adversarial machine learning that is the first to natively support multi-GPU setups for attacks, defenses, and evaluation. It is the first toolkit that supports both CLI and API interfaces and external YAML configuration files to enhance versatility and reproducibility. The toolkit includes multiple attacks, defenses and evaluation metrics. Rigiorous software engineering practices are followed to ensure high code quality and maintainability. The project is available as an open-source project on GitHub at https://github.com/melihcatal/advsecurenet and installable via PyPI.",
            "id": "2409.02629",
            "link": "http://arxiv.org/abs/2409.02629v1",
            "published": "2024-09-04T11:47:00+00:00",
            "updated": "2024-09-04T11:47:00+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CR",
                "cs.LG"
            ],
            "max_author_hindex": 27
        },
        "2409.02672": {
            "authors": [
                "Ruoyu Wang",
                "Lina Yao"
            ],
            "title": "Independence Constrained Disentangled Representation Learning from Epistemological Perspective",
            "abstract": "Disentangled Representation Learning aims to improve the explainability of deep learning methods by training a data encoder that identifies semantically meaningful latent variables in the data generation process. Nevertheless, there is no consensus regarding a universally accepted definition for the objective of disentangled representation learning. In particular, there is a considerable amount of discourse regarding whether should the latent variables be mutually independent or not. In this paper, we first investigate these arguments on the interrelationships between latent variables by establishing a conceptual bridge between Epistemology and Disentangled Representation Learning. Then, inspired by these interdisciplinary concepts, we introduce a two-level latent space framework to provide a general solution to the prior arguments on this issue. Finally, we propose a novel method for disentangled representation learning by employing an integration of mutual information constraint and independence constraint within the Generative Adversarial Network (GAN) framework. Experimental results demonstrate that our proposed method consistently outperforms baseline approaches in both quantitative and qualitative evaluations. The method exhibits strong performance across multiple commonly used metrics and demonstrates a great capability in disentangling various semantic factors, leading to an improved quality of controllable generation, which consequently benefits the explainability of the algorithm.",
            "id": "2409.02672",
            "link": "http://arxiv.org/abs/2409.02672v1",
            "published": "2024-09-04T13:00:59+00:00",
            "updated": "2024-09-04T13:00:59+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 45
        },
        "2409.02691": {
            "authors": [
                "Maeve Hutchinson",
                "Radu Jianu",
                "Aidan Slingsby",
                "Pranava Madhyastha"
            ],
            "title": "LLM-Assisted Visual Analytics: Opportunities and Challenges",
            "abstract": "We explore the integration of large language models (LLMs) into visual analytics (VA) systems to transform their capabilities through intuitive natural language interactions. We survey current research directions in this emerging field, examining how LLMs are integrated into data management, language interaction, visualisation generation, and language generation processes. We highlight the new possibilities that LLMs bring to VA, especially how they can change VA processes beyond the usual use cases. We especially highlight building new visualisation-language models, allowing access of a breadth of domain knowledge, multimodal interaction, and opportunities with guidance. Finally, we carefully consider the prominent challenges of using current LLMs in VA tasks. Our discussions in this paper aim to guide future researchers working on LLM-assisted VA systems and help them navigate common obstacles when developing these systems.",
            "id": "2409.02691",
            "link": "http://arxiv.org/abs/2409.02691v1",
            "published": "2024-09-04T13:24:03+00:00",
            "updated": "2024-09-04T13:24:03+00:00",
            "primary_category": "cs.HC",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "max_author_hindex": 22
        },
        "2409.02747": {
            "authors": [
                "Ahana Deb",
                "Roberto Cipollone",
                "Anders Jonsson",
                "Alessandro Ronca",
                "Mohammad Sadegh Talebi"
            ],
            "title": "Tractable Offline Learning of Regular Decision Processes",
            "abstract": "This work studies offline Reinforcement Learning (RL) in a class of non-Markovian environments called Regular Decision Processes (RDPs). In RDPs, the unknown dependency of future observations and rewards from the past interactions can be captured by some hidden finite-state automaton. For this reason, many RDP algorithms first reconstruct this unknown dependency using automata learning techniques. In this paper, we show that it is possible to overcome two strong limitations of previous offline RL algorithms for RDPs, notably RegORL. This can be accomplished via the introduction of two original techniques: the development of a new pseudometric based on formal languages, which removes a problematic dependency on $L_\\infty^\\mathsf{p}$-distinguishability parameters, and the adoption of Count-Min-Sketch (CMS), instead of naive counting. The former reduces the number of samples required in environments that are characterized by a low complexity in language-theoretic terms. The latter alleviates the memory requirements for long planning horizons. We derive the PAC sample complexity bounds associated to each of these techniques, and we validate the approach experimentally.",
            "id": "2409.02747",
            "link": "http://arxiv.org/abs/2409.02747v1",
            "published": "2024-09-04T14:26:58+00:00",
            "updated": "2024-09-04T14:26:58+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.FL"
            ],
            "max_author_hindex": 36
        },
        "2409.02779": {
            "authors": [
                "Akash R. Wasil",
                "Peter Barnett",
                "Michael Gerovitch",
                "Roman Hauksson",
                "Tom Reed",
                "Jack William Miller"
            ],
            "title": "Governing dual-use technologies: Case studies of international security agreements and lessons for AI governance",
            "abstract": "International AI governance agreements and institutions may play an important role in reducing global security risks from advanced AI. To inform the design of such agreements and institutions, we conducted case studies of historical and contemporary international security agreements. We focused specifically on those arrangements around dual-use technologies, examining agreements in nuclear security, chemical weapons, biosecurity, and export controls. For each agreement, we examined four key areas: (a) purpose, (b) core powers, (c) governance structure, and (d) instances of non-compliance. From these case studies, we extracted lessons for the design of international AI agreements and governance institutions. We discuss the importance of robust verification methods, strategies for balancing power between nations, mechanisms for adapting to rapid technological change, approaches to managing trade-offs between transparency and security, incentives for participation, and effective enforcement mechanisms.",
            "id": "2409.02779",
            "link": "http://arxiv.org/abs/2409.02779v1",
            "published": "2024-09-04T14:56:59+00:00",
            "updated": "2024-09-04T14:56:59+00:00",
            "primary_category": "cs.CY",
            "categories": [
                "cs.CY",
                "cs.AI"
            ],
            "max_author_hindex": 20
        },
        "2409.02850": {
            "authors": [
                "Raphael Lafargue",
                "Luke Smith",
                "Franck Vermet",
                "Mathias L\u00f6we",
                "Ian Reid",
                "Vincent Gripon",
                "Jack Valmadre"
            ],
            "title": "Oops, I Sampled it Again: Reinterpreting Confidence Intervals in Few-Shot Learning",
            "abstract": "The predominant method for computing confidence intervals (CI) in few-shot learning (FSL) is based on sampling the tasks with replacement, i.e.\\ allowing the same samples to appear in multiple tasks. This makes the CI misleading in that it takes into account the randomness of the sampler but not the data itself. To quantify the extent of this problem, we conduct a comparative analysis between CIs computed with and without replacement. These reveal a notable underestimation by the predominant method. This observation calls for a reevaluation of how we interpret confidence intervals and the resulting conclusions in FSL comparative studies. Our research demonstrates that the use of paired tests can partially address this issue. Additionally, we explore methods to further reduce the (size of the) CI by strategically sampling tasks of a specific size. We also introduce a new optimized benchmark, which can be accessed at https://github.com/RafLaf/FSL-benchmark-again",
            "id": "2409.02850",
            "link": "http://arxiv.org/abs/2409.02850v2",
            "published": "2024-09-04T16:20:57+00:00",
            "updated": "2024-09-06T08:30:57+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "stat.ML",
                "68T06",
                "I.2; I.4; I.5; G.3"
            ],
            "max_author_hindex": 25
        },
        "2409.02871": {
            "authors": [
                "Cristian Gariboldi",
                "Matteo Corno",
                "Beng Jin"
            ],
            "title": "Hybrid Imitation-Learning Motion Planner for Urban Driving",
            "abstract": "With the release of open source datasets such as nuPlan and Argoverse, the research around learning-based planners has spread a lot in the last years. Existing systems have shown excellent capabilities in imitating the human driver behaviour, but they struggle to guarantee safe closed-loop driving. Conversely, optimization-based planners offer greater security in short-term planning scenarios. To confront this challenge, in this paper we propose a novel hybrid motion planner that integrates both learning-based and optimization-based techniques. Initially, a multilayer perceptron (MLP) generates a human-like trajectory, which is then refined by an optimization-based component. This component not only minimizes tracking errors but also computes a trajectory that is both kinematically feasible and collision-free with obstacles and road boundaries. Our model effectively balances safety and human-likeness, mitigating the trade-off inherent in these objectives. We validate our approach through simulation experiments and further demonstrate its efficacy by deploying it in real-world self-driving vehicles.",
            "id": "2409.02871",
            "link": "http://arxiv.org/abs/2409.02871v1",
            "published": "2024-09-04T16:54:31+00:00",
            "updated": "2024-09-04T16:54:31+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 30
        },
        "2409.02958": {
            "authors": [
                "Dominykas Seputis",
                "Serghei Mihailov",
                "Soham Chatterjee",
                "Zehao Xiao"
            ],
            "title": "Multi-Modal Adapter for Vision-Language Models",
            "abstract": "Large pre-trained vision-language models, such as CLIP, have demonstrated state-of-the-art performance across a wide range of image classification tasks, without requiring retraining. Few-shot CLIP is competitive with existing specialized architectures that were trained on the downstream tasks. Recent research demonstrates that the performance of CLIP can be further improved using lightweight adaptation approaches. However, previous methods adapt different modalities of the CLIP model individually, ignoring the interactions and relationships between visual and textual representations. In this work, we propose Multi-Modal Adapter, an approach for Multi-Modal adaptation of CLIP. Specifically, we add a trainable Multi-Head Attention layer that combines text and image features to produce an additive adaptation of both. Multi-Modal Adapter demonstrates improved generalizability, based on its performance on unseen classes compared to existing adaptation methods. We perform additional ablations and investigations to validate and interpret the proposed approach.",
            "id": "2409.02958",
            "link": "http://arxiv.org/abs/2409.02958v1",
            "published": "2024-09-03T12:47:08+00:00",
            "updated": "2024-09-03T12:47:08+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 13
        },
        "2409.02960": {
            "authors": [
                "Shunichi Akatsuka",
                "Yaemi Teramoto",
                "Aaron Courville"
            ],
            "title": "Managing multiple agents by automatically adjusting incentives",
            "abstract": "In the coming years, AI agents will be used for making more complex decisions, including in situations involving many different groups of people. One big challenge is that AI agent tends to act in its own interest, unlike humans who often think about what will be the best for everyone in the long run. In this paper, we explore a method to get self-interested agents to work towards goals that benefit society as a whole. We propose a method to add a manager agent to mediate agent interactions by assigning incentives to certain actions. We tested our method with a supply-chain management problem and showed that this framework (1) increases the raw reward by 22.2%, (2) increases the agents' reward by 23.8%, and (3) increases the manager's reward by 20.1%.",
            "id": "2409.02960",
            "link": "http://arxiv.org/abs/2409.02960v1",
            "published": "2024-09-03T18:41:16+00:00",
            "updated": "2024-09-03T18:41:16+00:00",
            "primary_category": "cs.MA",
            "categories": [
                "cs.MA",
                "cs.AI",
                "cs.GT"
            ],
            "max_author_hindex": 88
        },
        "2409.02977": {
            "authors": [
                "Junwei Liu",
                "Kaixin Wang",
                "Yixuan Chen",
                "Xin Peng",
                "Zhenpeng Chen",
                "Lingming Zhang",
                "Yiling Lou"
            ],
            "title": "Large Language Model-Based Agents for Software Engineering: A Survey",
            "abstract": "The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools. To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE). The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems. In this work, we present a comprehensive and systematic survey on LLM-based agents for SE. We collect 106 papers and categorize them from two perspectives, i.e., the SE and agent perspectives. In addition, we discuss open challenges and future directions in this critical domain. The repository of this survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.",
            "id": "2409.02977",
            "link": "http://arxiv.org/abs/2409.02977v1",
            "published": "2024-09-04T15:59:41+00:00",
            "updated": "2024-09-04T15:59:41+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 42
        },
        "2409.03060": {
            "authors": [
                "Min Wu",
                "Xiaofu Li",
                "Haoze Wu",
                "Clark Barrett"
            ],
            "title": "Better Verified Explanations with Applications to Incorrectness and Out-of-Distribution Detection",
            "abstract": "Building on VeriX (Verified eXplainability, arXiv:2212.01051), a system for producing optimal verified explanations for machine learning model outputs, we present VeriX+, which significantly improves both the size and the generation time of verified explanations. We introduce a bound propagation-based sensitivity technique to improve the size, and a binary search-based traversal with confidence ranking for improving time -- the two techniques are orthogonal and can be used independently or together. We also show how to adapt the QuickXplain (Junker 2004) algorithm to our setting to provide a trade-off between size and time. Experimental evaluations on standard benchmarks demonstrate significant improvements on both metrics, e.g., a size reduction of 38% on the GTSRB dataset and a time reduction of 90% on MNIST. We also explore applications of our verified explanations and show that explanation size is a useful proxy for both incorrectness detection and out-of-distribution detection.",
            "id": "2409.03060",
            "link": "http://arxiv.org/abs/2409.03060v1",
            "published": "2024-09-04T20:20:37+00:00",
            "updated": "2024-09-04T20:20:37+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 50
        },
        "2409.03147": {
            "authors": [
                "Juan A. Berrios Moya"
            ],
            "title": "Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning",
            "abstract": "The rapid global aging trend has led to an increase in dementia cases, including Alzheimer's disease, underscoring the urgent need for early and accurate diagnostic methods. Traditional diagnostic techniques, such as cognitive tests, neuroimaging, and biomarker analysis, face significant limitations in sensitivity, accessibility, and cost, particularly in the early stages. This study explores the potential of machine learning (ML) as a transformative approach to enhance early dementia detection by leveraging ML models to analyze and integrate complex multimodal datasets, including cognitive assessments, neuroimaging, and genetic information. A comprehensive review of existing literature was conducted to evaluate various ML models, including supervised learning, deep learning, and advanced techniques such as ensemble learning and transformer models, assessing their accuracy, interpretability, and potential for clinical integration. The findings indicate that while ML models show significant promise in improving diagnostic precision and enabling earlier interventions, challenges remain in their generalizability, interpretability, and ethical deployment. This research concludes by outlining future directions aimed at enhancing the clinical utility of ML models in dementia detection, emphasizing interdisciplinary collaboration and ethically sound frameworks to improve early detection and intervention strategies for Alzheimer's disease and other forms of dementia.",
            "id": "2409.03147",
            "link": "http://arxiv.org/abs/2409.03147v1",
            "published": "2024-09-05T00:52:59+00:00",
            "updated": "2024-09-05T00:52:59+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 0
        },
        "2409.03239": {
            "authors": [
                "Jamshaid Ul Rahman",
                "Nimra"
            ],
            "title": "DiffGrad for Physics-Informed Neural Networks",
            "abstract": "Physics-Informed Neural Networks (PINNs) are regarded as state-of-the-art tools for addressing highly nonlinear problems based on partial differential equations. Despite their broad range of applications, PINNs encounter several performance challenges, including issues related to efficiency, minimization of computational cost, and enhancement of accuracy. Burgers' equation, a fundamental equation in fluid dynamics that is extensively used in PINNs, provides flexible results with the Adam optimizer that does not account for past gradients. This paper introduces a novel strategy for solving Burgers' equation by incorporating DiffGrad with PINNs, a method that leverages the difference between current and immediately preceding gradients to enhance performance. A comprehensive computational analysis is conducted using optimizers such as Adam, Adamax, RMSprop, and DiffGrad to evaluate and compare their effectiveness. Our approach includes visualizing the solutions over space at various time intervals to demonstrate the accuracy of the network. The results show that DiffGrad not only improves the accuracy of the solution but also reduces training time compared to the other optimizers.",
            "id": "2409.03239",
            "link": "http://arxiv.org/abs/2409.03239v1",
            "published": "2024-09-05T04:39:35+00:00",
            "updated": "2024-09-05T04:39:35+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "physics.comp-ph"
            ],
            "max_author_hindex": 8
        },
        "2409.03261": {
            "authors": [
                "Jinhee Kim",
                "Taesung Kim",
                "Jaegul Choo"
            ],
            "title": "Bones Can't Be Triangles: Accurate and Efficient Vertebrae Keypoint Estimation through Collaborative Error Revision",
            "abstract": "Recent advances in interactive keypoint estimation methods have enhanced accuracy while minimizing user intervention. However, these methods require user input for error correction, which can be costly in vertebrae keypoint estimation where inaccurate keypoints are densely clustered or overlap. We introduce a novel approach, KeyBot, specifically designed to identify and correct significant and typical errors in existing models, akin to user revision. By characterizing typical error types and using simulated errors for training, KeyBot effectively corrects these errors and significantly reduces user workload. Comprehensive quantitative and qualitative evaluations on three public datasets confirm that KeyBot significantly outperforms existing methods, achieving state-of-the-art performance in interactive vertebrae keypoint estimation. The source code and demo video are available at: https://ts-kim.github.io/KeyBot/",
            "id": "2409.03261",
            "link": "http://arxiv.org/abs/2409.03261v1",
            "published": "2024-09-05T06:03:52+00:00",
            "updated": "2024-09-05T06:03:52+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 41
        },
        "2409.03274": {
            "authors": [
                "Jing Cui",
                "Yishi Xu",
                "Zhewei Huang",
                "Shuchang Zhou",
                "Jianbin Jiao",
                "Junge Zhang"
            ],
            "title": "Recent Advances in Attack and Defense Approaches of Large Language Models",
            "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence and machine learning through their advanced text processing and generating capabilities. However, their widespread deployment has raised significant safety and reliability concerns. Established vulnerabilities in deep neural networks, coupled with emerging threat models, may compromise security evaluations and create a false sense of security. Given the extensive research in the field of LLM security, we believe that summarizing the current state of affairs will help the research community better understand the present landscape and inform future developments. This paper reviews current research on LLM vulnerabilities and threats, and evaluates the effectiveness of contemporary defense mechanisms. We analyze recent studies on attack vectors and model weaknesses, providing insights into attack mechanisms and the evolving threat landscape. We also examine current defense strategies, highlighting their strengths and limitations. By contrasting advancements in attack and defense methodologies, we identify research gaps and propose future directions to enhance LLM security. Our goal is to advance the understanding of LLM safety challenges and guide the development of more robust security measures.",
            "id": "2409.03274",
            "link": "http://arxiv.org/abs/2409.03274v2",
            "published": "2024-09-05T06:31:37+00:00",
            "updated": "2024-09-06T10:31:07+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 35
        },
        "2409.03377": {
            "authors": [
                "Yan Ru Pei",
                "Ritik Shrivastava",
                "FNU Sidharth"
            ],
            "title": "Real-time Speech Enhancement on Raw Signals with Deep State-space Modeling",
            "abstract": "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Code is available at github.com/Brainchip-Inc/aTENNuate",
            "id": "2409.03377",
            "link": "http://arxiv.org/abs/2409.03377v2",
            "published": "2024-09-05T09:28:56+00:00",
            "updated": "2024-09-07T23:13:20+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.LG",
                "eess.AS"
            ],
            "max_author_hindex": 89
        },
        "2409.03384": {
            "authors": [
                "Nikoletta Koilia",
                "Christoforos Kachris"
            ],
            "title": "Hardware Acceleration of LLMs: A comprehensive survey and comparison",
            "abstract": "Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. In this paper, we present a comprehensive survey of the several research efforts that have been presented for the acceleration of transformer networks for Large Language Models using hardware accelerators.   The survey presents the frameworks that have been proposed and then performs a qualitative and quantitative comparison regarding the technology, the processing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy efficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each framework. The main challenge in comparison is that every proposed scheme is implemented on a different process technology making hard a fair comparison. The main contribution of this paper is that we extrapolate the results of the performance and the energy efficiency on the same technology to make a fair comparison; one theoretical and one more practical. We implement part of the LLMs on several FPGA chips to extrapolate the results to the same process technology and then we make a fair comparison of the performance.",
            "id": "2409.03384",
            "link": "http://arxiv.org/abs/2409.03384v1",
            "published": "2024-09-05T09:43:25+00:00",
            "updated": "2024-09-05T09:43:25+00:00",
            "primary_category": "cs.AR",
            "categories": [
                "cs.AR",
                "cs.AI"
            ],
            "max_author_hindex": 18
        },
        "2409.03404": {
            "authors": [
                "Aoxiang Ning",
                "Minglong Xue",
                "Jinhong He",
                "Chengyun Song"
            ],
            "title": "KAN See In the Dark",
            "abstract": "Existing low-light image enhancement methods are difficult to fit the complex nonlinear relationship between normal and low-light images due to uneven illumination and noise effects. The recently proposed Kolmogorov-Arnold networks (KANs) feature spline-based convolutional layers and learnable activation functions, which can effectively capture nonlinear dependencies. In this paper, we design a KAN-Block based on KANs and innovatively apply it to low-light image enhancement. This method effectively alleviates the limitations of current methods constrained by linear network structures and lack of interpretability, further demonstrating the potential of KANs in low-level vision tasks. Given the poor perception of current low-light image enhancement methods and the stochastic nature of the inverse diffusion process, we further introduce frequency-domain perception for visually oriented enhancement. Extensive experiments demonstrate the competitive performance of our method on benchmark datasets. The code will be available at: https://github.com/AXNing/KSID}{https://github.com/AXNing/KSID.",
            "id": "2409.03404",
            "link": "http://arxiv.org/abs/2409.03404v1",
            "published": "2024-09-05T10:41:17+00:00",
            "updated": "2024-09-05T10:41:17+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 12
        },
        "2409.03463": {
            "authors": [
                "Lorenzo Bini",
                "Marco Sorbi",
                "Stephane Marchand-Maillet"
            ],
            "title": "Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks",
            "abstract": "Graph Neural Networks (GNNs) have become increasingly popular for effectively modeling data with graph structures. Recently, attention mechanisms have been integrated into GNNs to improve their ability to capture complex patterns. This paper presents the first comprehensive study revealing a critical, unexplored consequence of this integration: the emergence of Massive Activations (MAs) within attention layers. We introduce a novel method for detecting and analyzing MAs, focusing on edge features in different graph transformer architectures. Our study assesses various GNN models using benchmark datasets, including ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing the direct link between attention mechanisms and MAs generation in GNNs, (2) developing a robust definition and detection method for MAs based on activation ratio distributions, (3) introducing the Explicit Bias Term (EBT) as a potential countermeasure and exploring it as an adversarial framework to assess models robustness based on the presence or absence of MAs. Our findings highlight the prevalence and impact of attention-induced MAs across different architectures, such as GraphTransformer, GraphiT, and SAN. The study reveals the complex interplay between attention mechanisms, model architecture, dataset characteristics, and MAs emergence, providing crucial insights for developing more robust and reliable graph models.",
            "id": "2409.03463",
            "link": "http://arxiv.org/abs/2409.03463v1",
            "published": "2024-09-05T12:19:07+00:00",
            "updated": "2024-09-05T12:19:07+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 25
        },
        "2409.03516": {
            "authors": [
                "Jeongsoo Kim",
                "Jongho Nang",
                "Junsuk Choe"
            ],
            "title": "LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution",
            "abstract": "Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have demonstrated impressive performance. However, they suffer from significant complexity, resulting in high inference times and memory usage. Additionally, ViT models using Window Self-Attention (WSA) face challenges in processing regions outside their windows. To address these issues, we propose the Low-to-high Multi-Level Transformer (LMLT), which employs attention with varying feature sizes for each head. LMLT divides image features along the channel dimension, gradually reduces spatial size for lower heads, and applies self-attention to each head. This approach effectively captures both local and global information. By integrating the results from lower heads into higher heads, LMLT overcomes the window boundary issues in self-attention. Extensive experiments show that our model significantly reduces inference time and GPU memory usage while maintaining or even surpassing the performance of state-of-the-art ViT-based Image Super-Resolution methods. Our codes are availiable at https://github.com/jwgdmkj/LMLT.",
            "id": "2409.03516",
            "link": "http://arxiv.org/abs/2409.03516v1",
            "published": "2024-09-05T13:29:50+00:00",
            "updated": "2024-09-05T13:29:50+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 13
        },
        "2409.03597": {
            "authors": [
                "Yucong Zhang",
                "Xin Zou",
                "Jinshan Yang",
                "Wenjun Chen",
                "Faya Liang",
                "Ming Li"
            ],
            "title": "Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis",
            "abstract": "This paper presents the Multimodal Analyzing System for Laryngoscope (MASL), a system that combines audio and video data to automatically extract key segments and metrics from laryngeal videostroboscopic videos for clinical assessment. MASL integrates glottis detection with keyword spotting to analyze patient vocalizations and refine video highlights for better inspection of vocal cord movements. The system includes a strobing video extraction module that identifies frames by analyzing hue, saturation, and value fluctuations. MASL also provides effective metrics for vocal cord paralysis detection, employing a two-stage glottis segmentation process using U-Net followed by diffusion-based refinement to reduce false positives. Instead of glottal area waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis masks, evaluating both left and right vocal cords to detect unilateral vocal cord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes between left and right paralysis. Ablation studies and experiments on public and real-world datasets validate MASL's segmentation module and demonstrate its ability to provide reliable metrics for UVFP diagnosis.",
            "id": "2409.03597",
            "link": "http://arxiv.org/abs/2409.03597v1",
            "published": "2024-09-05T14:56:38+00:00",
            "updated": "2024-09-05T14:56:38+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 15
        },
        "2409.03717": {
            "authors": [
                "Justin Lovelace",
                "Soham Ray",
                "Kwangyoun Kim",
                "Kilian Q. Weinberger",
                "Felix Wu"
            ],
            "title": "Sample-Efficient Diffusion for Text-To-Speech Synthesis",
            "abstract": "This work introduces Sample-Efficient Speech Diffusion (SESD), an algorithm for effective speech synthesis in modest data regimes through latent diffusion. It is based on a novel diffusion architecture, that we call U-Audio Transformer (U-AT), that efficiently scales to long sequences and operates in the latent space of a pre-trained audio autoencoder. Conditioned on character-aware language model representations, SESD achieves impressive results despite training on less than 1k hours of speech - far less than current state-of-the-art systems. In fact, it synthesizes more intelligible speech than the state-of-the-art auto-regressive model, VALL-E, while using less than 2% the training data.",
            "id": "2409.03717",
            "link": "http://arxiv.org/abs/2409.03717v1",
            "published": "2024-09-01T20:34:36+00:00",
            "updated": "2024-09-01T20:34:36+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 75
        },
        "2409.03735": {
            "authors": [
                "Yan Shvartzshnaider",
                "Vasisht Duddu",
                "John Lacalamita"
            ],
            "title": "LLM-CI: Assessing Contextual Integrity Norms in Language Models",
            "abstract": "Large language models (LLMs), while memorizing parts of their training data scraped from the Internet, may also inadvertently encode societal preferences and norms. As these models are integrated into sociotechnical systems, it is crucial that the norms they encode align with societal expectations. These norms could vary across models, hyperparameters, optimization techniques, and datasets. This is especially challenging due to prompt sensitivity$-$small variations in prompts yield different responses, rendering existing assessment methodologies unreliable. There is a need for a comprehensive framework covering various models, optimization, and datasets, along with a reliable methodology to assess encoded norms.   We present LLM-CI, the first open-sourced framework to assess privacy norms encoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette methodology to assess the encoded norms across different contexts and LLMs. We propose the multi-prompt assessment methodology to address prompt sensitivity by assessing the norms from only the prompts that yield consistent responses across multiple variants. Using LLM-CI and our proposed methodology, we comprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior work, examining the impact of model properties (e.g., hyperparameters, capacity) and optimization strategies (e.g., alignment, quantization).",
            "id": "2409.03735",
            "link": "http://arxiv.org/abs/2409.03735v1",
            "published": "2024-09-05T17:50:31+00:00",
            "updated": "2024-09-05T17:50:31+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "cs.CY"
            ],
            "max_author_hindex": 9
        },
        "2409.03789": {
            "authors": [
                "Ibrahim Alshehri",
                "Adnan Alshehri",
                "Abdulrahman Almalki",
                "Majed Bamardouf",
                "Alaqsa Akbar"
            ],
            "title": "BreachSeek: A Multi-Agent Automated Penetration Tester",
            "abstract": "The increasing complexity and scale of modern digital environments have exposed significant gaps in traditional cybersecurity penetration testing methods, which are often time-consuming, labor-intensive, and unable to rapidly adapt to emerging threats. There is a critical need for an automated solution that can efficiently identify and exploit vulnerabilities across diverse systems without extensive human intervention. BreachSeek addresses this challenge by providing an AI-driven multi-agent software platform that leverages Large Language Models (LLMs) integrated through LangChain and LangGraph in Python. This system enables autonomous agents to conduct thorough penetration testing by identifying vulnerabilities, simulating a variety of cyberattacks, executing exploits, and generating comprehensive security reports. In preliminary evaluations, BreachSeek successfully exploited vulnerabilities in exploitable machines within local networks, demonstrating its practical effectiveness. Future developments aim to expand its capabilities, positioning it as an indispensable tool for cybersecurity professionals.",
            "id": "2409.03789",
            "link": "http://arxiv.org/abs/2409.03789v1",
            "published": "2024-08-31T19:15:38+00:00",
            "updated": "2024-08-31T19:15:38+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 4
        },
        "2409.03795": {
            "authors": [
                "Ayush Thakur"
            ],
            "title": "Security Implications and Mitigation Strategies in MPLS Networks",
            "abstract": "Multiprotocol Label Switching (MPLS) is a high-performance telecommunications technology that directs data from one network node to another based on short path labels rather than long network addresses. Its efficiency and scalability have made it a popular choice for large-scale and enterprise networks. However, as MPLS networks grow and evolve, they encounter various security challenges. This paper explores the security implications associated with MPLS networks, including risks such as label spoofing, traffic interception, and denial of service attacks. Additionally, it evaluates advanced mitigation strategies to address these vulnerabilities, leveraging mathematical models and security protocols to enhance MPLS network resilience. By integrating theoretical analysis with practical solutions, this paper aims to provide a comprehensive understanding of MPLS security and propose effective methods for safeguarding network infrastructure.",
            "id": "2409.03795",
            "link": "http://arxiv.org/abs/2409.03795v1",
            "published": "2024-09-04T09:21:47+00:00",
            "updated": "2024-09-04T09:21:47+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.NI"
            ],
            "max_author_hindex": 3
        },
        "2409.03805": {
            "authors": [
                "Mattias Tiger",
                "Daniel Jakobsson",
                "Anders Ynnerman",
                "Fredrik Heintz",
                "Daniel J\u00f6nsson"
            ],
            "title": "Exploratory Visual Analysis for Increasing Data Readiness in Artificial Intelligence Projects",
            "abstract": "We present experiences and lessons learned from increasing data readiness of heterogeneous data for artificial intelligence projects using visual analysis methods. Increasing the data readiness level involves understanding both the data as well as the context in which it is used, which are challenges well suitable to visual analysis. For this purpose, we contribute a mapping between data readiness aspects and visual analysis techniques suitable for different data types. We use the defined mapping to increase data readiness levels in use cases involving time-varying data, including numerical, categorical, and text. In addition to the mapping, we extend the data readiness concept to better take aspects of the task and solution into account and explicitly address distribution shifts during data collection time. We report on our experiences in using the presented visual analysis techniques to aid future artificial intelligence projects in raising the data readiness level.",
            "id": "2409.03805",
            "link": "http://arxiv.org/abs/2409.03805v1",
            "published": "2024-09-05T09:57:14+00:00",
            "updated": "2024-09-05T09:57:14+00:00",
            "primary_category": "stat.ME",
            "categories": [
                "stat.ME",
                "cs.AI"
            ],
            "max_author_hindex": 34
        },
        "2409.03811": {
            "authors": [
                "Federico Berto",
                "Chuanbo Hua",
                "Laurin Luttmann",
                "Jiwoo Son",
                "Junyoung Park",
                "Kyuree Ahn",
                "Changhyun Kwon",
                "Lin Xie",
                "Jinkyoo Park"
            ],
            "title": "PARCO: Learning Parallel Autoregressive Policies for Efficient Multi-Agent Combinatorial Optimization",
            "abstract": "Multi-agent combinatorial optimization problems such as routing and scheduling have great practical relevance but present challenges due to their NP-hard combinatorial nature, hard constraints on the number of possible agents, and hard-to-optimize objective functions. This paper introduces PARCO (Parallel AutoRegressive Combinatorial Optimization), a novel approach that learns fast surrogate solvers for multi-agent combinatorial problems with reinforcement learning by employing parallel autoregressive decoding. We propose a model with a Multiple Pointer Mechanism to efficiently decode multiple decisions simultaneously by different agents, enhanced by a Priority-based Conflict Handling scheme. Moreover, we design specialized Communication Layers that enable effective agent collaboration, thus enriching decision-making. We evaluate PARCO in representative multi-agent combinatorial problems in routing and scheduling and demonstrate that our learned solvers offer competitive results against both classical and neural baselines in terms of both solution quality and speed. We make our code openly available at https://github.com/ai4co/parco.",
            "id": "2409.03811",
            "link": "http://arxiv.org/abs/2409.03811v1",
            "published": "2024-09-05T17:49:18+00:00",
            "updated": "2024-09-05T17:49:18+00:00",
            "primary_category": "cs.MA",
            "categories": [
                "cs.MA",
                "cs.AI"
            ],
            "max_author_hindex": 25
        },
        "2409.03844": {
            "authors": [
                "Haoxuan Liu",
                "Zihao Wang",
                "Haorong Hong",
                "Youwei Feng",
                "Jiaxin Yu",
                "Han Diao",
                "Yunfei Xu",
                "Kejun Zhang"
            ],
            "title": "MetaBGM: Dynamic Soundtrack Transformation For Continuous Multi-Scene Experiences With Ambient Awareness And Personalization",
            "abstract": "This paper introduces MetaBGM, a groundbreaking framework for generating background music that adapts to dynamic scenes and real-time user interactions. We define multi-scene as variations in environmental contexts, such as transitions in game settings or movie scenes. To tackle the challenge of converting backend data into music description texts for audio generation models, MetaBGM employs a novel two-stage generation approach that transforms continuous scene and user state data into these texts, which are then fed into an audio generation model for real-time soundtrack creation. Experimental results demonstrate that MetaBGM effectively generates contextually relevant and dynamic background music for interactive applications.",
            "id": "2409.03844",
            "link": "http://arxiv.org/abs/2409.03844v1",
            "published": "2024-09-05T18:12:11+00:00",
            "updated": "2024-09-05T18:12:11+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.HC",
                "cs.MM",
                "eess.AS"
            ],
            "max_author_hindex": 21
        },
        "2409.03874": {
            "authors": [
                "Anoop R Katti",
                "Rui C. Gon\u00e7alves",
                "Rinchin Iakovlev"
            ],
            "title": "Cost-Control in Display Advertising: Theory vs Practice",
            "abstract": "In display advertising, advertisers want to achieve a marketing objective with constraints on budget and cost-per-outcome. This is usually formulated as an optimization problem that maximizes the total utility under constraints. The optimization is carried out in an online fashion in the dual space - for an incoming Ad auction, a bid is placed using an optimal bidding formula, assuming optimal values for the dual variables; based on the outcome of the previous auctions, the dual variables are updated in an online fashion. While this approach is theoretically sound, in practice, the dual variables are not optimal from the beginning, but rather converge over time. Specifically, for the cost-constraint, the convergence is asymptotic. As a result, we find that cost-control is ineffective. In this work, we analyse the shortcomings of the optimal bidding formula and propose a modification that deviates from the theoretical derivation. We simulate various practical scenarios and study the cost-control behaviors of the two algorithms. Through a large-scale evaluation on the real-word data, we show that the proposed modification reduces the cost violations by 50%, thereby achieving a better cost-control than the theoretical bidding formula.",
            "id": "2409.03874",
            "link": "http://arxiv.org/abs/2409.03874v1",
            "published": "2024-09-05T19:22:33+00:00",
            "updated": "2024-09-05T19:22:33+00:00",
            "primary_category": "cs.GT",
            "categories": [
                "cs.GT",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 6
        },
        "2409.03881": {
            "authors": [
                "Han Zheng",
                "Zhongxia Yan",
                "Cathy Wu"
            ],
            "title": "Multi-agent Path Finding for Mixed Autonomy Traffic Coordination",
            "abstract": "In the evolving landscape of urban mobility, the prospective integration of Connected and Automated Vehicles (CAVs) with Human-Driven Vehicles (HDVs) presents a complex array of challenges and opportunities for autonomous driving systems. While recent advancements in robotics have yielded Multi-Agent Path Finding (MAPF) algorithms tailored for agent coordination task characterized by simplified kinematics and complete control over agent behaviors, these solutions are inapplicable in mixed-traffic environments where uncontrollable HDVs must coexist and interact with CAVs. Addressing this gap, we propose the Behavior Prediction Kinematic Priority Based Search (BK-PBS), which leverages an offline-trained conditional prediction model to forecast HDV responses to CAV maneuvers, integrating these insights into a Priority Based Search (PBS) where the A* search proceeds over motion primitives to accommodate kinematic constraints. We compare BK-PBS with CAV planning algorithms derived by rule-based car-following models, and reinforcement learning. Through comprehensive simulation on a highway merging scenario across diverse scenarios of CAV penetration rate and traffic density, BK-PBS outperforms these baselines in reducing collision rates and enhancing system-level travel delay. Our work is directly applicable to many scenarios of multi-human multi-robot coordination.",
            "id": "2409.03881",
            "link": "http://arxiv.org/abs/2409.03881v1",
            "published": "2024-09-05T19:37:01+00:00",
            "updated": "2024-09-05T19:37:01+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.MA"
            ],
            "max_author_hindex": 61
        },
        "2409.03911": {
            "authors": [
                "\u00c8ric \u015aanchez",
                "Adri\u00e0 Molina",
                "Oriol Ramos Terrades"
            ],
            "title": "The Role of Generative Systems in Historical Photography Management: A Case Study on Catalan Archives",
            "abstract": "The use of image analysis in automated photography management is an increasing trend in heritage institutions. Such tools alleviate the human cost associated with the manual and expensive annotation of new data sources while facilitating fast access to the citizenship through online indexes and search engines. However, available tagging and description tools are usually designed around modern photographs in English, neglecting historical corpora in minoritized languages, each of which exhibits intrinsic particularities. The primary objective of this research is to study the quantitative contribution of generative systems in the description of historical sources. This is done by contextualizing the task of captioning historical photographs from the Catalan archives as a case study. Our findings provide practitioners with tools and directions on transfer learning for captioning models based on visual adaptation and linguistic proximity.",
            "id": "2409.03911",
            "link": "http://arxiv.org/abs/2409.03911v1",
            "published": "2024-09-05T21:08:25+00:00",
            "updated": "2024-09-05T21:08:25+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 6
        },
        "2409.03944": {
            "authors": [
                "Shashank Tripathi",
                "Omid Taheri",
                "Christoph Lassner",
                "Michael J. Black",
                "Daniel Holden",
                "Carsten Stoll"
            ],
            "title": "HUMOS: Human Motion Model Conditioned on Body Shape",
            "abstract": "Generating realistic human motion is essential for many computer vision and graphics applications. The wide variety of human body shapes and sizes greatly impacts how people move. However, most existing motion models ignore these differences, relying on a standardized, average body. This leads to uniform motion across different body types, where movements don't match their physical characteristics, limiting diversity. To solve this, we introduce a new approach to develop a generative motion model based on body shape. We show that it's possible to train this model using unpaired data by applying cycle consistency, intuitive physics, and stability constraints, which capture the relationship between identity and movement. The resulting model generates diverse, physically plausible, and dynamically stable human motions that are both quantitatively and qualitatively more realistic than current state-of-the-art methods. More details are available on our project page https://CarstenEpic.github.io/humos/.",
            "id": "2409.03944",
            "link": "http://arxiv.org/abs/2409.03944v1",
            "published": "2024-09-05T23:50:57+00:00",
            "updated": "2024-09-05T23:50:57+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 126
        },
        "2409.04040": {
            "authors": [
                "Huan Yang",
                "Deyu Zhang",
                "Yudong Zhao",
                "Yuanchun Li",
                "Yunxin Liu"
            ],
            "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV Leakage",
            "abstract": "Running LLMs on end devices has garnered significant attention recently due to their advantages in privacy preservation. With the advent of lightweight LLM models and specially designed GPUs, on-device LLM inference has achieved the necessary accuracy and performance metrics. However, we have identified that LLM inference on GPUs can leak privacy-sensitive intermediate information, specifically the KV pairs. An attacker could exploit these KV pairs to reconstruct the entire user conversation, leading to significant vulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE) and Trusted Execution Environments (TEE), are either too computation-intensive or resource-limited. To address these issues, we designed KV-Shield, which operates in two phases. In the initialization phase, it permutes the weight matrices so that all KV pairs are correspondingly permuted. During the runtime phase, the attention vector is inversely permuted to ensure the correctness of the layer output. All permutation-related operations are executed within the TEE, ensuring that insecure GPUs cannot access the original KV pairs, thus preventing conversation reconstruction. Finally, we theoretically analyze the correctness of KV-Shield, along with its advantages and overhead.",
            "id": "2409.04040",
            "link": "http://arxiv.org/abs/2409.04040v1",
            "published": "2024-09-06T06:16:55+00:00",
            "updated": "2024-09-06T06:16:55+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 37
        },
        "2409.04053": {
            "authors": [
                "Koen Kraaijveld",
                "Yifan Jiang",
                "Kaixin Ma",
                "Filip Ilievski"
            ],
            "title": "COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes",
            "abstract": "While visual question-answering (VQA) benchmarks have catalyzed the development of reasoning techniques, they have focused on vertical thinking. Effective problem-solving also necessitates lateral thinking, which remains understudied in AI and has not been used to test visual perception systems. To bridge this gap, we formulate visual lateral thinking as a multiple-choice question-answering task and describe a three-step taxonomy-driven methodology for instantiating task examples. Then, we develop COLUMBUS, a synthetic benchmark that applies the task pipeline to create QA sets with text and icon rebus puzzles based on publicly available collections of compounds and common phrases. COLUMBUS comprises over 1,000 puzzles, each with four answer candidates. While the SotA vision-language models (VLMs) achieve decent performance, our evaluation demonstrates a substantial gap between humans and models. VLMs benefit from human-curated descriptions but struggle to self-generate such representations at the right level of abstraction.",
            "id": "2409.04053",
            "link": "http://arxiv.org/abs/2409.04053v1",
            "published": "2024-09-06T06:49:55+00:00",
            "updated": "2024-09-06T06:49:55+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.04082": {
            "authors": [
                "Yi Tian",
                "Juan Andrade-Cetto"
            ],
            "title": "SDformerFlow: Spatiotemporal swin spikeformer for event-based optical flow estimation",
            "abstract": "Event cameras generate asynchronous and sparse event streams capturing changes in light intensity. They offer significant advantages over conventional frame-based cameras, such as a higher dynamic range and an extremely faster data rate, making them particularly useful in scenarios involving fast motion or challenging lighting conditions. Spiking neural networks (SNNs) share similar asynchronous and sparse characteristics and are well-suited for processing data from event cameras. Inspired by the potential of transformers and spike-driven transformers (spikeformers) in other computer vision tasks, we propose two solutions for fast and robust optical flow estimation for event cameras: STTFlowNet and SDformerFlow. STTFlowNet adopts a U-shaped artificial neural network (ANN) architecture with spatiotemporal shifted window self-attention (swin) transformer encoders, while SDformerFlow presents its fully spiking counterpart, incorporating swin spikeformer encoders. Furthermore, we present two variants of the spiking version with different neuron models. Our work is the first to make use of spikeformers for dense optical flow estimation. We conduct end-to-end training for all models using supervised learning. Our results yield state-of-the-art performance among SNN-based event optical flow methods on both the DSEC and MVSEC datasets, and show significant reduction in power consumption compared to the equivalent ANNs.",
            "id": "2409.04082",
            "link": "http://arxiv.org/abs/2409.04082v1",
            "published": "2024-09-06T07:48:18+00:00",
            "updated": "2024-09-06T07:48:18+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 33
        },
        "2409.04103": {
            "authors": [
                "Alberto Cattaneo",
                "Stephen Bonner",
                "Thomas Martynec",
                "Carlo Luschi",
                "Ian P Barrett",
                "Daniel Justus"
            ],
            "title": "The Role of Graph Topology in the Performance of Biomedical Knowledge Graph Completion Models",
            "abstract": "Knowledge Graph Completion has been increasingly adopted as a useful method for several tasks in biomedical research, like drug repurposing or drug-target identification. To that end, a variety of datasets and Knowledge Graph Embedding models has been proposed over the years. However, little is known about the properties that render a dataset useful for a given task and, even though theoretical properties of Knowledge Graph Embedding models are well understood, their practical utility in this field remains controversial. We conduct a comprehensive investigation into the topological properties of publicly available biomedical Knowledge Graphs and establish links to the accuracy observed in real-world applications. By releasing all model predictions and a new suite of analysis tools we invite the community to build upon our work and continue improving the understanding of these crucial applications.",
            "id": "2409.04103",
            "link": "http://arxiv.org/abs/2409.04103v1",
            "published": "2024-09-06T08:09:15+00:00",
            "updated": "2024-09-06T08:09:15+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "q-bio.QM"
            ],
            "max_author_hindex": 28
        },
        "2409.04230": {
            "authors": [
                "Inmo Jang"
            ],
            "title": "SPACE: A Python-based Simulator for Evaluating Decentralized Multi-Robot Task Allocation Algorithms",
            "abstract": "Swarm robotics explores the coordination of multiple robots to achieve collective goals, with collective decision-making being a central focus. This process involves decentralized robots autonomously making local decisions and communicating them, which influences the overall emergent behavior. Testing such decentralized algorithms in real-world scenarios with hundreds or more robots is often impractical, underscoring the need for effective simulation tools. We propose SPACE (Swarm Planning and Control Evaluation), a Python-based simulator designed to support the research, evaluation, and comparison of decentralized Multi-Robot Task Allocation (MRTA) algorithms. SPACE streamlines core algorithmic development by allowing users to implement decision-making algorithms as Python plug-ins, easily construct agent behavior trees via an intuitive GUI, and leverage built-in support for inter-agent communication and local task awareness. To demonstrate its practical utility, we implement and evaluate CBBA and GRAPE within the simulator, comparing their performance across different metrics, particularly in scenarios with dynamically introduced tasks. This evaluation shows the usefulness of SPACE in conducting rigorous and standardized comparisons of MRTA algorithms, helping to support future research in the field.",
            "id": "2409.04230",
            "link": "http://arxiv.org/abs/2409.04230v1",
            "published": "2024-09-06T12:38:24+00:00",
            "updated": "2024-09-06T12:38:24+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.MA"
            ],
            "max_author_hindex": 9
        },
        "2409.04249": {
            "authors": [
                "Xueyuan Han",
                "Zinuo Cai",
                "Yichu Zhang",
                "Chongxin Fan",
                "Junhan Liu",
                "Ruhui Ma",
                "Rajkumar Buyya"
            ],
            "title": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge Devices",
            "abstract": "The application of Transformer-based large models has achieved numerous success in recent years. However, the exponential growth in the parameters of large models introduces formidable memory challenge for edge deployment. Prior works to address this challenge mainly focus on optimizing the model structure and adopting memory swapping methods. However, the former reduces the inference accuracy, and the latter raises the inference latency. This paper introduces PIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces memory usage by incorporating dynamic memory management and minimizes inference latency by employing parallel model loading. Based on PIPELOAD mechanism, we present Hermes, a framework optimized for large model inference on edge devices. We evaluate Hermes on Transformer-based models of different sizes. Our experiments illustrate that Hermes achieves up to 4.24 X increase in inference speed and 86.7% lower memory consumption than the state-of-the-art pipeline mechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3% lower memory consumption for GPT-style models.",
            "id": "2409.04249",
            "link": "http://arxiv.org/abs/2409.04249v2",
            "published": "2024-09-06T12:55:49+00:00",
            "updated": "2024-09-09T18:25:01+00:00",
            "primary_category": "cs.DC",
            "categories": [
                "cs.DC",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 138
        },
        "2409.04272": {
            "authors": [
                "Changsong Liu",
                "Wei Zhang",
                "Yanyan Liu",
                "Mingyang Li",
                "Wenlin Li",
                "Yimeng Fan",
                "Xiangnan Bai",
                "Liang Zhangd"
            ],
            "title": "Cycle Pixel Difference Network for Crisp Edge Detection",
            "abstract": "Edge detection, as a fundamental task in computer vision, has garnered increasing attention. The advent of deep learning has significantly advanced this field. However, recent deep learning-based methods which rely on large-scale pre-trained weights cannot be trained from scratch, with very limited research addressing this issue. This paper proposes a novel cycle pixel difference convolution (CPDC), which effectively integrates image gradient information with modern convolution operations. Based on the CPDC, we develop a U-shape encoder-decoder model named CPD-Net, which is a purely end-to-end network. Additionally, to address the issue of edge thickness produced by most existing methods, we construct a multi-scale information enhancement module (MSEM) to enhance the discriminative ability of the model, thereby generating crisp and clean contour maps. Comprehensive experiments conducted on three standard benchmarks demonstrate that our method achieves competitive performance on the BSDS500 dataset (ODS=0.813), NYUD-V2 (ODS=0.760), and BIPED dataset (ODS=0.898). Our approach provides a novel perspective for addressing these challenges in edge detection.",
            "id": "2409.04272",
            "link": "http://arxiv.org/abs/2409.04272v1",
            "published": "2024-09-06T13:28:05+00:00",
            "updated": "2024-09-06T13:28:05+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 29
        },
        "2409.04306": {
            "authors": [
                "Felix Herrmann",
                "Sebastian Zach",
                "Jacopo Banfi",
                "Jan Peters",
                "Georgia Chalvatzaki",
                "Davide Tateo"
            ],
            "title": "Safe and Efficient Path Planning under Uncertainty via Deep Collision Probability Fields",
            "abstract": "Estimating collision probabilities between robots and environmental obstacles or other moving agents is crucial to ensure safety during path planning. This is an important building block of modern planning algorithms in many application scenarios such as autonomous driving, where noisy sensors perceive obstacles. While many approaches exist, they either provide too conservative estimates of the collision probabilities or are computationally intensive due to their sampling-based nature. To deal with these issues, we introduce Deep Collision Probability Fields, a neural-based approach for computing collision probabilities of arbitrary objects with arbitrary unimodal uncertainty distributions. Our approach relegates the computationally intensive estimation of collision probabilities via sampling at the training step, allowing for fast neural network inference of the constraints during planning. In extensive experiments, we show that Deep Collision Probability Fields can produce reasonably accurate collision probabilities (up to 10^{-3}) for planning and that our approach can be easily plugged into standard path planning approaches to plan safe paths on 2-D maps containing uncertain static and dynamic obstacles. Additional material, code, and videos are available at https://sites.google.com/view/ral-dcpf.",
            "id": "2409.04306",
            "link": "http://arxiv.org/abs/2409.04306v1",
            "published": "2024-09-06T14:28:41+00:00",
            "updated": "2024-09-06T14:28:41+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 42
        },
        "2409.04388": {
            "authors": [
                "Hangyu Qin",
                "Junbin Xiao",
                "Angela Yao"
            ],
            "title": "Question-Answering Dense Video Events",
            "abstract": "Multimodal Large Language Models (MLLMs) have shown excellent performance in question-answering of single-event videos. In this paper, we present question-answering dense video events, a novel task that requires answering and grounding the dense-event questions in long videos, thus challenging MLLMs to faithfully comprehend and reason about multiple events occurring over extended time periods. To facilitate the study, we construct DeVE-QA - a dataset featuring 78K questions about 26K events on 10.6K long videos. We then benchmark and show that existing MLLMs excelling at single-event QA struggle to perform well in DeVE-QA. For improvement, we propose DeVi, a novel training-free MLLM approach that highlights a hierarchical captioning module, a temporal event memory module, and a self-consistency checking module to respectively detect, contextualize and memorize, and ground dense-events in long videos for question answering. Extensive experiments show that DeVi is superior at answering dense-event questions and grounding relevant video moments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1 percent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA respectively.",
            "id": "2409.04388",
            "link": "http://arxiv.org/abs/2409.04388v3",
            "published": "2024-09-06T16:27:52+00:00",
            "updated": "2024-09-10T09:46:58+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ],
            "max_author_hindex": 12
        },
        "2409.04410": {
            "authors": [
                "Zhuoyan Luo",
                "Fengyuan Shi",
                "Yixiao Ge",
                "Yujiu Yang",
                "Limin Wang",
                "Ying Shan"
            ],
            "title": "Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation",
            "abstract": "We present Open-MAGVIT2, a family of auto-regressive image generation models ranging from 300M to 1.5B. The Open-MAGVIT2 project produces an open-source replication of Google's MAGVIT-v2 tokenizer, a tokenizer with a super-large codebook (i.e., $2^{18}$ codes), and achieves the state-of-the-art reconstruction performance (1.17 rFID) on ImageNet $256 \\times 256$. Furthermore, we explore its application in plain auto-regressive models and validate scalability properties. To assist auto-regressive models in predicting with a super-large vocabulary, we factorize it into two sub-vocabulary of different sizes by asymmetric token factorization, and further introduce \"next sub-token prediction\" to enhance sub-token interaction for better generation quality. We release all models and codes to foster innovation and creativity in the field of auto-regressive visual generation.",
            "id": "2409.04410",
            "link": "http://arxiv.org/abs/2409.04410v1",
            "published": "2024-09-06T17:14:53+00:00",
            "updated": "2024-09-06T17:14:53+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.04434": {
            "authors": [
                "Boris Knyazev",
                "Abhinav Moudgil",
                "Guillaume Lajoie",
                "Eugene Belilovsky",
                "Simon Lacoste-Julien"
            ],
            "title": "Accelerating Training with Neuron Interaction and Nowcasting Networks",
            "abstract": "Neural network training can be accelerated when a learnable update rule is used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable update rules can be costly and unstable to train and use. A simpler recently proposed approach to accelerate training is to use Adam for most of the optimization steps and periodically, only every few steps, nowcast (predict future) parameters. We improve this approach by Neuron interaction and Nowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neural networks to more accurately nowcast parameters by learning in a supervised way from a set of training trajectories over multiple tasks. We show that in some networks, such as Transformers, neuron connectivity is non-trivial. By accurately modeling neuron connectivity, we allow NiNo to accelerate Adam training by up to 50\\% in vision and language tasks.",
            "id": "2409.04434",
            "link": "http://arxiv.org/abs/2409.04434v1",
            "published": "2024-09-06T17:55:49+00:00",
            "updated": "2024-09-06T17:55:49+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ],
            "max_author_hindex": 44
        },
        "2409.04473": {
            "authors": [
                "Xianbing Zhao",
                "Lizhen Qu",
                "Tao Feng",
                "Jianfei Cai",
                "Buzhou Tang"
            ],
            "title": "Learning in Order! A Sequential Strategy to Learn Invariant Features for Multimodal Sentiment Analysis",
            "abstract": "This work proposes a novel and simple sequential learning strategy to train models on videos and texts for multimodal sentiment analysis. To estimate sentiment polarities on unseen out-of-distribution data, we introduce a multimodal model that is trained either in a single source domain or multiple source domains using our learning strategy. This strategy starts with learning domain invariant features from text, followed by learning sparse domain-agnostic features from videos, assisted by the selected features learned in text. Our experimental results demonstrate that our model achieves significantly better performance than the state-of-the-art approaches on average in both single-source and multi-source settings. Our feature selection procedure favors the features that are independent to each other and are strongly correlated with their polarity labels. To facilitate research on this topic, the source code of this work will be publicly available upon acceptance.",
            "id": "2409.04473",
            "link": "http://arxiv.org/abs/2409.04473v1",
            "published": "2024-09-05T11:55:05+00:00",
            "updated": "2024-09-05T11:55:05+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 35
        },
        "2409.04478": {
            "authors": [
                "Maheep Chaudhary",
                "Atticus Geiger"
            ],
            "title": "Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small",
            "abstract": "A popular new method in mechanistic interpretability is to train high-dimensional sparse autoencoders (SAEs) on neuron activations and use SAE features as the atomic units of analysis. However, the body of evidence on whether SAE feature spaces are useful for causal analysis is underdeveloped. In this work, we use the RAVEL benchmark to evaluate whether SAEs trained on hidden representations of GPT-2 small have sets of features that separately mediate knowledge of which country a city is in and which continent it is in. We evaluate four open-source SAEs for GPT-2 small against each other, with neurons serving as a baseline, and linear features learned via distributed alignment search (DAS) serving as a skyline. For each, we learn a binary mask to select features that will be patched to change the country of a city without changing the continent, or vice versa. Our results show that SAEs struggle to reach the neuron baseline, and none come close to the DAS skyline. We release code here: https://github.com/MaheepChaudhary/SAE-Ravel",
            "id": "2409.04478",
            "link": "http://arxiv.org/abs/2409.04478v1",
            "published": "2024-09-05T18:00:37+00:00",
            "updated": "2024-09-05T18:00:37+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.NE"
            ],
            "max_author_hindex": 17
        },
        "2409.04481": {
            "authors": [
                "Yizhen Zheng",
                "Huan Yee Koh",
                "Maddie Yang",
                "Li Li",
                "Lauren T. May",
                "Geoffrey I. Webb",
                "Shirui Pan",
                "George Church"
            ],
            "title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
            "abstract": "The integration of Large Language Models (LLMs) into the drug discovery and development field marks a significant paradigm shift, offering novel methodologies for understanding disease mechanisms, facilitating drug discovery, and optimizing clinical trial processes. This review highlights the expanding role of LLMs in revolutionizing various stages of the drug development pipeline. We investigate how these advanced computational models can uncover target-disease linkage, interpret complex biomedical data, enhance drug molecule design, predict drug efficacy and safety profiles, and facilitate clinical trial processes. Our paper aims to provide a comprehensive overview for researchers and practitioners in computational biology, pharmacology, and AI4Science by offering insights into the potential transformative impact of LLMs on drug discovery and development.",
            "id": "2409.04481",
            "link": "http://arxiv.org/abs/2409.04481v1",
            "published": "2024-09-06T02:03:38+00:00",
            "updated": "2024-09-06T02:03:38+00:00",
            "primary_category": "q-bio.QM",
            "categories": [
                "q-bio.QM",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 76
        },
        "2409.04519": {
            "authors": [
                "Jack Y. Araz",
                "Michael Spannowsky"
            ],
            "title": "The role of data embedding in quantum autoencoders for improved anomaly detection",
            "abstract": "The performance of Quantum Autoencoders (QAEs) in anomaly detection tasks is critically dependent on the choice of data embedding and ansatz design. This study explores the effects of three data embedding techniques, data re-uploading, parallel embedding, and alternate embedding, on the representability and effectiveness of QAEs in detecting anomalies. Our findings reveal that even with relatively simple variational circuits, enhanced data embedding strategies can substantially improve anomaly detection accuracy and the representability of underlying data across different datasets. Starting with toy examples featuring low-dimensional data, we visually demonstrate the effect of different embedding techniques on the representability of the model. We then extend our analysis to complex, higher-dimensional datasets, highlighting the significant impact of embedding methods on QAE performance.",
            "id": "2409.04519",
            "link": "http://arxiv.org/abs/2409.04519v1",
            "published": "2024-09-06T18:00:01+00:00",
            "updated": "2024-09-06T18:00:01+00:00",
            "primary_category": "quant-ph",
            "categories": [
                "quant-ph",
                "cs.AI",
                "cs.LG",
                "physics.data-an"
            ],
            "max_author_hindex": 57
        },
        "2409.04576": {
            "authors": [
                "Niklas Funk",
                "Julen Urain",
                "Joao Carvalho",
                "Vignesh Prasad",
                "Georgia Chalvatzaki",
                "Jan Peters"
            ],
            "title": "ActionFlow: Equivariant, Accurate, and Efficient Policies with Spatially Symmetric Flow Matching",
            "abstract": "Spatial understanding is a critical aspect of most robotic tasks, particularly when generalization is important. Despite the impressive results of deep generative models in complex manipulation tasks, the absence of a representation that encodes intricate spatial relationships between observations and actions often limits spatial generalization, necessitating large amounts of demonstrations. To tackle this problem, we introduce a novel policy class, ActionFlow. ActionFlow integrates spatial symmetry inductive biases while generating expressive action sequences. On the representation level, ActionFlow introduces an SE(3) Invariant Transformer architecture, which enables informed spatial reasoning based on the relative SE(3) poses between observations and actions. For action generation, ActionFlow leverages Flow Matching, a state-of-the-art deep generative model known for generating high-quality samples with fast inference - an essential property for feedback control. In combination, ActionFlow policies exhibit strong spatial and locality biases and SE(3)-equivariant action generation. Our experiments demonstrate the effectiveness of ActionFlow and its two main components on several simulated and real-world robotic manipulation tasks and confirm that we can obtain equivariant, accurate, and efficient policies with spatially symmetric flow matching. Project website: https://flowbasedpolicies.github.io/",
            "id": "2409.04576",
            "link": "http://arxiv.org/abs/2409.04576v1",
            "published": "2024-09-06T19:30:36+00:00",
            "updated": "2024-09-06T19:30:36+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 18
        },
        "2409.04585": {
            "authors": [
                "Wei Wen",
                "Quanyu Zhu",
                "Weiwei Chu",
                "Wen-Yen Chen",
                "Jiyan Yang"
            ],
            "title": "CubicML: Automated ML for Distributed ML Systems Co-design with ML Prediction of Performance",
            "abstract": "Scaling up deep learning models has been proven effective to improve intelligence of machine learning (ML) models, especially for industry recommendation models and large language models. The co-design of distributed ML systems and algorithms (to maximize training performance) plays a pivotal role for its success. As it scales, the number of co-design hyper-parameters grows rapidly which brings challenges to feasibly find the optimal setup for system performance maximization. In this paper, we propose CubicML which uses ML to automatically optimize training performance of distributed ML systems. In CubicML, we use a ML model as a proxy to predict the training performance for search efficiency and performance modeling flexibility. We proved that CubicML can effectively optimize training speed of in-house ads recommendation models and large language models at Meta.",
            "id": "2409.04585",
            "link": "http://arxiv.org/abs/2409.04585v1",
            "published": "2024-09-06T19:55:21+00:00",
            "updated": "2024-09-06T19:55:21+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.DC"
            ],
            "max_author_hindex": 26
        },
        "2409.04602": {
            "authors": [
                "Guang Ping He"
            ],
            "title": "Training quantum machine learning model on cloud without uploading the data",
            "abstract": "Based on the linearity of quantum unitary operations, we propose a method that runs the parameterized quantum circuits before encoding the input data. It enables a dataset owner to train machine learning models on quantum cloud computation platforms, without the risk of leaking the information of the data. It is also capable of encoding a huge number of data effectively at a later time using classical computations, thus saving the runtime on quantum computation devices. The trained quantum machine learning model can be run completely on classical computers, so that the dataset owner does not need to have any quantum hardware, nor even quantum simulators. Moreover, the method can mitigate the encoding bottom neck by reducing the required circuit depth from $O(2^{n})$ to $n/2$. These results manifest yet another advantage of quantum and quantum-inspired machine learning models over existing classical neural networks, and broaden the approaches for data security.",
            "id": "2409.04602",
            "link": "http://arxiv.org/abs/2409.04602v1",
            "published": "2024-09-06T20:14:52+00:00",
            "updated": "2024-09-06T20:14:52+00:00",
            "primary_category": "quant-ph",
            "categories": [
                "quant-ph",
                "cs.AI",
                "cs.CR",
                "cs.LG"
            ],
            "max_author_hindex": 15
        },
        "2409.04615": {
            "authors": [
                "S. Hemati",
                "Krishna R. Kalari",
                "H. R. Tizhoosh"
            ],
            "title": "A Short Survey on Set-Based Aggregation Techniques for Single-Vector WSI Representation in Digital Pathology",
            "abstract": "Digital pathology is revolutionizing the field of pathology by enabling the digitization, storage, and analysis of tissue samples as whole slide images (WSIs). WSIs are gigapixel files that capture the intricate details of tissue samples, providing a rich source of information for diagnostic and research purposes. However, due to their enormous size, representing these images as one compact vector is essential for many computational pathology tasks, such as search and retrieval, to ensure efficiency and scalability. Most current methods are \"patch-oriented,\" meaning they divide WSIs into smaller patches for processing, which prevents a holistic analysis of the entire slide. Additionally, the necessity for compact representation is driven by the expensive high-performance storage required for WSIs. Not all hospitals have access to such extensive storage solutions, leading to potential disparities in healthcare quality and accessibility. This paper provides an overview of existing set-based approaches to single-vector WSI representation, highlighting the innovations that allow for more efficient and effective use of these complex images in digital pathology, thus addressing both computational challenges and storage limitations.",
            "id": "2409.04615",
            "link": "http://arxiv.org/abs/2409.04615v1",
            "published": "2024-09-06T20:56:25+00:00",
            "updated": "2024-09-06T20:56:25+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 45
        },
        "2409.04631": {
            "authors": [
                "Saghir Alfasly",
                "Ghazal Alabtah",
                "Sobhan Hemati",
                "Krishna Rani Kalari",
                "H. R. Tizhoosh"
            ],
            "title": "Zero-Shot Whole Slide Image Retrieval in Histopathology Using Embeddings of Foundation Models",
            "abstract": "We have tested recently published foundation models for histopathology for image retrieval. We report macro average of F1 score for top-1 retrieval, majority of top-3 retrievals, and majority of top-5 retrievals. We perform zero-shot retrievals, i.e., we do not alter embeddings and we do not train any classifier. As test data, we used diagnostic slides of TCGA, The Cancer Genome Atlas, consisting of 23 organs and 117 cancer subtypes. As a search platform we used Yottixel that enabled us to perform WSI search using patches. Achieved F1 scores show low performance, e.g., for top-5 retrievals, 27% +/- 13% (Yottixel-DenseNet), 42% +/- 14% (Yottixel-UNI), 40%+/-13% (Yottixel-Virchow), 41%+/-13% (Yottixel-GigaPath), and 41%+/-14% (GigaPath WSI).",
            "id": "2409.04631",
            "link": "http://arxiv.org/abs/2409.04631v2",
            "published": "2024-09-06T21:43:00+00:00",
            "updated": "2024-09-12T15:37:30+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 45
        },
        "2409.04641": {
            "authors": [
                "Ian Cannon",
                "Washington Garcia",
                "Thomas Gresavage",
                "Joseph Saurine",
                "Ian Leong",
                "Jared Culbertson"
            ],
            "title": "Stacked Universal Successor Feature Approximators for Safety in Reinforcement Learning",
            "abstract": "Real-world problems often involve complex objective structures that resist distillation into reinforcement learning environments with a single objective. Operation costs must be balanced with multi-dimensional task performance and end-states' effects on future availability, all while ensuring safety for other agents in the environment and the reinforcement learning agent itself. System redundancy through secondary backup controllers has proven to be an effective method to ensure safety in real-world applications where the risk of violating constraints is extremely high. In this work, we investigate the utility of a stacked, continuous-control variation of universal successor feature approximation (USFA) adapted for soft actor-critic (SAC) and coupled with a suite of secondary safety controllers, which we call stacked USFA for safety (SUSFAS). Our method improves performance on secondary objectives compared to SAC baselines using an intervening secondary controller such as a runtime assurance (RTA) controller.",
            "id": "2409.04641",
            "link": "http://arxiv.org/abs/2409.04641v1",
            "published": "2024-09-06T22:20:07+00:00",
            "updated": "2024-09-06T22:20:07+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 6
        },
        "2409.04653": {
            "authors": [
                "Marcos Abel Zuzu\u00e1rregui",
                "Stefano Carpin"
            ],
            "title": "Solving Stochastic Orienteering Problems with Chance Constraints Using a GNN Powered Monte Carlo Tree Search",
            "abstract": "Leveraging the power of a graph neural network (GNN) with message passing, we present a Monte Carlo Tree Search (MCTS) method to solve stochastic orienteering problems with chance constraints. While adhering to an assigned travel budget the algorithm seeks to maximize collected reward while incurring stochastic travel costs. In this context, the acceptable probability of exceeding the assigned budget is expressed as a chance constraint. Our MCTS solution is an online and anytime algorithm alternating planning and execution that determines the next vertex to visit by continuously monitoring the remaining travel budget. The novelty of our work is that the rollout phase in the MCTS framework is implemented using a message passing GNN, predicting both the utility and failure probability of each available action. This allows to enormously expedite the search process. Our experimental evaluation shows that with the proposed method and architecture we manage to efficiently solve complex problem instances while incurring in moderate losses in terms of collected reward. Moreover, we demonstrate how the approach is capable of generalizing beyond the characteristics of the training dataset. The paper's website, open-source code, and supplementary documentation can be found at ucmercedrobotics.github.io/gnn-sop.",
            "id": "2409.04653",
            "link": "http://arxiv.org/abs/2409.04653v1",
            "published": "2024-09-06T23:31:01+00:00",
            "updated": "2024-09-06T23:31:01+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 31
        },
        "2409.04704": {
            "authors": [
                "Cheng Wan",
                "Chenjie Xie",
                "Longfei Liu",
                "Dan Wu",
                "Ye Li"
            ],
            "title": "A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting",
            "abstract": "Continuous blood pressure (BP) monitoring is essential for timely diagnosis and intervention in critical care settings. However, BP varies significantly across individuals, this inter-patient variability motivates the development of personalized models tailored to each patient's physiology. In this work, we propose a personalized BP forecasting model mainly using electrocardiogram (ECG) and photoplethysmogram (PPG) signals. This time-series model incorporates 2D representation learning to capture complex physiological relationships. Experiments are conducted on datasets collected from three diverse scenarios with BP measurements from 60 subjects total. Results demonstrate that the model achieves accurate and robust BP forecasts across scenarios within the Association for the Advancement of Medical Instrumentation (AAMI) standard criteria. This reliable early detection of abnormal fluctuations in BP is crucial for at-risk patients undergoing surgery or intensive care. The proposed model provides a valuable addition for continuous BP tracking to reduce mortality and improve prognosis.",
            "id": "2409.04704",
            "link": "http://arxiv.org/abs/2409.04704v1",
            "published": "2024-09-07T04:24:15+00:00",
            "updated": "2024-09-07T04:24:15+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 23
        },
        "2409.04720": {
            "authors": [
                "Junyu Gao",
                "Mengyuan Chen",
                "Liangyu Xiang",
                "Changsheng Xu"
            ],
            "title": "A Comprehensive Survey on Evidential Deep Learning and Its Applications",
            "abstract": "Reliable uncertainty estimation has become a crucial requirement for the industrial deployment of deep learning algorithms, particularly in high-risk applications such as autonomous driving and medical diagnosis. However, mainstream uncertainty estimation methods, based on deep ensembling or Bayesian neural networks, generally impose substantial computational overhead. To address this challenge, a novel paradigm called Evidential Deep Learning (EDL) has emerged, providing reliable uncertainty estimation with minimal additional computation in a single forward pass. This survey provides a comprehensive overview of the current research on EDL, designed to offer readers a broad introduction to the field without assuming prior knowledge. Specifically, we first delve into the theoretical foundation of EDL, the subjective logic theory, and discuss its distinctions from other uncertainty estimation frameworks. We further present existing theoretical advancements in EDL from four perspectives: reformulating the evidence collection process, improving uncertainty estimation via OOD samples, delving into various training strategies, and evidential regression networks. Thereafter, we elaborate on its extensive applications across various machine learning paradigms and downstream tasks. In the end, an outlook on future directions for better performances and broader adoption of EDL is provided, highlighting potential research avenues.",
            "id": "2409.04720",
            "link": "http://arxiv.org/abs/2409.04720v1",
            "published": "2024-09-07T05:55:06+00:00",
            "updated": "2024-09-07T05:55:06+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 71
        },
        "2409.04723": {
            "authors": [
                "Debaditya Shome",
                "Nasim Montazeri Ghahjaverestan",
                "Ali Etemad"
            ],
            "title": "NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series",
            "abstract": "Sleep is known to be a key factor in emotional regulation and overall mental health. In this study, we explore the integration of sleep measures from the previous night into wearable-based mood recognition. To this end, we propose NapTune, a novel prompt-tuning framework that utilizes sleep-related measures as additional inputs to a frozen pre-trained wearable time-series encoder by adding and training lightweight prompt parameters to each Transformer layer. Through rigorous empirical evaluation, we demonstrate that the inclusion of sleep data using NapTune not only improves mood recognition performance across different wearable time-series namely ECG, PPG, and EDA, but also makes it more sample-efficient. Our method demonstrates significant improvements over the best baselines and unimodal variants. Furthermore, we analyze the impact of adding sleep-related measures on recognizing different moods as well as the influence of individual sleep-related measures.",
            "id": "2409.04723",
            "link": "http://arxiv.org/abs/2409.04723v1",
            "published": "2024-09-07T06:06:04+00:00",
            "updated": "2024-09-07T06:06:04+00:00",
            "primary_category": "eess.SP",
            "categories": [
                "eess.SP",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 25
        },
        "2409.04740": {
            "authors": [
                "Fu Lin",
                "Jiasheng Shi",
                "Shijie Luo",
                "Qinpei Zhao",
                "Weixiong Rao",
                "Lei Chen"
            ],
            "title": "Up-sampling-only and Adaptive Mesh-based GNN for Simulating Physical Systems",
            "abstract": "Traditional simulation of complex mechanical systems relies on numerical solvers of Partial Differential Equations (PDEs), e.g., using the Finite Element Method (FEM). The FEM solvers frequently suffer from intensive computation cost and high running time. Recent graph neural network (GNN)-based simulation models can improve running time meanwhile with acceptable accuracy. Unfortunately, they are hard to tailor GNNs for complex mechanical systems, including such disadvantages as ineffective representation and inefficient message propagation (MP). To tackle these issues, in this paper, with the proposed Up-sampling-only and Adaptive MP techniques, we develop a novel hierarchical Mesh Graph Network, namely UA-MGN, for efficient and effective mechanical simulation. Evaluation on two synthetic and one real datasets demonstrates the superiority of the UA-MGN. For example, on the Beam dataset, compared to the state-of-the-art MS-MGN, UA-MGN leads to 40.99% lower errors but using only 43.48% fewer network parameters and 4.49% fewer floating point operations (FLOPs).",
            "id": "2409.04740",
            "link": "http://arxiv.org/abs/2409.04740v1",
            "published": "2024-09-07T07:09:58+00:00",
            "updated": "2024-09-07T07:09:58+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CE"
            ],
            "max_author_hindex": 27
        },
        "2409.04744": {
            "authors": [
                "Yongxin Deng",
                "Xihe Qiu",
                "Xiaoyu Tan",
                "Wei Chu",
                "Yinghui Xu"
            ],
            "title": "LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs",
            "abstract": "The uncertainty inherent in the environmental transition model of Reinforcement Learning (RL) necessitates a careful balance between exploration and exploitation to optimize the use of computational resources for accurately estimating an agent's expected reward. Achieving balance in control systems is particularly challenging in scenarios with sparse rewards. However, given the extensive prior knowledge available for many environments, it is redundant to begin learning from scratch in such settings. To address this, we introduce \\textbf{L}anguage \\textbf{M}odel \\textbf{G}uided \\textbf{T}rade-offs (i.e., \\textbf{LMGT}), a novel, sample-efficient framework that leverages the comprehensive prior knowledge embedded in Large Language Models (LLMs) and their adeptness at processing non-standard data forms, such as wiki tutorials. LMGT proficiently manages the exploration-exploitation trade-off by employing reward shifts guided by LLMs, which direct agents' exploration endeavors, thereby improving sample efficiency. We have thoroughly tested LMGT across various RL tasks and deployed it in industrial-grade RL recommendation systems, where it consistently outperforms baseline methods. The results indicate that our framework can significantly reduce the time cost required during the training phase in RL.",
            "id": "2409.04744",
            "link": "http://arxiv.org/abs/2409.04744v1",
            "published": "2024-09-07T07:40:43+00:00",
            "updated": "2024-09-07T07:40:43+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 19
        },
        "2409.04819": {
            "authors": [
                "Lars Nieradzik",
                "Henrike Stephani",
                "Janis Keuper"
            ],
            "title": "Top-GAP: Integrating Size Priors in CNNs for more Interpretability, Robustness, and Bias Mitigation",
            "abstract": "This paper introduces Top-GAP, a novel regularization technique that enhances the explainability and robustness of convolutional neural networks. By constraining the spatial size of the learned feature representation, our method forces the network to focus on the most salient image regions, effectively reducing background influence. Using adversarial attacks and the Effective Receptive Field, we show that Top-GAP directs more attention towards object pixels rather than the background. This leads to enhanced interpretability and robustness. We achieve over 50% robust accuracy on CIFAR-10 with PGD $\\epsilon=\\frac{8}{255}$ and $20$ iterations while maintaining the original clean accuracy. Furthermore, we see increases of up to 5% accuracy against distribution shifts. Our approach also yields more precise object localization, as evidenced by up to 25% improvement in Intersection over Union (IOU) compared to methods like GradCAM and Recipro-CAM.",
            "id": "2409.04819",
            "link": "http://arxiv.org/abs/2409.04819v1",
            "published": "2024-09-07T13:24:59+00:00",
            "updated": "2024-09-07T13:24:59+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.04820": {
            "authors": [
                "Tom Bekor",
                "Niv Nayman",
                "Lihi Zelnik-Manor"
            ],
            "title": "FreeAugment: Data Augmentation Search Across All Degrees of Freedom",
            "abstract": "Data augmentation has become an integral part of deep learning, as it is known to improve the generalization capabilities of neural networks. Since the most effective set of image transformations differs between tasks and domains, automatic data augmentation search aims to alleviate the extreme burden of manually finding the optimal image transformations. However, current methods are not able to jointly optimize all degrees of freedom: (1) the number of transformations to be applied, their (2) types, (3) order, and (4) magnitudes. Many existing methods risk picking the same transformation more than once, limit the search to two transformations only, or search for the number of transformations exhaustively or iteratively in a myopic manner. Our approach, FreeAugment, is the first to achieve global optimization of all four degrees of freedom simultaneously, using a fully differentiable method. It efficiently learns the number of transformations and a probability distribution over their permutations, inherently refraining from redundant repetition while sampling. Our experiments demonstrate that this joint learning of all degrees of freedom significantly improves performance, achieving state-of-the-art results on various natural image benchmarks and beyond across other domains. Project page at https://tombekor.github.io/FreeAugment-web",
            "id": "2409.04820",
            "link": "http://arxiv.org/abs/2409.04820v1",
            "published": "2024-09-07T13:26:12+00:00",
            "updated": "2024-09-07T13:26:12+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "math.OC",
                "I.2; I.4"
            ],
            "max_author_hindex": 38
        },
        "2409.04832": {
            "authors": [
                "Xuefeng Gao",
                "Jiale Zha",
                "Xun Yu Zhou"
            ],
            "title": "Reward-Directed Score-Based Diffusion Models via q-Learning",
            "abstract": "We propose a new reinforcement learning (RL) formulation for training continuous-time score-based diffusion models for generative AI to generate samples that maximize reward functions while keeping the generated distributions close to the unknown target data distributions. Different from most existing studies, our formulation does not involve any pretrained model for the unknown score functions of the noise-perturbed data distributions. We present an entropy-regularized continuous-time RL problem and show that the optimal stochastic policy has a Gaussian distribution with a known covariance matrix. Based on this result, we parameterize the mean of Gaussian policies and develop an actor-critic type (little) q-learning algorithm to solve the RL problem. A key ingredient in our algorithm design is to obtain noisy observations from the unknown score function via a ratio estimator. Numerically, we show the effectiveness of our approach by comparing its performance with two state-of-the-art RL methods that fine-tune pretrained models. Finally, we discuss extensions of our RL formulation to probability flow ODE implementation of diffusion models and to conditional diffusion models.",
            "id": "2409.04832",
            "link": "http://arxiv.org/abs/2409.04832v1",
            "published": "2024-09-07T13:55:45+00:00",
            "updated": "2024-09-07T13:55:45+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "math.OC"
            ],
            "max_author_hindex": 55
        },
        "2409.04834": {
            "authors": [
                "Lingzhe Zhang",
                "Tong Jia",
                "Kangjin Wang",
                "Mengxi Jia",
                "Yang Yong",
                "Ying Li"
            ],
            "title": "Reducing Events to Augment Log-based Anomaly Detection Models: An Empirical Study",
            "abstract": "As software systems grow increasingly intricate, the precise detection of anomalies have become both essential and challenging. Current log-based anomaly detection methods depend heavily on vast amounts of log data leading to inefficient inference and potential misguidance by noise logs. However, the quantitative effects of log reduction on the effectiveness of anomaly detection remain unexplored. Therefore, we first conduct a comprehensive study on six distinct models spanning three datasets. Through the study, the impact of log quantity and their effectiveness in representing anomalies is qualifies, uncovering three distinctive log event types that differently influence model performance. Drawing from these insights, we propose LogCleaner: an efficient methodology for the automatic reduction of log events in the context of anomaly detection. Serving as middleware between software systems and models, LogCleaner continuously updates and filters anti-events and duplicative-events in the raw generated logs. Experimental outcomes highlight LogCleaner's capability to reduce over 70% of log events in anomaly detection, accelerating the model's inference speed by approximately 300%, and universally improving the performance of models for anomaly detection.",
            "id": "2409.04834",
            "link": "http://arxiv.org/abs/2409.04834v1",
            "published": "2024-09-07T14:02:02+00:00",
            "updated": "2024-09-07T14:02:02+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 37
        },
        "2409.04840": {
            "authors": [
                "Zakaria Mhammedi"
            ],
            "title": "Sample- and Oracle-Efficient Reinforcement Learning for MDPs with Linearly-Realizable Value Functions",
            "abstract": "Designing sample-efficient and computationally feasible reinforcement learning (RL) algorithms is particularly challenging in environments with large or infinite state and action spaces. In this paper, we advance this effort by presenting an efficient algorithm for Markov Decision Processes (MDPs) where the state-action value function of any policy is linear in a given feature map. This challenging setting can model environments with infinite states and actions, strictly generalizes classic linear MDPs, and currently lacks a computationally efficient algorithm under online access to the MDP. Specifically, we introduce a new RL algorithm that efficiently finds a near-optimal policy in this setting, using a number of episodes and calls to a cost-sensitive classification (CSC) oracle that are both polynomial in the problem parameters. Notably, our CSC oracle can be efficiently implemented when the feature dimension is constant, representing a clear improvement over state-of-the-art methods, which require solving non-convex problems with horizon-many variables and can incur computational costs that are \\emph{exponential} in the horizon.",
            "id": "2409.04840",
            "link": "http://arxiv.org/abs/2409.04840v1",
            "published": "2024-09-07T14:38:05+00:00",
            "updated": "2024-09-07T14:38:05+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 12
        },
        "2409.04882": {
            "authors": [
                "Mike Zhang",
                "Yuntao Ma",
                "Takahiro Miki",
                "Marco Hutter"
            ],
            "title": "Learning to Open and Traverse Doors with a Legged Manipulator",
            "abstract": "Using doors is a longstanding challenge in robotics and is of significant practical interest in giving robots greater access to human-centric spaces. The task is challenging due to the need for online adaptation to varying door properties and precise control in manipulating the door panel and navigating through the confined doorway. To address this, we propose a learning-based controller for a legged manipulator to open and traverse through doors. The controller is trained using a teacher-student approach in simulation to learn robust task behaviors as well as estimate crucial door properties during the interaction. Unlike previous works, our approach is a single control policy that can handle both push and pull doors through learned behaviour which infers the opening direction during deployment without prior knowledge. The policy was deployed on the ANYmal legged robot with an arm and achieved a success rate of 95.0% in repeated trials conducted in an experimental setting. Additional experiments validate the policy's effectiveness and robustness to various doors and disturbances. A video overview of the method and experiments can be found at youtu.be/tQDZXN_k5NU.",
            "id": "2409.04882",
            "link": "http://arxiv.org/abs/2409.04882v1",
            "published": "2024-09-07T18:27:46+00:00",
            "updated": "2024-09-07T18:27:46+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 61
        },
        "2409.04896": {
            "authors": [
                "Kavish Chawla"
            ],
            "title": "Reinforcement Learning-Based Adaptive Load Balancing for Dynamic Cloud Environments",
            "abstract": "Efficient load balancing is crucial in cloud computing environments to ensure optimal resource utilization, minimize response times, and prevent server overload. Traditional load balancing algorithms, such as round-robin or least connections, are often static and unable to adapt to the dynamic and fluctuating nature of cloud workloads. In this paper, we propose a novel adaptive load balancing framework using Reinforcement Learning (RL) to address these challenges. The RL-based approach continuously learns and improves the distribution of tasks by observing real-time system performance and making decisions based on traffic patterns and resource availability. Our framework is designed to dynamically reallocate tasks to minimize latency and ensure balanced resource usage across servers. Experimental results show that the proposed RL-based load balancer outperforms traditional algorithms in terms of response time, resource utilization, and adaptability to changing workloads. These findings highlight the potential of AI-driven solutions for enhancing the efficiency and scalability of cloud infrastructures.",
            "id": "2409.04896",
            "link": "http://arxiv.org/abs/2409.04896v1",
            "published": "2024-09-07T19:40:48+00:00",
            "updated": "2024-09-07T19:40:48+00:00",
            "primary_category": "cs.DC",
            "categories": [
                "cs.DC",
                "cs.AI",
                "cs.NI",
                "68M14, 68T05"
            ],
            "max_author_hindex": 0
        },
        "2409.04909": {
            "authors": [
                "Shivesh Prakash"
            ],
            "title": "Efficient Training of Transformers for Molecule Property Prediction on Small-scale Datasets",
            "abstract": "The blood-brain barrier (BBB) serves as a protective barrier that separates the brain from the circulatory system, regulating the passage of substances into the central nervous system. Assessing the BBB permeability of potential drugs is crucial for effective drug targeting. However, traditional experimental methods for measuring BBB permeability are challenging and impractical for large-scale screening. Consequently, there is a need to develop computational approaches to predict BBB permeability. This paper proposes a GPS Transformer architecture augmented with Self Attention, designed to perform well in the low-data regime. The proposed approach achieved a state-of-the-art performance on the BBB permeability prediction task using the BBBP dataset, surpassing existing models. With a ROC-AUC of 78.8%, the approach sets a state-of-the-art by 5.5%. We demonstrate that standard Self Attention coupled with GPS transformer performs better than other variants of attention coupled with GPS Transformer.",
            "id": "2409.04909",
            "link": "http://arxiv.org/abs/2409.04909v1",
            "published": "2024-09-07T21:07:12+00:00",
            "updated": "2024-09-07T21:07:12+00:00",
            "primary_category": "q-bio.QM",
            "categories": [
                "q-bio.QM",
                "cs.AI",
                "cs.CE",
                "cs.LG"
            ],
            "max_author_hindex": 18
        },
        "2409.04953": {
            "authors": [
                "Francesco Papaleo",
                "Xavier Lizarraga-Seijas",
                "Frederic Font"
            ],
            "title": "Evaluating Neural Networks Architectures for Spring Reverb Modelling",
            "abstract": "Reverberation is a key element in spatial audio perception, historically achieved with the use of analogue devices, such as plate and spring reverb, and in the last decades with digital signal processing techniques that have allowed different approaches for Virtual Analogue Modelling (VAM). The electromechanical functioning of the spring reverb makes it a nonlinear system that is difficult to fully emulate in the digital domain with white-box modelling techniques. In this study, we compare five different neural network architectures, including convolutional and recurrent models, to assess their effectiveness in replicating the characteristics of this audio effect. The evaluation is conducted on two datasets at sampling rates of 16 kHz and 48 kHz. This paper specifically focuses on neural audio architectures that offer parametric control, aiming to advance the boundaries of current black-box modelling techniques in the domain of spring reverberation.",
            "id": "2409.04953",
            "link": "http://arxiv.org/abs/2409.04953v1",
            "published": "2024-09-08T02:37:42+00:00",
            "updated": "2024-09-08T02:37:42+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI"
            ],
            "max_author_hindex": 34
        },
        "2409.04971": {
            "authors": [
                "Luca Della Libera"
            ],
            "title": "Soft Actor-Critic with Beta Policy via Implicit Reparameterization Gradients",
            "abstract": "Recent advances in deep reinforcement learning have achieved impressive results in a wide range of complex tasks, but poor sample efficiency remains a major obstacle to real-world deployment. Soft actor-critic (SAC) mitigates this problem by combining stochastic policy optimization and off-policy learning, but its applicability is restricted to distributions whose gradients can be computed through the reparameterization trick. This limitation excludes several important examples such as the beta distribution, which was shown to improve the convergence rate of actor-critic algorithms in high-dimensional continuous control problems thanks to its bounded support. To address this issue, we investigate the use of implicit reparameterization, a powerful technique that extends the class of reparameterizable distributions. In particular, we use implicit reparameterization gradients to train SAC with the beta policy on simulated robot locomotion environments and compare its performance with common baselines. Experimental results show that the beta policy is a viable alternative, as it outperforms the normal policy and is on par with the squashed normal policy, which is the go-to choice for SAC. The code is available at https://github.com/lucadellalib/sac-beta.",
            "id": "2409.04971",
            "link": "http://arxiv.org/abs/2409.04971v1",
            "published": "2024-09-08T04:30:51+00:00",
            "updated": "2024-09-08T04:30:51+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "62M45",
                "I.2.8; I.2.6; I.5.1"
            ],
            "max_author_hindex": 2
        },
        "2409.04976": {
            "authors": [
                "Sonu Kumar",
                "Komal Gupta",
                "Gopal Raut",
                "Mukul Lokhande",
                "Santosh Kumar Vishvakarma"
            ],
            "title": "HYDRA: Hybrid Data Multiplexing and Run-time Layer Configurable DNN Accelerator",
            "abstract": "Deep neural networks (DNNs) offer plenty of challenges in executing efficient computation at edge nodes, primarily due to the huge hardware resource demands. The article proposes HYDRA, hybrid data multiplexing, and runtime layer configurable DNN accelerators to overcome the drawbacks. The work proposes a layer-multiplexed approach, which further reuses a single activation function within the execution of a single layer with improved Fused-Multiply-Accumulate (FMA). The proposed approach works in iterative mode to reuse the same hardware and execute different layers in a configurable fashion. The proposed architectures achieve reductions over 90% of power consumption and resource utilization improvements of state-of-the-art works, with 35.21 TOPSW. The proposed architecture reduces the area overhead (N-1) times required in bandwidth, AF and layer architecture. This work shows HYDRA architecture supports optimal DNN computations while improving performance on resource-constrained edge devices.",
            "id": "2409.04976",
            "link": "http://arxiv.org/abs/2409.04976v1",
            "published": "2024-09-08T05:10:02+00:00",
            "updated": "2024-09-08T05:10:02+00:00",
            "primary_category": "cs.AR",
            "categories": [
                "cs.AR",
                "cs.AI",
                "eess.IV"
            ],
            "max_author_hindex": 20
        },
        "2409.05001": {
            "authors": [
                "Huan Zhang",
                "Wei Cheng",
                "Yuhan Wu",
                "Wei Hu"
            ],
            "title": "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement",
            "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00%-162.43% compared to prompting LLMs directly.",
            "id": "2409.05001",
            "link": "http://arxiv.org/abs/2409.05001v1",
            "published": "2024-09-08T07:22:19+00:00",
            "updated": "2024-09-08T07:22:19+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 38
        },
        "2409.05007": {
            "authors": [
                "Pujin Shi",
                "Fei Gao"
            ],
            "title": "Audio-Guided Fusion Techniques for Multimodal Emotion Analysis",
            "abstract": "In this paper, we propose a solution for the semi-supervised learning track (MER-SEMI) in MER2024. First, in order to enhance the performance of the feature extractor on sentiment classification tasks,we fine-tuned video and text feature extractors, specifically CLIP-vit-large and Baichuan-13B, using labeled data. This approach effectively preserves the original emotional information conveyed in the videos. Second, we propose an Audio-Guided Transformer (AGT) fusion mechanism, which leverages the robustness of Hubert-large, showing superior effectiveness in fusing both inter-channel and intra-channel information. Third, To enhance the accuracy of the model, we iteratively apply self-supervised learning by using high-confidence unlabeled data as pseudo-labels. Finally, through black-box probing, we discovered an imbalanced data distribution between the training and test sets. Therefore, We adopt a prior-knowledge-based voting mechanism. The results demonstrate the effectiveness of our strategy, ultimately earning us third place in the MER-SEMI track.",
            "id": "2409.05007",
            "link": "http://arxiv.org/abs/2409.05007v1",
            "published": "2024-09-08T07:28:27+00:00",
            "updated": "2024-09-08T07:28:27+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 23
        },
        "2409.05035": {
            "authors": [
                "Phurich Saengthong",
                "Takahiro Shinozaki"
            ],
            "title": "Deep Generic Representations for Domain-Generalized Anomalous Sound Detection",
            "abstract": "Developing a reliable anomalous sound detection (ASD) system requires robustness to noise, adaptation to domain shifts, and effective performance with limited training data. Current leading methods rely on extensive labeled data for each target machine type to train feature extractors using Outlier-Exposure (OE) techniques, yet their performance on the target domain remains sub-optimal. In this paper, we present \\textit{GenRep}, which utilizes generic feature representations from a robust, large-scale pre-trained feature extractor combined with kNN for domain-generalized ASD, without the need for fine-tuning. \\textit{GenRep} incorporates MemMixup, a simple approach for augmenting the target memory bank using nearest source samples, paired with a domain normalization technique to address the imbalance between source and target domains. \\textit{GenRep} outperforms the best OE-based approach without a need for labeled data with an Official Score of 73.79\\% on the DCASE2023T2 Eval set and demonstrates robustness under limited data scenarios. The code is available open-source.",
            "id": "2409.05035",
            "link": "http://arxiv.org/abs/2409.05035v1",
            "published": "2024-09-08T09:20:30+00:00",
            "updated": "2024-09-08T09:20:30+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 21
        },
        "2409.05215": {
            "authors": [
                "Emmanouil Panagiotou",
                "Arjun Roy",
                "Eirini Ntoutsi"
            ],
            "title": "Synthetic Tabular Data Generation for Class Imbalance and Fairness: A Comparative Study",
            "abstract": "Due to their data-driven nature, Machine Learning (ML) models are susceptible to bias inherited from data, especially in classification problems where class and group imbalances are prevalent. Class imbalance (in the classification target) and group imbalance (in protected attributes like sex or race) can undermine both ML utility and fairness. Although class and group imbalances commonly coincide in real-world tabular datasets, limited methods address this scenario. While most methods use oversampling techniques, like interpolation, to mitigate imbalances, recent advancements in synthetic tabular data generation offer promise but have not been adequately explored for this purpose. To this end, this paper conducts a comparative analysis to address class and group imbalances using state-of-the-art models for synthetic tabular data generation and various sampling strategies. Experimental results on four datasets, demonstrate the effectiveness of generative models for bias mitigation, creating opportunities for further exploration in this direction.",
            "id": "2409.05215",
            "link": "http://arxiv.org/abs/2409.05215v1",
            "published": "2024-09-08T20:08:09+00:00",
            "updated": "2024-09-08T20:08:09+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 24
        },
        "2409.07482": {
            "authors": [
                "Qi Li",
                "Jinfeng Huang",
                "Hongliang He",
                "Xinran Zhang",
                "Feibin Zhang",
                "Zhaoye Qin",
                "Fulei Chu"
            ],
            "title": "VSLLaVA: a pipeline of large multimodal foundation model for industrial vibration signal analysis",
            "abstract": "Large multimodal foundation models have been extensively utilized for image recognition tasks guided by instructions, yet there remains a scarcity of domain expertise in industrial vibration signal analysis. This paper presents a pipeline named VSLLaVA that leverages a large language model to integrate expert knowledge for identification of signal parameters and diagnosis of faults. Within this pipeline, we first introduce an expert rule-assisted signal generator. The generator merges signal provided by vibration analysis experts with domain-specific parameter identification and fault diagnosis question-answer pairs to build signal-question-answer triplets. Then we use these triplets to apply low-rank adaptation methods for fine-tuning the linear layers of the Contrastive Language-Image Pretraining (CLIP) and large language model, injecting multimodal signal processing knowledge. Finally, the fine-tuned model is assessed through the combined efforts of large language model and expert rules to evaluate answer accuracy and relevance, which showcases enhanced performance in identifying, analyzing various signal parameters, and diagnosing faults. These enhancements indicate the potential of this pipeline to build a foundational model for future industrial signal analysis and monitoring.",
            "id": "2409.07482",
            "link": "http://arxiv.org/abs/2409.07482v1",
            "published": "2024-09-03T06:21:26+00:00",
            "updated": "2024-09-03T06:21:26+00:00",
            "primary_category": "eess.SP",
            "categories": [
                "eess.SP",
                "cs.AI"
            ],
            "max_author_hindex": 63
        },
        "2409.00986": {
            "authors": [
                "Jeong Hun Yeo",
                "Chae Won Kim",
                "Hyunjun Kim",
                "Hyeongseop Rha",
                "Seunghee Han",
                "Wen-Huang Cheng",
                "Yong Man Ro"
            ],
            "title": "Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language",
            "abstract": "Lip reading aims to predict spoken language by analyzing lip movements. Despite advancements in lip reading technologies, performance degrades when models are applied to unseen speakers due to their sensitivity to variations in visual information such as lip appearances. To address this challenge, speaker adaptive lip reading technologies have advanced by focusing on effectively adapting a lip reading model to target speakers in the visual modality. The effectiveness of adapting language information, such as vocabulary choice, of the target speaker has not been explored in the previous works. Moreover, existing datasets for speaker adaptation have limited vocabulary size and pose variations, limiting the validation of previous speaker-adaptive methods in real-world scenarios. To address these issues, we propose a novel speaker-adaptive lip reading method that adapts a pre-trained model to target speakers at both vision and language levels. Specifically, we integrate prompt tuning and the LoRA approach, applying them to a pre-trained lip reading model to effectively adapt the model to target speakers. In addition, to validate its effectiveness in real-world scenarios, we introduce a new dataset, VoxLRS-SA, derived from VoxCeleb2 and LRS3. It contains a vocabulary of approximately 100K words, offers diverse pose variations, and enables the validation of adaptation methods in wild, sentence-level lip reading for the first time. Through various experiments, we demonstrate that the existing speaker-adaptive method also improves performance in the wild at the sentence level. Moreover, with the proposed adaptation method, we show that the proposed method achieves larger improvements when applied to the target speaker, compared to the previous works.",
            "id": "2409.00986",
            "link": "http://arxiv.org/abs/2409.00986v1",
            "published": "2024-09-02T07:05:12+00:00",
            "updated": "2024-09-02T07:05:12+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.CL",
                "eess.AS",
                "eess.IV"
            ],
            "max_author_hindex": 42
        },
        "2409.01193": {
            "authors": [
                "Rui Zeng",
                "Xi Chen",
                "Yuwen Pu",
                "Xuhong Zhang",
                "Tianyu Du",
                "Shouling Ji"
            ],
            "title": "CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models",
            "abstract": "Backdoors can be injected into NLP models to induce misbehavior when the input text contains a specific feature, known as a trigger, which the attacker secretly selects. Unlike fixed words, phrases, or sentences used in the static text trigger, NLP dynamic backdoor attacks design triggers associated with abstract and latent text features, making them considerably stealthier than traditional static backdoor attacks. However, existing research on NLP backdoor detection primarily focuses on defending against static backdoor attacks, while detecting dynamic backdoors in NLP models remains largely unexplored. This paper presents CLIBE, the first framework to detect dynamic backdoors in Transformer-based NLP models. CLIBE injects a \"few-shot perturbation\" into the suspect Transformer model by crafting optimized weight perturbation in the attention layers to make the perturbed model classify a limited number of reference samples as a target label. Subsequently, CLIBE leverages the generalization ability of this few-shot perturbation to determine whether the original model contains a dynamic backdoor. Extensive evaluation on three advanced NLP dynamic backdoor attacks, two widely-used Transformer frameworks, and four real-world classification tasks strongly validates the effectiveness of CLIBE. We also demonstrate the robustness of CLIBE against various adaptive attacks. Furthermore, we employ CLIBE to scrutinize 49 popular Transformer models on Hugging Face and discover one exhibiting a high probability of containing a dynamic backdoor. We have contacted Hugging Face and provided detailed evidence of this model's backdoor behavior. Moreover, we extend CLIBE to detect backdoor text generation models modified to exhibit toxic behavior. To the best of our knowledge, CLIBE is the first framework capable of detecting backdoors in text generation models without access to trigger input test samples.",
            "id": "2409.01193",
            "link": "http://arxiv.org/abs/2409.01193v2",
            "published": "2024-09-02T11:59:56+00:00",
            "updated": "2024-09-11T12:29:02+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.CL",
                "cs.LG"
            ],
            "max_author_hindex": 26
        },
        "2409.03512": {
            "authors": [
                "Jifan Yu",
                "Zheyuan Zhang",
                "Daniel Zhang-li",
                "Shangqing Tu",
                "Zhanxin Hao",
                "Rui Miao Li",
                "Haoxuan Li",
                "Yuanchun Wang",
                "Hanming Li",
                "Linlu Gong",
                "Jie Cao",
                "Jiayin Lin",
                "Jinchang Zhou",
                "Fei Qin",
                "Haohua Wang",
                "Jianxiao Jiang",
                "Lijun Deng",
                "Yisi Zhan",
                "Chaojun Xiao",
                "Xusheng Dai",
                "Xuan Yan",
                "Nianyi Lin",
                "Nan Zhang",
                "Ruixin Ni",
                "Yang Dang",
                "Lei Hou",
                "Yu Zhang",
                "Xu Han",
                "Manli Li",
                "Juanzi Li",
                "Zhiyuan Liu",
                "Huiqin Liu",
                "Maosong Sun"
            ],
            "title": "From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents",
            "abstract": "Since the first instances of online education, where courses were uploaded to accessible and shared online platforms, this form of scaling the dissemination of human knowledge to reach a broader audience has sparked extensive discussion and widespread adoption. Recognizing that personalized learning still holds significant potential for improvement, new AI technologies have been continuously integrated into this learning format, resulting in a variety of educational AI applications such as educational recommendation and intelligent tutoring. The emergence of intelligence in large language models (LLMs) has allowed for these educational enhancements to be built upon a unified foundational model, enabling deeper integration. In this context, we propose MAIC (Massive AI-empowered Course), a new form of online education that leverages LLM-driven multi-agent systems to construct an AI-augmented classroom, balancing scalability with adaptivity. Beyond exploring the conceptual framework and technical innovations, we conduct preliminary experiments at Tsinghua University, one of China's leading universities. Drawing from over 100,000 learning records of more than 500 students, we obtain a series of valuable observations and initial analyses. This project will continue to evolve, ultimately aiming to establish a comprehensive open platform that supports and unifies research, technology, and applications in exploring the possibilities of online education in the era of large model AI. We envision this platform as a collaborative hub, bringing together educators, researchers, and innovators to collectively explore the future of AI-driven online education.",
            "id": "2409.03512",
            "link": "http://arxiv.org/abs/2409.03512v1",
            "published": "2024-09-05T13:22:51+00:00",
            "updated": "2024-09-05T13:22:51+00:00",
            "primary_category": "cs.CY",
            "categories": [
                "cs.CY",
                "cs.CL"
            ],
            "max_author_hindex": 87
        },
        "2409.00310": {
            "authors": [
                "Mikhail Borisenkov",
                "Andrei Velichko",
                "Maksim Belyaev",
                "Dmitry Korzun",
                "Tatyana Tserne",
                "Larisa Bakutova",
                "Denis Gubin"
            ],
            "title": "Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning",
            "abstract": "This study investigates machine learning algorithms to identify objective features for diagnosing food addiction (FA) and assessing confirmed symptoms (SC). Data were collected from 81 participants (mean age: 21.5 years, range: 18-61 years, women: 77.8%) whose FA and SC were measured using the Yale Food Addiction Scale (YFAS). Participants provided demographic and anthropometric data, completed the YFAS, the Zung Self-Rating Depression Scale, and the Dutch Eating Behavior Questionnaire, and wore an actimeter on the non-dominant wrist for a week to record motor activity. Analysis of the actimetric data identified significant statistical and entropy-based features that accurately predicted FA and SC using ML. The Matthews correlation coefficient (MCC) was the primary metric. Activity-related features were more effective for FA prediction (MCC=0.88) than rest-related features (MCC=0.68). For SC, activity segments yielded MCC=0.47, rest segments MCC=0.38, and their combination MCC=0.51. Significant correlations were also found between actimetric features related to FA, emotional, and restrained eating behaviors, supporting the model's validity. Our results support the concept of a human bionic suite composed of IoT devices and ML sensors, which implements health digital assistance with real-time monitoring and analysis of physiological indicators related to FA and SC.",
            "id": "2409.00310",
            "link": "http://arxiv.org/abs/2409.00310v1",
            "published": "2024-08-31T00:33:17+00:00",
            "updated": "2024-08-31T00:33:17+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "eess.SP",
                "physics.med-ph"
            ],
            "max_author_hindex": 31
        },
        "2409.00338": {
            "authors": [
                "Xiaoyu Zhang",
                "Wenchuan Yang",
                "Jiawei Feng",
                "Bitao Dai",
                "Tianci Bu",
                "Xin Lu"
            ],
            "title": "GSpect: Spectral Filtering for Cross-Scale Graph Classification",
            "abstract": "Identifying structures in common forms the basis for networked systems design and optimization. However, real structures represented by graphs are often of varying sizes, leading to the low accuracy of traditional graph classification methods. These graphs are called cross-scale graphs. To overcome this limitation, in this study, we propose GSpect, an advanced spectral graph filtering model for cross-scale graph classification tasks. Compared with other methods, we use graph wavelet neural networks for the convolution layer of the model, which aggregates multi-scale messages to generate graph representations. We design a spectral-pooling layer which aggregates nodes to one node to reduce the cross-scale graphs to the same size. We collect and construct the cross-scale benchmark data set, MSG (Multi Scale Graphs). Experiments reveal that, on open data sets, GSpect improves the performance of classification accuracy by 1.62% on average, and for a maximum of 3.33% on PROTEINS. On MSG, GSpect improves the performance of classification accuracy by 15.55% on average. GSpect fills the gap in cross-scale graph classification studies and has potential to provide assistance in application research like diagnosis of brain disease by predicting the brain network's label and developing new drugs with molecular structures learned from their counterparts in other systems.",
            "id": "2409.00338",
            "link": "http://arxiv.org/abs/2409.00338v1",
            "published": "2024-08-31T03:26:32+00:00",
            "updated": "2024-08-31T03:26:32+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ],
            "max_author_hindex": 23
        },
        "2409.00356": {
            "authors": [
                "Weinan Dai",
                "Yifeng Jiang",
                "Yuanjing Liu",
                "Jinkun Chen",
                "Xin Sun",
                "Jinglei Tao"
            ],
            "title": "Contrastive Augmentation: An Unsupervised Learning Approach for Keyword Spotting in Speech Technology",
            "abstract": "This paper addresses the persistent challenge in Keyword Spotting (KWS), a fundamental component in speech technology, regarding the acquisition of substantial labeled data for training. Given the difficulty in obtaining large quantities of positive samples and the laborious process of collecting new target samples when the keyword changes, we introduce a novel approach combining unsupervised contrastive learning and a unique augmentation-based technique. Our method allows the neural network to train on unlabeled data sets, potentially improving performance in downstream tasks with limited labeled data sets. We also propose that similar high-level feature representations should be employed for speech utterances with the same keyword despite variations in speed or volume. To achieve this, we present a speech augmentation-based unsupervised learning method that utilizes the similarity between the bottleneck layer feature and the audio reconstructing information for auxiliary training. Furthermore, we propose a compressed convolutional architecture to address potential redundancy and non-informative information in KWS tasks, enabling the model to simultaneously learn local features and focus on long-term information. This method achieves strong performance on the Google Speech Commands V2 Dataset. Inspired by recent advancements in sign spotting and spoken term detection, our method underlines the potential of our contrastive learning approach in KWS and the advantages of Query-by-Example Spoken Term Detection strategies. The presented CAB-KWS provide new perspectives in the field of KWS, demonstrating effective ways to reduce data collection efforts and increase the system's robustness.",
            "id": "2409.00356",
            "link": "http://arxiv.org/abs/2409.00356v1",
            "published": "2024-08-31T05:40:37+00:00",
            "updated": "2024-08-31T05:40:37+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 52
        },
        "2409.00438": {
            "authors": [
                "Anoushka Harit",
                "Zhongtian Sun",
                "Jongmin Yu",
                "Noura Al Moubayed"
            ],
            "title": "Breaking Down Financial News Impact: A Novel AI Approach with Geometric Hypergraphs",
            "abstract": "In the fast-paced and volatile financial markets, accurately predicting stock movements based on financial news is critical for investors and analysts. Traditional models often struggle to capture the intricate and dynamic relationships between news events and market reactions, limiting their ability to provide actionable insights. This paper introduces a novel approach leveraging Explainable Artificial Intelligence (XAI) through the development of a Geometric Hypergraph Attention Network (GHAN) to analyze the impact of financial news on market behaviours. Geometric hypergraphs extend traditional graph structures by allowing edges to connect multiple nodes, effectively modelling high-order relationships and interactions among financial entities and news events. This unique capability enables the capture of complex dependencies, such as the simultaneous impact of a single news event on multiple stocks or sectors, which traditional models frequently overlook.   By incorporating attention mechanisms within hypergraphs, GHAN enhances the model's ability to focus on the most relevant information, ensuring more accurate predictions and better interpretability. Additionally, we employ BERT-based embeddings to capture the semantic richness of financial news texts, providing a nuanced understanding of the content. Using a comprehensive financial news dataset, our GHAN model addresses key challenges in financial news impact analysis, including the complexity of high-order interactions, the necessity for model interpretability, and the dynamic nature of financial markets. Integrating attention mechanisms and SHAP values within GHAN ensures transparency, highlighting the most influential factors driving market predictions.   Empirical validation demonstrates the superior effectiveness of our approach over traditional sentiment analysis and time-series models.",
            "id": "2409.00438",
            "link": "http://arxiv.org/abs/2409.00438v1",
            "published": "2024-08-31T12:18:45+00:00",
            "updated": "2024-08-31T12:18:45+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.00547": {
            "authors": [
                "Fazle Rahat",
                "M Shifat Hossain",
                "Md Rubel Ahmed",
                "Sumit Kumar Jha",
                "Rickard Ewetz"
            ],
            "title": "Data Augmentation for Image Classification using Generative AI",
            "abstract": "Scaling laws dictate that the performance of AI models is proportional to the amount of available data. Data augmentation is a promising solution to expanding the dataset size. Traditional approaches focused on augmentation using rotation, translation, and resizing. Recent approaches use generative AI models to improve dataset diversity. However, the generative methods struggle with issues such as subject corruption and the introduction of irrelevant artifacts. In this paper, we propose the Automated Generative Data Augmentation (AGA). The framework combines the utility of large language models (LLMs), diffusion models, and segmentation models to augment data. AGA preserves foreground authenticity while ensuring background diversity. Specific contributions include: i) segment and superclass based object extraction, ii) prompt diversity with combinatorial complexity using prompt decomposition, and iii) affine subject manipulation. We evaluate AGA against state-of-the-art (SOTA) techniques on three representative datasets, ImageNet, CUB, and iWildCam. The experimental evaluation demonstrates an accuracy improvement of 15.6% and 23.5% for in and out-of-distribution data compared to baseline models, respectively. There is also a 64.3% improvement in SIC score compared to the baselines.",
            "id": "2409.00547",
            "link": "http://arxiv.org/abs/2409.00547v1",
            "published": "2024-08-31T21:16:43+00:00",
            "updated": "2024-08-31T21:16:43+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "I.2.10; I.5.1"
            ],
            "max_author_hindex": 19
        },
        "2409.00571": {
            "authors": [
                "Nafis Tanveer Islam",
                "Joseph Khoury",
                "Andrew Seong",
                "Elias Bou-Harb",
                "Peyman Najafirad"
            ],
            "title": "Enhancing Source Code Security with LLMs: Demystifying The Challenges and Generating Reliable Repairs",
            "abstract": "With the recent unprecedented advancements in Artificial Intelligence (AI) computing, progress in Large Language Models (LLMs) is accelerating rapidly, presenting challenges in establishing clear guidelines, particularly in the field of security. That being said, we thoroughly identify and describe three main technical challenges in the security and software engineering literature that spans the entire LLM workflow, namely; \\textbf{\\textit{(i)}} Data Collection and Labeling; \\textbf{\\textit{(ii)}} System Design and Learning; and \\textbf{\\textit{(iii)}} Performance Evaluation. Building upon these challenges, this paper introduces \\texttt{SecRepair}, an instruction-based LLM system designed to reliably \\textit{identify}, \\textit{describe}, and automatically \\textit{repair} vulnerable source code. Our system is accompanied by a list of actionable guides on \\textbf{\\textit{(i)}} Data Preparation and Augmentation Techniques; \\textbf{\\textit{(ii)}} Selecting and Adapting state-of-the-art LLM Models; \\textbf{\\textit{(iii)}} Evaluation Procedures. \\texttt{SecRepair} uses a reinforcement learning-based fine-tuning with a semantic reward that caters to the functionality and security aspects of the generated code. Our empirical analysis shows that \\texttt{SecRepair} achieves a \\textit{12}\\% improvement in security code repair compared to other LLMs when trained using reinforcement learning. Furthermore, we demonstrate the capabilities of \\texttt{SecRepair} in generating reliable, functional, and compilable security code repairs against real-world test cases using automated evaluation metrics.",
            "id": "2409.00571",
            "link": "http://arxiv.org/abs/2409.00571v1",
            "published": "2024-09-01T00:41:40+00:00",
            "updated": "2024-09-01T00:41:40+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 27
        },
        "2409.00620": {
            "authors": [
                "Xiaoyu Zhang",
                "Guangwei Liu",
                "Zihao Liu",
                "Ningyi Xu",
                "Yunhui Liu",
                "Ji Zhao"
            ],
            "title": "Enhancing Vectorized Map Perception with Historical Rasterized Maps",
            "abstract": "In autonomous driving, there is growing interest in end-to-end online vectorized map perception in bird's-eye-view (BEV) space, with an expectation that it could replace traditional high-cost offline high-definition (HD) maps. However, the accuracy and robustness of these methods can be easily compromised in challenging conditions, such as occlusion or adverse weather, when relying only on onboard sensors. In this paper, we propose HRMapNet, leveraging a low-cost Historical Rasterized Map to enhance online vectorized map perception. The historical rasterized map can be easily constructed from past predicted vectorized results and provides valuable complementary information. To fully exploit a historical map, we propose two novel modules to enhance BEV features and map element queries. For BEV features, we employ a feature aggregation module to encode features from both onboard images and the historical map. For map element queries, we design a query initialization module to endow queries with priors from the historical map. The two modules contribute to leveraging map information in online perception. Our HRMapNet can be integrated with most online vectorized map perception methods. We integrate it in two state-of-the-art methods, significantly improving their performance on both the nuScenes and Argoverse 2 datasets. The source code is released at https://github.com/HXMap/HRMapNet.",
            "id": "2409.00620",
            "link": "http://arxiv.org/abs/2409.00620v1",
            "published": "2024-09-01T05:22:33+00:00",
            "updated": "2024-09-01T05:22:33+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 24
        },
        "2409.00639": {
            "authors": [
                "Tanisha Singh",
                "Shreshtha Jha",
                "Nidhi Bhatt",
                "Palak Handa",
                "Nidhi Goel",
                "Sreedevi Indu"
            ],
            "title": "Artificial Intelligence in Gastrointestinal Bleeding Analysis for Video Capsule Endoscopy: Insights, Innovations, and Prospects (2008-2023)",
            "abstract": "The escalating global mortality and morbidity rates associated with gastrointestinal (GI) bleeding, compounded by the complexities and limitations of traditional endoscopic methods, underscore the urgent need for a critical review of current methodologies used for addressing this condition. With an estimated 300,000 annual deaths worldwide, the demand for innovative diagnostic and therapeutic strategies is paramount. The introduction of Video Capsule Endoscopy (VCE) has marked a significant advancement, offering a comprehensive, non-invasive visualization of the digestive tract that is pivotal for detecting bleeding sources unattainable by traditional methods. Despite its benefits, the efficacy of VCE is hindered by diagnostic challenges, including time-consuming analysis and susceptibility to human error. This backdrop sets the stage for exploring Machine Learning (ML) applications in automating GI bleeding detection within capsule endoscopy, aiming to enhance diagnostic accuracy, reduce manual labor, and improve patient outcomes. Through an exhaustive analysis of 113 papers published between 2008 and 2023, this review assesses the current state of ML methodologies in bleeding detection, highlighting their effectiveness, challenges, and prospective directions. It contributes an in-depth examination of AI techniques in VCE frame analysis, offering insights into open-source datasets, mathematical performance metrics, and technique categorization. The paper sets a foundation for future research to overcome existing challenges, advancing gastrointestinal diagnostics through interdisciplinary collaboration and innovation in ML applications.",
            "id": "2409.00639",
            "link": "http://arxiv.org/abs/2409.00639v1",
            "published": "2024-09-01T07:13:28+00:00",
            "updated": "2024-09-01T07:13:28+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.00658": {
            "authors": [
                "Seyed Mohammad Ali Jafari",
                "Ehsan Chitsaz"
            ],
            "title": "Nasdaq-100 Companies' Hiring Insights: A Topic-based Classification Approach to the Labor Market",
            "abstract": "The emergence of new and disruptive technologies makes the economy and labor market more unstable. To overcome this kind of uncertainty and to make the labor market more comprehensible, we must employ labor market intelligence techniques, which are predominantly based on data analysis. Companies use job posting sites to advertise their job vacancies, known as online job vacancies (OJVs). LinkedIn is one of the most utilized websites for matching the supply and demand sides of the labor market; companies post their job vacancies on their job pages, and LinkedIn recommends these jobs to job seekers who are likely to be interested. However, with the vast number of online job vacancies, it becomes challenging to discern overarching trends in the labor market. In this paper, we propose a data mining-based approach for job classification in the modern online labor market. We employed structural topic modeling as our methodology and used the NASDAQ-100 indexed companies' online job vacancies on LinkedIn as the input data. We discover that among all 13 job categories, Marketing, Branding, and Sales; Software Engineering; Hardware Engineering; Industrial Engineering; and Project Management are the most frequently posted job classifications. This study aims to provide a clearer understanding of job market trends, enabling stakeholders to make informed decisions in a rapidly evolving employment landscape.",
            "id": "2409.00658",
            "link": "http://arxiv.org/abs/2409.00658v1",
            "published": "2024-09-01T08:18:56+00:00",
            "updated": "2024-09-01T08:18:56+00:00",
            "primary_category": "econ.GN",
            "categories": [
                "econ.GN",
                "cs.AI",
                "q-fin.EC"
            ],
            "max_author_hindex": 19
        },
        "2409.00667": {
            "authors": [
                "Rahul Yumlembam",
                "Biju Issac",
                "Seibu Mary Jacob",
                "Longzhi Yang"
            ],
            "title": "Comprehensive Botnet Detection by Mitigating Adversarial Attacks, Navigating the Subtleties of Perturbation Distances and Fortifying Predictions with Conformal Layers",
            "abstract": "Botnets are computer networks controlled by malicious actors that present significant cybersecurity challenges. They autonomously infect, propagate, and coordinate to conduct cybercrimes, necessitating robust detection methods. This research addresses the sophisticated adversarial manipulations posed by attackers, aiming to undermine machine learning-based botnet detection systems. We introduce a flow-based detection approach, leveraging machine learning and deep learning algorithms trained on the ISCX and ISOT datasets. The detection algorithms are optimized using the Genetic Algorithm and Particle Swarm Optimization to obtain a baseline detection method. The Carlini & Wagner (C&W) attack and Generative Adversarial Network (GAN) generate deceptive data with subtle perturbations, targeting each feature used for classification while preserving their semantic and syntactic relationships, which ensures that the adversarial samples retain meaningfulness and realism. An in-depth analysis of the required L2 distance from the original sample for the malware sample to misclassify is performed across various iteration checkpoints, showing different levels of misclassification at different L2 distances of the Pertrub sample from the original sample. Our work delves into the vulnerability of various models, examining the transferability of adversarial examples from a Neural Network surrogate model to Tree-based algorithms. Subsequently, models that initially misclassified the perturbed samples are retrained, enhancing their resilience and detection capabilities. In the final phase, a conformal prediction layer is integrated, significantly rejecting incorrect predictions, of 58.20 % in the ISCX dataset and 98.94 % in the ISOT dataset.",
            "id": "2409.00667",
            "link": "http://arxiv.org/abs/2409.00667v1",
            "published": "2024-09-01T08:53:21+00:00",
            "updated": "2024-09-01T08:53:21+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 28
        },
        "2409.00718": {
            "authors": [
                "Pragya Gupta",
                "Subhamoy Mandal",
                "Debashree Guha",
                "Debjani Chakraborty"
            ],
            "title": "Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images",
            "abstract": "Automatic diagnosis techniques have evolved to identify age-related macular degeneration (AMD) by employing single modality Fundus images or optical coherence tomography (OCT). To classify ocular diseases, fundus and OCT images are the most crucial imaging modalities used in the clinical setting. Most deep learning-based techniques are established on a single imaging modality, which contemplates the ocular disorders to a specific extent and disregards other modality that comprises exhaustive information among distinct imaging modalities. This paper proposes a modality-specific multiscale color space embedding integrated with the attention mechanism based on transfer learning for classification (MCGAEc), which can efficiently extract the distinct modality information at various scales using the distinct color spaces. In this work, we first introduce the modality-specific multiscale color space encoder model, which includes diverse feature representations by integrating distinct characteristic color spaces on a multiscale into a unified framework. The extracted features from the prior encoder module are incorporated with the attention mechanism to extract the global features representation, which is integrated with the prior extracted features and transferred to the random forest classifier for the classification of AMD. To analyze the performance of the proposed MCGAEc method, a publicly available multi-modality dataset from Project Macula for AMD is utilized and compared with the existing models.",
            "id": "2409.00718",
            "link": "http://arxiv.org/abs/2409.00718v1",
            "published": "2024-09-01T13:17:45+00:00",
            "updated": "2024-09-01T13:17:45+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 23
        },
        "2409.00724": {
            "authors": [
                "Shams Nafisa Ali",
                "Afia Zahin",
                "Samiul Based Shuvo",
                "Nusrat Binta Nizam",
                "Shoyad Ibn Sabur Khan Nuhash",
                "Sayeed Sajjad Razin",
                "S. M. Sakeef Sani",
                "Farihin Rahman",
                "Nawshad Binta Nizam",
                "Farhat Binte Azam",
                "Rakib Hossen",
                "Sumaiya Ohab",
                "Nawsabah Noor",
                "Taufiq Hasan"
            ],
            "title": "BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems",
            "abstract": "Cardiac auscultation, an integral tool in diagnosing cardiovascular diseases (CVDs), often relies on the subjective interpretation of clinicians, presenting a limitation in consistency and accuracy. Addressing this, we introduce the BUET Multi-disease Heart Sound (BMD-HS) dataset - a comprehensive and meticulously curated collection of heart sound recordings. This dataset, encompassing 864 recordings across five distinct classes of common heart sounds, represents a broad spectrum of valvular heart diseases, with a focus on diagnostically challenging cases. The standout feature of the BMD-HS dataset is its innovative multi-label annotation system, which captures a diverse range of diseases and unique disease states. This system significantly enhances the dataset's utility for developing advanced machine learning models in automated heart sound classification and diagnosis. By bridging the gap between traditional auscultation practices and contemporary data-driven diagnostic methods, the BMD-HS dataset is poised to revolutionize CVD diagnosis and management, providing an invaluable resource for the advancement of cardiac health research. The dataset is publicly available at this link: https://github.com/mHealthBuet/BMD-HS-Dataset.",
            "id": "2409.00724",
            "link": "http://arxiv.org/abs/2409.00724v1",
            "published": "2024-09-01T13:55:04+00:00",
            "updated": "2024-09-01T13:55:04+00:00",
            "primary_category": "eess.SP",
            "categories": [
                "eess.SP",
                "cs.AI",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ],
            "max_author_hindex": 20
        },
        "2409.00750": {
            "authors": [
                "Yuancheng Wang",
                "Haoyue Zhan",
                "Liwei Liu",
                "Ruihong Zeng",
                "Haotian Guo",
                "Jiachen Zheng",
                "Qiang Zhang",
                "Shunsi Zhang",
                "Zhizheng Wu"
            ],
            "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
            "abstract": "Nowadays, large-scale text-to-speech (TTS) systems are primarily divided into two types: autoregressive and non-autoregressive. The autoregressive systems have certain deficiencies in robustness and cannot control speech duration. In contrast, non-autoregressive systems require explicit prediction of phone-level duration, which may compromise their naturalness. We introduce the Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive model for TTS that does not require precise alignment information between text and speech. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the \\textit{mask-and-predict} learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. We scale MaskGCT to a large-scale multilingual dataset with 100K hours of in-the-wild speech. Our experiments demonstrate that MaskGCT achieves superior or competitive performance compared to state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility while offering higher generation efficiency than diffusion-based or autoregressive TTS models. Audio samples are available at https://maskgct.github.io.",
            "id": "2409.00750",
            "link": "http://arxiv.org/abs/2409.00750v1",
            "published": "2024-09-01T15:26:30+00:00",
            "updated": "2024-09-01T15:26:30+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.LG",
                "eess.AS"
            ],
            "max_author_hindex": 18
        },
        "2409.00755": {
            "authors": [
                "Haojian Huang",
                "Chuanyu Qin",
                "Zhe Liu",
                "Kaijing Ma",
                "Jin Chen",
                "Han Fang",
                "Chao Ban",
                "Hao Sun",
                "Zhongjiang He"
            ],
            "title": "Trusted Unified Feature-Neighborhood Dynamics for Multi-View Classification",
            "abstract": "Multi-view classification (MVC) faces inherent challenges due to domain gaps and inconsistencies across different views, often resulting in uncertainties during the fusion process. While Evidential Deep Learning (EDL) has been effective in addressing view uncertainty, existing methods predominantly rely on the Dempster-Shafer combination rule, which is sensitive to conflicting evidence and often neglects the critical role of neighborhood structures within multi-view data. To address these limitations, we propose a Trusted Unified Feature-NEighborhood Dynamics (TUNED) model for robust MVC. This method effectively integrates local and global feature-neighborhood (F-N) structures for robust decision-making. Specifically, we begin by extracting local F-N structures within each view. To further mitigate potential uncertainties and conflicts in multi-view fusion, we employ a selective Markov random field that adaptively manages cross-view neighborhood dependencies. Additionally, we employ a shared parameterized evidence extractor that learns global consensus conditioned on local F-N structures, thereby enhancing the global integration of multi-view features. Experiments on benchmark datasets show that our method improves accuracy and robustness over existing approaches, particularly in scenarios with high uncertainty and conflicting views. The code will be made available at https://github.com/JethroJames/TUNED.",
            "id": "2409.00755",
            "link": "http://arxiv.org/abs/2409.00755v1",
            "published": "2024-09-01T15:48:20+00:00",
            "updated": "2024-09-01T15:48:20+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 33
        },
        "2409.00807": {
            "authors": [
                "Haoyu Lan",
                "Bino A. Varghese",
                "Nasim Sheikh-Bahaei",
                "Farshid Sepehrband",
                "Arthur W Toga",
                "Jeiran Choupan"
            ],
            "title": "Diffusion based multi-domain neuroimaging harmonization method with preservation of anatomical details",
            "abstract": "Multi-center neuroimaging studies face technical variability due to batch differences across sites, which potentially hinders data aggregation and impacts study reliability.Recent efforts in neuroimaging harmonization have aimed to minimize these technical gaps and reduce technical variability across batches. While Generative Adversarial Networks (GAN) has been a prominent method for addressing image harmonization tasks, GAN-harmonized images suffer from artifacts or anatomical distortions. Given the advancements of denoising diffusion probabilistic model which produces high-fidelity images, we have assessed the efficacy of the diffusion model for neuroimaging harmonization. we have demonstrated the diffusion model's superior capability in harmonizing images from multiple domains, while GAN-based methods are limited to harmonizing images between two domains per model. Our experiments highlight that the learned domain invariant anatomical condition reinforces the model to accurately preserve the anatomical details while differentiating batch differences at each diffusion step. Our proposed method has been tested on two public neuroimaging dataset ADNI1 and ABIDE II, yielding harmonization results with consistent anatomy preservation and superior FID score compared to the GAN-based methods. We have conducted multiple analysis including extensive quantitative and qualitative evaluations against the baseline models, ablation study showcasing the benefits of the learned conditions, and improvements in the consistency of perivascular spaces (PVS) segmentation through harmonization.",
            "id": "2409.00807",
            "link": "http://arxiv.org/abs/2409.00807v1",
            "published": "2024-09-01T18:54:00+00:00",
            "updated": "2024-09-01T18:54:00+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "eess.IV"
            ],
            "max_author_hindex": 172
        },
        "2409.00810": {
            "authors": [
                "Kanthimathi S",
                "Shravan Venkatraman",
                "Jayasankar K S",
                "Pranay Jiljith T",
                "Jashwanth R"
            ],
            "title": "A Novel Self-Attention-Enabled Weighted Ensemble-Based Convolutional Neural Network Framework for Distributed Denial of Service Attack Classification",
            "abstract": "Distributed Denial of Service (DDoS) attacks are a major concern in network security, as they overwhelm systems with excessive traffic, compromise sensitive data, and disrupt network services. Accurately detecting these attacks is crucial to protecting network infrastructure. Traditional approaches, such as single Convolutional Neural Networks (CNNs) or conventional Machine Learning (ML) algorithms like Decision Trees (DTs) and Support Vector Machines (SVMs), struggle to extract the diverse features needed for precise classification, resulting in suboptimal performance. This research addresses this gap by introducing a novel approach for DDoS attack detection. The proposed method combines three distinct CNN architectures: SA-Enabled CNN with XGBoost, SA-Enabled CNN with LSTM, and SA-Enabled CNN with Random Forest. Each model extracts features at multiple scales, while self-attention mechanisms enhance feature integration and relevance. The weighted ensemble approach ensures that both prominent and subtle features contribute to the final classification, improving adaptability to evolving attack patterns and novel threats. The proposed method achieves a precision of 98.71%, an F1-score of 98.66%, a recall of 98.63%, and an accuracy of 98.69%, outperforming traditional methods and setting a new benchmark in DDoS attack detection. This innovative approach addresses critical limitations in current models and advances the state of the art in network security.",
            "id": "2409.00810",
            "link": "http://arxiv.org/abs/2409.00810v1",
            "published": "2024-09-01T18:58:33+00:00",
            "updated": "2024-09-01T18:58:33+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.LG",
                "I.2.6"
            ],
            "max_author_hindex": 25
        },
        "2409.00839": {
            "authors": [
                "Haobo Yang",
                "Shiyan Zhang",
                "Zhuoyi Yang",
                "Xinyu Zhang",
                "Li Wang",
                "Yifan Tang",
                "Jilong Guo",
                "Jun Li"
            ],
            "title": "Entropy Loss: An Interpretability Amplifier of 3D Object Detection Network for Intelligent Driving",
            "abstract": "With the increasing complexity of the traffic environment, the significance of safety perception in intelligent driving is intensifying. Traditional methods in the field of intelligent driving perception rely on deep learning, which suffers from limited interpretability, often described as a \"black box.\" This paper introduces a novel type of loss function, termed \"Entropy Loss,\" along with an innovative training strategy. Entropy Loss is formulated based on the functionality of feature compression networks within the perception model. Drawing inspiration from communication systems, the information transmission process in a feature compression network is expected to demonstrate steady changes in information volume and a continuous decrease in information entropy. By modeling network layer outputs as continuous random variables, we construct a probabilistic model that quantifies changes in information volume. Entropy Loss is then derived based on these expectations, guiding the update of network parameters to enhance network interpretability. Our experiments indicate that the Entropy Loss training strategy accelerates the training process. Utilizing the same 60 training epochs, the accuracy of 3D object detection models using Entropy Loss on the KITTI test set improved by up to 4.47\\% compared to models without Entropy Loss, underscoring the method's efficacy. The implementation code is available at \\url{https://github.com/yhbcode000/Eloss-Interpretability}.",
            "id": "2409.00839",
            "link": "http://arxiv.org/abs/2409.00839v1",
            "published": "2024-09-01T20:55:50+00:00",
            "updated": "2024-09-01T20:55:50+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.IT",
                "math.IT"
            ],
            "max_author_hindex": 23
        },
        "2409.00858": {
            "authors": [
                "Zilin Huang",
                "Zihao Sheng",
                "Sikai Chen"
            ],
            "title": "Trustworthy Human-AI Collaboration: Reinforcement Learning with Human Feedback and Physics Knowledge for Safe Autonomous Driving",
            "abstract": "In the field of autonomous driving, developing safe and trustworthy autonomous driving policies remains a significant challenge. Recently, Reinforcement Learning with Human Feedback (RLHF) has attracted substantial attention due to its potential to enhance training safety and sampling efficiency. Nevertheless, existing RLHF-enabled methods often falter when faced with imperfect human demonstrations, potentially leading to training oscillations or even worse performance than rule-based approaches. Inspired by the human learning process, we propose Physics-enhanced Reinforcement Learning with Human Feedback (PE-RLHF). This novel framework synergistically integrates human feedback (e.g., human intervention and demonstration) and physics knowledge (e.g., traffic flow model) into the training loop of reinforcement learning. The key advantage of PE-RLHF is its guarantee that the learned policy will perform at least as well as the given physics-based policy, even when human feedback quality deteriorates, thus ensuring trustworthy safety improvements. PE-RLHF introduces a Physics-enhanced Human-AI (PE-HAI) collaborative paradigm for dynamic action selection between human and physics-based actions, employs a reward-free approach with a proxy value function to capture human preferences, and incorporates a minimal intervention mechanism to reduce the cognitive load on human mentors. Extensive experiments across diverse driving scenarios demonstrate that PE-RLHF significantly outperforms traditional methods, achieving state-of-the-art (SOTA) performance in safety, efficiency, and generalizability, even with varying quality of human feedback. The philosophy behind PE-RLHF not only advances autonomous driving technology but can also offer valuable insights for other safety-critical domains. Demo video and code are available at: \\https://zilin-huang.github.io/PE-RLHF-website/",
            "id": "2409.00858",
            "link": "http://arxiv.org/abs/2409.00858v2",
            "published": "2024-09-01T22:20:32+00:00",
            "updated": "2024-09-05T08:07:27+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ],
            "max_author_hindex": 18
        },
        "2409.00873": {
            "authors": [
                "Sajib Acharjee Dip",
                "Kazi Hasan Ibn Arif",
                "Uddip Acharjee Shuvo",
                "Ishtiaque Ahmed Khan",
                "Na Meng"
            ],
            "title": "Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation",
            "abstract": "In the realm of dermatology, the complexity of diagnosing skin conditions manually necessitates the expertise of dermatologists. Accurate identification of various skin ailments, ranging from cancer to inflammatory diseases, is paramount. However, existing artificial intelligence (AI) models in dermatology face challenges, particularly in accurately diagnosing diseases across diverse skin tones, with a notable performance gap in darker skin. Additionally, the scarcity of publicly available, unbiased datasets hampers the development of inclusive AI diagnostic tools. To tackle the challenges in accurately predicting skin conditions across diverse skin tones, we employ a transfer-learning approach that capitalizes on the rich, transferable knowledge from various image domains. Our method integrates multiple pre-trained models from a wide range of sources, including general and specific medical images, to improve the robustness and inclusiveness of the skin condition predictions. We rigorously evaluated the effectiveness of these models using the Diverse Dermatology Images (DDI) dataset, which uniquely encompasses both underrepresented and common skin tones, making it an ideal benchmark for assessing our approach. Among all methods, Med-ViT emerged as the top performer due to its comprehensive feature representation learned from diverse image sources. To further enhance performance, we conducted domain adaptation using additional skin image datasets such as HAM10000. This adaptation significantly improved model performance across all models.",
            "id": "2409.00873",
            "link": "http://arxiv.org/abs/2409.00873v1",
            "published": "2024-09-01T23:48:26+00:00",
            "updated": "2024-09-01T23:48:26+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 13
        },
        "2409.00904": {
            "authors": [
                "Zhanwen Liu",
                "Chao Li",
                "Yang Wang",
                "Nan Yang",
                "Xing Fan",
                "Jiaqi Ma",
                "Xiangmo Zhao"
            ],
            "title": "Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction",
            "abstract": "Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.",
            "id": "2409.00904",
            "link": "http://arxiv.org/abs/2409.00904v1",
            "published": "2024-09-02T02:36:18+00:00",
            "updated": "2024-09-02T02:36:18+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 33
        },
        "2409.00951": {
            "authors": [
                "Zoey Chen",
                "Zhao Mandi",
                "Homanga Bharadhwaj",
                "Mohit Sharma",
                "Shuran Song",
                "Abhishek Gupta",
                "Vikash Kumar"
            ],
            "title": "Semantically Controllable Augmentations for Generalizable Robot Learning",
            "abstract": "Generalization to unseen real-world scenarios for robot manipulation requires exposure to diverse datasets during training. However, collecting large real-world datasets is intractable due to high operational costs. For robot learning to generalize despite these challenges, it is essential to leverage sources of data or priors beyond the robot's direct experience. In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a data source. These generative models encompass a broad range of real-world scenarios beyond a robot's direct experience and can synthesize novel synthetic experiences that expose robotic agents to additional world priors aiding real-world generalization at no extra cost.   In particular, our approach leverages pre-trained generative models as an effective tool for data augmentation. We propose a generative augmentation framework for semantically controllable augmentations and rapidly multiplying robot datasets while inducing rich variations that enable real-world generalization. Based on diverse augmentations of robot data, we show how scalable robot manipulation policies can be trained and deployed both in simulation and in unseen real-world environments such as kitchens and table-tops. By demonstrating the effectiveness of image-text generative models in diverse real-world robotic applications, our generative augmentation framework provides a scalable and efficient path for boosting generalization in robot learning at no extra human cost.",
            "id": "2409.00951",
            "link": "http://arxiv.org/abs/2409.00951v1",
            "published": "2024-09-02T05:25:34+00:00",
            "updated": "2024-09-02T05:25:34+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ],
            "max_author_hindex": 26
        },
        "2409.00974": {
            "authors": [
                "Riccardo Taiello",
                "Sergen Cansiz",
                "Marc Vesin",
                "Francesco Cremonesi",
                "Lucia Innocenti",
                "Melek \u00d6nen",
                "Marco Lorenzi"
            ],
            "title": "Enhancing Privacy in Federated Learning: Secure Aggregation for Real-World Healthcare Applications",
            "abstract": "Deploying federated learning (FL) in real-world scenarios, particularly in healthcare, poses challenges in communication and security. In particular, with respect to the federated aggregation procedure, researchers have been focusing on the study of secure aggregation (SA) schemes to provide privacy guarantees over the model's parameters transmitted by the clients. Nevertheless, the practical availability of SA in currently available FL frameworks is currently limited, due to computational and communication bottlenecks. To fill this gap, this study explores the implementation of SA within the open-source Fed-BioMed framework. We implement and compare two SA protocols, Joye-Libert (JL) and Low Overhead Masking (LOM), by providing extensive benchmarks in a panel of healthcare data analysis problems. Our theoretical and experimental evaluations on four datasets demonstrate that SA protocols effectively protect privacy while maintaining task accuracy. Computational overhead during training is less than 1% on a CPU and less than 50% on a GPU for large models, with protection phases taking less than 10 seconds. Incorporating SA into Fed-BioMed impacts task accuracy by no more than 2% compared to non-SA scenarios. Overall this study demonstrates the feasibility of SA in real-world healthcare applications and contributes in reducing the gap towards the adoption of privacy-preserving technologies in sensitive applications.",
            "id": "2409.00974",
            "link": "http://arxiv.org/abs/2409.00974v1",
            "published": "2024-09-02T06:43:22+00:00",
            "updated": "2024-09-02T06:43:22+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 37
        },
        "2409.00991": {
            "authors": [
                "Xiaobin Lu",
                "Xiaobin Hu",
                "Jun Luo",
                "Ben Zhu",
                "Yaping Ruan",
                "Wenqi Ren"
            ],
            "title": "3D Priors-Guided Diffusion for Blind Face Restoration",
            "abstract": "Blind face restoration endeavors to restore a clear face image from a degraded counterpart. Recent approaches employing Generative Adversarial Networks (GANs) as priors have demonstrated remarkable success in this field. However, these methods encounter challenges in achieving a balance between realism and fidelity, particularly in complex degradation scenarios. To inherit the exceptional realism generative ability of the diffusion model and also constrained by the identity-aware fidelity, we propose a novel diffusion-based framework by embedding the 3D facial priors as structure and identity constraints into a denoising diffusion process. Specifically, in order to obtain more accurate 3D prior representations, the 3D facial image is reconstructed by a 3D Morphable Model (3DMM) using an initial restored face image that has been processed by a pretrained restoration network. A customized multi-level feature extraction method is employed to exploit both structural and identity information of 3D facial images, which are then mapped into the noise estimation process. In order to enhance the fusion of identity information into the noise estimation, we propose a Time-Aware Fusion Block (TAFB). This module offers a more efficient and adaptive fusion of weights for denoising, considering the dynamic nature of the denoising process in the diffusion model, which involves initial structure refinement followed by texture detail enhancement. Extensive experiments demonstrate that our network performs favorably against state-of-the-art algorithms on synthetic and real-world datasets for blind face restoration. The Code is released on our project page at https://github.com/838143396/3Diffusion.",
            "id": "2409.00991",
            "link": "http://arxiv.org/abs/2409.00991v2",
            "published": "2024-09-02T07:13:32+00:00",
            "updated": "2024-09-12T07:10:41+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 11
        },
        "2409.01014": {
            "authors": [
                "Xiaojie Xu",
                "Tianshuo Xu",
                "Fulong Ma",
                "Yingcong Chen"
            ],
            "title": "From Bird's-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model",
            "abstract": "We explore Bird's-Eye View (BEV) generation, converting a BEV map into its corresponding multi-view street images. Valued for its unified spatial representation aiding multi-sensor fusion, BEV is pivotal for various autonomous driving applications. Creating accurate street-view images from BEV maps is essential for portraying complex traffic scenarios and enhancing driving algorithms. Concurrently, diffusion-based conditional image generation models have demonstrated remarkable outcomes, adept at producing diverse, high-quality, and condition-aligned results. Nonetheless, the training of these models demands substantial data and computational resources. Hence, exploring methods to fine-tune these advanced models, like Stable Diffusion, for specific conditional generation tasks emerges as a promising avenue. In this paper, we introduce a practical framework for generating images from a BEV layout. Our approach comprises two main components: the Neural View Transformation and the Street Image Generation. The Neural View Transformation phase converts the BEV map into aligned multi-view semantic segmentation maps by learning the shape correspondence between the BEV and perspective views. Subsequently, the Street Image Generation phase utilizes these segmentations as a condition to guide a fine-tuned latent diffusion model. This finetuning process ensures both view and style consistency. Our model leverages the generative capacity of large pretrained diffusion models within traffic contexts, effectively yielding diverse and condition-coherent street view images.",
            "id": "2409.01014",
            "link": "http://arxiv.org/abs/2409.01014v1",
            "published": "2024-09-02T07:47:16+00:00",
            "updated": "2024-09-02T07:47:16+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.01038": {
            "authors": [
                "Yu Xiang Tan",
                "Malika Meghjani"
            ],
            "title": "Robust Vehicle Localization and Tracking in Rain using Street Maps",
            "abstract": "GPS-based vehicle localization and tracking suffers from unstable positional information commonly experienced in tunnel segments and in dense urban areas. Also, both Visual Odometry (VO) and Visual Inertial Odometry (VIO) are susceptible to adverse weather conditions that causes occlusions or blur on the visual input. In this paper, we propose a novel approach for vehicle localization that uses street network based map information to correct drifting odometry estimates and intermittent GPS measurements especially, in adversarial scenarios such as driving in rain and tunnels. Specifically, our approach is a flexible fusion algorithm that integrates intermittent GPS, drifting IMU and VO estimates together with 2D map information for robust vehicle localization and tracking. We refer to our approach as Map-Fusion. We robustly evaluate our proposed approach on four geographically diverse datasets from different countries ranging across clear and rain weather conditions. These datasets also include challenging visual segments in tunnels and underpasses. We show that with the integration of the map information, our Map-Fusion algorithm reduces the error of the state-of-the-art VO and VIO approaches across all datasets. We also validate our proposed algorithm in a real-world environment and in real-time on a hardware constrained mobile robot. Map-Fusion achieved 2.46m error in clear weather and 6.05m error in rain weather for a 150m route.",
            "id": "2409.01038",
            "link": "http://arxiv.org/abs/2409.01038v1",
            "published": "2024-09-02T08:15:12+00:00",
            "updated": "2024-09-02T08:15:12+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 16
        },
        "2409.01081": {
            "authors": [
                "Dingshuo Chen",
                "Zhixun Li",
                "Yuyan Ni",
                "Guibin Zhang",
                "Ding Wang",
                "Qiang Liu",
                "Shu Wu",
                "Jeffrey Xu Yu",
                "Liang Wang"
            ],
            "title": "Beyond Efficiency: Molecular Data Pruning for Enhanced Generalization",
            "abstract": "With the emergence of various molecular tasks and massive datasets, how to perform efficient training has become an urgent yet under-explored issue in the area. Data pruning (DP), as an oft-stated approach to saving training burdens, filters out less influential samples to form a coreset for training. However, the increasing reliance on pretrained models for molecular tasks renders traditional in-domain DP methods incompatible. Therefore, we propose a Molecular data Pruning framework for enhanced Generalization (MolPeg), which focuses on the source-free data pruning scenario, where data pruning is applied with pretrained models. By maintaining two models with different updating paces during training, we introduce a novel scoring function to measure the informativeness of samples based on the loss discrepancy. As a plug-and-play framework, MolPeg realizes the perception of both source and target domain and consistently outperforms existing DP methods across four downstream tasks. Remarkably, it can surpass the performance obtained from full-dataset training, even when pruning up to 60-70% of the data on HIV and PCBA dataset. Our work suggests that the discovery of effective data-pruning metrics could provide a viable path to both enhanced efficiency and superior generalization in transfer learning.",
            "id": "2409.01081",
            "link": "http://arxiv.org/abs/2409.01081v1",
            "published": "2024-09-02T09:06:04+00:00",
            "updated": "2024-09-02T09:06:04+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "q-bio.BM"
            ],
            "max_author_hindex": 77
        },
        "2409.01086": {
            "authors": [
                "Xiaolong Wang",
                "Zhi-Qi Cheng",
                "Jue Wang",
                "Xiaojiang Peng"
            ],
            "title": "DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image Editing",
            "abstract": "Fashion image editing is a crucial tool for designers to convey their creative ideas by visualizing design concepts interactively. Current fashion image editing techniques, though advanced with multimodal prompts and powerful diffusion models, often struggle to accurately identify editing regions and preserve the desired garment texture detail. To address these challenges, we introduce a new multimodal fashion image editing architecture based on latent diffusion models, called Detail-Preserved Diffusion Models (DPDEdit). DPDEdit guides the fashion image generation of diffusion models by integrating text prompts, region masks, human pose images, and garment texture images. To precisely locate the editing region, we first introduce Grounded-SAM to predict the editing region based on the user's textual description, and then combine it with other conditions to perform local editing. To transfer the detail of the given garment texture into the target fashion image, we propose a texture injection and refinement mechanism. Specifically, this mechanism employs a decoupled cross-attention layer to integrate textual descriptions and texture images, and incorporates an auxiliary U-Net to preserve the high-frequency details of generated garment texture. Additionally, we extend the VITON-HD dataset using a multimodal large language model to generate paired samples with texture images and textual descriptions. Extensive experiments show that our DPDEdit outperforms state-of-the-art methods in terms of image fidelity and coherence with the given multimodal inputs.",
            "id": "2409.01086",
            "link": "http://arxiv.org/abs/2409.01086v1",
            "published": "2024-09-02T09:15:26+00:00",
            "updated": "2024-09-02T09:15:26+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 22
        },
        "2409.01092": {
            "authors": [
                "Wenshuai Liu",
                "Yaru Fu",
                "Yongna Guo",
                "Fu Lee Wang",
                "Wen Sun",
                "Yan Zhang"
            ],
            "title": "Two-Timescale Synchronization and Migration for Digital Twin Networks: A Multi-Agent Deep Reinforcement Learning Approach",
            "abstract": "Digital twins (DTs) have emerged as a promising enabler for representing the real-time states of physical worlds and realizing self-sustaining systems. In practice, DTs of physical devices, such as mobile users (MUs), are commonly deployed in multi-access edge computing (MEC) networks for the sake of reducing latency. To ensure the accuracy and fidelity of DTs, it is essential for MUs to regularly synchronize their status with their DTs. However, MU mobility introduces significant challenges to DT synchronization. Firstly, MU mobility triggers DT migration which could cause synchronization failures. Secondly, MUs require frequent synchronization with their DTs to ensure DT fidelity. Nonetheless, DT migration among MEC servers, caused by MU mobility, may occur infrequently. Accordingly, we propose a two-timescale DT synchronization and migration framework with reliability consideration by establishing a non-convex stochastic problem to minimize the long-term average energy consumption of MUs. We use Lyapunov theory to convert the reliability constraints and reformulate the new problem as a partially observable Markov decision-making process (POMDP). Furthermore, we develop a heterogeneous agent proximal policy optimization with Beta distribution (Beta-HAPPO) method to solve it. Numerical results show that our proposed Beta-HAPPO method achieves significant improvements in energy savings when compared with other benchmarks.",
            "id": "2409.01092",
            "link": "http://arxiv.org/abs/2409.01092v1",
            "published": "2024-09-02T09:20:46+00:00",
            "updated": "2024-09-02T09:20:46+00:00",
            "primary_category": "cs.ET",
            "categories": [
                "cs.ET",
                "cs.AI",
                "cs.NI",
                "C.2.3; C.2.4"
            ],
            "max_author_hindex": 25
        },
        "2409.01124": {
            "authors": [
                "Jin Song",
                "Ming Zhong",
                "George Em Karniadakis",
                "Zhenya Yan"
            ],
            "title": "Two-stage initial-value iterative physics-informed neural networks for simulating solitary waves of nonlinear wave equations",
            "abstract": "We propose a new two-stage initial-value iterative neural network (IINN) algorithm for solitary wave computations of nonlinear wave equations based on traditional numerical iterative methods and physics-informed neural networks (PINNs). Specifically, the IINN framework consists of two subnetworks, one of which is used to fit a given initial value, and the other incorporates physical information and continues training on the basis of the first subnetwork. Importantly, the IINN method does not require any additional data information including boundary conditions, apart from the given initial value. Corresponding theoretical guarantees are provided to demonstrate the effectiveness of our IINN method. The proposed IINN method is efficiently applied to learn some types of solutions in different nonlinear wave equations, including the one-dimensional (1D) nonlinear Schr\\\"odinger equations (NLS) equation (with and without potentials), the 1D saturable NLS equation with PT -symmetric optical lattices, the 1D focusing-defocusing coupled NLS equations, the KdV equation, the two-dimensional (2D) NLS equation with potentials, the 2D amended GP equation with a potential, the (2+1)-dimensional KP equation, and the 3D NLS equation with a potential. These applications serve as evidence for the efficacy of our method. Finally, by comparing with the traditional methods, we demonstrate the advantages of the proposed IINN method.",
            "id": "2409.01124",
            "link": "http://arxiv.org/abs/2409.01124v1",
            "published": "2024-09-02T10:00:02+00:00",
            "updated": "2024-09-02T10:00:02+00:00",
            "primary_category": "physics.comp-ph",
            "categories": [
                "physics.comp-ph",
                "cs.AI",
                "cs.LG",
                "math-ph",
                "math.MP",
                "nlin.PS",
                "nlin.SI"
            ],
            "max_author_hindex": 128
        },
        "2409.01145": {
            "authors": [
                "Haoran Yang",
                "Xiangyu Zhao",
                "Sirui Huang",
                "Qing Li",
                "Guandong Xu"
            ],
            "title": "LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning",
            "abstract": "Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised graph learning that has attracted attention across various application scenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet to be explored. Because conventional augmentation techniques like feature embedding masking cannot directly process textual attributes on TAGs. A naive strategy for applying GCL to TAGs is to encode the textual attributes into feature embeddings via a language model and then feed the embeddings into the following GCL module for processing. Such a strategy faces three key challenges: I) failure to avoid information loss, II) semantic loss during the text encoding phase, and III) implicit augmentation constraints that lead to uncontrollable and incomprehensible results. In this paper, we propose a novel GCL framework named LATEX-GCL to utilize Large Language Models (LLMs) to produce textual augmentations and LLMs' powerful natural language processing (NLP) abilities to address the three limitations aforementioned to pave the way for applying GCL to TAG tasks. Extensive experiments on four high-quality TAG datasets illustrate the superiority of the proposed LATEX-GCL method. The source codes and datasets are released to ease the reproducibility, which can be accessed via this link: https://anonymous.4open.science/r/LATEX-GCL-0712.",
            "id": "2409.01145",
            "link": "http://arxiv.org/abs/2409.01145v1",
            "published": "2024-09-02T10:30:55+00:00",
            "updated": "2024-09-02T10:30:55+00:00",
            "primary_category": "cs.SI",
            "categories": [
                "cs.SI",
                "cs.AI"
            ],
            "max_author_hindex": 23
        },
        "2409.01148": {
            "authors": [
                "Mingyuan Yao",
                "Yukang Huo",
                "Qingbin Tian",
                "Jiayin Zhao",
                "Xiao Liu",
                "Ruifeng Wang",
                "Haihua Wang"
            ],
            "title": "FMRFT: Fusion Mamba and DETR for Query Time Sequence Intersection Fish Tracking",
            "abstract": "Growth, abnormal behavior, and diseases of fish can be early detected by monitoring fish tracking through the method of image processing, which is of great significance for factory aquaculture. However, underwater reflections and some reasons with fish, such as the high similarity , rapid swimming caused by stimuli and multi-object occlusion bring challenges to multi-target tracking of fish. To address these challenges, this paper establishes a complex multi-scene sturgeon tracking dataset and proposes a real-time end-to-end fish tracking model, FMRFT. In this model, the Mamba In Mamba (MIM) architecture with low memory consumption is introduced into the tracking algorithm to realize multi-frame video timing memory and fast feature extraction, which improves the efficiency of correlation analysis for contiguous frames in multi-fish video. Additionally, the superior feature interaction and a priori frame processing capabilities of RT-DETR are leveraged to provide an effective tracking algorithm. By incorporating the QTSI query interaction processing module, the model effectively handles occluded objects and redundant tracking frames, resulting in more accurate and stable fish tracking. Trained and tested on the dataset, the model achieves an IDF1 score of 90.3% and a MOTA accuracy of 94.3%. Experimental results demonstrate that the proposed FMRFT model effectively addresses the challenges of high similarity and mutual occlusion in fish populations, enabling accurate tracking in factory farming environments.",
            "id": "2409.01148",
            "link": "http://arxiv.org/abs/2409.01148v1",
            "published": "2024-09-02T10:33:45+00:00",
            "updated": "2024-09-02T10:33:45+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 11
        },
        "2409.01216": {
            "authors": [
                "Luoyu Mei",
                "Shuai Wang",
                "Yun Cheng",
                "Ruofeng Liu",
                "Zhimeng Yin",
                "Wenchao Jiang",
                "Shuai Wang",
                "Wei Gong"
            ],
            "title": "ESP-PCT: Enhanced VR Semantic Performance through Efficient Compression of Temporal and Spatial Redundancies in Point Cloud Transformers",
            "abstract": "Semantic recognition is pivotal in virtual reality (VR) applications, enabling immersive and interactive experiences. A promising approach is utilizing millimeter-wave (mmWave) signals to generate point clouds. However, the high computational and memory demands of current mmWave point cloud models hinder their efficiency and reliability. To address this limitation, our paper introduces ESP-PCT, a novel Enhanced Semantic Performance Point Cloud Transformer with a two-stage semantic recognition framework tailored for VR applications. ESP-PCT takes advantage of the accuracy of sensory point cloud data and optimizes the semantic recognition process, where the localization and focus stages are trained jointly in an end-to-end manner. We evaluate ESP-PCT on various VR semantic recognition conditions, demonstrating substantial enhancements in recognition efficiency. Notably, ESP-PCT achieves a remarkable accuracy of 93.2% while reducing the computational requirements (FLOPs) by 76.9% and memory usage by 78.2% compared to the existing Point Transformer model simultaneously. These underscore ESP-PCT's potential in VR semantic recognition by achieving high accuracy and reducing redundancy. The code and data of this project are available at \\url{https://github.com/lymei-SEU/ESP-PCT}.",
            "id": "2409.01216",
            "link": "http://arxiv.org/abs/2409.01216v1",
            "published": "2024-09-02T12:48:40+00:00",
            "updated": "2024-09-02T12:48:40+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 19
        },
        "2409.01354": {
            "authors": [
                "Shahbaz Rezaei",
                "Xin Liu"
            ],
            "title": "Explanation Space: A New Perspective into Time Series Interpretability",
            "abstract": "Human understandable explanation of deep learning models is necessary for many critical and sensitive applications. Unlike image or tabular data where the importance of each input feature (for the classifier's decision) can be directly projected into the input, time series distinguishable features (e.g. dominant frequency) are often hard to manifest in time domain for a user to easily understand. Moreover, most explanation methods require a baseline value as an indication of the absence of any feature. However, the notion of lack of feature, which is often defined as black pixels for vision tasks or zero/mean values for tabular data, is not well-defined in time series. Despite the adoption of explainable AI methods (XAI) from tabular and vision domain into time series domain, these differences limit the application of these XAI methods in practice. In this paper, we propose a simple yet effective method that allows a model originally trained on time domain to be interpreted in other explanation spaces using existing methods. We suggest four explanation spaces that each can potentially alleviate these issues in certain types of time series. Our method can be readily adopted in existing platforms without any change to trained models or XAI methods. The code is available at https://github.com/shrezaei/TS-X-spaces.",
            "id": "2409.01354",
            "link": "http://arxiv.org/abs/2409.01354v2",
            "published": "2024-09-02T16:15:26+00:00",
            "updated": "2024-09-05T02:00:12+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 11
        },
        "2409.01362": {
            "authors": [
                "Xinyu Chen",
                "HanQin Cai",
                "Fuqiang Liu",
                "Jinhua Zhao"
            ],
            "title": "Correlating Time Series with Interpretable Convolutional Kernels",
            "abstract": "This study addresses the problem of convolutional kernel learning in univariate, multivariate, and multidimensional time series data, which is crucial for interpreting temporal patterns in time series and supporting downstream machine learning tasks. First, we propose formulating convolutional kernel learning for univariate time series as a sparse regression problem with a non-negative constraint, leveraging the properties of circular convolution and circulant matrices. Second, to generalize this approach to multivariate and multidimensional time series data, we use tensor computations, reformulating the convolutional kernel learning problem in the form of tensors. This is further converted into a standard sparse regression problem through vectorization and tensor unfolding operations. In the proposed methodology, the optimization problem is addressed using the existing non-negative subspace pursuit method, enabling the convolutional kernel to capture temporal correlations and patterns. To evaluate the proposed model, we apply it to several real-world time series datasets. On the multidimensional rideshare and taxi trip data from New York City and Chicago, the convolutional kernels reveal interpretable local correlations and cyclical patterns, such as weekly seasonality. In the context of multidimensional fluid flow data, both local and nonlocal correlations captured by the convolutional kernels can reinforce tensor factorization, leading to performance improvements in fluid flow reconstruction tasks. Thus, this study lays an insightful foundation for automatically learning convolutional kernels from time series data, with an emphasis on interpretability through sparsity and non-negativity constraints.",
            "id": "2409.01362",
            "link": "http://arxiv.org/abs/2409.01362v1",
            "published": "2024-09-02T16:29:21+00:00",
            "updated": "2024-09-02T16:29:21+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 45
        },
        "2409.01382": {
            "authors": [
                "Musfiqur Rahman",
                "SayedHassan Khatoonabadi",
                "Ahmad Abdellatif",
                "Emad Shihab"
            ],
            "title": "Automatic Detection of LLM-generated Code: A Case Study of Claude 3 Haiku",
            "abstract": "Using Large Language Models (LLMs) has gained popularity among software developers for generating source code. However, the use of LLM-generated code can introduce risks of adding suboptimal, defective, and vulnerable code. This makes it necessary to devise methods for the accurate detection of LLM-generated code. Toward this goal, we perform a case study of Claude 3 Haiku (or Claude 3 for brevity) on CodeSearchNet dataset. We divide our analyses into two parts: function-level and class-level. We extract 22 software metric features, such as Code Lines and Cyclomatic Complexity, for each level of granularity. We then analyze code snippets generated by Claude 3 and their human-authored counterparts using the extracted features to understand how unique the code generated by Claude 3 is. In the following step, we use the unique characteristics of Claude 3-generated code to build Machine Learning (ML) models and identify which features of the code snippets make them more detectable by ML models. Our results indicate that Claude 3 tends to generate longer functions, but shorter classes than humans, and this characteristic can be used to detect Claude 3-generated code with ML models with 82% and 66% accuracies for function-level and class-level snippets, respectively.",
            "id": "2409.01382",
            "link": "http://arxiv.org/abs/2409.01382v1",
            "published": "2024-09-02T17:25:15+00:00",
            "updated": "2024-09-02T17:25:15+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 43
        },
        "2409.01411": {
            "authors": [
                "Zirui Xu",
                "Vasileios Tzoumas"
            ],
            "title": "Performance-Aware Self-Configurable Multi-Agent Networks: A Distributed Submodular Approach for Simultaneous Coordination and Network Design",
            "abstract": "We introduce the first, to our knowledge, rigorous approach that enables multi-agent networks to self-configure their communication topology to balance the trade-off between scalability and optimality during multi-agent planning. We are motivated by the future of ubiquitous collaborative autonomy where numerous distributed agents will be coordinating via agent-to-agent communication to execute complex tasks such as traffic monitoring, event detection, and environmental exploration. But the explosion of information in such large-scale networks currently curtails their deployment due to impractical decision times induced by the computational and communication requirements of the existing near-optimal coordination algorithms. To overcome this challenge, we present the AlterNAting COordination and Network-Design Algorithm (Anaconda), a scalable algorithm that also enjoys near-optimality guarantees. Subject to the agents' bandwidth constraints, Anaconda enables the agents to optimize their local communication neighborhoods such that the action-coordination approximation performance of the network is maximized. Compared to the state of the art, Anaconda is an anytime self-configurable algorithm that quantifies its suboptimality guarantee for any type of network, from fully disconnected to fully centralized, and that, for sparse networks, is one order faster in terms of decision speed. To develop the algorithm, we quantify the suboptimality cost due to decentralization, i.e., due to communication-minimal distributed coordination. We also employ tools inspired by the literature on multi-armed bandits and submodular maximization subject to cardinality constraints. We demonstrate Anaconda in simulated scenarios of area monitoring and compare it with a state-of-the-art algorithm.",
            "id": "2409.01411",
            "link": "http://arxiv.org/abs/2409.01411v1",
            "published": "2024-09-02T18:11:33+00:00",
            "updated": "2024-09-02T18:11:33+00:00",
            "primary_category": "eess.SY",
            "categories": [
                "eess.SY",
                "cs.AI",
                "cs.MA",
                "cs.RO",
                "cs.SY",
                "math.OC"
            ],
            "max_author_hindex": 19
        },
        "2409.01534": {
            "authors": [
                "Yaozong Gan",
                "Guang Li",
                "Ren Togo",
                "Keisuke Maeda",
                "Takahiro Ogawa",
                "Miki Haseyama"
            ],
            "title": "Think Twice Before Recognizing: Large Multimodal Models for General Fine-grained Traffic Sign Recognition",
            "abstract": "We propose a new strategy called think twice before recognizing to improve fine-grained traffic sign recognition (TSR). Fine-grained TSR in the wild is difficult due to the complex road conditions, and existing approaches particularly struggle with cross-country TSR when data is lacking. Our strategy achieves effective fine-grained TSR by stimulating the multiple-thinking capability of large multimodal models (LMM). We introduce context, characteristic, and differential descriptions to design multiple thinking processes for the LMM. The context descriptions with center coordinate prompt optimization help the LMM to locate the target traffic sign in the original road images containing multiple traffic signs and filter irrelevant answers through the proposed prior traffic sign hypothesis. The characteristic description is based on few-shot in-context learning of template traffic signs, which decreases the cross-domain difference and enhances the fine-grained recognition capability of the LMM. The differential descriptions of similar traffic signs optimize the multimodal thinking capability of the LMM. The proposed method is independent of training data and requires only simple and uniform instructions. We conducted extensive experiments on three benchmark datasets and two real-world datasets from different countries, and the proposed method achieves state-of-the-art TSR results on all five datasets.",
            "id": "2409.01534",
            "link": "http://arxiv.org/abs/2409.01534v1",
            "published": "2024-09-03T02:08:47+00:00",
            "updated": "2024-09-03T02:08:47+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ],
            "max_author_hindex": 40
        },
        "2409.01540": {
            "authors": [
                "Deniz Aykac",
                "Joel Brogan",
                "Nell Barber",
                "Ryan Shivers",
                "Bob Zhang",
                "Dallas Sacca",
                "Ryan Tipton",
                "Gavin Jager",
                "Austin Garret",
                "Matthew Love",
                "Jim Goddard",
                "David Cornett III",
                "David S. Bolme"
            ],
            "title": "Long-Range Biometric Identification in Real World Scenarios: A Comprehensive Evaluation Framework Based on Missions",
            "abstract": "The considerable body of data available for evaluating biometric recognition systems in Research and Development (R\\&D) environments has contributed to the increasingly common problem of target performance mismatch. Biometric algorithms are frequently tested against data that may not reflect the real world applications they target. From a Testing and Evaluation (T\\&E) standpoint, this domain mismatch causes difficulty assessing when improvements in State-of-the-Art (SOTA) research actually translate to improved applied outcomes. This problem can be addressed with thoughtful preparation of data and experimental methods to reflect specific use-cases and scenarios.   To that end, this paper evaluates research solutions for identifying individuals at ranges and altitudes, which could support various application areas such as counterterrorism, protection of critical infrastructure facilities, military force protection, and border security. We address challenges including image quality issues and reliance on face recognition as the sole biometric modality. By fusing face and body features, we propose developing robust biometric systems for effective long-range identification from both the ground and steep pitch angles. Preliminary results show promising progress in whole-body recognition. This paper presents these early findings and discusses potential future directions for advancing long-range biometric identification systems based on mission-driven metrics.",
            "id": "2409.01540",
            "link": "http://arxiv.org/abs/2409.01540v1",
            "published": "2024-09-03T02:17:36+00:00",
            "updated": "2024-09-03T02:17:36+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 24
        },
        "2409.01560": {
            "authors": [
                "Bin Fu",
                "Qiyang Wan",
                "Jialin Li",
                "Ruiping Wang",
                "Xilin Chen"
            ],
            "title": "Blocks as Probes: Dissecting Categorization Ability of Large Multimodal Models",
            "abstract": "Categorization, a core cognitive ability in humans that organizes objects based on common features, is essential to cognitive science as well as computer vision. To evaluate the categorization ability of visual AI models, various proxy tasks on recognition from datasets to open world scenarios have been proposed. Recent development of Large Multimodal Models (LMMs) has demonstrated impressive results in high-level visual tasks, such as visual question answering, video temporal reasoning, etc., utilizing the advanced architectures and large-scale multimodal instruction tuning. Previous researchers have developed holistic benchmarks to measure the high-level visual capability of LMMs, but there is still a lack of pure and in-depth quantitative evaluation of the most fundamental categorization ability. According to the research on human cognitive process, categorization can be seen as including two parts: category learning and category use. Inspired by this, we propose a novel, challenging, and efficient benchmark based on composite blocks, called ComBo, which provides a disentangled evaluation framework and covers the entire categorization process from learning to use. By analyzing the results of multiple evaluation tasks, we find that although LMMs exhibit acceptable generalization ability in learning new categories, there are still gaps compared to humans in many ways, such as fine-grained perception of spatial relationship and abstract category understanding. Through the study of categorization, we can provide inspiration for the further development of LMMs in terms of interpretability and generalization.",
            "id": "2409.01560",
            "link": "http://arxiv.org/abs/2409.01560v1",
            "published": "2024-09-03T02:55:36+00:00",
            "updated": "2024-09-03T02:55:36+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 35
        },
        "2409.01572": {
            "authors": [
                "Hamza Farooq",
                "Zuhair Zafar",
                "Ahsan Saadat",
                "Tariq M Khan",
                "Shahzaib Iqbal",
                "Imran Razzak"
            ],
            "title": "LSSF-Net: Lightweight Segmentation with Self-Awareness, Spatial Attention, and Focal Modulation",
            "abstract": "Accurate segmentation of skin lesions within dermoscopic images plays a crucial role in the timely identification of skin cancer for computer-aided diagnosis on mobile platforms. However, varying shapes of the lesions, lack of defined edges, and the presence of obstructions such as hair strands and marker colors make this challenge more complex. \\textcolor{red}Additionally, skin lesions often exhibit subtle variations in texture and color that are difficult to differentiate from surrounding healthy skin, necessitating models that can capture both fine-grained details and broader contextual information. Currently, melanoma segmentation models are commonly based on fully connected networks and U-Nets. However, these models often struggle with capturing the complex and varied characteristics of skin lesions, such as the presence of indistinct boundaries and diverse lesion appearances, which can lead to suboptimal segmentation performance.To address these challenges, we propose a novel lightweight network specifically designed for skin lesion segmentation utilizing mobile devices, featuring a minimal number of learnable parameters (only 0.8 million). This network comprises an encoder-decoder architecture that incorporates conformer-based focal modulation attention, self-aware local and global spatial attention, and split channel-shuffle. The efficacy of our model has been evaluated on four well-established benchmark datasets for skin lesion segmentation: ISIC 2016, ISIC 2017, ISIC 2018, and PH2. Empirical findings substantiate its state-of-the-art performance, notably reflected in a high Jaccard index.",
            "id": "2409.01572",
            "link": "http://arxiv.org/abs/2409.01572v1",
            "published": "2024-09-03T03:06:32+00:00",
            "updated": "2024-09-03T03:06:32+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 30
        },
        "2409.01573": {
            "authors": [
                "Liang Geng"
            ],
            "title": "Improving Apple Object Detection with Occlusion-Enhanced Distillation",
            "abstract": "Apples growing in natural environments often face severe visual obstructions from leaves and branches. This significantly increases the risk of false detections in object detection tasks, thereby escalating the challenge. Addressing this issue, we introduce a technique called \"Occlusion-Enhanced Distillation\" (OED). This approach utilizes occlusion information to regularize the learning of semantically aligned features on occluded datasets and employs Exponential Moving Average (EMA) to enhance training stability. Specifically, we first design an occlusion-enhanced dataset that integrates Grounding DINO and SAM methods to extract occluding elements such as leaves and branches from each sample, creating occlusion examples that reflect the natural growth state of fruits. Additionally, we propose a multi-scale knowledge distillation strategy, where the student network uses images with increased occlusions as inputs, while the teacher network employs images without natural occlusions. Through this setup, the strategy guides the student network to learn from the teacher across scales of semantic and local features alignment, effectively narrowing the feature distance between occluded and non-occluded targets and enhancing the robustness of object detection. Lastly, to improve the stability of the student network, we introduce the EMA strategy, which aids the student network in learning more generalized feature expressions that are less affected by the noise of individual image occlusions. Our method significantly outperforms current state-of-the-art techniques through extensive comparative experiments.",
            "id": "2409.01573",
            "link": "http://arxiv.org/abs/2409.01573v1",
            "published": "2024-09-03T03:11:48+00:00",
            "updated": "2024-09-03T03:11:48+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 7
        },
        "2409.01581": {
            "authors": [
                "Zixuan Guo",
                "Yifan Xie",
                "Weijing Xie",
                "Peng Huang",
                "Fei Ma",
                "Fei Richard Yu"
            ],
            "title": "GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color Point Clouds via 3D Gaussian Splatting",
            "abstract": "Dense colored point clouds enhance visual perception and are of significant value in various robotic applications. However, existing learning-based point cloud upsampling methods are constrained by computational resources and batch processing strategies, which often require subdividing point clouds into smaller patches, leading to distortions that degrade perceptual quality. To address this challenge, we propose a novel 2D-3D hybrid colored point cloud upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for robotic perception. This approach leverages 3DGS to bridge 3D point clouds with their 2D rendered images in robot vision systems. A dual scale rendered image restoration network transforms sparse point cloud renderings into dense representations, which are then input into 3DGS along with precise robot camera poses and interpolated sparse point clouds to reconstruct dense 3D point clouds. We have made a series of enhancements to the vanilla 3DGS, enabling precise control over the number of points and significantly boosting the quality of the upsampled point cloud for robotic scene understanding. Our framework supports processing entire point clouds on a single consumer-grade GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation and thus producing high-quality, dense colored point clouds with millions of points for robot navigation and manipulation tasks. Extensive experimental results on generating million-level point cloud data validate the effectiveness of our method, substantially improving the quality of colored point clouds and demonstrating significant potential for applications involving large-scale point clouds in autonomous robotics and human-robot interaction scenarios.",
            "id": "2409.01581",
            "link": "http://arxiv.org/abs/2409.01581v1",
            "published": "2024-09-03T03:35:04+00:00",
            "updated": "2024-09-03T03:35:04+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 72
        },
        "2409.01596": {
            "authors": [
                "Ruben D. Fonnegra",
                "Maria Liliana Hern\u00e1ndez",
                "Juan C. Caicedo",
                "Gloria M. D\u00edaz"
            ],
            "title": "A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models",
            "abstract": "Contrast-enhancement pattern analysis is critical in breast magnetic resonance imaging (MRI) to distinguish benign from probably malignant tumors. However, contrast-enhanced image acquisitions are time-consuming and very expensive. As an alternative to physical acquisition, this paper proposes a comprehensive pipeline for the generation of accurate long-term (late) contrast-enhanced breast MRI from the early counterpart. The proposed strategy focuses on preserving the contrast agent pattern in the enhanced regions while maintaining visual properties in the entire synthesized images. To that end, a novel loss function that leverages the biological behavior of contrast agent (CA) in tissue, given by the Time-Intensity (TI) enhancement curve, is proposed to optimize a pixel-attention based generative model. In addition, unlike traditional normalization and standardization methods, we developed a new normalization strategy that maintains the contrast enhancement pattern across the image sequences at multiple timestamps. This ensures the prevalence of the CA pattern after image preprocessing, unlike conventional approaches. Furthermore, in order to objectively evaluate the clinical quality of the synthesized images, two metrics are also introduced to measure the differences between the TI curves of enhanced regions of the acquired and synthesized images. The experimental results showed that the proposed strategy generates images that significantly outperform diagnostic quality in contrast-enhanced regions while maintaining the spatial features of the entire image. This results suggest a potential use of synthetic late enhanced images generated via deep learning in clinical scenarios.",
            "id": "2409.01596",
            "link": "http://arxiv.org/abs/2409.01596v1",
            "published": "2024-09-03T04:31:49+00:00",
            "updated": "2024-09-03T04:31:49+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 29
        },
        "2409.01652": {
            "authors": [
                "Wenlong Huang",
                "Chen Wang",
                "Yunzhu Li",
                "Ruohan Zhang",
                "Li Fei-Fei"
            ],
            "title": "ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation",
            "abstract": "Representing robotic manipulation tasks as constraints that associate the robot and the environment is a promising way to encode desired robot behaviors. However, it remains unclear how to formulate the constraints such that they are 1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable by off-the-shelf solvers to produce robot actions in real-time. In this work, we introduce Relational Keypoint Constraints (ReKep), a visually-grounded representation for constraints in robotic manipulation. Specifically, ReKep is expressed as Python functions mapping a set of 3D keypoints in the environment to a numerical cost. We demonstrate that by representing a manipulation task as a sequence of Relational Keypoint Constraints, we can employ a hierarchical optimization procedure to solve for robot actions (represented by a sequence of end-effector poses in SE(3)) with a perception-action loop at a real-time frequency. Furthermore, in order to circumvent the need for manual specification of ReKep for each new task, we devise an automated procedure that leverages large vision models and vision-language models to produce ReKep from free-form language instructions and RGB-D observations. We present system implementations on a wheeled single-arm platform and a stationary dual-arm platform that can perform a large variety of manipulation tasks, featuring multi-stage, in-the-wild, bimanual, and reactive behaviors, all without task-specific data or environment models. Website at https://rekep-robot.github.io.",
            "id": "2409.01652",
            "link": "http://arxiv.org/abs/2409.01652v1",
            "published": "2024-09-03T06:45:22+00:00",
            "updated": "2024-09-03T06:45:22+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 17
        },
        "2409.01672": {
            "authors": [
                "Avraham Chapman",
                "Haiming Xu",
                "Lingqiao Liu"
            ],
            "title": "Enhancing Fine-Grained Visual Recognition in the Low-Data Regime Through Feature Magnitude Regularization",
            "abstract": "Training a fine-grained image recognition model with limited data presents a significant challenge, as the subtle differences between categories may not be easily discernible amidst distracting noise patterns. One commonly employed strategy is to leverage pretrained neural networks, which can generate effective feature representations for constructing an image classification model with a restricted dataset. However, these pretrained neural networks are typically trained for different tasks than the fine-grained visual recognition (FGVR) task at hand, which can lead to the extraction of less relevant features. Moreover, in the context of building FGVR models with limited data, these irrelevant features can dominate the training process, overshadowing more useful, generalizable discriminative features. Our research has identified a surprisingly simple solution to this challenge: we introduce a regularization technique to ensure that the magnitudes of the extracted features are evenly distributed. This regularization is achieved by maximizing the uniformity of feature magnitude distribution, measured through the entropy of the normalized features. The motivation behind this regularization is to remove bias in feature magnitudes from pretrained models, where some features may be more prominent and, consequently, more likely to be used for classification. Additionally, we have developed a dynamic weighting mechanism to adjust the strength of this regularization throughout the learning process. Despite its apparent simplicity, our approach has demonstrated significant performance improvements across various fine-grained visual recognition datasets.",
            "id": "2409.01672",
            "link": "http://arxiv.org/abs/2409.01672v2",
            "published": "2024-09-03T07:32:46+00:00",
            "updated": "2024-09-07T05:36:52+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 25
        },
        "2409.01909": {
            "authors": [
                "Lipeng Ma",
                "Weidong Yang",
                "Sihang Jiang",
                "Ben Fei",
                "Mingjie Zhou",
                "Shuhao Li",
                "Bo Xu",
                "Yanghua Xiao"
            ],
            "title": "LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models",
            "abstract": "Logs play a critical role in providing essential information for system monitoring and troubleshooting. Recently, with the success of pre-trained language models (PLMs) and large language models (LLMs) in natural language processing (NLP), smaller PLMs (such as BERT) and LLMs (like ChatGPT) have become the current mainstream approaches for log analysis. While LLMs possess rich knowledge, their high computational costs and unstable performance make LLMs impractical for analyzing logs directly. In contrast, smaller PLMs can be fine-tuned for specific tasks even with limited computational resources, making them more practical. However, these smaller PLMs face challenges in understanding logs comprehensively due to their limited expert knowledge. To better utilize the knowledge embedded within LLMs for log understanding, this paper introduces a novel knowledge enhancement framework, called LUK, which acquires expert knowledge from LLMs to empower log understanding on a smaller PLM. Specifically, we design a multi-expert collaboration framework based on LLMs consisting of different roles to acquire expert knowledge. In addition, we propose two novel pre-training tasks to enhance the log pre-training with expert knowledge. LUK achieves state-of-the-art results on different log analysis tasks and extensive experiments demonstrate expert knowledge from LLMs can be utilized more effectively to understand logs.",
            "id": "2409.01909",
            "link": "http://arxiv.org/abs/2409.01909v1",
            "published": "2024-09-03T13:58:34+00:00",
            "updated": "2024-09-03T13:58:34+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 57
        },
        "2409.01952": {
            "authors": [
                "Abdullah Arafat Miah",
                "Yu Bi"
            ],
            "title": "Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor",
            "abstract": "Deep neural networks (DNNs) have long been recognized as vulnerable to backdoor attacks. By providing poisoned training data in the fine-tuning process, the attacker can implant a backdoor into the victim model. This enables input samples meeting specific textual trigger patterns to be classified as target labels of the attacker's choice. While such black-box attacks have been well explored in both computer vision and natural language processing (NLP), backdoor attacks relying on white-box attack philosophy have hardly been thoroughly investigated. In this paper, we take the first step to introduce a new type of backdoor attack that conceals itself within the underlying model architecture. Specifically, we propose to design separate backdoor modules consisting of two functions: trigger detection and noise injection. The add-on modules of model architecture layers can detect the presence of input trigger tokens and modify layer weights using Gaussian noise to disturb the feature distribution of the baseline model. We conduct extensive experiments to evaluate our attack methods using two model architecture settings on five different large language datasets. We demonstrate that the training-free architectural backdoor on a large language model poses a genuine threat. Unlike the-state-of-art work, it can survive the rigorous fine-tuning and retraining process, as well as evade output probability-based defense methods (i.e. BDDR). All the code and data is available https://github.com/SiSL-URI/Arch_Backdoor_LLM.",
            "id": "2409.01952",
            "link": "http://arxiv.org/abs/2409.01952v2",
            "published": "2024-09-03T14:54:16+00:00",
            "updated": "2024-09-09T15:37:15+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.AR"
            ],
            "max_author_hindex": 8
        },
        "2409.01995": {
            "authors": [
                "Yiwei Guo",
                "Zhihan Li",
                "Junjie Li",
                "Chenpeng Du",
                "Hankun Wang",
                "Shuai Wang",
                "Xie Chen",
                "Kai Yu"
            ],
            "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
            "abstract": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
            "id": "2409.01995",
            "link": "http://arxiv.org/abs/2409.01995v2",
            "published": "2024-09-03T15:41:07+00:00",
            "updated": "2024-09-11T04:20:50+00:00",
            "primary_category": "eess.AS",
            "categories": [
                "eess.AS",
                "cs.AI",
                "cs.SD"
            ],
            "max_author_hindex": 26
        },
        "2409.02045": {
            "authors": [
                "Chenghao Qian",
                "Mahdi Rezaei",
                "Saeed Anwar",
                "Wenjing Li",
                "Tanveer Hussain",
                "Mohsen Azarmi",
                "Wei Wang"
            ],
            "title": "AllWeatherNet:Unified Image enhancement for autonomous driving under adverse weather and lowlight-conditions",
            "abstract": "Adverse conditions like snow, rain, nighttime, and fog, pose challenges for autonomous driving perception systems. Existing methods have limited effectiveness in improving essential computer vision tasks, such as semantic segmentation, and often focus on only one specific condition, such as removing rain or translating nighttime images into daytime ones. To address these limitations, we propose a method to improve the visual quality and clarity degraded by such adverse conditions. Our method, AllWeather-Net, utilizes a novel hierarchical architecture to enhance images across all adverse conditions. This architecture incorporates information at three semantic levels: scene, object, and texture, by discriminating patches at each level. Furthermore, we introduce a Scaled Illumination-aware Attention Mechanism (SIAM) that guides the learning towards road elements critical for autonomous driving perception. SIAM exhibits robustness, remaining unaffected by changes in weather conditions or environmental scenes. AllWeather-Net effectively transforms images into normal weather and daytime scenes, demonstrating superior image enhancement results and subsequently enhancing the performance of semantic segmentation, with up to a 5.3% improvement in mIoU in the trained domain. We also show our model's generalization ability by applying it to unseen domains without re-training, achieving up to 3.9% mIoU improvement. Code can be accessed at: https://github.com/Jumponthemoon/AllWeatherNet.",
            "id": "2409.02045",
            "link": "http://arxiv.org/abs/2409.02045v1",
            "published": "2024-09-03T16:47:01+00:00",
            "updated": "2024-09-03T16:47:01+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 31
        },
        "2409.02095": {
            "authors": [
                "Wenbo Hu",
                "Xiangjun Gao",
                "Xiaoyu Li",
                "Sijie Zhao",
                "Xiaodong Cun",
                "Yong Zhang",
                "Long Quan",
                "Ying Shan"
            ],
            "title": "DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
            "abstract": "Despite significant advancements in monocular depth estimation for static images, estimating video depth in the open world remains challenging, since open-world videos are extremely diverse in content, motion, camera movement, and length. We present DepthCrafter, an innovative method for generating temporally consistent long depth sequences with intricate details for open-world videos, without requiring any supplementary information such as camera poses or optical flow. DepthCrafter achieves generalization ability to open-world videos by training a video-to-depth model from a pre-trained image-to-video diffusion model, through our meticulously designed three-stage training strategy with the compiled paired video-depth datasets. Our training approach enables the model to generate depth sequences with variable lengths at one time, up to 110 frames, and harvest both precise depth details and rich content diversity from realistic and synthetic datasets. We also propose an inference strategy that processes extremely long videos through segment-wise estimation and seamless stitching. Comprehensive evaluations on multiple datasets reveal that DepthCrafter achieves state-of-the-art performance in open-world video depth estimation under zero-shot settings. Furthermore, DepthCrafter facilitates various downstream applications, including depth-based visual effects and conditional video generation.",
            "id": "2409.02095",
            "link": "http://arxiv.org/abs/2409.02095v1",
            "published": "2024-09-03T17:52:03+00:00",
            "updated": "2024-09-03T17:52:03+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ],
            "max_author_hindex": 64
        },
        "2409.02124": {
            "authors": [
                "Jinming Wang",
                "Hai Wang",
                "Hongkai Wen",
                "Geyong Min",
                "Man Luo"
            ],
            "title": "TrajWeaver: Trajectory Recovery with State Propagation Diffusion Model",
            "abstract": "With the proliferation of location-aware devices, large amount of trajectories have been generated when agents such as people, vehicles and goods flow around the urban environment. These raw trajectories, typically collected from various sources such as GPS in cars, personal mobile devices, and public transport, are often sparse and fragmented due to limited sampling rates, infrastructure coverage and data loss. In this context, trajectory recovery aims to reconstruct such sparse raw trajectories into their dense and continuous counterparts, so that fine-grained movement of agents across space and time can be captured faithfully. Existing trajectory recovery approaches typically rely on the prior knowledge of travel mode or motion patterns, and often fail in densely populated urban areas where accurate maps are absent. In this paper, we present a new recovery framework called TrajWeaver based on probabilistic diffusion models, which is able to recover dense and refined trajectories from the sparse raw ones, conditioned on various auxiliary features such as Areas of Interest along the way, user identity and waybill information. The core of TrajWeaver is a novel State Propagation Diffusion Model (SPDM), which introduces a new state propagation mechanism on top of the standard diffusion models, so that knowledge computed in earlier diffusion steps can be reused later, improving the recovery performance while reducing the number of steps needed. Extensive experiments show that the proposed TrajWeaver can recover from raw trajectories of various lengths, sparsity levels and heterogeneous travel modes, and outperform the state-of-the-art baselines significantly in recovery accuracy. Our code is available at: https://anonymous.4open.science/r/TrajWeaver/",
            "id": "2409.02124",
            "link": "http://arxiv.org/abs/2409.02124v1",
            "published": "2024-09-01T06:42:19+00:00",
            "updated": "2024-09-01T06:42:19+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 55
        },
        "2409.02127": {
            "authors": [
                "Senthil Kumar Jagatheesaperumal",
                "Mohamed Rahouti",
                "Ali Alfatemi",
                "Nasir Ghani",
                "Vu Khanh Quy",
                "Abdellah Chehri"
            ],
            "title": "Enabling Trustworthy Federated Learning in Industrial IoT: Bridging the Gap Between Interpretability and Robustness",
            "abstract": "Federated Learning (FL) represents a paradigm shift in machine learning, allowing collaborative model training while keeping data localized. This approach is particularly pertinent in the Industrial Internet of Things (IIoT) context, where data privacy, security, and efficient utilization of distributed resources are paramount. The essence of FL in IIoT lies in its ability to learn from diverse, distributed data sources without requiring central data storage, thus enhancing privacy and reducing communication overheads. However, despite its potential, several challenges impede the widespread adoption of FL in IIoT, notably in ensuring interpretability and robustness. This article focuses on enabling trustworthy FL in IIoT by bridging the gap between interpretability and robustness, which is crucial for enhancing trust, improving decision-making, and ensuring compliance with regulations. Moreover, the design strategies summarized in this article ensure that FL systems in IIoT are transparent and reliable, vital in industrial settings where decisions have significant safety and economic impacts. The case studies in the IIoT environment driven by trustworthy FL models are provided, wherein the practical insights of trustworthy communications between IIoT systems and their end users are highlighted.",
            "id": "2409.02127",
            "link": "http://arxiv.org/abs/2409.02127v1",
            "published": "2024-09-01T15:13:39+00:00",
            "updated": "2024-09-01T15:13:39+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "68Txx",
                "I.2.6"
            ],
            "max_author_hindex": 28
        },
        "2409.02139": {
            "authors": [
                "Tianxu Liu",
                "Yanbin Wang",
                "Jianguo Sun",
                "Ye Tian",
                "Yanyu Huang",
                "Tao Xue",
                "Peiyue Li",
                "Yiwei Liu"
            ],
            "title": "The Role of Transformer Models in Advancing Blockchain Technology: A Systematic Survey",
            "abstract": "As blockchain technology rapidly evolves, the demand for enhanced efficiency, security, and scalability grows.Transformer models, as powerful deep learning architectures,have shown unprecedented potential in addressing various blockchain challenges. However, a systematic review of Transformer applications in blockchain is lacking. This paper aims to fill this research gap by surveying over 200 relevant papers, comprehensively reviewing practical cases and research progress of Transformers in blockchain applications. Our survey covers key areas including anomaly detection, smart contract security analysis, cryptocurrency prediction and trend analysis, and code summary generation. To clearly articulate the advancements of Transformers across various blockchain domains, we adopt a domain-oriented classification system, organizing and introducing representative methods based on major challenges in current blockchain research. For each research domain,we first introduce its background and objectives, then review previous representative methods and analyze their limitations,and finally introduce the advancements brought by Transformer models. Furthermore, we explore the challenges of utilizing Transformer, such as data privacy, model complexity, and real-time processing requirements. Finally, this article proposes future research directions, emphasizing the importance of exploring the Transformer architecture in depth to adapt it to specific blockchain applications, and discusses its potential role in promoting the development of blockchain technology. This review aims to provide new perspectives and a research foundation for the integrated development of blockchain technology and machine learning, supporting further innovation and application expansion of blockchain technology.",
            "id": "2409.02139",
            "link": "http://arxiv.org/abs/2409.02139v2",
            "published": "2024-09-02T19:12:54+00:00",
            "updated": "2024-09-05T11:09:38+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ],
            "max_author_hindex": 64
        },
        "2409.02219": {
            "authors": [
                "Don Byrd"
            ],
            "title": "A+AI: Threats to Society, Remedies, and Governance",
            "abstract": "This document focuses on the threats, especially near-term threats, that Artificial Intelligence (AI) brings to society. Most of the threats discussed here can result from any algorithmic process, not just AI; in addition, defining AI is notoriously difficult. For both reasons, it is important to think of \"A+AI\": Algorithms and Artificial Intelligence.   In addition to the threats, this paper discusses countermeasures to them, and it includes a table showing which countermeasures are likely to mitigate which threats. Thoughtful governance could manage the risks without seriously impeding progress; in fact, chances are it would accelerate progress by reducing the social chaos that would otherwise be likely. The paper lists specific actions government should take as soon as possible, namely:   * Require all social media platforms accessible in the U.S. to offer users verification that their accounts are owned by citizens, and to display every account's verification status   * Establish regulations to require that all products created or significantly modified with A+AI be clearly labeled as such; to restrict use of generative AI to create likenesses of persons; and to require creators of generative AI software to disclose materials used to train their software and to compensate the creators of any copyrighted material used   * Fund a crash project of research on mitigating the threats   * Fund educational campaigns to raise awareness of the threats",
            "id": "2409.02219",
            "link": "http://arxiv.org/abs/2409.02219v2",
            "published": "2024-09-03T18:43:47+00:00",
            "updated": "2024-09-07T01:25:30+00:00",
            "primary_category": "cs.CY",
            "categories": [
                "cs.CY",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.02251": {
            "authors": [
                "Abdullah Arafat Miah",
                "Kaan Icer",
                "Resit Sendag",
                "Yu Bi"
            ],
            "title": "NoiseAttack: An Evasive Sample-Specific Multi-Targeted Backdoor Attack Through White Gaussian Noise",
            "abstract": "Backdoor attacks pose a significant threat when using third-party data for deep learning development. In these attacks, data can be manipulated to cause a trained model to behave improperly when a specific trigger pattern is applied, providing the adversary with unauthorized advantages. While most existing works focus on designing trigger patterns in both visible and invisible to poison the victim class, they typically result in a single targeted class upon the success of the backdoor attack, meaning that the victim class can only be converted to another class based on the adversary predefined value. In this paper, we address this issue by introducing a novel sample-specific multi-targeted backdoor attack, namely NoiseAttack. Specifically, we adopt White Gaussian Noise (WGN) with various Power Spectral Densities (PSD) as our underlying triggers, coupled with a unique training strategy to execute the backdoor attack. This work is the first of its kind to launch a vision backdoor attack with the intent to generate multiple targeted classes with minimal input configuration. Furthermore, our extensive experimental results demonstrate that NoiseAttack can achieve a high attack success rate against popular network architectures and datasets, as well as bypass state-of-the-art backdoor detection methods. Our source code and experiments are available at https://github.com/SiSL-URI/NoiseAttack/tree/main.",
            "id": "2409.02251",
            "link": "http://arxiv.org/abs/2409.02251v1",
            "published": "2024-09-03T19:24:46+00:00",
            "updated": "2024-09-03T19:24:46+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CR",
                "cs.LG"
            ],
            "max_author_hindex": 10
        },
        "2409.02313": {
            "authors": [
                "Ricardo Buitrago Ruiz",
                "Tanya Marwah",
                "Albert Gu",
                "Andrej Risteski"
            ],
            "title": "On the Benefits of Memory for Modeling Time-Dependent PDEs",
            "abstract": "Data-driven techniques have emerged as a promising alternative to traditional numerical methods for solving partial differential equations (PDEs). These techniques frequently offer a better trade-off between computational cost and accuracy for many PDE families of interest. For time-dependent PDEs, existing methodologies typically treat PDEs as Markovian systems, i.e., the evolution of the system only depends on the ``current state'', and not the past states. However, distortion of the input signals -- e.g., due to discretization or low-pass filtering -- can render the evolution of the distorted signals non-Markovian. In this work, motivated by the Mori-Zwanzig theory of model reduction, we investigate the impact of architectures with memory for modeling PDEs: that is, when past states are explicitly used to predict the future. We introduce Memory Neural Operator (MemNO), a network based on the recent SSM architectures and Fourier Neural Operator (FNO). We empirically demonstrate on a variety of PDE families of interest that when the input is given on a low-resolution grid, MemNO significantly outperforms the baselines without memory, achieving more than 6 times less error on unseen PDEs. Via a combination of theory and experiments, we show that the effect of memory is particularly significant when the solution of the PDE has high frequency Fourier components (e.g., low-viscosity fluid dynamics), and it also increases robustness to observation noise.",
            "id": "2409.02313",
            "link": "http://arxiv.org/abs/2409.02313v1",
            "published": "2024-09-03T21:56:13+00:00",
            "updated": "2024-09-03T21:56:13+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 23
        },
        "2409.02322": {
            "authors": [
                "Defu Cao",
                "Wen Ye",
                "Yizhou Zhang",
                "Yan Liu"
            ],
            "title": "TimeDiT: General-purpose Diffusion Transformers for Time Series Foundation Model",
            "abstract": "With recent advances in building foundation models for texts and video data, there is a surge of interest in foundation models for time series. A family of models have been developed, utilizing a temporal auto-regressive generative Transformer architecture, whose effectiveness has been proven in Large Language Models. While the empirical results are promising, almost all existing time series foundation models have only been tested on well-curated ``benchmark'' datasets very similar to texts. However, real-world time series exhibit unique challenges, such as variable channel sizes across domains, missing values, and varying signal sampling intervals due to the multi-resolution nature of real-world data. Additionally, the uni-directional nature of temporally auto-regressive decoding limits the incorporation of domain knowledge, such as physical laws expressed as partial differential equations (PDEs). To address these challenges, we introduce the Time Diffusion Transformer (TimeDiT), a general foundation model for time series that employs a denoising diffusion paradigm instead of temporal auto-regressive generation. TimeDiT leverages the Transformer architecture to capture temporal dependencies and employs diffusion processes to generate high-quality candidate samples without imposing stringent assumptions on the target distribution via novel masking schemes and a channel alignment strategy. Furthermore, we propose a finetuning-free model editing strategy that allows the seamless integration of external knowledge during the sampling process without updating any model parameters. Extensive experiments conducted on a varity of tasks such as forecasting, imputation, and anomaly detection, demonstrate the effectiveness of TimeDiT.",
            "id": "2409.02322",
            "link": "http://arxiv.org/abs/2409.02322v1",
            "published": "2024-09-03T22:31:57+00:00",
            "updated": "2024-09-03T22:31:57+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 20
        },
        "2409.02337": {
            "authors": [
                "Deepak Raina",
                "Mythra V. Balakuntala",
                "Byung Wook Kim",
                "Juan Wachs",
                "Richard Voyles"
            ],
            "title": "Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback",
            "abstract": "Ultrasound is widely employed for clinical intervention and diagnosis, due to its advantages of offering non-invasive, radiation-free, and real-time imaging. However, the accessibility of this dexterous procedure is limited due to the substantial training and expertise required of operators. The robotic ultrasound (RUS) offers a viable solution to address this limitation; nonetheless, achieving human-level proficiency remains challenging. Learning from demonstrations (LfD) methods have been explored in RUS, which learns the policy prior from a dataset of offline demonstrations to encode the mental model of the expert sonographer. However, active engagement of experts, i.e. Coaching, during the training of RUS has not been explored thus far. Coaching is known for enhancing efficiency and performance in human training. This paper proposes a coaching framework for RUS to amplify its performance. The framework combines DRL (self-supervised practice) with sparse expert's feedback through coaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a reward based on image quality rating. The coaching by experts is modeled as a Partially Observable Markov Decision Process (POMDP), which updates the policy parameters based on the correction by the expert. The validation study on phantoms showed that coaching increases the learning rate by $25\\%$ and the number of high-quality image acquisition by $74.5\\%$.",
            "id": "2409.02337",
            "link": "http://arxiv.org/abs/2409.02337v1",
            "published": "2024-09-03T23:52:33+00:00",
            "updated": "2024-09-03T23:52:33+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 31
        },
        "2409.02376": {
            "authors": [
                "Jie Fu",
                "Shun Fu",
                "Mick Grierson"
            ],
            "title": "Coral Model Generation from Single Images for Virtual Reality Applications",
            "abstract": "With the rapid development of VR technology, the demand for high-quality 3D models is increasing. Traditional methods struggle with efficiency and quality in large-scale customization. This paper introduces a deep-learning framework that generates high-precision 3D coral models from a single image. Using the Coral dataset, the framework extracts geometric and texture features, performs 3D reconstruction, and optimizes design and material blending. Advanced optimization and polygon count control ensure shape accuracy, detail retention, and flexible output for various complexities, catering to high-quality rendering and real-time interaction needs.The project incorporates Explainable AI (XAI) to transform AI-generated models into interactive \"artworks,\" best viewed in VR and XR. This enhances model interpretability and human-machine collaboration. Real-time feedback in VR interactions displays information like coral species and habitat, enriching user experience. The generated models surpass traditional methods in detail, visual quality, and efficiency. This research offers an intelligent approach to 3D content creation for VR, lowering production barriers, and promoting widespread VR applications. Additionally, integrating XAI provides new insights into AI-generated visual content and advances research in 3D vision interpretability.",
            "id": "2409.02376",
            "link": "http://arxiv.org/abs/2409.02376v1",
            "published": "2024-09-04T01:54:20+00:00",
            "updated": "2024-09-04T01:54:20+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.GR",
                "cs.HC",
                "cs.MM"
            ],
            "max_author_hindex": 43
        },
        "2409.02389": {
            "authors": [
                "Xiongkun Linghu",
                "Jiangyong Huang",
                "Xuesong Niu",
                "Xiaojian Ma",
                "Baoxiong Jia",
                "Siyuan Huang"
            ],
            "title": "Multi-modal Situated Reasoning in 3D Scenes",
            "abstract": "Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.",
            "id": "2409.02389",
            "link": "http://arxiv.org/abs/2409.02389v1",
            "published": "2024-09-04T02:37:38+00:00",
            "updated": "2024-09-04T02:37:38+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 13
        },
        "2409.02390": {
            "authors": [
                "Jie Su",
                "Fang Cai",
                "Shu-Kuo Zhao",
                "Xin-Yi Wang",
                "Tian-Yi Qian",
                "Da-Hui Wang",
                "Bo Hong"
            ],
            "title": "Neural Dynamics Model of Visual Decision-Making: Learning from Human Experts",
            "abstract": "Uncovering the fundamental neural correlates of biological intelligence, developing mathematical models, and conducting computational simulations are critical for advancing new paradigms in artificial intelligence (AI). In this study, we implemented a comprehensive visual decision-making model that spans from visual input to behavioral output, using a neural dynamics modeling approach. Drawing inspiration from the key components of the dorsal visual pathway in primates, our model not only aligns closely with human behavior but also reflects neural activities in primates, and achieving accuracy comparable to convolutional neural networks (CNNs). Moreover, magnetic resonance imaging (MRI) identified key neuroimaging features such as structural connections and functional connectivity that are associated with performance in perceptual decision-making tasks. A neuroimaging-informed fine-tuning approach was introduced and applied to the model, leading to performance improvements that paralleled the behavioral variations observed among subjects. Compared to classical deep learning models, our model more accurately replicates the behavioral performance of biological intelligence, relying on the structural characteristics of biological neural networks rather than extensive training data, and demonstrating enhanced resilience to perturbation.",
            "id": "2409.02390",
            "link": "http://arxiv.org/abs/2409.02390v1",
            "published": "2024-09-04T02:38:52+00:00",
            "updated": "2024-09-04T02:38:52+00:00",
            "primary_category": "cs.NE",
            "categories": [
                "cs.NE",
                "cs.AI",
                "cs.CV",
                "q-bio.NC"
            ],
            "max_author_hindex": 41
        },
        "2409.02404": {
            "authors": [
                "Shiming Ge",
                "Bochao Liu",
                "Pengju Wang",
                "Yong Li",
                "Dan Zeng"
            ],
            "title": "Learning Privacy-Preserving Student Networks via Discriminative-Generative Distillation",
            "abstract": "While deep models have proved successful in learning rich knowledge from massive well-annotated data, they may pose a privacy leakage risk in practical deployment. It is necessary to find an effective trade-off between high utility and strong privacy. In this work, we propose a discriminative-generative distillation approach to learn privacy-preserving deep models. Our key idea is taking models as bridge to distill knowledge from private data and then transfer it to learn a student network via two streams. First, discriminative stream trains a baseline classifier on private data and an ensemble of teachers on multiple disjoint private subsets, respectively. Then, generative stream takes the classifier as a fixed discriminator and trains a generator in a data-free manner. After that, the generator is used to generate massive synthetic data which are further applied to train a variational autoencoder (VAE). Among these synthetic data, a few of them are fed into the teacher ensemble to query labels via differentially private aggregation, while most of them are embedded to the trained VAE for reconstructing synthetic data. Finally, a semi-supervised student learning is performed to simultaneously handle two tasks: knowledge transfer from the teachers with distillation on few privately labeled synthetic data, and knowledge enhancement with tangent-normal adversarial regularization on many triples of reconstructed synthetic data. In this way, our approach can control query cost over private data and mitigate accuracy degradation in a unified manner, leading to a privacy-preserving student model. Extensive experiments and analysis clearly show the effectiveness of the proposed approach.",
            "id": "2409.02404",
            "link": "http://arxiv.org/abs/2409.02404v1",
            "published": "2024-09-04T03:06:13+00:00",
            "updated": "2024-09-04T03:06:13+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ],
            "max_author_hindex": 44
        },
        "2409.02423": {
            "authors": [
                "Lang Xu",
                "Quentin Anthony",
                "Qinghua Zhou",
                "Nawras Alnaasan",
                "Radha R. Gulhane",
                "Aamir Shafi",
                "Hari Subramoni",
                "Dhabaleswar K. Panda"
            ],
            "title": "Accelerating Large Language Model Training with Hybrid GPU-based Compression",
            "abstract": "Data Parallelism (DP), Tensor Parallelism (TP), and Pipeline Parallelism (PP) are the three strategies widely adopted to enable fast and efficient Large Language Model (LLM) training. However, these approaches rely on data-intensive communication routines to collect, aggregate, and re-distribute gradients, activations, and other important model information, which pose significant overhead. Co-designed with GPU-based compression libraries, MPI libraries have been proven to reduce message size significantly, and leverage interconnect bandwidth, thus increasing training efficiency while maintaining acceptable accuracy.   In this work, we investigate the efficacy of compression-assisted MPI collectives under the context of distributed LLM training using 3D parallelism and ZeRO optimizations. We scaled up to 192 V100 GPUs on the Lassen supercomputer. First, we enabled a na\\\"ive compression scheme across all collectives and observed a 22.5\\% increase in TFLOPS per GPU and a 23.6\\% increase in samples per second for GPT-NeoX-20B training. Nonetheless, such a strategy ignores the sparsity discrepancy among messages communicated in each parallelism degree, thus introducing more errors and causing degradation in training loss. Therefore, we incorporated hybrid compression settings toward each parallel dimension and adjusted the compression intensity accordingly. Given their low-rank structure (arXiv:2301.02654), we apply aggressive compression on gradients when performing DP All-reduce. We adopt milder compression to preserve precision while communicating activations, optimizer states, and model parameters in TP and PP. Using the adjusted hybrid compression scheme, we demonstrate a 17.3\\% increase in TFLOPS per GPU and a 12.7\\% increase in samples per second while reaching baseline loss convergence.",
            "id": "2409.02423",
            "link": "http://arxiv.org/abs/2409.02423v1",
            "published": "2024-09-04T04:05:30+00:00",
            "updated": "2024-09-04T04:05:30+00:00",
            "primary_category": "cs.DC",
            "categories": [
                "cs.DC",
                "cs.AI"
            ],
            "max_author_hindex": 57
        },
        "2409.02483": {
            "authors": [
                "Yunfeng Diao",
                "Baiqi Wu",
                "Ruixuan Zhang",
                "Ajian Liu",
                "Xingxing Wei",
                "Meng Wang",
                "He Wang"
            ],
            "title": "TASAR: Transferable Attack on Skeletal Action Recognition",
            "abstract": "Skeletal sequences, as well-structured representations of human behaviors, are crucial in Human Activity Recognition (HAR). The transferability of adversarial skeletal sequences enables attacks in real-world HAR scenarios, such as autonomous driving, intelligent surveillance, and human-computer interactions. However, existing Skeleton-based HAR (S-HAR) attacks exhibit weak adversarial transferability and, therefore, cannot be considered true transfer-based S-HAR attacks. More importantly, the reason for this failure remains unclear. In this paper, we study this phenomenon through the lens of loss surface, and find that its sharpness contributes to the poor transferability in S-HAR. Inspired by this observation, we assume and empirically validate that smoothening the rugged loss landscape could potentially improve adversarial transferability in S-HAR. To this end, we propose the first Transfer-based Attack on Skeletal Action Recognition, TASAR. TASAR explores the smoothed model posterior without re-training the pre-trained surrogates, which is achieved by a new post-train Dual Bayesian optimization strategy. Furthermore, unlike previous transfer-based attacks that treat each frame independently and overlook temporal coherence within sequences, TASAR incorporates motion dynamics into the Bayesian attack gradient, effectively disrupting the spatial-temporal coherence of S-HARs. To exhaustively evaluate the effectiveness of existing methods and our method, we build the first large-scale robust S-HAR benchmark, comprising 7 S-HAR models, 10 attack methods, 3 S-HAR datasets and 2 defense models. Extensive results demonstrate the superiority of TASAR. Our benchmark enables easy comparisons for future studies, with the code available in the supplementary material.",
            "id": "2409.02483",
            "link": "http://arxiv.org/abs/2409.02483v1",
            "published": "2024-09-04T07:20:01+00:00",
            "updated": "2024-09-04T07:20:01+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 22
        },
        "2409.02485": {
            "authors": [
                "Takanori Fujiwara",
                "Kostiantyn Kucher",
                "Junpeng Wang",
                "Rafael M. Martins",
                "Andreas Kerren",
                "Anders Ynnerman"
            ],
            "title": "Adversarial Attacks on Machine Learning-Aided Visualizations",
            "abstract": "Research in ML4VIS investigates how to use machine learning (ML) techniques to generate visualizations, and the field is rapidly growing with high societal impact. However, as with any computational pipeline that employs ML processes, ML4VIS approaches are susceptible to a range of ML-specific adversarial attacks. These attacks can manipulate visualization generations, causing analysts to be tricked and their judgments to be impaired. Due to a lack of synthesis from both visualization and ML perspectives, this security aspect is largely overlooked by the current ML4VIS literature. To bridge this gap, we investigate the potential vulnerabilities of ML-aided visualizations from adversarial attacks using a holistic lens of both visualization and ML perspectives. We first identify the attack surface (i.e., attack entry points) that is unique in ML-aided visualizations. We then exemplify five different adversarial attacks. These examples highlight the range of possible attacks when considering the attack surface and multiple different adversary capabilities. Our results show that adversaries can induce various attacks, such as creating arbitrary and deceptive visualizations, by systematically identifying input attributes that are influential in ML inferences. Based on our observations of the attack surface characteristics and the attack examples, we underline the importance of comprehensive studies of security issues and defense mechanisms as a call of urgency for the ML4VIS community.",
            "id": "2409.02485",
            "link": "http://arxiv.org/abs/2409.02485v1",
            "published": "2024-09-04T07:23:12+00:00",
            "updated": "2024-09-04T07:23:12+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.HC",
                "cs.LG",
                "stat.ML"
            ],
            "max_author_hindex": 34
        },
        "2409.02486": {
            "authors": [
                "Cho-Ying Wu",
                "Yiqi Zhong",
                "Junying Wang",
                "Ulrich Neumann"
            ],
            "title": "Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization",
            "abstract": "Indoor robots rely on depth to perform tasks like navigation or obstacle detection, and single-image depth estimation is widely used to assist perception. Most indoor single-image depth prediction focuses less on model generalizability to unseen datasets, concerned with in-the-wild robustness for system deployment. This work leverages gradient-based meta-learning to gain higher generalizability on zero-shot cross-dataset inference. Unlike the most-studied meta-learning of image classification associated with explicit class labels, no explicit task boundaries exist for continuous depth values tied to highly varying indoor environments regarding object arrangement and scene composition. We propose fine-grained task that treats each RGB-D mini-batch as a task in our meta-learning formulation. We first show that our method on limited data induces a much better prior (max 27.8% in RMSE). Then, finetuning on meta-learned initialization consistently outperforms baselines without the meta approach. Aiming at generalization, we propose zero-shot cross-dataset protocols and validate higher generalizability induced by our meta-initialization, as a simple and useful plugin to many existing depth estimation methods. The work at the intersection of depth and meta-learning potentially drives both research to step closer to practical robotic and machine perception usage.",
            "id": "2409.02486",
            "link": "http://arxiv.org/abs/2409.02486v1",
            "published": "2024-09-04T07:25:50+00:00",
            "updated": "2024-09-04T07:25:50+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 62
        },
        "2409.02495": {
            "authors": [
                "Hao Wu",
                "Likun Zhang",
                "Shucheng Li",
                "Fengyuan Xu",
                "Sheng Zhong"
            ],
            "title": "CoAst: Validation-Free Contribution Assessment for Federated Learning based on Cross-Round Valuation",
            "abstract": "In the federated learning (FL) process, since the data held by each participant is different, it is necessary to figure out which participant has a higher contribution to the model performance. Effective contribution assessment can help motivate data owners to participate in the FL training. Research works in this field can be divided into two directions based on whether a validation dataset is required. Validation-based methods need to use representative validation data to measure the model accuracy, which is difficult to obtain in practical FL scenarios. Existing validation-free methods assess the contribution based on the parameters and gradients of local models and the global model in a single training round, which is easily compromised by the stochasticity of model training. In this work, we propose CoAst, a practical method to assess the FL participants' contribution without access to any validation data. The core idea of CoAst involves two aspects: one is to only count the most important part of model parameters through a weights quantization, and the other is a cross-round valuation based on the similarity between the current local parameters and the global parameter updates in several subsequent communication rounds. Extensive experiments show that CoAst has comparable assessment reliability to existing validation-based methods and outperforms existing validation-free methods.",
            "id": "2409.02495",
            "link": "http://arxiv.org/abs/2409.02495v1",
            "published": "2024-09-04T07:46:28+00:00",
            "updated": "2024-09-04T07:46:28+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 24
        },
        "2409.02555": {
            "authors": [
                "Kangkai Zhang",
                "Shiming Ge",
                "Ruixin Shi",
                "Dan Zeng"
            ],
            "title": "Low-Resolution Object Recognition with Cross-Resolution Relational Contrastive Distillation",
            "abstract": "Recognizing objects in low-resolution images is a challenging task due to the lack of informative details. Recent studies have shown that knowledge distillation approaches can effectively transfer knowledge from a high-resolution teacher model to a low-resolution student model by aligning cross-resolution representations. However, these approaches still face limitations in adapting to the situation where the recognized objects exhibit significant representation discrepancies between training and testing images. In this study, we propose a cross-resolution relational contrastive distillation approach to facilitate low-resolution object recognition. Our approach enables the student model to mimic the behavior of a well-trained teacher model which delivers high accuracy in identifying high-resolution objects. To extract sufficient knowledge, the student learning is supervised with contrastive relational distillation loss, which preserves the similarities in various relational structures in contrastive representation space. In this manner, the capability of recovering missing details of familiar low-resolution objects can be effectively enhanced, leading to a better knowledge transfer. Extensive experiments on low-resolution object classification and low-resolution face recognition clearly demonstrate the effectiveness and adaptability of our approach.",
            "id": "2409.02555",
            "link": "http://arxiv.org/abs/2409.02555v1",
            "published": "2024-09-04T09:21:13+00:00",
            "updated": "2024-09-04T09:21:13+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ],
            "max_author_hindex": 8
        },
        "2409.02572": {
            "authors": [
                "Fatma Yasmine Loumachi",
                "Mohamed Chahine Ghanem"
            ],
            "title": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and Large Language Models",
            "abstract": "Timeline Analysis (TA) is a key part of Timeline Forensics (TF) in Digital Forensics (DF), focusing primarily on examining and analysing temporal digital artefacts such as timestamps, derived from event logs, file metadata, and other related data to correlate events resulting from cyber incidents and reconstruct their chronological timeline. Traditional tools often struggle to efficiently process the vast volume and variety of data acquired during DF investigations and Incident Response (IR) processes. This paper presents a novel framework, GenDFIR, that combines Rule-Based Artificial Intelligence (R-BAI) algorithms with Large Language Models (LLMs) to advance and automate the TA process. Our approach consists of two main stages (1) We use R-BAI to identify and select anomalous digital artefacts based on predefined rules. (2) The selected artefacts are then converted into embeddings for processing by an LLM with the help of a Retrieval-Augmented Generation (RAG) agent. The LLM consequently leverages its capabilities to perform automated TA on the artefacts and predict potential incident scenarios. To validate our framework, we evaluate GenDFIR performance, efficiency, and reliability using various metrics across synthetic cyber incident simulation scenarios. This paper presents a proof of concept, where the findings demonstrate the significant potential of integrating R-BAI and LLMs for TA. This novel approach highlights the power of Generative AI (GenAI), specifically LLMs, and opens new avenues for advanced threat detection and incident reconstruction, representing a significant step forward in the field.",
            "id": "2409.02572",
            "link": "http://arxiv.org/abs/2409.02572v1",
            "published": "2024-09-04T09:46:33+00:00",
            "updated": "2024-09-04T09:46:33+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.ET",
                "cs.LG"
            ],
            "max_author_hindex": 5
        },
        "2409.02574": {
            "authors": [
                "Taesung Kwon",
                "Jong Chul Ye"
            ],
            "title": "Solving Video Inverse Problems Using Image Diffusion Models",
            "abstract": "Recently, diffusion model-based inverse problem solvers (DIS) have emerged as state-of-the-art approaches for addressing inverse problems, including image super-resolution, deblurring, inpainting, etc. However, their application to video inverse problems arising from spatio-temporal degradation remains largely unexplored due to the challenges in training video diffusion models. To address this issue, here we introduce an innovative video inverse solver that leverages only image diffusion models. Specifically, by drawing inspiration from the success of the recent decomposed diffusion sampler (DDS), our method treats the time dimension of a video as the batch dimension of image diffusion models and solves spatio-temporal optimization problems within denoised spatio-temporal batches derived from each image diffusion model. Moreover, we introduce a batch-consistent diffusion sampling strategy that encourages consistency across batches by synchronizing the stochastic noise components in image diffusion models. Our approach synergistically combines batch-consistent sampling with simultaneous optimization of denoised spatio-temporal batches at each reverse diffusion step, resulting in a novel and efficient diffusion sampling strategy for video inverse problems. Experimental results demonstrate that our method effectively addresses various spatio-temporal degradations in video inverse problems, achieving state-of-the-art reconstructions. Project page: https://solving-video-inverse.github.io/main/",
            "id": "2409.02574",
            "link": "http://arxiv.org/abs/2409.02574v1",
            "published": "2024-09-04T09:48:27+00:00",
            "updated": "2024-09-04T09:48:27+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "stat.ML"
            ],
            "max_author_hindex": 63
        },
        "2409.02598": {
            "authors": [
                "Wenwu Guo",
                "Jinlin Wu",
                "Zhen Chen",
                "Qingxiang Zhao",
                "Miao Xu",
                "Zhen Lei",
                "Hongbin Liu"
            ],
            "title": "SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments",
            "abstract": "Vision-based surgical navigation has received increasing attention due to its non-invasive, cost-effective, and flexible advantages. In particular, a critical element of the vision-based navigation system is tracking surgical instruments. Compared with 2D instrument tracking methods, 3D instrument tracking has broader value in clinical practice, but is also more challenging due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models for 3D registration. To solve these challenges, we propose the SurgTrack, a two-stage 3D instrument tracking method for CAD-free and robust real-world applications. In the first registration stage, we incorporate an Instrument Signed Distance Field (SDF) modeling the 3D representation of instruments, achieving CAD-freed 3D registration. Due to this, we can obtain the location and orientation of instruments in the 3D space by matching the video stream with the registered SDF model. In the second tracking stage, we devise a posture graph optimization module, leveraging the historical tracking results of the posture memory pool to optimize the tracking results and improve the occlusion robustness. Furthermore, we collect the Instrument3D dataset to comprehensively evaluate the 3D tracking of surgical instruments. The extensive experiments validate the superiority and scalability of our SurgTrack, by outperforming the state-of-the-arts with a remarkable improvement. The code and dataset are available at https://github.com/wenwucode/SurgTrack.",
            "id": "2409.02598",
            "link": "http://arxiv.org/abs/2409.02598v1",
            "published": "2024-09-04T10:29:59+00:00",
            "updated": "2024-09-04T10:29:59+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 38
        },
        "2409.02669": {
            "authors": [
                "Ruoyu Wang",
                "Yao Liu",
                "Yuanjiang Cao",
                "Lina Yao"
            ],
            "title": "Causality-Aware Transformer Networks for Robotic Navigation",
            "abstract": "Recent advances in machine learning algorithms have garnered growing interest in developing versatile Embodied AI systems. However, current research in this domain reveals opportunities for improvement. First, the direct adoption of RNNs and Transformers often overlooks the specific differences between Embodied AI and traditional sequential data modelling, potentially limiting its performance in Embodied AI tasks. Second, the reliance on task-specific configurations, such as pre-trained modules and dataset-specific logic, compromises the generalizability of these methods. We address these constraints by initially exploring the unique differences between Embodied AI tasks and other sequential data tasks through the lens of Causality, presenting a causal framework to elucidate the inadequacies of conventional sequential methods for Embodied AI. By leveraging this causal perspective, we propose Causality-Aware Transformer (CAT) Networks for Navigation, featuring a Causal Understanding Module to enhance the models's Environmental Understanding capability. Meanwhile, our method is devoid of task-specific inductive biases and can be trained in an End-to-End manner, which enhances the method's generalizability across various contexts. Empirical evaluations demonstrate that our methodology consistently surpasses benchmark performances across a spectrum of settings, tasks and simulation environments. Extensive ablation studies reveal that the performance gains can be attributed to the Causal Understanding Module, which demonstrates effectiveness and efficiency in both Reinforcement Learning and Supervised Learning settings.",
            "id": "2409.02669",
            "link": "http://arxiv.org/abs/2409.02669v1",
            "published": "2024-09-04T12:53:26+00:00",
            "updated": "2024-09-04T12:53:26+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 45
        },
        "2409.02681": {
            "authors": [
                "Ramon Tavares"
            ],
            "title": "Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon",
            "abstract": "This study presents a comprehensive methodology for modeling and forecasting the historical time series of fire spots detected by the AQUA_M-T satellite in the Amazon, Brazil. The approach utilizes a mixed Recurrent Neural Network (RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures to predict monthly accumulations of daily detected fire spots. A summary of the data revealed a consistent seasonality over time, with annual maximum and minimum fire spot values tending to repeat at the same periods each year. The primary objective is to verify whether the forecasts capture this inherent seasonality through rigorous statistical analysis. The methodology involved careful data preparation, model configuration, and training using cross-validation with two seeds, ensuring that the data generalizes well to the test and validation sets, and confirming the convergence of the model parameters. The results indicate that the mixed LSTM and GRU model offers improved accuracy in forecasting 12 months ahead, demonstrating its effectiveness in capturing complex temporal patterns and modeling the observed time series. This research significantly contributes to the application of deep learning techniques in environmental monitoring, specifically in fire spot forecasting. In addition to improving forecast accuracy, the proposed approach highlights the potential for adaptation to other time series forecasting challenges, opening new avenues for research and development in machine learning and natural phenomenon prediction. Keywords: Time Series Forecasting, Recurrent Neural Networks, Deep Learning.",
            "id": "2409.02681",
            "link": "http://arxiv.org/abs/2409.02681v1",
            "published": "2024-09-04T13:11:59+00:00",
            "updated": "2024-09-04T13:11:59+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "stat.AP"
            ],
            "max_author_hindex": 13
        },
        "2409.02693": {
            "authors": [
                "Ahmed Ramadan",
                "Husam Yasin",
                "Burhan Pektas"
            ],
            "title": "The Role of Artificial Intelligence and Machine Learning in Software Testing",
            "abstract": "Artificial Intelligence (AI) and Machine Learning (ML) have significantly impacted various industries, including software development. Software testing, a crucial part of the software development lifecycle (SDLC), ensures the quality and reliability of software products. Traditionally, software testing has been a labor-intensive process requiring significant manual effort. However, the advent of AI and ML has transformed this landscape by introducing automation and intelligent decision-making capabilities. AI and ML technologies enhance the efficiency and effectiveness of software testing by automating complex tasks such as test case generation, test execution, and result analysis. These technologies reduce the time required for testing and improve the accuracy of defect detection, ultimately leading to higher quality software. AI can predict potential areas of failure by analyzing historical data and identifying patterns, which allows for more targeted and efficient testing. This paper explores the role of AI and ML in software testing by reviewing existing literature, analyzing current tools and techniques, and presenting case studies that demonstrate the practical benefits of these technologies. The literature review provides a comprehensive overview of the advancements in AI and ML applications in software testing, highlighting key methodologies and findings from various studies. The analysis of current tools showcases the capabilities of popular AI-driven testing tools such as Eggplant AI, Test.ai, Selenium, Appvance, Applitools Eyes, Katalon Studio, and Tricentis Tosca, each offering unique features and advantages. Case studies included in this paper illustrate real-world applications of AI and ML in software testing, showing significant improvements in testing efficiency, accuracy, and overall software quality.",
            "id": "2409.02693",
            "link": "http://arxiv.org/abs/2409.02693v1",
            "published": "2024-09-04T13:25:13+00:00",
            "updated": "2024-09-04T13:25:13+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 7
        },
        "2409.02702": {
            "authors": [
                "Chunyan An",
                "Yunhan Li",
                "Qiang Yang",
                "Winston K. G. Seah",
                "Zhixu Li",
                "Conghao Yang"
            ],
            "title": "Incorporating Like-Minded Peers to Overcome Friend Data Sparsity in Session-Based Social Recommendations",
            "abstract": "Session-based Social Recommendation (SSR) leverages social relationships within online networks to enhance the performance of Session-based Recommendation (SR). However, existing SSR algorithms often encounter the challenge of \"friend data sparsity\". Moreover, significant discrepancies can exist between the purchase preferences of social network friends and those of the target user, reducing the influence of friends relative to the target user's own preferences. To address these challenges, this paper introduces the concept of \"Like-minded Peers\" (LMP), representing users whose preferences align with the target user's current session based on their historical sessions. This is the first work, to our knowledge, that uses LMP to enhance the modeling of social influence in SSR. This approach not only alleviates the problem of friend data sparsity but also effectively incorporates users with similar preferences to the target user. We propose a novel model named Transformer Encoder with Graph Attention Aggregator Recommendation (TEGAARec), which includes the TEGAA module and the GAT-based social aggregation module. The TEGAA module captures and merges both long-term and short-term interests for target users and LMP users. Concurrently, the GAT-based social aggregation module is designed to aggregate the target users' dynamic interests and social influence in a weighted manner. Extensive experiments on four real-world datasets demonstrate the efficacy and superiority of our proposed model and ablation studies are done to illustrate the contributions of each component in TEGAARec.",
            "id": "2409.02702",
            "link": "http://arxiv.org/abs/2409.02702v2",
            "published": "2024-09-04T13:39:12+00:00",
            "updated": "2024-09-07T00:40:09+00:00",
            "primary_category": "cs.SI",
            "categories": [
                "cs.SI",
                "cs.AI"
            ],
            "max_author_hindex": 37
        },
        "2409.02720": {
            "authors": [
                "Huawei Sun",
                "Zixu Wang",
                "Hao Feng",
                "Julius Ott",
                "Lorenzo Servadei",
                "Robert Wille"
            ],
            "title": "GET-UP: GEomeTric-aware Depth Estimation with Radar Points UPsampling",
            "abstract": "Depth estimation plays a pivotal role in autonomous driving, facilitating a comprehensive understanding of the vehicle's 3D surroundings. Radar, with its robustness to adverse weather conditions and capability to measure distances, has drawn significant interest for radar-camera depth estimation. However, existing algorithms process the inherently noisy and sparse radar data by projecting 3D points onto the image plane for pixel-level feature extraction, overlooking the valuable geometric information contained within the radar point cloud. To address this gap, we propose GET-UP, leveraging attention-enhanced Graph Neural Networks (GNN) to exchange and aggregate both 2D and 3D information from radar data. This approach effectively enriches the feature representation by incorporating spatial relationships compared to traditional methods that rely only on 2D feature extraction. Furthermore, we incorporate a point cloud upsampling task to densify the radar point cloud, rectify point positions, and derive additional 3D features under the guidance of lidar data. Finally, we fuse radar and camera features during the decoding phase for depth estimation. We benchmark our proposed GET-UP on the nuScenes dataset, achieving state-of-the-art performance with a 15.3% and 14.7% improvement in MAE and RMSE over the previously best-performing model. Code: https://github.com/harborsarah/GET-UP",
            "id": "2409.02720",
            "link": "http://arxiv.org/abs/2409.02720v2",
            "published": "2024-09-02T14:15:09+00:00",
            "updated": "2024-09-08T18:43:06+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "eess.SP"
            ],
            "max_author_hindex": 46
        },
        "2409.02810": {
            "authors": [
                "Xiaodong Feng",
                "Haojiong Shangguan",
                "Tao Tang",
                "Xiaoliang Wan",
                "Tao Zhou"
            ],
            "title": "A hybrid FEM-PINN method for time-dependent partial differential equations",
            "abstract": "In this work, we present a hybrid numerical method for solving evolution partial differential equations (PDEs) by merging the time finite element method with deep neural networks. In contrast to the conventional deep learning-based formulation where the neural network is defined on a spatiotemporal domain, our methodology utilizes finite element basis functions in the time direction where the space-dependent coefficients are defined as the output of a neural network. We then apply the Galerkin or collocation projection in the time direction to obtain a system of PDEs for the space-dependent coefficients which is approximated in the framework of PINN. The advantages of such a hybrid formulation are twofold: statistical errors are avoided for the integral in the time direction, and the neural network's output can be regarded as a set of reduced spatial basis functions. To further alleviate the difficulties from high dimensionality and low regularity, we have developed an adaptive sampling strategy that refines the training set. More specifically, we use an explicit density model to approximate the distribution induced by the PDE residual and then augment the training set with new time-dependent random samples given by the learned density model. The effectiveness and efficiency of our proposed method have been demonstrated through a series of numerical experiments.",
            "id": "2409.02810",
            "link": "http://arxiv.org/abs/2409.02810v1",
            "published": "2024-09-04T15:28:25+00:00",
            "updated": "2024-09-04T15:28:25+00:00",
            "primary_category": "math.NA",
            "categories": [
                "math.NA",
                "cs.AI",
                "cs.NA"
            ],
            "max_author_hindex": 57
        },
        "2409.02883": {
            "authors": [
                "Junyoung Park",
                "Eun Hyun Seo",
                "Sunjun Kim",
                "SangHak Yi",
                "Kun Ho Lee",
                "Sungho Won"
            ],
            "title": "Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test",
            "abstract": "Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to assess cognitive functions such as visuospatial skills and memory, making them valuable tools for detecting mild cognitive impairment (MCI). Despite their utility, existing predictive models based on these tests often suffer from limitations like small sample sizes and lack of external validation, which undermine their reliability. We developed a multi-stream deep learning framework that integrates two distinct processing streams: a multi-head self-attention based spatial stream using raw RCFT images and a scoring stream employing a previously developed automated scoring system. Our model was trained on data from 1,740 subjects in the Korean cohort and validated on an external hospital dataset of 222 subjects from Korea. The proposed multi-stream model demonstrated superior performance over baseline models (AUC = 0.872, Accuracy = 0.781) in external validation. The integration of both spatial and scoring streams enables the model to capture intricate visual details from the raw images while also incorporating structured scoring data, which together enhance its ability to detect subtle cognitive impairments. This dual approach not only improves predictive accuracy but also increases the robustness of the model, making it more reliable in diverse clinical settings. Our model has practical implications for clinical settings, where it could serve as a cost-effective tool for early MCI screening.",
            "id": "2409.02883",
            "link": "http://arxiv.org/abs/2409.02883v1",
            "published": "2024-09-04T17:08:04+00:00",
            "updated": "2024-09-04T17:08:04+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 32
        },
        "2409.02917": {
            "authors": [
                "Jiaxin Guo",
                "Jiangliu Wang",
                "Ruofeng Wei",
                "Di Kang",
                "Qi Dou",
                "Yun-hui Liu"
            ],
            "title": "UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from Endoscopic Sparse Views",
            "abstract": "Visualizing surgical scenes is crucial for revealing internal anatomical structures during minimally invasive procedures. Novel View Synthesis is a vital technique that offers geometry and appearance reconstruction, enhancing understanding, planning, and decision-making in surgical scenes. Despite the impressive achievements of Neural Radiance Field (NeRF), its direct application to surgical scenes produces unsatisfying results due to two challenges: endoscopic sparse views and significant photometric inconsistencies. In this paper, we propose uncertainty-aware conditional NeRF for novel view synthesis to tackle the severe shape-radiance ambiguity from sparse surgical views. The core of UC-NeRF is to incorporate the multi-view uncertainty estimation to condition the neural radiance field for modeling the severe photometric inconsistencies adaptively. Specifically, our UC-NeRF first builds a consistency learner in the form of multi-view stereo network, to establish the geometric correspondence from sparse views and generate uncertainty estimation and feature priors. In neural rendering, we design a base-adaptive NeRF network to exploit the uncertainty estimation for explicitly handling the photometric inconsistencies. Furthermore, an uncertainty-guided geometry distillation is employed to enhance geometry learning. Experiments on the SCARED and Hamlyn datasets demonstrate our superior performance in rendering appearance and geometry, consistently outperforming the current state-of-the-art approaches. Our code will be released at \\url{https://github.com/wrld/UC-NeRF}.",
            "id": "2409.02917",
            "link": "http://arxiv.org/abs/2409.02917v1",
            "published": "2024-09-04T17:53:42+00:00",
            "updated": "2024-09-04T17:53:42+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 60
        },
        "2409.03043": {
            "authors": [
                "Christiaan Viviers",
                "Amaan Valiuddin",
                "Francisco Caetano",
                "Lemar Abdi",
                "Lena Filatova",
                "Peter de With",
                "Fons van der Sommen"
            ],
            "title": "Can Your Generative Model Detect Out-of-Distribution Covariate Shift?",
            "abstract": "Detecting Out-of-Distribution~(OOD) sensory data and covariate distribution shift aims to identify new test examples with different high-level image statistics to the captured, normal and In-Distribution (ID) set. Existing OOD detection literature largely focuses on semantic shift with little-to-no consensus over covariate shift. Generative models capture the ID data in an unsupervised manner, enabling them to effectively identify samples that deviate significantly from this learned distribution, irrespective of the downstream task. In this work, we elucidate the ability of generative models to detect and quantify domain-specific covariate shift through extensive analyses that involves a variety of models. To this end, we conjecture that it is sufficient to detect most occurring sensory faults (anomalies and deviations in global signals statistics) by solely modeling high-frequency signal-dependent and independent details. We propose a novel method, CovariateFlow, for OOD detection, specifically tailored to covariate heteroscedastic high-frequency image-components using conditional Normalizing Flows (cNFs). Our results on CIFAR10 vs. CIFAR10-C and ImageNet200 vs. ImageNet200-C demonstrate the effectiveness of the method by accurately detecting OOD covariate shift. This work contributes to enhancing the fidelity of imaging systems and aiding machine learning models in OOD detection in the presence of covariate shift.",
            "id": "2409.03043",
            "link": "http://arxiv.org/abs/2409.03043v1",
            "published": "2024-09-04T19:27:56+00:00",
            "updated": "2024-09-04T19:27:56+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 35
        },
        "2409.03062": {
            "authors": [
                "Shehan Perera",
                "Yunus Erzurumlu",
                "Deepak Gulati",
                "Alper Yilmaz"
            ],
            "title": "MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation",
            "abstract": "Skin cancer segmentation poses a significant challenge in medical image analysis. Numerous existing solutions, predominantly CNN-based, face issues related to a lack of global contextual understanding. Alternatively, some approaches resort to large-scale Transformer models to bridge the global contextual gaps, but at the expense of model size and computational complexity. Finally many Transformer based approaches rely primarily on CNN based decoders overlooking the benefits of Transformer based decoding models. Recognizing these limitations, we address the need efficient lightweight solutions by introducing MobileUNETR, which aims to overcome the performance constraints associated with both CNNs and Transformers while minimizing model size, presenting a promising stride towards efficient image segmentation. MobileUNETR has 3 main features. 1) MobileUNETR comprises of a lightweight hybrid CNN-Transformer encoder to help balance local and global contextual feature extraction in an efficient manner; 2) A novel hybrid decoder that simultaneously utilizes low-level and global features at different resolutions within the decoding stage for accurate mask generation; 3) surpassing large and complex architectures, MobileUNETR achieves superior performance with 3 million parameters and a computational complexity of 1.3 GFLOP resulting in 10x and 23x reduction in parameters and FLOPS, respectively. Extensive experiments have been conducted to validate the effectiveness of our proposed method on four publicly available skin lesion segmentation datasets, including ISIC 2016, ISIC 2017, ISIC 2018, and PH2 datasets. The code will be publicly available at: https://github.com/OSUPCVLab/MobileUNETR.git",
            "id": "2409.03062",
            "link": "http://arxiv.org/abs/2409.03062v1",
            "published": "2024-09-04T20:23:37+00:00",
            "updated": "2024-09-04T20:23:37+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 13
        },
        "2409.03077": {
            "authors": [
                "Paul Christiano",
                "Jacob Hilton",
                "Victor Lecomte",
                "Mark Xu"
            ],
            "title": "Backdoor defense, learnability and obfuscation",
            "abstract": "We introduce a formal notion of defendability against backdoors using a game between an attacker and a defender. In this game, the attacker modifies a function to behave differently on a particular input known as the \"trigger\", while behaving the same almost everywhere else. The defender then attempts to detect the trigger at evaluation time. If the defender succeeds with high enough probability, then the function class is said to be defendable. The key constraint on the attacker that makes defense possible is that the attacker's strategy must work for a randomly-chosen trigger.   Our definition is simple and does not explicitly mention learning, yet we demonstrate that it is closely connected to learnability. In the computationally unbounded setting, we use a voting algorithm of Hanneke et al. (2022) to show that defendability is essentially determined by the VC dimension of the function class, in much the same way as PAC learnability. In the computationally bounded setting, we use a similar argument to show that efficient PAC learnability implies efficient defendability, but not conversely. On the other hand, we use indistinguishability obfuscation to show that the class of polynomial size circuits is not efficiently defendable. Finally, we present polynomial size decision trees as a natural example for which defense is strictly easier than learning. Thus, we identify efficient defendability as a notable intermediate concept in between efficient learnability and obfuscation.",
            "id": "2409.03077",
            "link": "http://arxiv.org/abs/2409.03077v1",
            "published": "2024-09-04T21:05:42+00:00",
            "updated": "2024-09-04T21:05:42+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ],
            "max_author_hindex": 15
        },
        "2409.03206": {
            "authors": [
                "Mingze Gao",
                "Jingyu Liu",
                "Mingda Li",
                "Jiangtao Xie",
                "Qingbin Liu",
                "Bo Zhao",
                "Xi Chen",
                "Hui Xiong"
            ],
            "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
            "abstract": "Multimodal Large Language Models (MLLMs) have significantly improved performance across various image-language applications. Recently, there has been a growing interest in adapting image pre-trained MLLMs for video-related tasks. However, most efforts concentrate on enhancing the vision encoder and projector components, while the core part, Large Language Models (LLMs), remains comparatively under-explored. In this paper, we propose two strategies to enhance the model's capability in video understanding tasks by improving inter-layer attention computation in LLMs. Specifically, the first approach focuses on the enhancement of Rotary Position Embedding (RoPE) with Temporal-Aware Dual RoPE, which introduces temporal position information to strengthen the MLLM's temporal modeling capabilities while preserving the relative position relationships of both visual and text tokens. The second approach involves enhancing the Attention Mask with the Frame-wise Block Causal Attention Mask, a simple yet effective method that broadens visual token interactions within and across video frames while maintaining the causal inference mechanism. Based on these proposed methods, we adapt LLaVA for video understanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our TC-LLaVA achieves new state-of-the-art performance across various video understanding benchmarks with only supervised fine-tuning (SFT) on video-related datasets.",
            "id": "2409.03206",
            "link": "http://arxiv.org/abs/2409.03206v1",
            "published": "2024-09-05T02:54:17+00:00",
            "updated": "2024-09-05T02:54:17+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.03219": {
            "authors": [
                "Tao Huang"
            ],
            "title": "Content Moderation by LLM: From Accuracy to Legitimacy",
            "abstract": "One trending application of LLM (large language model) is to use it for content moderation in online platforms. Most current studies on this application have focused on the metric of accuracy - the extent to which LLM makes correct decisions about content. This article argues that accuracy is insufficient and misleading, because it fails to grasp the distinction between easy cases and hard cases as well as the inevitable trade-offs in achieving higher accuracy. Closer examination reveals that content moderation is a constitutive part of platform governance, the key of which is to gain and enhance legitimacy. Instead of making moderation decisions correct, the chief goal of LLM is to make them legitimate. In this regard, this article proposes a paradigm shift from the single benchmark of accuracy towards a legitimacy-based framework of evaluating the performance of LLM moderators. The framework suggests that for easy cases, the key is to ensure accuracy, speed and transparency, while for hard cases, what matters is reasoned justification and user participation. Examined under this framework, LLM's real potential in moderation is not accuracy improvement. Rather, LLM can better contribute in four other aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for moderation decisions, to assist human reviewers in getting more contextual information, and to facilitate user participation in a more interactive way. Using normative theories from law and social sciences to critically assess the new technological application, this article seeks to redefine LLM's role in content moderation and redirect relevant research in this field.",
            "id": "2409.03219",
            "link": "http://arxiv.org/abs/2409.03219v1",
            "published": "2024-09-05T03:33:54+00:00",
            "updated": "2024-09-05T03:33:54+00:00",
            "primary_category": "cs.CY",
            "categories": [
                "cs.CY",
                "cs.AI",
                "cs.ET",
                "cs.HC",
                "cs.LG"
            ],
            "max_author_hindex": 27
        },
        "2409.03254": {
            "authors": [
                "Dawei Dai",
                "Hao Zhu",
                "Shuyin Xia",
                "Guoyin Wang"
            ],
            "title": "Granular-ball Representation Learning for Deep CNN on Learning with Label Noise",
            "abstract": "In actual scenarios, whether manually or automatically annotated, label noise is inevitably generated in the training data, which can affect the effectiveness of deep CNN models. The popular solutions require data cleaning or designing additional optimizations to punish the data with mislabeled data, thereby enhancing the robustness of models. However, these methods come at the cost of weakening or even losing some data during the training process. As we know, content is the inherent attribute of an image that does not change with changes in annotations. In this study, we propose a general granular-ball computing (GBC) module that can be embedded into a CNN model, where the classifier finally predicts the label of granular-ball ($gb$) samples instead of each individual samples. Specifically, considering the classification task: (1) in forward process, we split the input samples as $gb$ samples at feature-level, each of which can correspond to multiple samples with varying numbers and share one single label; (2) during the backpropagation process, we modify the gradient allocation strategy of the GBC module to enable it to propagate normally; and (3) we develop an experience replay policy to ensure the stability of the training process. Experiments demonstrate that the proposed method can improve the robustness of CNN models with no additional data or optimization.",
            "id": "2409.03254",
            "link": "http://arxiv.org/abs/2409.03254v1",
            "published": "2024-09-05T05:18:31+00:00",
            "updated": "2024-09-05T05:18:31+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 18
        },
        "2409.03429": {
            "authors": [
                "Sara Roos-Hoefgeest",
                "Mario Roos-Hoefgeest",
                "Ignacio Alvarez",
                "Rafael C. Gonz\u00e1lez"
            ],
            "title": "Reinforcement Learning Approach to Optimizing Profilometric Sensor Trajectories for Surface Inspection",
            "abstract": "High-precision surface defect detection in manufacturing is essential for ensuring quality control. Laser triangulation profilometric sensors are key to this process, providing detailed and accurate surface measurements over a line. To achieve a complete and precise surface scan, accurate relative motion between the sensor and the workpiece is required. It is crucial to control the sensor pose to maintain optimal distance and relative orientation to the surface. It is also important to ensure uniform profile distribution throughout the scanning process. This paper presents a novel Reinforcement Learning (RL) based approach to optimize robot inspection trajectories for profilometric sensors. Building upon the Boustrophedon scanning method, our technique dynamically adjusts the sensor position and tilt to maintain optimal orientation and distance from the surface, while also ensuring a consistent profile distance for uniform and high-quality scanning. Utilizing a simulated environment based on the CAD model of the part, we replicate real-world scanning conditions, including sensor noise and surface irregularities. This simulation-based approach enables offline trajectory planning based on CAD models. Key contributions include the modeling of the state space, action space, and reward function, specifically designed for inspection applications using profilometric sensors. We use Proximal Policy Optimization (PPO) algorithm to efficiently train the RL agent, demonstrating its capability to optimize inspection trajectories with profilometric sensors. To validate our approach, we conducted several experiments where a model trained on a specific training piece was tested on various parts in simulation. Also, we conducted a real-world experiment by executing the optimized trajectory, generated offline from a CAD model, to inspect a part using a UR3e robotic arm model.",
            "id": "2409.03429",
            "link": "http://arxiv.org/abs/2409.03429v1",
            "published": "2024-09-05T11:20:12+00:00",
            "updated": "2024-09-05T11:20:12+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "max_author_hindex": 17
        },
        "2409.03470": {
            "authors": [
                "Prerak Mody",
                "Nicolas F. Chaves-de-Plaza",
                "Chinmay Rao",
                "Eleftheria Astrenidou",
                "Mischa de Ridder",
                "Nienke Hoekstra",
                "Klaus Hildebrandt",
                "Marius Staring"
            ],
            "title": "Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation",
            "abstract": "Increased usage of automated tools like deep learning in medical image segmentation has alleviated the bottleneck of manual contouring. This has shifted manual labour to quality assessment (QA) of automated contours which involves detecting errors and correcting them. A potential solution to semi-automated QA is to use deep Bayesian uncertainty to recommend potentially erroneous regions, thus reducing time spent on error detection. Previous work has investigated the correspondence between uncertainty and error, however, no work has been done on improving the \"utility\" of Bayesian uncertainty maps such that it is only present in inaccurate regions and not in the accurate ones. Our work trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which promotes uncertainty to be present only in inaccurate regions. We apply this method on datasets of two radiotherapy body sites, c.f. head-and-neck CT and prostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated against voxel inaccuracies using Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves. Numerical results show that when compared to the Bayesian baseline the proposed method successfully suppresses uncertainty for accurate voxels, with similar presence of uncertainty for inaccurate voxels. Code to reproduce experiments is available at https://github.com/prerakmody/bayesuncertainty-error-correspondence",
            "id": "2409.03470",
            "link": "http://arxiv.org/abs/2409.03470v1",
            "published": "2024-09-05T12:31:51+00:00",
            "updated": "2024-09-05T12:31:51+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ],
            "max_author_hindex": 36
        },
        "2409.03500": {
            "authors": [
                "Fabrizio Gilardi",
                "Sabrina Di Lorenzo",
                "Juri Ezzaini",
                "Beryl Santa",
                "Benjamin Streiff",
                "Eric Zurfluh",
                "Emma Hoes"
            ],
            "title": "Disclosure of AI-Generated News Increases Engagement but Does Not Reduce Aversion, Despite Positive Quality Ratings",
            "abstract": "The advancement of artificial intelligence (AI) has led to its application in many areas, including journalism. One key issue is the public's perception of AI-generated content. This preregistered study investigates (i) the perceived quality of AI-assisted and AI-generated versus human-generated news articles, (ii) whether disclosure of AI's involvement in generating these news articles influences engagement with them, and (iii) whether such awareness affects the willingness to read AI-generated articles in the future. We employed a between-subjects survey experiment with 599 participants from the German-speaking part of Switzerland, who evaluated the credibility, readability, and expertise of news articles. These articles were either written by journalists (control group), rewritten by AI (AI-assisted group), or entirely generated by AI (AI-generated group). Our results indicate that all news articles, regardless of whether they were written by journalists or AI, were perceived to be of equal quality. When participants in the treatment groups were subsequently made aware of AI's involvement in generating the articles, they expressed a higher willingness to engage with (i.e., continue reading) the articles than participants in the control group. However, they were not more willing to read AI-generated news in the future. These results suggest that aversion to AI usage in news media is not primarily rooted in a perceived lack of quality, and that by disclosing using AI, journalists could attract more immediate engagement with their content, at least in the short term.",
            "id": "2409.03500",
            "link": "http://arxiv.org/abs/2409.03500v1",
            "published": "2024-09-05T13:12:16+00:00",
            "updated": "2024-09-05T13:12:16+00:00",
            "primary_category": "cs.CY",
            "categories": [
                "cs.CY",
                "cs.AI"
            ],
            "max_author_hindex": 3
        },
        "2409.03543": {
            "authors": [
                "Fabian Diet",
                "Moussa Kassem Sbeyti",
                "Michelle Karg"
            ],
            "title": "Prediction Accuracy & Reliability: Classification and Object Localization under Distribution Shift",
            "abstract": "Natural distribution shift causes a deterioration in the perception performance of convolutional neural networks (CNNs). This comprehensive analysis for real-world traffic data addresses: 1) investigating the effect of natural distribution shift and weather augmentations on both detection quality and confidence estimation, 2) evaluating model performance for both classification and object localization, and 3) benchmarking two common uncertainty quantification methods - Ensembles and different variants of Monte-Carlo (MC) Dropout - under natural and close-to-natural distribution shift. For this purpose, a novel dataset has been curated from publicly available autonomous driving datasets. The in-distribution (ID) data is based on cutouts of a single object, for which both class and bounding box annotations are available. The six distribution-shift datasets cover adverse weather scenarios, simulated rain and fog, corner cases, and out-of-distribution data. A granular analysis of CNNs under distribution shift allows to quantize the impact of different types of shifts on both, task performance and confidence estimation: ConvNeXt-Tiny is more robust than EfficientNet-B0; heavy rain degrades classification stronger than localization, contrary to heavy fog; integrating MC-Dropout into selected layers only has the potential to enhance task performance and confidence estimation, whereby the identification of these layers depends on the type of distribution shift and the considered task.",
            "id": "2409.03543",
            "link": "http://arxiv.org/abs/2409.03543v1",
            "published": "2024-09-05T14:06:56+00:00",
            "updated": "2024-09-05T14:06:56+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 15
        },
        "2409.03550": {
            "authors": [
                "Qianlong Xiang",
                "Miao Zhang",
                "Yuzhang Shang",
                "Jianlong Wu",
                "Yan Yan",
                "Liqiang Nie"
            ],
            "title": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture",
            "abstract": "Diffusion models (DMs) have demonstrated exceptional generative capabilities across various areas, while they are hindered by slow inference speeds and high computational demands during deployment. The most common way to accelerate DMs involves reducing the number of denoising steps during generation, achieved through faster sampling solvers or knowledge distillation (KD). In contrast to prior approaches, we propose a novel method that transfers the capability of large pretrained DMs to faster architectures. Specifically, we employ KD in a distinct manner to compress DMs by distilling their generative ability into more rapid variants. Furthermore, considering that the source data is either unaccessible or too enormous to store for current generative models, we introduce a new paradigm for their distillation without source data, termed Data-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our established DKDM framework comprises two main components: 1) a DKDM objective that uses synthetic denoising data produced by pretrained DMs to optimize faster DMs without source data, and 2) a dynamic iterative distillation method that flexibly organizes the synthesis of denoising data, preventing it from slowing down the optimization process as the generation is slow. To our knowledge, this is the first attempt at using KD to distill DMs into any architecture in a data-free manner. Importantly, our DKDM is orthogonal to most existing acceleration methods, such as denoising step reduction, quantization and pruning. Experiments show that our DKDM is capable of deriving 2x faster DMs with performance remaining on par with the baseline. Notably, our DKDM enables pretrained DMs to function as \"datasets\" for training new DMs.",
            "id": "2409.03550",
            "link": "http://arxiv.org/abs/2409.03550v1",
            "published": "2024-09-05T14:12:22+00:00",
            "updated": "2024-09-05T14:12:22+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 66
        },
        "2409.03646": {
            "authors": [
                "Manshan Guo",
                "Bhavin Choksi",
                "Sari Sadiya",
                "Alessandro T. Gifford",
                "Martina G. Vilas",
                "Radoslaw M. Cichy",
                "Gemma Roig"
            ],
            "title": "Limited but consistent gains in adversarial robustness by co-training object recognition models with human EEG",
            "abstract": "In contrast to human vision, artificial neural networks (ANNs) remain relatively susceptible to adversarial attacks. To address this vulnerability, efforts have been made to transfer inductive bias from human brains to ANNs, often by training the ANN representations to match their biological counterparts. Previous works relied on brain data acquired in rodents or primates using invasive techniques, from specific regions of the brain, under non-natural conditions (anesthetized animals), and with stimulus datasets lacking diversity and naturalness. In this work, we explored whether aligning model representations to human EEG responses to a rich set of real-world images increases robustness to ANNs. Specifically, we trained ResNet50-backbone models on a dual task of classification and EEG prediction; and evaluated their EEG prediction accuracy and robustness to adversarial attacks. We observed significant correlation between the networks' EEG prediction accuracy, often highest around 100 ms post stimulus onset, and their gains in adversarial robustness. Although effect size was limited, effects were consistent across different random initializations and robust for architectural variants. We further teased apart the data from individual EEG channels and observed strongest contribution from electrodes in the parieto-occipital regions. The demonstrated utility of human EEG for such tasks opens up avenues for future efforts that scale to larger datasets under diverse stimuli conditions with the promise of stronger effects.",
            "id": "2409.03646",
            "link": "http://arxiv.org/abs/2409.03646v1",
            "published": "2024-09-05T16:04:57+00:00",
            "updated": "2024-09-05T16:04:57+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.HC"
            ],
            "max_author_hindex": 29
        },
        "2409.03685": {
            "authors": [
                "Stephen Tian",
                "Blake Wulfe",
                "Kyle Sargent",
                "Katherine Liu",
                "Sergey Zakharov",
                "Vitor Guizilini",
                "Jiajun Wu"
            ],
            "title": "View-Invariant Policy Learning via Zero-Shot Novel View Synthesis",
            "abstract": "Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image. For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks. Videos and additional visualizations are available at https://s-tian.github.io/projects/vista.",
            "id": "2409.03685",
            "link": "http://arxiv.org/abs/2409.03685v1",
            "published": "2024-09-05T16:39:21+00:00",
            "updated": "2024-09-05T16:39:21+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ],
            "max_author_hindex": 26
        },
        "2409.03715": {
            "authors": [
                "Yanxu Chen",
                "Linshu Huang",
                "Tian Gou"
            ],
            "title": "Applications and Advances of Artificial Intelligence in Music Generation:A Review",
            "abstract": "In recent years, artificial intelligence (AI) has made significant progress in the field of music generation, driving innovation in music creation and applications. This paper provides a systematic review of the latest research advancements in AI music generation, covering key technologies, models, datasets, evaluation methods, and their practical applications across various fields. The main contributions of this review include: (1) presenting a comprehensive summary framework that systematically categorizes and compares different technological approaches, including symbolic generation, audio generation, and hybrid models, helping readers better understand the full spectrum of technologies in the field; (2) offering an extensive survey of current literature, covering emerging topics such as multimodal datasets and emotion expression evaluation, providing a broad reference for related research; (3) conducting a detailed analysis of the practical impact of AI music generation in various application domains, particularly in real-time interaction and interdisciplinary applications, offering new perspectives and insights; (4) summarizing the existing challenges and limitations of music quality evaluation methods and proposing potential future research directions, aiming to promote the standardization and broader adoption of evaluation techniques. Through these innovative summaries and analyses, this paper serves as a comprehensive reference tool for researchers and practitioners in AI music generation, while also outlining future directions for the field.",
            "id": "2409.03715",
            "link": "http://arxiv.org/abs/2409.03715v1",
            "published": "2024-09-03T13:50:55+00:00",
            "updated": "2024-09-03T13:50:55+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 11
        },
        "2409.03793": {
            "authors": [
                "Ishaan Domkundwar",
                "Mukunda N S"
            ],
            "title": "Safeguarding AI Agents: Developing and Analyzing Safety Architectures",
            "abstract": "AI agents, specifically powered by large language models, have demonstrated exceptional capabilities in various applications where precision and efficacy are necessary. However, these agents come with inherent risks, including the potential for unsafe or biased actions, vulnerability to adversarial attacks, lack of transparency, and tendency to generate hallucinations. As AI agents become more prevalent in critical sectors of the industry, the implementation of effective safety protocols becomes increasingly important. This paper addresses the critical need for safety measures in AI systems, especially ones that collaborate with human teams. We propose and evaluate three frameworks to enhance safety protocols in AI agent systems: an LLM-powered input-output filter, a safety agent integrated within the system, and a hierarchical delegation-based system with embedded safety checks. Our methodology involves implementing these frameworks and testing them against a set of unsafe agentic use cases, providing a comprehensive evaluation of their effectiveness in mitigating risks associated with AI agent deployment. We conclude that these frameworks can significantly strengthen the safety and security of AI agent systems, minimizing potential harmful actions or outputs. Our work contributes to the ongoing effort to create safe and reliable AI applications, particularly in automated operations, and provides a foundation for developing robust guardrails to ensure the responsible use of AI agents in real-world applications.",
            "id": "2409.03793",
            "link": "http://arxiv.org/abs/2409.03793v1",
            "published": "2024-09-03T10:14:51+00:00",
            "updated": "2024-09-03T10:14:51+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 1
        },
        "2409.03796": {
            "authors": [
                "Guangjing Wang",
                "Hanqing Guo",
                "Yuanda Wang",
                "Bocheng Chen",
                "Ce Zhou",
                "Qiben Yan"
            ],
            "title": "Protecting Activity Sensing Data Privacy Using Hierarchical Information Dissociation",
            "abstract": "Smartphones and wearable devices have been integrated into our daily lives, offering personalized services. However, many apps become overprivileged as their collected sensing data contains unnecessary sensitive information. For example, mobile sensing data could reveal private attributes (e.g., gender and age) and unintended sensitive features (e.g., hand gestures when entering passwords). To prevent sensitive information leakage, existing methods must obtain private labels and users need to specify privacy policies. However, they only achieve limited control over information disclosure. In this work, we present Hippo to dissociate hierarchical information including private metadata and multi-grained activity information from the sensing data. Hippo achieves fine-grained control over the disclosure of sensitive information without requiring private labels. Specifically, we design a latent guidance-based diffusion model, which generates multi-grained versions of raw sensor data conditioned on hierarchical latent activity features. Hippo enables users to control the disclosure of sensitive information in sensing data, ensuring their privacy while preserving the necessary features to meet the utility requirements of applications. Hippo is the first unified model that achieves two goals: perturbing the sensitive attributes and controlling the disclosure of sensitive information in mobile sensing data. Extensive experiments show that Hippo can anonymize personal attributes and transform activity information at various resolutions across different types of sensing data.",
            "id": "2409.03796",
            "link": "http://arxiv.org/abs/2409.03796v1",
            "published": "2024-09-04T15:38:00+00:00",
            "updated": "2024-09-04T15:38:00+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 24
        },
        "2409.03806": {
            "authors": [
                "Yudara Kularathne",
                "Prathapa Janitha",
                "Sithira Ambepitiya"
            ],
            "title": "Mpox Screen Lite: AI-Driven On-Device Offline Mpox Screening for Low-Resource African Mpox Emergency Response",
            "abstract": "Background: The 2024 Mpox outbreak, particularly severe in Africa with clade 1b emergence, has highlighted critical gaps in diagnostic capabilities in resource-limited settings. This study aimed to develop and validate an artificial intelligence (AI)-driven, on-device screening tool for Mpox, designed to function offline in low-resource environments.   Methods: We developed a YOLOv8n-based deep learning model trained on 2,700 images (900 each of Mpox, other skin conditions, and normal skin), including synthetic data. The model was validated on 360 images and tested on 540 images. A larger external validation was conducted using 1,500 independent images. Performance metrics included accuracy, precision, recall, F1-score, sensitivity, and specificity.   Findings: The model demonstrated high accuracy (96%) in the final test set. For Mpox detection, it achieved 93% precision, 97% recall, and an F1-score of 95%. Sensitivity and specificity for Mpox detection were 97% and 96%, respectively. Performance remained consistent in the larger external validation, confirming the model's robustness and generalizability.   Interpretation: This AI-driven screening tool offers a rapid, accurate, and scalable solution for Mpox detection in resource-constrained settings. Its offline functionality and high performance across diverse datasets suggest significant potential for improving Mpox surveillance and management, particularly in areas lacking traditional diagnostic infrastructure.",
            "id": "2409.03806",
            "link": "http://arxiv.org/abs/2409.03806v1",
            "published": "2024-09-05T11:18:34+00:00",
            "updated": "2024-09-05T11:18:34+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 1
        },
        "2409.03833": {
            "authors": [
                "Victoria Tiki",
                "Kiet Pham",
                "Eliu Huerta"
            ],
            "title": "AI forecasting of higher-order wave modes of spinning binary black hole mergers",
            "abstract": "We present a physics-inspired transformer model that predicts the non-linear dynamics of higher-order wave modes emitted by quasi-circular, spinning, non-precessing binary black hole mergers. The model forecasts the waveform evolution from the pre-merger phase through the ringdown, starting with an input time-series spanning $ t \\in [-5000\\textrm{M}, -100\\textrm{M}) $. The merger event, defined as the peak amplitude of waveforms that include the $l = |m| = 2$ modes, occurs at $ t = 0\\textrm{M} $. The transformer then generates predictions over the time range $ t \\in [-100\\textrm{M}, 130\\textrm{M}] $. We produced training, evaluation and test sets using the NRHybSur3dq8 model, considering a signal manifold defined by mass ratios $ q \\in [1, 8] $; spin components $ s^z_{\\{1,2\\}} \\in [-0.8, 0.8] $; modes up to $l \\leq 4$, including the $(5,5)$ mode but excluding the $(4,0)$ and $(4,1)$ modes; and inclination angles $\\theta \\in [0, \\pi]$. We trained the model on 14,440,761 waveforms, completing the training in 15 hours using 16 NVIDIA A100 GPUs in the Delta supercomputer. We used 4 H100 GPUs in the DeltaAI supercomputer to compute, within 7 hours, the overlap between ground truth and predicted waveforms using a test set of 840,000 waveforms, finding that the mean and median overlaps over the test set are 0.996 and 0.997, respectively. Additionally, we conducted interpretability studies to elucidate the waveform features utilized by our transformer model to produce accurate predictions. The scientific software used for this work is released with this manuscript.",
            "id": "2409.03833",
            "link": "http://arxiv.org/abs/2409.03833v1",
            "published": "2024-09-05T18:00:11+00:00",
            "updated": "2024-09-05T18:00:11+00:00",
            "primary_category": "gr-qc",
            "categories": [
                "gr-qc",
                "astro-ph.IM",
                "cs.AI",
                "68T10, 85-08, 83C35, 83C57",
                "I.2"
            ],
            "max_author_hindex": 78
        },
        "2409.03933": {
            "authors": [
                "Esther Lagemann",
                "Julia Roeb",
                "Steven L. Brunton",
                "Christian Lagemann"
            ],
            "title": "A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application",
            "abstract": "The accurate quantification of wall-shear stress dynamics is of substantial importance for various applications in fundamental and applied research, spanning areas from human health to aircraft design and optimization. Despite significant progress in experimental measurement techniques and post-processing algorithms, temporally resolved wall-shear stress dynamics with adequate spatial resolution and within a suitable spatial domain remain an elusive goal. To address this gap, we introduce a deep learning architecture that ingests wall-parallel velocity fields from the logarithmic layer of turbulent wall-bounded flows and outputs the corresponding 2D wall-shear stress fields with identical spatial resolution and domain size. From a physical perspective, our framework acts as a surrogate model encapsulating the various mechanisms through which highly energetic outer-layer flow structures influence the governing wall-shear stress dynamics. The network is trained in a supervised fashion on a unified dataset comprising direct numerical simulations of statistically 1D turbulent channel and spatially developing turbulent boundary layer flows at friction Reynolds numbers ranging from 390 to 1,500. We demonstrate a zero-shot applicability to experimental velocity fields obtained from Particle-Image Velocimetry measurements and verify the physical accuracy of the wall-shear stress estimates with synchronized wall-shear stress measurements using the Micro-Pillar Shear-Stress Sensor for Reynolds numbers up to 2,000. In summary, the presented framework lays the groundwork for extracting inaccessible experimental wall-shear stress information from readily available velocity measurements and thus, facilitates advancements in a variety of experimental applications.",
            "id": "2409.03933",
            "link": "http://arxiv.org/abs/2409.03933v1",
            "published": "2024-09-05T22:59:23+00:00",
            "updated": "2024-09-05T22:59:23+00:00",
            "primary_category": "physics.flu-dyn",
            "categories": [
                "physics.flu-dyn",
                "cs.AI"
            ],
            "max_author_hindex": 65
        },
        "2409.03947": {
            "authors": [
                "Kai Shu",
                "Yuzhuo Jia",
                "Ziyang Zhang",
                "Jiechao Gao"
            ],
            "title": "FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes",
            "abstract": "Automatic Medical Imaging Narrative generation aims to alleviate the workload of radiologists by producing accurate clinical descriptions directly from radiological images. However, the subtle visual nuances and domain-specific terminology in medical images pose significant challenges compared to generic image captioning tasks. Existing approaches often neglect the vital distinction between normal and abnormal findings, leading to suboptimal performance. In this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive Partitioning Graph framework that addresses these limitations through domain-adaptive learning. FODA-PG constructs a granular graphical representation of radiological findings by separating disease-related attributes into distinct \"disease-specific\" and \"disease-free\" categories based on their clinical significance and location. This adaptive partitioning enables our model to capture the nuanced differences between normal and pathological states, mitigating the impact of data biases. By integrating this fine-grained semantic knowledge into a powerful transformer-based architecture and providing rigorous mathematical justifications for its effectiveness, FODA-PG generates precise and clinically coherent reports with enhanced generalization capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks demonstrate the superiority of our approach over state-of-the-art methods, highlighting the importance of domain adaptation in medical report generation.",
            "id": "2409.03947",
            "link": "http://arxiv.org/abs/2409.03947v1",
            "published": "2024-09-06T00:04:35+00:00",
            "updated": "2024-09-06T00:04:35+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 15
        },
        "2409.04007": {
            "authors": [
                "Byunggun Kim",
                "Younghun Kwon"
            ],
            "title": "Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition",
            "abstract": "Speech emotion recognition (SER) classifies human emotions in speech with a computer model. Recently, performance in SER has steadily increased as deep learning techniques have adapted. However, unlike many domains that use speech data, data for training in the SER model is insufficient. This causes overfitting of training of the neural network, resulting in performance degradation. In fact, successful emotion recognition requires an effective preprocessing method and a model structure that efficiently uses the number of weight parameters. In this study, we propose using eight dataset versions with different frequency-time resolutions to search for an effective emotional speech preprocessing method. We propose a 6-layer convolutional neural network (CNN) model with efficient channel attention (ECA) to pursue an efficient model structure. In particular, the well-positioned ECA blocks can improve channel feature representation with only a few parameters. With the interactive emotional dyadic motion capture (IEMOCAP) dataset, increasing the frequency resolution in preprocessing emotional speech can improve emotion recognition performance. Also, ECA after the deep convolution layer can effectively increase channel feature representation. Consequently, the best result (79.37UA 79.68WA) can be obtained, exceeding the performance of previous SER models. Furthermore, to compensate for the lack of emotional speech data, we experiment with multiple preprocessing data methods that augment trainable data preprocessed with all different settings from one sample. In the experiment, we can achieve the highest result (80.28UA 80.46WA).",
            "id": "2409.04007",
            "link": "http://arxiv.org/abs/2409.04007v1",
            "published": "2024-09-06T03:17:25+00:00",
            "updated": "2024-09-06T03:17:25+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 12
        },
        "2409.04025": {
            "authors": [
                "Yangguang Chen",
                "Tong Wang",
                "Guanzhou Chen",
                "Kun Zhu",
                "Xiaoliang Tan",
                "Jiaqi Wang",
                "Hong Xie",
                "Wenlin Zhou",
                "Jingyi Zhao",
                "Qing Wang",
                "Xiaolong Luo",
                "Xiaodong Zhang"
            ],
            "title": "BFA-YOLO: Balanced multiscale object detection network for multi-view building facade attachments detection",
            "abstract": "Detection of building facade attachments such as doors, windows, balconies, air conditioner units, billboards, and glass curtain walls plays a pivotal role in numerous applications. Building facade attachments detection aids in vbuilding information modeling (BIM) construction and meeting Level of Detail 3 (LOD3) standards. Yet, it faces challenges like uneven object distribution, small object detection difficulty, and background interference. To counter these, we propose BFA-YOLO, a model for detecting facade attachments in multi-view images. BFA-YOLO incorporates three novel innovations: the Feature Balanced Spindle Module (FBSM) for addressing uneven distribution, the Target Dynamic Alignment Task Detection Head (TDATH) aimed at improving small object detection, and the Position Memory Enhanced Self-Attention Mechanism (PMESA) to combat background interference, with each component specifically designed to solve its corresponding challenge. Detection efficacy of deep network models deeply depends on the dataset's characteristics. Existing open source datasets related to building facades are limited by their single perspective, small image pool, and incomplete category coverage. We propose a novel method for building facade attachments detection dataset construction and construct the BFA-3D dataset for facade attachments detection. The BFA-3D dataset features multi-view, accurate labels, diverse categories, and detailed classification. BFA-YOLO surpasses YOLOv8 by 1.8% and 2.9% in mAP@0.5 on the multi-view BFA-3D and street-view Facade-WHU datasets, respectively. These results underscore BFA-YOLO's superior performance in detecting facade attachments.",
            "id": "2409.04025",
            "link": "http://arxiv.org/abs/2409.04025v1",
            "published": "2024-09-06T04:44:52+00:00",
            "updated": "2024-09-06T04:44:52+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 43
        },
        "2409.04060": {
            "authors": [
                "Kentaro Hirahara",
                "Chikahito Nakane",
                "Hajime Ebisawa",
                "Tsuyoshi Kuroda",
                "Yohei Iwaki",
                "Tomoyoshi Utsumi",
                "Yuichiro Nomura",
                "Makoto Koike",
                "Hiroshi Mineno"
            ],
            "title": "D4: Text-guided diffusion model-based domain adaptive data augmentation for vineyard shoot detection",
            "abstract": "In an agricultural field, plant phenotyping using object detection models is gaining attention. However, collecting the training data necessary to create generic and high-precision models is extremely challenging due to the difficulty of annotation and the diversity of domains. Furthermore, it is difficult to transfer training data across different crops, and although machine learning models effective for specific environments, conditions, or crops have been developed, they cannot be widely applied in actual fields. In this study, we propose a generative data augmentation method (D4) for vineyard shoot detection. D4 uses a pre-trained text-guided diffusion model based on a large number of original images culled from video data collected by unmanned ground vehicles or other means, and a small number of annotated datasets. The proposed method generates new annotated images with background information adapted to the target domain while retaining annotation information necessary for object detection. In addition, D4 overcomes the lack of training data in agriculture, including the difficulty of annotation and diversity of domains. We confirmed that this generative data augmentation method improved the mean average precision by up to 28.65% for the BBox detection task and the average precision by up to 13.73% for the keypoint detection task for vineyard shoot detection. Our generative data augmentation method D4 is expected to simultaneously solve the cost and domain diversity issues of training data generation in agriculture and improve the generalization performance of detection models.",
            "id": "2409.04060",
            "link": "http://arxiv.org/abs/2409.04060v1",
            "published": "2024-09-06T07:04:27+00:00",
            "updated": "2024-09-06T07:04:27+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 45
        },
        "2409.04104": {
            "authors": [
                "Phairot Autthasan",
                "Rattanaphon Chaisaen",
                "Huy Phan",
                "Maarten De Vos",
                "Theerawit Wilaiprasitporn"
            ],
            "title": "MixNet: Joining Force of Classical and Modern Approaches Toward the Comprehensive Pipeline in Motor Imagery EEG Classification",
            "abstract": "Recent advances in deep learning (DL) have significantly impacted motor imagery (MI)-based brain-computer interface (BCI) systems, enhancing the decoding of electroencephalography (EEG) signals. However, most studies struggle to identify discriminative patterns across subjects during MI tasks, limiting MI classification performance. In this article, we propose MixNet, a novel classification framework designed to overcome this limitation by utilizing spectral-spatial signals from MI data, along with a multitask learning architecture named MIN2Net, for classification. Here, the spectral-spatial signals are generated using the filter-bank common spatial patterns (FBCSPs) method on MI data. Since the multitask learning architecture is used for the classification task, the learning in each task may exhibit different generalization rates and potential overfitting across tasks. To address this issue, we implement adaptive gradient blending, simultaneously regulating multiple loss weights and adjusting the learning pace for each task based on its generalization/overfitting tendencies. Experimental results on six benchmark data sets of different data sizes demonstrate that MixNet consistently outperforms all state-of-the-art algorithms in subject-dependent and -independent settings. Finally, the low-density EEG MI classification results show that MixNet outperforms all state-of-the-art algorithms, offering promising implications for Internet of Thing (IoT) applications, such as lightweight and portable EEG wearable devices based on low-density montages.",
            "id": "2409.04104",
            "link": "http://arxiv.org/abs/2409.04104v1",
            "published": "2024-09-06T08:14:58+00:00",
            "updated": "2024-09-06T08:14:58+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "cs.HC",
                "eess.SP"
            ],
            "max_author_hindex": 38
        },
        "2409.04142": {
            "authors": [
                "Gorka Abad",
                "Stjepan Picek",
                "Lorenzo Cavallaro",
                "Aitor Urbieta"
            ],
            "title": "Context is the Key: Backdoor Attacks for In-Context Learning with Vision Transformers",
            "abstract": "Due to the high cost of training, large model (LM) practitioners commonly use pretrained models downloaded from untrusted sources, which could lead to owning compromised models. In-context learning is the ability of LMs to perform multiple tasks depending on the prompt or context. This can enable new attacks, such as backdoor attacks with dynamic behavior depending on how models are prompted.   In this paper, we leverage the ability of vision transformers (ViTs) to perform different tasks depending on the prompts. Then, through data poisoning, we investigate two new threats: i) task-specific backdoors where the attacker chooses a target task to attack, and only the selected task is compromised at test time under the presence of the trigger. At the same time, any other task is not affected, even if prompted with the trigger. We succeeded in attacking every tested model, achieving up to 89.90\\% degradation on the target task. ii) We generalize the attack, allowing the backdoor to affect \\emph{any} task, even tasks unseen during the training phase. Our attack was successful on every tested model, achieving a maximum of $13\\times$ degradation. Finally, we investigate the robustness of prompts and fine-tuning as techniques for removing the backdoors from the model. We found that these methods fall short and, in the best case, reduce the degradation from 89.90\\% to 73.46\\%.",
            "id": "2409.04142",
            "link": "http://arxiv.org/abs/2409.04142v1",
            "published": "2024-09-06T09:16:39+00:00",
            "updated": "2024-09-06T09:16:39+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 35
        },
        "2409.04180": {
            "authors": [
                "George Andriopoulos",
                "Zixuan Dong",
                "Li Guo",
                "Zifan Zhao",
                "Keith Ross"
            ],
            "title": "The Prevalence of Neural Collapse in Neural Multivariate Regression",
            "abstract": "Recently it has been observed that neural networks exhibit Neural Collapse (NC) during the final stage of training for the classification problem. We empirically show that multivariate regression, as employed in imitation learning and other applications, exhibits Neural Regression Collapse (NRC), a new form of neural collapse: (NRC1) The last-layer feature vectors collapse to the subspace spanned by the $n$ principal components of the feature vectors, where $n$ is the dimension of the targets (for univariate regression, $n=1$); (NRC2) The last-layer feature vectors also collapse to the subspace spanned by the last-layer weight vectors; (NRC3) The Gram matrix for the weight vectors converges to a specific functional form that depends on the covariance matrix of the targets. After empirically establishing the prevalence of (NRC1)-(NRC3) for a variety of datasets and network architectures, we provide an explanation of these phenomena by modeling the regression task in the context of the Unconstrained Feature Model (UFM), in which the last layer feature vectors are treated as free variables when minimizing the loss function. We show that when the regularization parameters in the UFM model are strictly positive, then (NRC1)-(NRC3) also emerge as solutions in the UFM optimization problem. We also show that if the regularization parameters are equal to zero, then there is no collapse. To our knowledge, this is the first empirical and theoretical study of neural collapse in the context of regression. This extension is significant not only because it broadens the applicability of neural collapse to a new category of problems but also because it suggests that the phenomena of neural collapse could be a universal behavior in deep learning.",
            "id": "2409.04180",
            "link": "http://arxiv.org/abs/2409.04180v1",
            "published": "2024-09-06T10:45:58+00:00",
            "updated": "2024-09-06T10:45:58+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 68
        },
        "2409.04196": {
            "authors": [
                "Lorenza Prospero",
                "Abdullah Hamdi",
                "Joao F. Henriques",
                "Christian Rupprecht"
            ],
            "title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers",
            "abstract": "Reconstructing realistic 3D human models from monocular images has significant applications in creative industries, human-computer interfaces, and healthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene representation composed of a mixture of Gaussians. Predicting such mixtures for a human from a single input image is challenging, as it is a non-uniform density (with a many-to-one relationship with input pixels) with strict physical constraints. At the same time, it needs to be flexible to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate density and approximate initial position for Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other Gaussians' attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve fast inference of 3D human models from a single image without test-time optimization, expensive diffusion models, or 3D points supervision. We also show that it can improve 3D pose estimation by better fitting human models that account for clothes and other variations. The code is available on the project website https://abdullahamdi.com/gst/ .",
            "id": "2409.04196",
            "link": "http://arxiv.org/abs/2409.04196v1",
            "published": "2024-09-06T11:34:24+00:00",
            "updated": "2024-09-06T11:34:24+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 32
        },
        "2409.04290": {
            "authors": [
                "William Knottenbelt",
                "Zeyu Gao",
                "Rebecca Wray",
                "Woody Zhidong Zhang",
                "Jiashuai Liu",
                "Mireia Crispin-Ortuzar"
            ],
            "title": "CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis",
            "abstract": "Survival analysis is a branch of statistics used for modeling the time until a specific event occurs and is widely used in medicine, engineering, finance, and many other fields. When choosing survival models, there is typically a trade-off between performance and interpretability, where the highest performance is achieved by black-box models based on deep learning. This is a major problem in fields such as medicine where practitioners are reluctant to blindly trust black-box models to make important patient decisions. Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable and accurate alternative to multi-layer perceptrons (MLPs). We introduce CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable, high-performance survival analysis. We evaluate the proposed CoxKAN on 4 synthetic datasets and 9 real medical datasets. The synthetic experiments demonstrate that CoxKAN accurately recovers interpretable symbolic formulae for the hazard function, and effectively performs automatic feature selection. Evaluation on the 9 real datasets show that CoxKAN consistently outperforms the Cox proportional hazards model and achieves performance that is superior or comparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies complex interactions between predictor variables that would be extremely difficult to recognise using existing survival methods, and automatically finds symbolic formulae which uncover the precise effect of important biomarkers on patient risk.",
            "id": "2409.04290",
            "link": "http://arxiv.org/abs/2409.04290v1",
            "published": "2024-09-06T13:59:58+00:00",
            "updated": "2024-09-06T13:59:58+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 42
        },
        "2409.04341": {
            "authors": [
                "Xiyuan Zhao",
                "Xinhao Deng",
                "Qi Li",
                "Yunpeng Liu",
                "Zhuotao Liu",
                "Kun Sun",
                "Ke Xu"
            ],
            "title": "Towards Fine-Grained Webpage Fingerprinting at Scale",
            "abstract": "Website Fingerprinting (WF) attacks can effectively identify the websites visited by Tor clients via analyzing encrypted traffic patterns. Existing attacks focus on identifying different websites, but their accuracy dramatically decreases when applied to identify fine-grained webpages, especially when distinguishing among different subpages of the same website. WebPage Fingerprinting (WPF) attacks face the challenges of highly similar traffic patterns and a much larger scale of webpages. Furthermore, clients often visit multiple webpages concurrently, increasing the difficulty of extracting the traffic patterns of each webpage from the obfuscated traffic. In this paper, we propose Oscar, a WPF attack based on multi-label metric learning that identifies different webpages from obfuscated traffic by transforming the feature space. Oscar can extract the subtle differences among various webpages, even those with similar traffic patterns. In particular, Oscar combines proxy-based and sample-based metric learning losses to extract webpage features from obfuscated traffic and identify multiple webpages. We prototype Oscar and evaluate its performance using traffic collected from 1,000 monitored webpages and over 9,000 unmonitored webpages in the real world. Oscar demonstrates an 88.6% improvement in the multi-label metric Recall@5 compared to the state-of-the-art attacks.",
            "id": "2409.04341",
            "link": "http://arxiv.org/abs/2409.04341v1",
            "published": "2024-09-06T15:21:00+00:00",
            "updated": "2024-09-06T15:21:00+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 29
        },
        "2409.04360": {
            "authors": [
                "Gianluca Carloni",
                "Sara Colantonio"
            ],
            "title": "Connectivity-Inspired Network for Context-Aware Recognition",
            "abstract": "The aim of this paper is threefold. We inform the AI practitioner about the human visual system with an extensive literature review; we propose a novel biologically motivated neural network for image classification; and, finally, we present a new plug-and-play module to model context awareness. We focus on the effect of incorporating circuit motifs found in biological brains to address visual recognition. Our convolutional architecture is inspired by the connectivity of human cortical and subcortical streams, and we implement bottom-up and top-down modulations that mimic the extensive afferent and efferent connections between visual and cognitive areas. Our Contextual Attention Block is simple and effective and can be integrated with any feed-forward neural network. It infers weights that multiply the feature maps according to their causal influence on the scene, modeling the co-occurrence of different objects in the image. We place our module at different bottlenecks to infuse a hierarchical context awareness into the model. We validated our proposals through image classification experiments on benchmark data and found a consistent improvement in performance and the robustness of the produced explanations via class activation. Our code is available at https://github.com/gianlucarloni/CoCoReco.",
            "id": "2409.04360",
            "link": "http://arxiv.org/abs/2409.04360v1",
            "published": "2024-09-06T15:42:10+00:00",
            "updated": "2024-09-06T15:42:10+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "eess.IV",
                "I.2; I.4; I.5; J.3; J.6"
            ],
            "max_author_hindex": 16
        },
        "2409.04367": {
            "authors": [
                "Maria-Florina Balcan",
                "Anh Tuan Nguyen",
                "Dravyansh Sharma"
            ],
            "title": "Provable Hyperparameter Tuning for Structured Pfaffian Settings",
            "abstract": "Data-driven algorithm design automatically adapts algorithms to specific application domains, achieving better performance. In the context of parameterized algorithms, this approach involves tuning the algorithm parameters using problem instances drawn from the problem distribution of the target application domain. While empirical evidence supports the effectiveness of data-driven algorithm design, providing theoretical guarantees for several parameterized families remains challenging. This is due to the intricate behaviors of their corresponding utility functions, which typically admit piece-wise and discontinuity structures. In this work, we present refined frameworks for providing learning guarantees for parameterized data-driven algorithm design problems in both distributional and online learning settings. For the distributional learning setting, we introduce the Pfaffian GJ framework, an extension of the classical GJ framework, capable of providing learning guarantees for function classes for which the computation involves Pfaffian functions. Unlike the GJ framework, which is limited to function classes with computation characterized by rational functions, our proposed framework can deal with function classes involving Pfaffian functions, which are much more general and widely applicable. We then show that for many parameterized algorithms of interest, their utility function possesses a refined piece-wise structure, which automatically translates to learning guarantees using our proposed framework. For the online learning setting, we provide a new tool for verifying dispersion property of a sequence of loss functions. This sufficient condition allows no-regret learning for sequences of piece-wise structured loss functions where the piece-wise structure involves Pfaffian transition boundaries.",
            "id": "2409.04367",
            "link": "http://arxiv.org/abs/2409.04367v1",
            "published": "2024-09-06T15:58:20+00:00",
            "updated": "2024-09-06T15:58:20+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ],
            "max_author_hindex": 51
        },
        "2409.04368": {
            "authors": [
                "Gregory Szumel",
                "Brian Guo",
                "Darui Lu",
                "Rongze Gui",
                "Tingyu Wang",
                "Nicholas Konz",
                "Maciej A. Mazurowski"
            ],
            "title": "The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study",
            "abstract": "Purpose: Medical images acquired using different scanners and protocols can differ substantially in their appearance. This phenomenon, scanner domain shift, can result in a drop in the performance of deep neural networks which are trained on data acquired by one scanner and tested on another. This significant practical issue is well-acknowledged, however, no systematic study of the issue is available across different modalities and diagnostic tasks. Materials and Methods: In this paper, we present a broad experimental study evaluating the impact of scanner domain shift on convolutional neural network performance for different automated diagnostic tasks. We evaluate this phenomenon in common radiological modalities, including X-ray, CT, and MRI. Results: We find that network performance on data from a different scanner is almost always worse than on same-scanner data, and we quantify the degree of performance drop across different datasets. Notably, we find that this drop is most severe for MRI, moderate for X-ray, and quite small for CT, on average, which we attribute to the standardized nature of CT acquisition systems which is not present in MRI or X-ray. We also study how injecting varying amounts of target domain data into the training set, as well as adding noise to the training data, helps with generalization. Conclusion: Our results provide extensive experimental evidence and quantification of the extent of performance drop caused by scanner domain shift in deep learning across different modalities, with the goal of guiding the future development of robust deep learning models for medical image analysis.",
            "id": "2409.04368",
            "link": "http://arxiv.org/abs/2409.04368v1",
            "published": "2024-09-06T15:59:30+00:00",
            "updated": "2024-09-06T15:59:30+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 35
        },
        "2409.04428": {
            "authors": [
                "Alexandru Vasilache",
                "Jann Krausse",
                "Klaus Knobloch",
                "Juergen Becker"
            ],
            "title": "Hybrid Spiking Neural Networks for Low-Power Intra-Cortical Brain-Machine Interfaces",
            "abstract": "Intra-cortical brain-machine interfaces (iBMIs) have the potential to dramatically improve the lives of people with paraplegia by restoring their ability to perform daily activities. However, current iBMIs suffer from scalability and mobility limitations due to bulky hardware and wiring. Wireless iBMIs offer a solution but are constrained by a limited data rate. To overcome this challenge, we are investigating hybrid spiking neural networks for embedded neural decoding in wireless iBMIs. The networks consist of a temporal convolution-based compression followed by recurrent processing and a final interpolation back to the original sequence length. As recurrent units, we explore gated recurrent units (GRUs), leaky integrate-and-fire (LIF) neurons, and a combination of both - spiking GRUs (sGRUs) and analyze their differences in terms of accuracy, footprint, and activation sparsity. To that end, we train decoders on the \"Nonhuman Primate Reaching with Multichannel Sensorimotor Cortex Electrophysiology\" dataset and evaluate it using the NeuroBench framework, targeting both tracks of the IEEE BioCAS Grand Challenge on Neural Decoding. Our approach achieves high accuracy in predicting velocities of primate reaching movements from multichannel primary motor cortex recordings while maintaining a low number of synaptic operations, surpassing the current baseline models in the NeuroBench framework. This work highlights the potential of hybrid neural networks to facilitate wireless iBMIs with high decoding precision and a substantial increase in the number of monitored neurons, paving the way toward more advanced neuroprosthetic technologies.",
            "id": "2409.04428",
            "link": "http://arxiv.org/abs/2409.04428v1",
            "published": "2024-09-06T17:48:44+00:00",
            "updated": "2024-09-06T17:48:44+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "q-bio.NC"
            ],
            "max_author_hindex": 22
        },
        "2409.04495": {
            "authors": [
                "Runzhong Wang",
                "Yang Li",
                "Junchi Yan",
                "Xiaokang Yang"
            ],
            "title": "Learning to Solve Combinatorial Optimization under Positive Linear Constraints via Non-Autoregressive Neural Networks",
            "abstract": "Combinatorial optimization (CO) is the fundamental problem at the intersection of computer science, applied mathematics, etc. The inherent hardness in CO problems brings up challenge for solving CO exactly, making deep-neural-network-based solvers a research frontier. In this paper, we design a family of non-autoregressive neural networks to solve CO problems under positive linear constraints with the following merits. First, the positive linear constraint covers a wide range of CO problems, indicating that our approach breaks the generality bottleneck of existing non-autoregressive networks. Second, compared to existing autoregressive neural network solvers, our non-autoregressive networks have the advantages of higher efficiency and preserving permutation invariance. Third, our offline unsupervised learning has lower demand on high-quality labels, getting rid of the demand of optimal labels in supervised learning. Fourth, our online differentiable search method significantly improves the generalizability of our neural network solver to unseen problems. We validate the effectiveness of this framework in solving representative CO problems including facility location, max-set covering, and traveling salesman problem. Our non-autoregressive neural solvers are competitive to and can be even superior to state-of-the-art solvers such as SCIP and Gurobi, especially when both efficiency and efficacy are considered. Code is available at https://github.com/Thinklab-SJTU/NAR-CO-Solver",
            "id": "2409.04495",
            "link": "http://arxiv.org/abs/2409.04495v1",
            "published": "2024-09-06T14:58:31+00:00",
            "updated": "2024-09-06T14:58:31+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 26
        },
        "2409.04559": {
            "authors": [
                "Gemma Canet Tarr\u00e9s",
                "Zhe Lin",
                "Zhifei Zhang",
                "Jianming Zhang",
                "Yizhi Song",
                "Dan Ruta",
                "Andrew Gilbert",
                "John Collomosse",
                "Soo Ye Kim"
            ],
            "title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing",
            "abstract": "Compositing an object into an image involves multiple non-trivial sub-tasks such as object placement and scaling, color/lighting harmonization, viewpoint/geometry adjustment, and shadow/reflection generation. Recent generative image compositing methods leverage diffusion models to handle multiple sub-tasks at once. However, existing models face limitations due to their reliance on masking the original object during training, which constrains their generation to the input mask. Furthermore, obtaining an accurate input mask specifying the location and scale of the object in a new image can be highly challenging. To overcome such limitations, we define a novel problem of unconstrained generative object compositing, i.e., the generation is not bounded by the mask, and train a diffusion-based model on a synthesized paired dataset. Our first-of-its-kind model is able to generate object effects such as shadows and reflections that go beyond the mask, enhancing image realism. Additionally, if an empty mask is provided, our model automatically places the object in diverse natural locations and scales, accelerating the compositing workflow. Our model outperforms existing object placement and compositing models in various quality metrics and user studies.",
            "id": "2409.04559",
            "link": "http://arxiv.org/abs/2409.04559v2",
            "published": "2024-09-06T18:42:30+00:00",
            "updated": "2024-09-11T11:05:56+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 112
        },
        "2409.04609": {
            "authors": [
                "Abhijeet Sahu",
                "Truc Nguyen",
                "Kejun Chen",
                "Xiangyu Zhang",
                "Malik Hassanaly"
            ],
            "title": "Detection of False Data Injection Attacks (FDIA) on Power Dynamical Systems With a State Prediction Method",
            "abstract": "With the deeper penetration of inverter-based resources in power systems, false data injection attacks (FDIA) are a growing cyber-security concern. They have the potential to disrupt the system's stability like frequency stability, thereby leading to catastrophic failures. Therefore, an FDIA detection method would be valuable to protect power systems. FDIAs typically induce a discrepancy between the desired and the effective behavior of the power system dynamics. A suitable detection method can leverage power dynamics predictions to identify whether such a discrepancy was induced by an FDIA. This work investigates the efficacy of temporal and spatio-temporal state prediction models, such as Long Short-Term Memory (LSTM) and a combination of Graph Neural Networks (GNN) with LSTM, for predicting frequency dynamics in the absence of an FDIA but with noisy measurements, and thereby identify FDIA events. For demonstration purposes, the IEEE 39 New England Kron-reduced model simulated with a swing equation is considered. It is shown that the proposed state prediction models can be used as a building block for developing an effective FDIA detection method that can maintain high detection accuracy across various attack and deployment settings. It is also shown how the FDIA detection should be deployed to limit its exposure to detection inaccuracies and mitigate its computational burden.",
            "id": "2409.04609",
            "link": "http://arxiv.org/abs/2409.04609v1",
            "published": "2024-09-06T20:47:21+00:00",
            "updated": "2024-09-06T20:47:21+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 14
        },
        "2409.04613": {
            "authors": [
                "Chinmay Maheshwari",
                "Manxi Wu",
                "Shankar Sastry"
            ],
            "title": "Decentralized Learning in General-sum Markov Games",
            "abstract": "The Markov game framework is widely used to model interactions among agents with heterogeneous utilities in dynamic and uncertain societal-scale systems. In these systems, agents typically operate in a decentralized manner due to privacy and scalability concerns, often acting without any information about other agents. The design and analysis of decentralized learning algorithms that provably converge to rational outcomes remain elusive, especially beyond Markov zero-sum games and Markov potential games, which do not adequately capture the nature of many real-world interactions that is neither fully competitive nor fully cooperative. This paper investigates the design of decentralized learning algorithms for general-sum Markov games, aiming to provide provable guarantees of convergence to approximate Nash equilibria in the long run. Our approach builds on constructing a Markov Near-Potential Function (MNPF) to address the intractability of designing algorithms that converge to exact Nash equilibria. We demonstrate that MNPFs play a central role in ensuring the convergence of an actor-critic-based decentralized learning algorithm to approximate Nash equilibria. By leveraging a two-timescale approach, where Q-function estimates are updated faster than policy updates, we show that the system converges to a level set of the MNPF over the set of approximate Nash equilibria. This convergence result is further strengthened if the set of Nash equilibria is assumed to be finite. Our findings provide a new perspective on the analysis and design of decentralized learning algorithms in multi-agent systems.",
            "id": "2409.04613",
            "link": "http://arxiv.org/abs/2409.04613v1",
            "published": "2024-09-06T20:49:11+00:00",
            "updated": "2024-09-06T20:49:11+00:00",
            "primary_category": "cs.MA",
            "categories": [
                "cs.MA",
                "cs.AI",
                "cs.GT",
                "cs.SY",
                "eess.SY",
                "math.OC"
            ],
            "max_author_hindex": 9
        },
        "2409.04637": {
            "authors": [
                "Pingzhi Li",
                "Tianlong Chen",
                "Junyu Liu"
            ],
            "title": "Enhancing Quantum Security over Federated Learning via Post-Quantum Cryptography",
            "abstract": "Federated learning (FL) has become one of the standard approaches for deploying machine learning models on edge devices, where private training data are distributed across clients, and a shared model is learned by aggregating locally computed updates from each client. While this paradigm enhances communication efficiency by only requiring updates at the end of each training epoch, the transmitted model updates remain vulnerable to malicious tampering, posing risks to the integrity of the global model. Although current digital signature algorithms can protect these communicated model updates, they fail to ensure quantum security in the era of large-scale quantum computing. Fortunately, various post-quantum cryptography algorithms have been developed to address this vulnerability, especially the three NIST-standardized algorithms - Dilithium, FALCON, and SPHINCS+. In this work, we empirically investigate the impact of these three NIST-standardized PQC algorithms for digital signatures within the FL procedure, covering a wide range of models, tasks, and FL settings. Our results indicate that Dilithium stands out as the most efficient PQC algorithm for digital signature in federated learning. Additionally, we offer an in-depth discussion of the implications of our findings and potential directions for future research.",
            "id": "2409.04637",
            "link": "http://arxiv.org/abs/2409.04637v1",
            "published": "2024-09-06T22:02:08+00:00",
            "updated": "2024-09-06T22:02:08+00:00",
            "primary_category": "quant-ph",
            "categories": [
                "quant-ph",
                "cs.AI",
                "cs.CR",
                "cs.LG"
            ],
            "max_author_hindex": 22
        },
        "2409.04707": {
            "authors": [
                "Yuhan Ma",
                "Dan Sun",
                "Erdi Gao",
                "Ningjing Sang",
                "Iris Li",
                "Guanming Huang"
            ],
            "title": "Enhancing Deep Learning with Optimized Gradient Descent: Bridging Numerical Methods and Neural Network Training",
            "abstract": "Optimization theory serves as a pivotal scientific instrument for achieving optimal system performance, with its origins in economic applications to identify the best investment strategies for maximizing benefits. Over the centuries, from the geometric inquiries of ancient Greece to the calculus contributions by Newton and Leibniz, optimization theory has significantly advanced. The persistent work of scientists like Lagrange, Cauchy, and von Neumann has fortified its progress. The modern era has seen an unprecedented expansion of optimization theory applications, particularly with the growth of computer science, enabling more sophisticated computational practices and widespread utilization across engineering, decision analysis, and operations research. This paper delves into the profound relationship between optimization theory and deep learning, highlighting the omnipresence of optimization problems in the latter. We explore the gradient descent algorithm and its variants, which are the cornerstone of optimizing neural networks. The chapter introduces an enhancement to the SGD optimizer, drawing inspiration from numerical optimization methods, aiming to enhance interpretability and accuracy. Our experiments on diverse deep learning tasks substantiate the improved algorithm's efficacy. The paper concludes by emphasizing the continuous development of optimization theory and its expanding role in solving intricate problems, enhancing computational capabilities, and informing better policy decisions.",
            "id": "2409.04707",
            "link": "http://arxiv.org/abs/2409.04707v1",
            "published": "2024-09-07T04:37:20+00:00",
            "updated": "2024-09-07T04:37:20+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 37
        },
        "2409.04732": {
            "authors": [
                "Mohammadmahdi Honarmand",
                "Muhammad Abdullah Jamal",
                "Omid Mohareri"
            ],
            "title": "VidLPRO: A $\\underline{Vid}$eo-$\\underline{L}$anguage $\\underline{P}$re-training Framework for $\\underline{Ro}$botic and Laparoscopic Surgery",
            "abstract": "We introduce VidLPRO, a novel video-language (VL) pre-training framework designed specifically for robotic and laparoscopic surgery. While existing surgical VL models primarily rely on contrastive learning, we propose a more comprehensive approach to capture the intricate temporal dynamics and align video with language. VidLPRO integrates video-text contrastive learning, video-text matching, and masked language modeling objectives to learn rich VL representations. To support this framework, we present GenSurg+, a carefully curated dataset derived from GenSurgery, comprising 17k surgical video clips paired with captions generated by GPT-4 using transcripts extracted by the Whisper model. This dataset addresses the need for large-scale, high-quality VL data in the surgical domain. Extensive experiments on benchmark datasets, including Cholec80 and AutoLaparo, demonstrate the efficacy of our approach. VidLPRO achieves state-of-the-art performance in zero-shot surgical phase recognition, significantly outperforming existing surgical VL models such as SurgVLP and HecVL. Our model demonstrates improvements of up to 21.5\\% in accuracy and 15.7% in F1 score, setting a new benchmark in the field. Notably, VidLPRO exhibits robust performance even with single-frame inference, while effectively scaling with increased temporal context. Ablation studies reveal the impact of frame sampling strategies on model performance and computational efficiency. These results underscore VidLPRO's potential as a foundation model for surgical video understanding.",
            "id": "2409.04732",
            "link": "http://arxiv.org/abs/2409.04732v2",
            "published": "2024-09-07T06:33:12+00:00",
            "updated": "2024-09-11T23:12:53+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 14
        },
        "2409.04792": {
            "authors": [
                "Hongyao Tang",
                "Glen Berseth"
            ],
            "title": "Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn",
            "abstract": "Deep neural networks provide Reinforcement Learning (RL) powerful function approximators to address large-scale decision-making problems. However, these approximators introduce challenges due to the non-stationary nature of RL training. One source of the challenges in RL is that output predictions can churn, leading to uncontrolled changes after each batch update for states not included in the batch. Although such a churn phenomenon exists in each step of network training, how churn occurs and impacts RL remains under-explored. In this work, we start by characterizing churn in a view of Generalized Policy Iteration with function approximation, and we discover a chain effect of churn that leads to a cycle where the churns in value estimation and policy improvement compound and bias the learning dynamics throughout the iteration. Further, we concretize the study and focus on the learning issues caused by the chain effect in different settings, including greedy action deviation in value-based methods, trust region violation in proximal policy optimization, and dual bias of policy value in actor-critic methods. We then propose a method to reduce the chain effect across different settings, called Churn Approximated ReductIoN (CHAIN), which can be easily plugged into most existing DRL algorithms. Our experiments demonstrate the effectiveness of our method in both reducing churn and improving learning performance across online and offline, value-based and policy-based RL settings, as well as a scaling setting.",
            "id": "2409.04792",
            "link": "http://arxiv.org/abs/2409.04792v1",
            "published": "2024-09-07T11:08:20+00:00",
            "updated": "2024-09-07T11:08:20+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 20
        },
        "2409.04813": {
            "authors": [
                "Mustafa Co\u015fkun",
                "Ananth Grama",
                "Mehmet Koyut\u00fcrk"
            ],
            "title": "Generalized Learning of Coefficients in Spectral Graph Convolutional Networks",
            "abstract": "Spectral Graph Convolutional Networks (GCNs) have gained popularity in graph machine learning applications due, in part, to their flexibility in specification of network propagation rules. These propagation rules are often constructed as polynomial filters whose coefficients are learned using label information during training. In contrast to learned polynomial filters, explicit filter functions are useful in capturing relationships between network topology and distribution of labels across the network. A number of algorithms incorporating either approach have been proposed; however the relationship between filter functions and polynomial approximations is not fully resolved. This is largely due to the ill-conditioned nature of the linear systems that must be solved to derive polynomial approximations of filter functions. To address this challenge, we propose a novel Arnoldi orthonormalization-based algorithm, along with a unifying approach, called G-Arnoldi-GCN that can efficiently and effectively approximate a given filter function with a polynomial. We evaluate G-Arnoldi-GCN in the context of multi-class node classification across ten datasets with diverse topological characteristics. Our experiments show that G-Arnoldi-GCN consistently outperforms state-of-the-art methods when suitable filter functions are employed. Overall, G-Arnoldi-GCN opens important new directions in graph machine learning by enabling the explicit design and application of diverse filter functions. Code link: https://anonymous.4open.science/r/GArnoldi-GCN-F7E2/README.md",
            "id": "2409.04813",
            "link": "http://arxiv.org/abs/2409.04813v1",
            "published": "2024-09-07T12:53:44+00:00",
            "updated": "2024-09-07T12:53:44+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 46
        },
        "2409.04828": {
            "authors": [
                "Yuan Liu",
                "Zhongyin Zhao",
                "Ziyuan Zhuang",
                "Le Tian",
                "Xiao Zhou",
                "Jie Zhou"
            ],
            "title": "POINTS: Improving Your Vision-language Model with Affordable Strategies",
            "abstract": "In recent years, vision-language models have made significant strides, excelling in tasks like optical character recognition and geometric problem-solving. However, several critical issues remain: 1) Proprietary models often lack transparency about their architectures, while open-source models need more detailed ablations of their training strategies. 2) Pre-training data in open-source works is under-explored, with datasets added empirically, making the process cumbersome. 3) Fine-tuning often focuses on adding datasets, leading to diminishing returns. To address these issues, we propose the following contributions: 1) We trained a robust baseline model using the latest advancements in vision-language models, introducing effective improvements and conducting comprehensive ablation and validation for each technique. 2) Inspired by recent work on large language models, we filtered pre-training data using perplexity, selecting the lowest perplexity data for training. This approach allowed us to train on a curated 1M dataset, achieving competitive performance. 3) During visual instruction tuning, we used model soup on different datasets when adding more datasets yielded marginal improvements. These innovations resulted in a 9B parameter model that performs competitively with state-of-the-art models. Our strategies are efficient and lightweight, making them easily adoptable by the community.",
            "id": "2409.04828",
            "link": "http://arxiv.org/abs/2409.04828v1",
            "published": "2024-09-07T13:41:37+00:00",
            "updated": "2024-09-07T13:41:37+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ],
            "max_author_hindex": 17
        },
        "2409.04829": {
            "authors": [
                "Yang Xu",
                "Huihong Shi",
                "Zhongfeng Wang"
            ],
            "title": "NASH: Neural Architecture and Accelerator Search for Multiplication-Reduced Hybrid Models",
            "abstract": "The significant computational cost of multiplications hinders the deployment of deep neural networks (DNNs) on edge devices. While multiplication-free models offer enhanced hardware efficiency, they typically sacrifice accuracy. As a solution, multiplication-reduced hybrid models have emerged to combine the benefits of both approaches. Particularly, prior works, i.e., NASA and NASA-F, leverage Neural Architecture Search (NAS) to construct such hybrid models, enhancing hardware efficiency while maintaining accuracy. However, they either entail costly retraining or encounter gradient conflicts, limiting both search efficiency and accuracy. Additionally, they overlook the acceleration opportunity introduced by accelerator search, yielding sub-optimal hardware performance. To overcome these limitations, we propose NASH, a Neural architecture and Accelerator Search framework for multiplication-reduced Hybrid models. Specifically, as for NAS, we propose a tailored zero-shot metric to pre-identify promising hybrid models before training, enhancing search efficiency while alleviating gradient conflicts. Regarding accelerator search, we innovatively introduce coarse-to-fine search to streamline the search process. Furthermore, we seamlessly integrate these two levels of searches to unveil NASH, obtaining the optimal model and accelerator pairing. Experiments validate our effectiveness, e.g., when compared with the state-of-the-art multiplication-based system, we can achieve $\\uparrow$$2.14\\times$ throughput and $\\uparrow$$2.01\\times$ FPS with $\\uparrow$$0.25\\%$ accuracy on CIFAR-100, and $\\uparrow$$1.40\\times$ throughput and $\\uparrow$$1.19\\times$ FPS with $\\uparrow$$0.56\\%$ accuracy on Tiny-ImageNet. Codes are available at \\url{https://github.com/xuyang527/NASH.}",
            "id": "2409.04829",
            "link": "http://arxiv.org/abs/2409.04829v1",
            "published": "2024-09-07T13:42:40+00:00",
            "updated": "2024-09-07T13:42:40+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 35
        },
        "2409.04849": {
            "authors": [
                "Chuyi Chen",
                "Zhe Zhang",
                "Yanchao Zhao"
            ],
            "title": "FedModule: A Modular Federated Learning Framework",
            "abstract": "Federated learning (FL) has been widely adopted across various applications, such as healthcare, finance, and smart cities. However, as experimental scenarios become more complex, existing FL frameworks and benchmarks have struggled to keep pace. This paper introduces FedModule, a flexible and extensible FL experimental framework that has been open-sourced to support diverse FL paradigms and provide comprehensive benchmarks for complex experimental scenarios. FedModule adheres to the \"one code, all scenarios\" principle and employs a modular design that breaks the FL process into individual components, allowing for the seamless integration of different FL paradigms. The framework supports synchronous, asynchronous, and personalized federated learning, with over 20 implemented algorithms. Experiments conducted on public datasets demonstrate the flexibility and extensibility of FedModule. The framework offers multiple execution modes-including linear, threaded, process-based, and distributed-enabling users to tailor their setups to various experimental needs. Additionally, FedModule provides extensive logging and testing capabilities, which facilitate detailed performance analysis of FL algorithms. Comparative evaluations against existing FL toolkits, such as TensorFlow Federated, PySyft, Flower, and FLGo, highlight FedModule's superior scalability, flexibility, and comprehensive benchmark support. By addressing the limitations of current FL frameworks, FedModule marks a significant advancement in FL experimentation, providing researchers and practitioners with a robust tool for developing and evaluating FL algorithms across a wide range of scenarios.",
            "id": "2409.04849",
            "link": "http://arxiv.org/abs/2409.04849v1",
            "published": "2024-09-07T15:03:12+00:00",
            "updated": "2024-09-07T15:03:12+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 24
        },
        "2409.04880": {
            "authors": [
                "Neha Kumaru",
                "Garvit Gupta",
                "Shreyas Mongia",
                "Shubham Singh",
                "Ponnurangam Kumaraguru",
                "Arun Balaji Buduru"
            ],
            "title": "Towards identifying Source credibility on Information Leakage in Digital Gadget Market",
            "abstract": "The use of Social media to share content is on a constant rise. One of the capsize effect of information sharing on Social media includes the spread of sensitive information on the public domain. With the digital gadget market becoming highly competitive and ever-evolving, the trend of an increasing number of sensitive posts leaking information on devices in social media is observed. Many web-blogs on digital gadget market have mushroomed recently, making the problem of information leak all pervasive. Credible leaks on specifics of an upcoming device can cause a lot of financial damage to the respective organization. Hence, it is crucial to assess the credibility of the platforms that continuously post about a smartphone or digital gadget leaks. In this work, we analyze the headlines of leak web-blog posts and their corresponding official press-release. We first collect 54, 495 leak and press-release headlines for different smartphones. We train our custom NER model to capture the evolving smartphone names with an accuracy of 82.14% on manually annotated results. We further propose a credibility score metric for the web-blog, based on the number of falsified and authentic smartphone leak posts.",
            "id": "2409.04880",
            "link": "http://arxiv.org/abs/2409.04880v1",
            "published": "2024-09-07T18:20:33+00:00",
            "updated": "2024-09-07T18:20:33+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 41
        },
        "2409.04915": {
            "authors": [
                "Abdur Rahman",
                "Lu He",
                "Haifeng Wang"
            ],
            "title": "Activation Function Optimization Scheme for Image Classification",
            "abstract": "Activation function has a significant impact on the dynamics, convergence, and performance of deep neural networks. The search for a consistent and high-performing activation function has always been a pursuit during deep learning model development. Existing state-of-the-art activation functions are manually designed with human expertise except for Swish. Swish was developed using a reinforcement learning-based search strategy. In this study, we propose an evolutionary approach for optimizing activation functions specifically for image classification tasks, aiming to discover functions that outperform current state-of-the-art options. Through this optimization framework, we obtain a series of high-performing activation functions denoted as Exponential Error Linear Unit (EELU). The developed activation functions are evaluated for image classification tasks from two perspectives: (1) five state-of-the-art neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and Compact Convolutional Transformer which cover computationally heavy to light neural networks, and (2) eight standard datasets, including CIFAR10, Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15, and TinyImageNet which cover from typical machine vision benchmark, agricultural image applications to medical image applications. Finally, we statistically investigate the generalization of the resultant activation functions developed through the optimization scheme. With a Friedman test, we conclude that the optimization scheme is able to generate activation functions that outperform the existing standard ones in 92.8% cases among 28 different cases studied, and $-x\\cdot erf(e^{-x})$ is found to be the best activation function for image classification generated by the optimization scheme.",
            "id": "2409.04915",
            "link": "http://arxiv.org/abs/2409.04915v1",
            "published": "2024-09-07T21:40:15+00:00",
            "updated": "2024-09-07T21:40:15+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 16
        },
        "2409.04920": {
            "authors": [
                "Abdur Rahman",
                "Jason Street",
                "James Wooten",
                "Mohammad Marufuzzaman",
                "Veera G. Gude",
                "Randy Buchanan",
                "Haifeng Wang"
            ],
            "title": "MoistNet: Machine Vision-based Deep Learning Models for Wood Chip Moisture Content Measurement",
            "abstract": "Quick and reliable measurement of wood chip moisture content is an everlasting problem for numerous forest-reliant industries such as biofuel, pulp and paper, and bio-refineries. Moisture content is a critical attribute of wood chips due to its direct relationship with the final product quality. Conventional techniques for determining moisture content, such as oven-drying, possess some drawbacks in terms of their time-consuming nature, potential sample damage, and lack of real-time feasibility. Furthermore, alternative techniques, including NIR spectroscopy, electrical capacitance, X-rays, and microwaves, have demonstrated potential; nevertheless, they are still constrained by issues related to portability, precision, and the expense of the required equipment. Hence, there is a need for a moisture content determination method that is instant, portable, non-destructive, inexpensive, and precise. This study explores the use of deep learning and machine vision to predict moisture content classes from RGB images of wood chips. A large-scale image dataset comprising 1,600 RGB images of wood chips has been collected and annotated with ground truth labels, utilizing the results of the oven-drying technique. Two high-performing neural networks, MoistNetLite and MoistNetMax, have been developed leveraging Neural Architecture Search (NAS) and hyperparameter optimization. The developed models are evaluated and compared with state-of-the-art deep learning models. Results demonstrate that MoistNetLite achieves 87% accuracy with minimal computational overhead, while MoistNetMax exhibits exceptional precision with a 91% accuracy in wood chip moisture content class prediction. With improved accuracy and faster prediction speed, our proposed MoistNet models hold great promise for the wood chip processing industry.",
            "id": "2409.04920",
            "link": "http://arxiv.org/abs/2409.04920v1",
            "published": "2024-09-07T22:03:13+00:00",
            "updated": "2024-09-07T22:03:13+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 45
        },
        "2409.04922": {
            "authors": [
                "Sarwan Ali",
                "Prakash Chourasia",
                "Bipin Koirala",
                "Murray Patterson"
            ],
            "title": "Nearest Neighbor CCP-Based Molecular Sequence Analysis",
            "abstract": "Molecular sequence analysis is crucial for comprehending several biological processes, including protein-protein interactions, functional annotation, and disease classification. The large number of sequences and the inherently complicated nature of protein structures make it challenging to analyze such data. Finding patterns and enhancing subsequent research requires the use of dimensionality reduction and feature selection approaches. Recently, a method called Correlated Clustering and Projection (CCP) has been proposed as an effective method for biological sequencing data. The CCP technique is still costly to compute even though it is effective for sequence visualization. Furthermore, its utility for classifying molecular sequences is still uncertain. To solve these two problems, we present a Nearest Neighbor Correlated Clustering and Projection (CCP-NN)-based technique for efficiently preprocessing molecular sequence data. To group related molecular sequences and produce representative supersequences, CCP makes use of sequence-to-sequence correlations. As opposed to conventional methods, CCP doesn't rely on matrix diagonalization, therefore it can be applied to a range of machine-learning problems. We estimate the density map and compute the correlation using a nearest-neighbor search technique. We performed molecular sequence classification using CCP and CCP-NN representations to assess the efficacy of our proposed approach. Our findings show that CCP-NN considerably improves classification task accuracy as well as significantly outperforms CCP in terms of computational runtime.",
            "id": "2409.04922",
            "link": "http://arxiv.org/abs/2409.04922v1",
            "published": "2024-09-07T22:06:00+00:00",
            "updated": "2024-09-07T22:06:00+00:00",
            "primary_category": "q-bio.GN",
            "categories": [
                "q-bio.GN",
                "cs.AI",
                "cs.CC",
                "cs.LG"
            ],
            "max_author_hindex": 19
        },
        "2409.04949": {
            "authors": [
                "Nidula Elgiriyewithana",
                "N. D. Kodikara"
            ],
            "title": "Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings",
            "abstract": "In this research, we present an innovative, parameter-efficient model that utilizes the attention U-Net architecture for the automatic detection and eradication of non-speech vocal sounds, specifically breath sounds, in vocal recordings. This task is of paramount importance in the field of sound engineering, despite being relatively under-explored. The conventional manual process for detecting and eliminating these sounds requires significant expertise and is extremely time-intensive. Existing automated detection and removal methods often fall short in terms of efficiency and precision. Our proposed model addresses these limitations by offering a streamlined process and superior accuracy, achieved through the application of advanced deep learning techniques. A unique dataset, derived from Device and Produced Speech (DAPS), was employed for this purpose. The training phase of the model emphasizes a log spectrogram and integrates an early stopping mechanism to prevent overfitting. Our model not only conserves precious time for sound engineers but also enhances the quality and consistency of audio production. This constitutes a significant breakthrough, as evidenced by its comparative efficiency, necessitating only 1.9M parameters and a training duration of 3.2 hours - markedly less than the top-performing models in this domain. The model is capable of generating identical outputs as previous models with drastically improved precision, making it an optimal choice.",
            "id": "2409.04949",
            "link": "http://arxiv.org/abs/2409.04949v1",
            "published": "2024-09-08T02:11:33+00:00",
            "updated": "2024-09-08T02:11:33+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "eess.AS"
            ],
            "max_author_hindex": 8
        },
        "2409.04958": {
            "authors": [
                "Jun Yu",
                "WenJian Wang"
            ],
            "title": "DDNet: Deformable Convolution and Dense FPN for Surface Defect Detection in Recycled Books",
            "abstract": "Recycled and recirculated books, such as ancient texts and reused textbooks, hold significant value in the second-hand goods market, with their worth largely dependent on surface preservation. However, accurately assessing surface defects is challenging due to the wide variations in shape, size, and the often imprecise detection of defects. To address these issues, we propose DDNet, an innovative detection model designed to enhance defect localization and classification. DDNet introduces a surface defect feature extraction module based on a deformable convolution operator (DC) and a densely connected FPN module (DFPN). The DC module dynamically adjusts the convolution grid to better align with object contours, capturing subtle shape variations and improving boundary delineation and prediction accuracy. Meanwhile, DFPN leverages dense skip connections to enhance feature fusion, constructing a hierarchical structure that generates multi-resolution, high-fidelity feature maps, thus effectively detecting defects of various sizes. In addition to the model, we present a comprehensive dataset specifically curated for surface defect detection in recycled and recirculated books. This dataset encompasses a diverse range of defect types, shapes, and sizes, making it ideal for evaluating the robustness and effectiveness of defect detection models. Through extensive evaluations, DDNet achieves precise localization and classification of surface defects, recording a mAP value of 46.7% on our proprietary dataset - an improvement of 14.2% over the baseline model - demonstrating its superior detection capabilities.",
            "id": "2409.04958",
            "link": "http://arxiv.org/abs/2409.04958v1",
            "published": "2024-09-08T03:18:19+00:00",
            "updated": "2024-09-08T03:18:19+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 30
        },
        "2409.04977": {
            "authors": [
                "Qi Wang",
                "Zijun Gao",
                "Mingxiu Sui",
                "Taiyuan Mei",
                "Xiaohan Cheng",
                "Iris Li"
            ],
            "title": "Enhancing Convolutional Neural Networks with Higher-Order Numerical Difference Methods",
            "abstract": "With the rise of deep learning technology in practical applications, Convolutional Neural Networks (CNNs) have been able to assist humans in solving many real-world problems. To enhance the performance of CNNs, numerous network architectures have been explored. Some of these architectures are designed based on the accumulated experience of researchers over time, while others are designed through neural architecture search methods. The improvements made to CNNs by the aforementioned methods are quite significant, but most of the improvement methods are limited in reality by model size and environmental constraints, making it difficult to fully realize the improved performance. In recent years, research has found that many CNN structures can be explained by the discretization of ordinary differential equations. This implies that we can design theoretically supported deep network structures using higher-order numerical difference methods. It should be noted that most of the previous CNN model structures are based on low-order numerical methods. Therefore, considering that the accuracy of linear multi-step numerical difference methods is higher than that of the forward Euler method, this paper proposes a stacking scheme based on the linear multi-step method. This scheme enhances the performance of ResNet without increasing the model size and compares it with the Runge-Kutta scheme. The experimental results show that the performance of the stacking scheme proposed in this paper is superior to existing stacking schemes (ResNet and HO-ResNet), and it has the capability to be extended to other types of neural networks.",
            "id": "2409.04977",
            "link": "http://arxiv.org/abs/2409.04977v1",
            "published": "2024-09-08T05:13:58+00:00",
            "updated": "2024-09-08T05:13:58+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 53
        },
        "2409.05076": {
            "authors": [
                "Yudong Zhang",
                "Ruobing Xie",
                "Jiansheng Chen",
                "Xingwu Sun",
                "Yu Wang"
            ],
            "title": "PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions",
            "abstract": "Large Vision-Language Models (LVLMs) have demonstrated their powerful multimodal capabilities. However, they also face serious safety problems, as adversaries can induce robustness issues in LVLMs through the use of well-designed adversarial examples. Therefore, LVLMs are in urgent need of detection tools for adversarial examples to prevent incorrect responses. In this work, we first discover that LVLMs exhibit regular attention patterns for clean images when presented with probe questions. We propose an unconventional method named PIP, which utilizes the attention patterns of one randomly selected irrelevant probe question (e.g., \"Is there a clock?\") to distinguish adversarial examples from clean examples. Regardless of the image to be tested and its corresponding question, PIP only needs to perform one additional inference of the image to be tested and the probe question, and then achieves successful detection of adversarial examples. Even under black-box attacks and open dataset scenarios, our PIP, coupled with a simple SVM, still achieves more than 98% recall and a precision of over 90%. Our PIP is the first attempt to detect adversarial attacks on LVLMs via simple irrelevant probe questions, shedding light on deeper understanding and introspection within LVLMs. The code is available at https://github.com/btzyd/pip.",
            "id": "2409.05076",
            "link": "http://arxiv.org/abs/2409.05076v1",
            "published": "2024-09-08T12:38:25+00:00",
            "updated": "2024-09-08T12:38:25+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 33
        },
        "2409.05084": {
            "authors": [
                "Alexandre Lu\u00eds Magalh\u00e3es Levada",
                "Frank Nielsen",
                "Michel Ferreira Cardia Haddad"
            ],
            "title": "Adaptive $k$-nearest neighbor classifier based on the local estimation of the shape operator",
            "abstract": "The $k$-nearest neighbor ($k$-NN) algorithm is one of the most popular methods for nonparametric classification. However, a relevant limitation concerns the definition of the number of neighbors $k$. This parameter exerts a direct impact on several properties of the classifier, such as the bias-variance tradeoff, smoothness of decision boundaries, robustness to noise, and class imbalance handling. In the present paper, we introduce a new adaptive $k$-nearest neighbours ($kK$-NN) algorithm that explores the local curvature at a sample to adaptively defining the neighborhood size. The rationale is that points with low curvature could have larger neighborhoods (locally, the tangent space approximates well the underlying data shape), whereas points with high curvature could have smaller neighborhoods (locally, the tangent space is a loose approximation). We estimate the local Gaussian curvature by computing an approximation to the local shape operator in terms of the local covariance matrix as well as the local Hessian matrix. Results on many real-world datasets indicate that the new $kK$-NN algorithm yields superior balanced accuracy compared to the established $k$-NN method and also another adaptive $k$-NN algorithm. This is particularly evident when the number of samples in the training data is limited, suggesting that the $kK$-NN is capable of learning more discriminant functions with less data considering many relevant cases.",
            "id": "2409.05084",
            "link": "http://arxiv.org/abs/2409.05084v1",
            "published": "2024-09-08T13:08:45+00:00",
            "updated": "2024-09-08T13:08:45+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.IT",
                "math.IT"
            ],
            "max_author_hindex": 44
        },
        "2409.05144": {
            "authors": [
                "Junjie Zhao",
                "Chengxi Zhang",
                "Min Qin",
                "Peng Yang"
            ],
            "title": "QuantFactor REINFORCE: Mining Steady Formulaic Alpha Factors with Variance-bounded REINFORCE",
            "abstract": "The goal of alpha factor mining is to discover indicative signals of investment opportunities from the historical financial market data of assets. Deep learning based alpha factor mining methods have shown to be powerful, which, however, lack of the interpretability, making them unacceptable in the risk-sensitive real markets. Alpha factors in formulaic forms are more interpretable and therefore favored by market participants, while the search space is complex and powerful explorative methods are urged. Recently, a promising framework is proposed for generating formulaic alpha factors using deep reinforcement learning, and quickly gained research focuses from both academia and industries. This paper first argues that the originally employed policy training method, i.e., Proximal Policy Optimization (PPO), faces several important issues in the context of alpha factors mining, making it ineffective to explore the search space of the formula. Herein, a novel reinforcement learning based on the well-known REINFORCE algorithm is proposed. Given that the underlying state transition function adheres to the Dirac distribution, the Markov Decision Process within this framework exhibit minimal environmental variability, making REINFORCE algorithm more appropriate than PPO. A new dedicated baseline is designed to theoretically reduce the commonly suffered high variance of REINFORCE. Moreover, the information ratio is introduced as a reward shaping mechanism to encourage the generation of steady alpha factors that can better adapt to changes in market volatility. Experimental evaluations on various real assets data show that the proposed algorithm can increase the correlation with asset returns by 3.83%, and a stronger ability to obtain excess returns compared to the latest alpha factors mining methods, which meets the theoretical results well.",
            "id": "2409.05144",
            "link": "http://arxiv.org/abs/2409.05144v1",
            "published": "2024-09-08T15:57:58+00:00",
            "updated": "2024-09-08T15:57:58+00:00",
            "primary_category": "q-fin.CP",
            "categories": [
                "q-fin.CP",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 28
        },
        "2409.05202": {
            "authors": [
                "Xin Jin",
                "Hongyu Zhu",
                "Siyuan Li",
                "Zedong Wang",
                "Zicheng Liu",
                "Chang Yu",
                "Huafeng Qin",
                "Stan Z. Li"
            ],
            "title": "A Survey on Mixup Augmentations and Beyond",
            "abstract": "As Deep Neural Networks have achieved thrilling breakthroughs in the past decade, data augmentations have garnered increasing attention as regularization techniques when massive labeled data are unavailable. Among existing augmentations, Mixup and relevant data-mixing methods that convexly combine selected samples and the corresponding labels are widely adopted because they yield high performances by generating data-dependent virtual data while easily migrating to various domains. This survey presents a comprehensive review of foundational mixup methods and their applications. We first elaborate on the training pipeline with mixup augmentations as a unified framework containing modules. A reformulated framework could contain various mixup methods and give intuitive operational procedures. Then, we systematically investigate the applications of mixup augmentations on vision downstream tasks, various data modalities, and some analysis \\& theorems of mixup. Meanwhile, we conclude the current status and limitations of mixup research and point out further work for effective and efficient mixup augmentations. This survey can provide researchers with the current state of the art in mixup methods and provide some insights and guidance roles in the mixup arena. An online project with this survey is available at \\url{https://github.com/Westlake-AI/Awesome-Mixup}.",
            "id": "2409.05202",
            "link": "http://arxiv.org/abs/2409.05202v1",
            "published": "2024-09-08T19:32:22+00:00",
            "updated": "2024-09-08T19:32:22+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 38
        },
        "2409.05206": {
            "authors": [
                "E. V. Aretos",
                "D. G. Sotiropoulos"
            ],
            "title": "SEF: A Method for Computing Prediction Intervals by Shifting the Error Function in Neural Networks",
            "abstract": "In today's era, Neural Networks (NN) are applied in various scientific fields such as robotics, medicine, engineering, etc. However, the predictions of neural networks themselves contain a degree of uncertainty that must always be taken into account before any decision is made. This is why many researchers have focused on developing different ways to quantify the uncertainty of neural network predictions. Some of these methods are based on generating prediction intervals (PI) via neural networks for the requested target values. The SEF (Shifting the Error Function) method presented in this paper is a new method that belongs to this category of methods. The proposed approach involves training a single neural network three times, thus generating an estimate along with the corresponding upper and lower bounds for a given problem. A pivotal aspect of the method is the calculation of a parameter from the initial network's estimates, which is then integrated into the loss functions of the other two networks. This innovative process effectively produces PIs, resulting in a robust and efficient technique for uncertainty quantification. To evaluate the effectiveness of our method, a comparison in terms of successful PI generation between the SEF, PI3NN and PIVEN methods was made using two synthetic datasets.",
            "id": "2409.05206",
            "link": "http://arxiv.org/abs/2409.05206v1",
            "published": "2024-09-08T19:46:45+00:00",
            "updated": "2024-09-08T19:46:45+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ],
            "max_author_hindex": 9
        },
        "2409.05242": {
            "authors": [
                "Chamath Palihawadana",
                "Nirmalie Wiratunga",
                "Anjana Wijekoon",
                "Harsha Kalutarage"
            ],
            "title": "FedFT: Improving Communication Performance for Federated Learning with Frequency Space Transformation",
            "abstract": "Communication efficiency is a widely recognised research problem in Federated Learning (FL), with recent work focused on developing techniques for efficient compression, distribution and aggregation of model parameters between clients and the server. Particularly within distributed systems, it is important to balance the need for computational cost and communication efficiency. However, existing methods are often constrained to specific applications and are less generalisable. In this paper, we introduce FedFT (federated frequency-space transformation), a simple yet effective methodology for communicating model parameters in a FL setting. FedFT uses Discrete Cosine Transform (DCT) to represent model parameters in frequency space, enabling efficient compression and reducing communication overhead. FedFT is compatible with various existing FL methodologies and neural architectures, and its linear property eliminates the need for multiple transformations during federated aggregation. This methodology is vital for distributed solutions, tackling essential challenges like data privacy, interoperability, and energy efficiency inherent to these environments. We demonstrate the generalisability of the FedFT methodology on four datasets using comparative studies with three state-of-the-art FL baselines (FedAvg, FedProx, FedSim). Our results demonstrate that using FedFT to represent the differences in model parameters between communication rounds in frequency space results in a more compact representation compared to representing the entire model in frequency space. This leads to a reduction in communication overhead, while keeping accuracy levels comparable and in some cases even improving it. Our results suggest that this reduction can range from 5% to 30% per client, depending on dataset.",
            "id": "2409.05242",
            "link": "http://arxiv.org/abs/2409.05242v1",
            "published": "2024-09-08T23:05:35+00:00",
            "updated": "2024-09-08T23:05:35+00:00",
            "primary_category": "cs.DC",
            "categories": [
                "cs.DC",
                "cs.AI"
            ],
            "max_author_hindex": 25
        },
        "2409.05898": {
            "authors": [
                "Yihao Cai",
                "Hongpeng Cao",
                "Yanbing Mao",
                "Lui Sha",
                "Marco Caccamo"
            ],
            "title": "Simplex-enabled Safe Continual Learning Machine",
            "abstract": "This paper proposes the SeC-Learning Machine: Simplex-enabled safe continual learning for safety-critical autonomous systems. The SeC-learning machine is built on Simplex logic (that is, ``using simplicity to control complexity'') and physics-regulated deep reinforcement learning (Phy-DRL). The SeC-learning machine thus constitutes HP (high performance)-Student, HA (high assurance)-Teacher, and Coordinator. Specifically, the HP-Student is a pre-trained high-performance but not fully verified Phy-DRL, continuing to learn in a real plant to tune the action policy to be safe. In contrast, the HA-Teacher is a mission-reduced, physics-model-based, and verified design. As a complementary, HA-Teacher has two missions: backing up safety and correcting unsafe learning. The Coordinator triggers the interaction and the switch between HP-Student and HA-Teacher. Powered by the three interactive components, the SeC-learning machine can i) assure lifetime safety (i.e., safety guarantee in any continual-learning stage, regardless of HP-Student's success or convergence), ii) address the Sim2Real gap, and iii) learn to tolerate unknown unknowns in real plants. The experiments on a cart-pole system and a real quadruped robot demonstrate the distinguished features of the SeC-learning machine, compared with continual learning built on state-of-the-art safe DRL frameworks with approaches to addressing the Sim2Real gap.",
            "id": "2409.05898",
            "link": "http://arxiv.org/abs/2409.05898v1",
            "published": "2024-09-05T16:03:00+00:00",
            "updated": "2024-09-05T16:03:00+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ],
            "max_author_hindex": 61
        },
        "2409.05916": {
            "authors": [
                "Chunbin Gu",
                "Mutian He",
                "Hanqun Cao",
                "Guangyong Chen",
                "Chang-yu Hsieh",
                "Pheng Ann Heng"
            ],
            "title": "Unlocking Potential Binders: Multimodal Pretraining DEL-Fusion for Denoising DNA-Encoded Libraries",
            "abstract": "In the realm of drug discovery, DNA-encoded library (DEL) screening technology has emerged as an efficient method for identifying high-affinity compounds. However, DEL screening faces a significant challenge: noise arising from nonspecific interactions within complex biological systems. Neural networks trained on DEL libraries have been employed to extract compound features, aiming to denoise the data and uncover potential binders to the desired therapeutic target. Nevertheless, the inherent structure of DEL, constrained by the limited diversity of building blocks, impacts the performance of compound encoders. Moreover, existing methods only capture compound features at a single level, further limiting the effectiveness of the denoising strategy. To mitigate these issues, we propose a Multimodal Pretraining DEL-Fusion model (MPDF) that enhances encoder capabilities through pretraining and integrates compound features across various scales. We develop pretraining tasks applying contrastive objectives between different compound representations and their text descriptions, enhancing the compound encoders' ability to acquire generic features. Furthermore, we propose a novel DEL-fusion framework that amalgamates compound information at the atomic, submolecular, and molecular levels, as captured by various compound encoders. The synergy of these innovations equips MPDF with enriched, multi-scale features, enabling comprehensive downstream denoising. Evaluated on three DEL datasets, MPDF demonstrates superior performance in data processing and analysis for validation tasks. Notably, MPDF offers novel insights into identifying high-affinity molecules, paving the way for improved DEL utility in drug discovery.",
            "id": "2409.05916",
            "link": "http://arxiv.org/abs/2409.05916v1",
            "published": "2024-09-07T17:32:21+00:00",
            "updated": "2024-09-07T17:32:21+00:00",
            "primary_category": "q-bio.QM",
            "categories": [
                "q-bio.QM",
                "cs.AI",
                "cs.LG",
                "q-bio.BM"
            ],
            "max_author_hindex": 101
        },
        "2409.05919": {
            "authors": [
                "Roy Abitbol",
                "Eyal Cohen",
                "Muhammad Kanaan",
                "Bhavna Agrawal",
                "Yingjie Li",
                "Anuradha Bhamidipaty",
                "Erez Bilgory"
            ],
            "title": "KModels: Unlocking AI for Business Applications",
            "abstract": "As artificial intelligence (AI) continues to rapidly advance, there is a growing demand to integrate AI capabilities into existing business applications. However, a significant gap exists between the rapid progress in AI and how slowly AI is being embedded into business environments. Deploying well-performing lab models into production settings, especially in on-premise environments, often entails specialized expertise and imposes a heavy burden of model management, creating significant barriers to implementing AI models in real-world applications.   KModels leverages proven libraries and platforms (Kubeflow Pipelines, KServe) to streamline AI adoption by supporting both AI developers and consumers. It allows model developers to focus solely on model development and share models as transportable units (Templates), abstracting away complex production deployment concerns. KModels enables AI consumers to eliminate the need for a dedicated data scientist, as the templates encapsulate most data science considerations while providing business-oriented control.   This paper presents the architecture of KModels and the key decisions that shape it. We outline KModels' main components as well as its interfaces. Furthermore, we explain how KModels is highly suited for on-premise deployment but can also be used in cloud environments.   The efficacy of KModels is demonstrated through the successful deployment of three AI models within an existing Work Order Management system. These models operate in a client's data center and are trained on local data, without data scientist intervention. One model improved the accuracy of Failure Code specification for work orders from 46% to 83%, showcasing the substantial benefit of accessible and localized AI solutions.",
            "id": "2409.05919",
            "link": "http://arxiv.org/abs/2409.05919v1",
            "published": "2024-09-08T13:19:12+00:00",
            "updated": "2024-09-08T13:19:12+00:00",
            "primary_category": "cs.SE",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "max_author_hindex": 43
        },
        "2409.05921": {
            "authors": [
                "Zhiqi Shao",
                "Haoning Xi",
                "Haohui Lu",
                "Ze Wang",
                "Michael G. H. Bell",
                "Junbin Gao"
            ],
            "title": "STLLM-DF: A Spatial-Temporal Large Language Model with Diffusion for Enhanced Multi-Mode Traffic System Forecasting",
            "abstract": "The rapid advancement of Intelligent Transportation Systems (ITS) presents challenges, particularly with missing data in multi-modal transportation and the complexity of handling diverse sequential tasks within a centralized framework. To address these issues, we propose the Spatial-Temporal Large Language Model Diffusion (STLLM-DF), an innovative model that leverages Denoising Diffusion Probabilistic Models (DDPMs) and Large Language Models (LLMs) to improve multi-task transportation prediction. The DDPM's robust denoising capabilities enable it to recover underlying data patterns from noisy inputs, making it particularly effective in complex transportation systems. Meanwhile, the non-pretrained LLM dynamically adapts to spatial-temporal relationships within multi-modal networks, allowing the system to efficiently manage diverse transportation tasks in both long-term and short-term predictions. Extensive experiments demonstrate that STLLM-DF consistently outperforms existing models, achieving an average reduction of 2.40\\% in MAE, 4.50\\% in RMSE, and 1.51\\% in MAPE. This model significantly advances centralized ITS by enhancing predictive accuracy, robustness, and overall system performance across multiple tasks, thus paving the way for more effective spatio-temporal traffic forecasting through the integration of frozen transformer language models and diffusion techniques.",
            "id": "2409.05921",
            "link": "http://arxiv.org/abs/2409.05921v1",
            "published": "2024-09-08T15:29:27+00:00",
            "updated": "2024-09-08T15:29:27+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "I.2.7",
                "I.2.1"
            ],
            "max_author_hindex": 55
        },
        "2409.07480": {
            "authors": [
                "Sam Gijsen",
                "Kerstin Ritter"
            ],
            "title": "EEG-Language Modeling for Pathology Detection",
            "abstract": "Multimodal language modeling constitutes a recent breakthrough which leverages advances in large language models to pretrain capable multimodal models. The integration of natural language during pretraining has been shown to significantly improve learned representations, particularly in computer vision. However, the efficacy of multimodal language modeling in the realm of functional brain data, specifically for advancing pathology detection, remains unexplored. This study pioneers EEG-language models trained on clinical reports and 15000 EEGs. We extend methods for multimodal alignment to this novel domain and investigate which textual information in reports is useful for training EEG-language models. Our results indicate that models learn richer representations from being exposed to a variety of report segments, including the patient's clinical history, description of the EEG, and the physician's interpretation. Compared to models exposed to narrower clinical text information, we find such models to retrieve EEGs based on clinical reports (and vice versa) with substantially higher accuracy. Yet, this is only observed when using a contrastive learning approach. Particularly in regimes with few annotations, we observe that representations of EEG-language models can significantly improve pathology detection compared to those of EEG-only models, as demonstrated by both zero-shot classification and linear probes. In sum, these results highlight the potential of integrating brain activity data with clinical text, suggesting that EEG-language models represent significant progress for clinical applications.",
            "id": "2409.07480",
            "link": "http://arxiv.org/abs/2409.07480v1",
            "published": "2024-09-02T10:03:03+00:00",
            "updated": "2024-09-02T10:03:03+00:00",
            "primary_category": "eess.SP",
            "categories": [
                "eess.SP",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 12
        },
        "2409.07485": {
            "authors": [
                "Alessio Burrello",
                "Francesco Carlucci",
                "Giovanni Pollo",
                "Xiaying Wang",
                "Massimo Poncino",
                "Enrico Macii",
                "Luca Benini",
                "Daniele Jahier Pagliari"
            ],
            "title": "Optimization and Deployment of Deep Neural Networks for PPG-based Blood Pressure Estimation Targeting Low-power Wearables",
            "abstract": "PPG-based Blood Pressure (BP) estimation is a challenging biosignal processing task for low-power devices such as wearables. State-of-the-art Deep Neural Networks (DNNs) trained for this task implement either a PPG-to-BP signal-to-signal reconstruction or a scalar BP value regression and have been shown to outperform classic methods on the largest and most complex public datasets. However, these models often require excessive parameter storage or computational effort for wearable deployment, exceeding the available memory or incurring too high latency and energy consumption. In this work, we describe a fully-automated DNN design pipeline, encompassing HW-aware Neural Architecture Search (NAS) and Quantization, thanks to which we derive accurate yet lightweight models, that can be deployed on an ultra-low-power multicore System-on-Chip (SoC), GAP8. Starting from both regression and signal-to-signal state-of-the-art models on four public datasets, we obtain optimized versions that achieve up to 4.99% lower error or 73.36% lower size at iso-error. Noteworthy, while the most accurate SoA network on the largest dataset can not fit the GAP8 memory, all our optimized models can; our most accurate DNN consumes as little as 0.37 mJ while reaching the lowest MAE of 8.08 on Diastolic BP estimation.",
            "id": "2409.07485",
            "link": "http://arxiv.org/abs/2409.07485v1",
            "published": "2024-09-03T15:48:43+00:00",
            "updated": "2024-09-03T15:48:43+00:00",
            "primary_category": "eess.SP",
            "categories": [
                "eess.SP",
                "cs.AI",
                "cs.LG"
            ],
            "max_author_hindex": 104
        },
        "2409.07486": {
            "authors": [
                "Junjie Li",
                "Yang Liu",
                "Weiqing Liu",
                "Shikai Fang",
                "Lewen Wang",
                "Chang Xu",
                "Jiang Bian"
            ],
            "title": "MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model",
            "abstract": "Generative models aim to simulate realistic effects of various actions across different contexts, from text generation to visual effects. Despite efforts to build real-world simulators, leveraging generative models for virtual worlds, like financial markets, remains underexplored. In financial markets, generative models can simulate market effects of various behaviors, enabling interaction with market scenes and players, and training strategies without financial risk. This simulation relies on the finest structured data in financial market like orders thus building the finest realistic simulation. We propose Large Market Model (LMM), an order-level generative foundation model, for financial market simulation, akin to language modeling in the digital world. Our financial Market Simulation engine (MarS), powered by LMM, addresses the need for realistic, interactive and controllable order generation. Key objectives of this paper include evaluating LMM's scaling law in financial markets, assessing MarS's realism, balancing controlled generation with market impact, and demonstrating MarS's potential applications. We showcase MarS as a forecast tool, detection system, analysis platform, and agent training environment. Our contributions include pioneering a generative model for financial markets, designing MarS to meet domain-specific needs, and demonstrating MarS-based applications' industry potential.",
            "id": "2409.07486",
            "link": "http://arxiv.org/abs/2409.07486v1",
            "published": "2024-09-04T08:16:22+00:00",
            "updated": "2024-09-04T08:16:22+00:00",
            "primary_category": "q-fin.CP",
            "categories": [
                "q-fin.CP",
                "cs.AI",
                "cs.CE",
                "cs.LG",
                "q-fin.TR"
            ],
            "max_author_hindex": 55
        },
        "2409.07489": {
            "authors": [
                "Sakuna Harinda Jayasundara",
                "Nalin Asanka Gamagedara Arachchilage",
                "Giovanni Russello"
            ],
            "title": "RAGent: Retrieval-based Access Control Policy Generation",
            "abstract": "Manually generating access control policies from an organization's high-level requirement specifications poses significant challenges. It requires laborious efforts to sift through multiple documents containing such specifications and translate their access requirements into access control policies. Also, the complexities and ambiguities of these specifications often result in errors by system administrators during the translation process, leading to data breaches. However, the automated policy generation frameworks designed to help administrators in this process are unreliable due to limitations, such as the lack of domain adaptation. Therefore, to improve the reliability of access control policy generation, we propose RAGent, a novel retrieval-based access control policy generation framework based on language models. RAGent identifies access requirements from high-level requirement specifications with an average state-of-the-art F1 score of 87.9%. Through retrieval augmented generation, RAGent then translates the identified access requirements into access control policies with an F1 score of 77.9%. Unlike existing frameworks, RAGent generates policies with complex components like purposes and conditions, in addition to subjects, actions, and resources. Moreover, RAGent automatically verifies the generated policies and iteratively refines them through a novel verification-refinement mechanism, further improving the reliability of the process by 3%, reaching the F1 score of 80.6%. We also introduce three annotated datasets for developing access control policy generation frameworks in the future, addressing the data scarcity of the domain.",
            "id": "2409.07489",
            "link": "http://arxiv.org/abs/2409.07489v1",
            "published": "2024-09-08T00:23:37+00:00",
            "updated": "2024-09-08T00:23:37+00:00",
            "primary_category": "cs.CR",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "max_author_hindex": 23
        },
        "2409.00726": {
            "authors": [
                "Zhaojie Fang",
                "Xiao Yu",
                "Guanyu Zhou",
                "Ke Zhuang",
                "Yifei Chen",
                "Ruiquan Ge",
                "Changmiao Wang",
                "Gangyong Jia",
                "Qing Wu",
                "Juan Ye",
                "Maimaiti Nuliqiman",
                "Peifang Xu",
                "Ahmed Elazab"
            ],
            "title": "LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset",
            "abstract": "Ultra-Wide-Field Fluorescein Angiography (UWF-FA) enables precise identification of ocular diseases using sodium fluorescein, which can be potentially harmful. Existing research has developed methods to generate UWF-FA from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) to reduce the adverse reactions associated with injections. However, these methods have been less effective in producing high-quality late-phase UWF-FA, particularly in lesion areas and fine details. Two primary challenges hinder the generation of high-quality late-phase UWF-FA: the scarcity of paired UWF-SLO and early/late-phase UWF-FA datasets, and the need for realistic generation at lesion sites and potential blood leakage regions. This study introduces an improved latent diffusion model framework to generate high-quality late-phase UWF-FA from limited paired UWF images. To address the challenges as mentioned earlier, our approach employs a module utilizing Cross-temporal Regional Difference Loss, which encourages the model to focus on the differences between early and late phases. Additionally, we introduce a low-frequency enhanced noise strategy in the diffusion forward process to improve the realism of medical images. To further enhance the mapping capability of the variational autoencoder module, especially with limited datasets, we implement a Gated Convolutional Encoder to extract additional information from conditional images. Our Latent Diffusion Model for Ultra-Wide-Field Late-Phase Fluorescein Angiography (LPUWF-LDM) effectively reconstructs fine details in late-phase UWF-FA and achieves state-of-the-art results compared to other existing methods when working with limited datasets. Our source code is available at: https://github.com/Tinysqua/****.",
            "id": "2409.00726",
            "link": "http://arxiv.org/abs/2409.00726v1",
            "published": "2024-09-01T14:09:00+00:00",
            "updated": "2024-09-01T14:09:00+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 27
        },
        "2409.00879": {
            "authors": [
                "Youngseog Chung",
                "Dhruv Malik",
                "Jeff Schneider",
                "Yuanzhi Li",
                "Aarti Singh"
            ],
            "title": "Beyond Parameter Count: Implicit Bias in Soft Mixture of Experts",
            "abstract": "The traditional viewpoint on Sparse Mixture of Experts (MoE) models is that instead of training a single large expert, which is computationally expensive, we can train many small experts. The hope is that if the total parameter count of the small experts equals that of the singular large expert, then we retain the representation power of the large expert while gaining computational tractability and promoting expert specialization. The recently introduced Soft MoE replaces the Sparse MoE's discrete routing mechanism with a differentiable gating function that smoothly mixes tokens. While this smooth gating function successfully mitigates the various training instabilities associated with Sparse MoE, it is unclear whether it induces implicit biases that affect Soft MoE's representation power or potential for expert specialization. We prove that Soft MoE with a single arbitrarily powerful expert cannot represent simple convex functions. This justifies that Soft MoE's success cannot be explained by the traditional viewpoint of many small experts collectively mimicking the representation power of a single large expert, and that multiple experts are actually necessary to achieve good representation power (even for a fixed total parameter count). Continuing along this line of investigation, we introduce a notion of expert specialization for Soft MoE, and while varying the number of experts yet fixing the total parameter count, we consider the following (computationally intractable) task. Given any input, how can we discover the expert subset that is specialized to predict this input's label? We empirically show that when there are many small experts, the architecture is implicitly biased in a fashion that allows us to efficiently approximate the specialized expert subset. Our method can be easily implemented to potentially reduce computation during inference.",
            "id": "2409.00879",
            "link": "http://arxiv.org/abs/2409.00879v1",
            "published": "2024-09-02T00:39:00+00:00",
            "updated": "2024-09-02T00:39:00+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 39
        },
        "2409.01138": {
            "authors": [
                "Tuong Vy Nguyen",
                "Johannes Hoster",
                "Alexander Glaser",
                "Kristian Hildebrand",
                "Felix Biessmann"
            ],
            "title": "Generating Synthetic Satellite Imagery for Rare Objects: An Empirical Comparison of Models and Metrics",
            "abstract": "Generative deep learning architectures can produce realistic, high-resolution fake imagery -- with potentially drastic societal implications. A key question in this context is: How easy is it to generate realistic imagery, in particular for niche domains. The iterative process required to achieve specific image content is difficult to automate and control. Especially for rare classes, it remains difficult to assess fidelity, meaning whether generative approaches produce realistic imagery and alignment, meaning how (well) the generation can be guided by human input. In this work, we present a large-scale empirical evaluation of generative architectures which we fine-tuned to generate synthetic satellite imagery. We focus on nuclear power plants as an example of a rare object category - as there are only around 400 facilities worldwide, this restriction is exemplary for many other scenarios in which training and test data is limited by the restricted number of occurrences of real-world examples. We generate synthetic imagery by conditioning on two kinds of modalities, textual input and image input obtained from a game engine that allows for detailed specification of the building layout. The generated images are assessed by commonly used metrics for automatic evaluation and then compared with human judgement from our conducted user studies to assess their trustworthiness. Our results demonstrate that even for rare objects, generation of authentic synthetic satellite imagery with textual or detailed building layouts is feasible. In line with previous work, we find that automated metrics are often not aligned with human perception -- in fact, we find strong negative correlations between commonly used image quality metrics and human ratings.",
            "id": "2409.01138",
            "link": "http://arxiv.org/abs/2409.01138v1",
            "published": "2024-09-02T10:19:39+00:00",
            "updated": "2024-09-02T10:19:39+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ],
            "max_author_hindex": 24
        },
        "2409.01241": {
            "authors": [
                "Sorin Grigorescu",
                "Mihai Zaha"
            ],
            "title": "CyberCortex.AI: An AI-based Operating System for Autonomous Robotics and Complex Automation",
            "abstract": "The underlying framework for controlling autonomous robots and complex automation applications are Operating Systems (OS) capable of scheduling perception-and-control tasks, as well as providing real-time data communication to other robotic peers and remote cloud computers. In this paper, we introduce CyberCortex.AI, a robotics OS designed to enable heterogeneous AI-based robotics and complex automation applications. CyberCortex.AI is a decentralized distributed OS which enables robots to talk to each other, as well as to High Performance Computers (HPC) in the cloud. Sensory and control data from the robots is streamed towards HPC systems with the purpose of training AI algorithms, which are afterwards deployed on the robots. Each functionality of a robot (e.g. sensory data acquisition, path planning, motion control, etc.) is executed within a so-called DataBlock of Filters shared through the internet, where each filter is computed either locally on the robot itself, or remotely on a different robotic system. The data is stored and accessed via a so-called \\textit{Temporal Addressable Memory} (TAM), which acts as a gateway between each filter's input and output. CyberCortex.AI has two main components: i) the CyberCortex.AI.inference system, which is a real-time implementation of the DataBlock running on the robots' embedded hardware, and ii) the CyberCortex.AI.dojo, which runs on an HPC computer in the cloud, and it is used to design, train and deploy AI algorithms. We present a quantitative and qualitative performance analysis of the proposed approach using two collaborative robotics applications: \\textit{i}) a forest fires prevention system based on an Unitree A1 legged robot and an Anafi Parrot 4K drone, as well as \\textit{ii}) an autonomous driving system which uses CyberCortex.AI for collaborative perception and motion control.",
            "id": "2409.01241",
            "link": "http://arxiv.org/abs/2409.01241v1",
            "published": "2024-09-02T13:14:50+00:00",
            "updated": "2024-09-02T13:14:50+00:00",
            "primary_category": "cs.RO",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.OS"
            ],
            "max_author_hindex": 12
        },
        "2409.01330": {
            "authors": [
                "Iulian Emil Tampu",
                "Per Nyman",
                "Christoforos Spyretos",
                "Ida Blystad",
                "Alia Shamikh",
                "Gabriela Prochazka",
                "Teresita D\u00edaz de St\u00e5hl",
                "Johanna Sandgren",
                "Peter Lundberg",
                "Neda Haj-Hosseini"
            ],
            "title": "Pediatric brain tumor classification using digital histopathology and deep learning: evaluation of SOTA methods on a multi-center Swedish cohort",
            "abstract": "Brain tumors are the most common solid tumors in children and young adults, but the scarcity of large histopathology datasets has limited the application of computational pathology in this group. This study implements two weakly supervised multiple-instance learning (MIL) approaches on patch-features obtained from state-of-the-art histology-specific foundation models to classify pediatric brain tumors in hematoxylin and eosin whole slide images (WSIs) from a multi-center Swedish cohort. WSIs from 540 subjects (age 8.5$\\pm$4.9 years) diagnosed with brain tumor were gathered from the six Swedish university hospitals. Instance (patch)-level features were obtained from WSIs using three pre-trained feature extractors: ResNet50, UNI and CONCH. Instances were aggregated using attention-based MIL (ABMIL) or clustering-constrained attention MIL (CLAM) for patient-level classification. Models were evaluated on three classification tasks based on the hierarchical classification of pediatric brain tumors: tumor category, family and type. Model generalization was assessed by training on data from two of the centers and testing on data from four other centers. Model interpretability was evaluated through attention-mapping. The highest classification performance was achieved using UNI features and AMBIL aggregation, with Matthew's correlation coefficient of 0.86$\\pm$0.04, 0.63$\\pm$0.04, and 0.53$\\pm$0.05, for tumor category, family and type classification, respectively. When evaluating generalization, models utilizing UNI and CONCH features outperformed those using ResNet50. However, the drop in performance from the in-site to out-of-site testing was similar across feature extractors. These results show the potential of state-of-the-art computational pathology methods in diagnosing pediatric brain tumors at different hierarchical levels with fair generalizability on a multi-center national dataset.",
            "id": "2409.01330",
            "link": "http://arxiv.org/abs/2409.01330v1",
            "published": "2024-09-02T15:32:04+00:00",
            "updated": "2024-09-02T15:32:04+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 35
        },
        "2409.01555": {
            "authors": [
                "Zhiheng Peng",
                "Kai Zhao",
                "Xiaoran Chen",
                "Li Ma",
                "Siyu Xia",
                "Changjie Fan",
                "Weijian Shang",
                "Wei Jing"
            ],
            "title": "EA-RAS: Towards Efficient and Accurate End-to-End Reconstruction of Anatomical Skeleton",
            "abstract": "Efficient, accurate and low-cost estimation of human skeletal information is crucial for a range of applications such as biology education and human-computer interaction. However, current simple skeleton models, which are typically based on 2D-3D joint points, fall short in terms of anatomical fidelity, restricting their utility in fields. On the other hand, more complex models while anatomically precise, are hindered by sophisticate multi-stage processing and the need for extra data like skin meshes, making them unsuitable for real-time applications. To this end, we propose the EA-RAS (Towards Efficient and Accurate End-to-End Reconstruction of Anatomical Skeleton), a single-stage, lightweight, and plug-and-play anatomical skeleton estimator that can provide real-time, accurate anatomically realistic skeletons with arbitrary pose using only a single RGB image input. Additionally, EA-RAS estimates the conventional human-mesh model explicitly, which not only enhances the functionality but also leverages the outside skin information by integrating features into the inside skeleton modeling process. In this work, we also develop a progressive training strategy and integrated it with an enhanced optimization process, enabling the network to obtain initial weights using only a small skin dataset and achieve self-supervision in skeleton reconstruction. Besides, we also provide an optional lightweight post-processing optimization strategy to further improve accuracy for scenarios that prioritize precision over real-time processing. The experiments demonstrated that our regression method is over 800 times faster than existing methods, meeting real-time requirements. Additionally, the post-processing optimization strategy provided can enhance reconstruction accuracy by over 50% and achieve a speed increase of more than 7 times.",
            "id": "2409.01555",
            "link": "http://arxiv.org/abs/2409.01555v1",
            "published": "2024-09-03T02:46:28+00:00",
            "updated": "2024-09-03T02:46:28+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 28
        },
        "2409.01622": {
            "authors": [
                "Zach Eidex",
                "Mojtaba Safari",
                "Richard L. J. Qiu",
                "David S. Yu",
                "Hui-Kuo Shu",
                "Hui Mao",
                "Xiaofeng Yang"
            ],
            "title": "T1-contrast Enhanced MRI Generation from Multi-parametric MRI for Glioma Patients with Latent Tumor Conditioning",
            "abstract": "Objective: Gadolinium-based contrast agents (GBCAs) are commonly used in MRI scans of patients with gliomas to enhance brain tumor characterization using T1-weighted (T1W) MRI. However, there is growing concern about GBCA toxicity. This study develops a deep-learning framework to generate T1-postcontrast (T1C) from pre-contrast multiparametric MRI. Approach: We propose the tumor-aware vision transformer (TA-ViT) model that predicts high-quality T1C images. The predicted tumor region is significantly improved (P < .001) by conditioning the transformer layers from predicted segmentation maps through adaptive layer norm zero mechanism. The predicted segmentation maps were generated with the multi-parametric residual (MPR) ViT model and transformed into a latent space to produce compressed, feature-rich representations. The TA-ViT model predicted T1C MRI images of 501 glioma cases. Selected patients were split into training (N=400), validation (N=50), and test (N=51) sets. Main Results: Both qualitative and quantitative results demonstrate that the TA-ViT model performs superior against the benchmark MRP-ViT model. Our method produces synthetic T1C MRI with high soft tissue contrast and more accurately reconstructs both the tumor and whole brain volumes. The synthesized T1C images achieved remarkable improvements in both tumor and healthy tissue regions compared to the MRP-ViT model. For healthy tissue and tumor regions, the results were as follows: NMSE: 8.53 +/- 4.61E-4; PSNR: 31.2 +/- 2.2; NCC: 0.908 +/- .041 and NMSE: 1.22 +/- 1.27E-4, PSNR: 41.3 +/- 4.7, and NCC: 0.879 +/- 0.042, respectively. Significance: The proposed method generates synthetic T1C images that closely resemble real T1C images. Future development and application of this approach may enable contrast-agent-free MRI for brain tumor patients, eliminating the risk of GBCA toxicity and simplifying the MRI scan protocol.",
            "id": "2409.01622",
            "link": "http://arxiv.org/abs/2409.01622v1",
            "published": "2024-09-03T05:45:37+00:00",
            "updated": "2024-09-03T05:45:37+00:00",
            "primary_category": "eess.IV",
            "categories": [
                "eess.IV",
                "cs.AI",
                "cs.CV"
            ],
            "max_author_hindex": 43
        },
        "2409.01688": {
            "authors": [
                "Erzhi Liu",
                "Jerry Yao-Chieh Hu",
                "Alex Reneau",
                "Zhao Song",
                "Han Liu"
            ],
            "title": "Differentially Private Kernel Density Estimation",
            "abstract": "We introduce a refined differentially private (DP) data structure for kernel density estimation (KDE), offering not only improved privacy-utility tradeoff but also better efficiency over prior results. Specifically, we study the mathematical problem: given a similarity function $f$ (or DP KDE) and a private dataset $X \\subset \\mathbb{R}^d$, our goal is to preprocess $X$ so that for any query $y\\in\\mathbb{R}^d$, we approximate $\\sum_{x \\in X} f(x, y)$ in a differentially private fashion. The best previous algorithm for $f(x,y) =\\| x - y \\|_1$ is the node-contaminated balanced binary tree by [Backurs, Lin, Mahabadi, Silwal, and Tarnawski, ICLR 2024]. Their algorithm requires $O(nd)$ space and time for preprocessing with $n=|X|$. For any query point, the query time is $d \\log n$, with an error guarantee of $(1+\\alpha)$-approximation and $\\epsilon^{-1} \\alpha^{-0.5} d^{1.5} R \\log^{1.5} n$.   In this paper, we improve the best previous result [Backurs, Lin, Mahabadi, Silwal, and Tarnawski, ICLR 2024] in three aspects:   - We reduce query time by a factor of $\\alpha^{-1} \\log n$.   - We improve the approximation ratio from $\\alpha$ to 1.   - We reduce the error dependence by a factor of $\\alpha^{-0.5}$.   From a technical perspective, our method of constructing the search tree differs from previous work [Backurs, Lin, Mahabadi, Silwal, and Tarnawski, ICLR 2024]. In prior work, for each query, the answer is split into $\\alpha^{-1} \\log n$ numbers, each derived from the summation of $\\log n$ values in interval tree countings. In contrast, we construct the tree differently, splitting the answer into $\\log n$ numbers, where each is a smart combination of two distance values, two counting values, and $y$ itself. We believe our tree structure may be of independent interest.",
            "id": "2409.01688",
            "link": "http://arxiv.org/abs/2409.01688v1",
            "published": "2024-09-03T08:01:19+00:00",
            "updated": "2024-09-03T08:01:19+00:00",
            "primary_category": "cs.DS",
            "categories": [
                "cs.DS",
                "cs.AI",
                "cs.LG",
                "stat.ML"
            ],
            "max_author_hindex": 9
        },
        "2409.01695": {
            "authors": [
                "Yihao Chen",
                "Haochen Wu",
                "Nan Jiang",
                "Xiang Xia",
                "Qing Gu",
                "Yunqi Hao",
                "Pengfei Cai",
                "Yu Guan",
                "Jialong Wang",
                "Weilin Xie",
                "Lei Fang",
                "Sian Fang",
                "Yan Song",
                "Wu Guo",
                "Lin Liu",
                "Minqiang Xu"
            ],
            "title": "USTC-KXDIGIT System Description for ASVspoof5 Challenge",
            "abstract": "This paper describes the USTC-KXDIGIT system submitted to the ASVspoof5 Challenge for Track 1 (speech deepfake detection) and Track 2 (spoofing-robust automatic speaker verification, SASV). Track 1 showcases a diverse range of technical qualities from potential processing algorithms and includes both open and closed conditions. For these conditions, our system consists of a cascade of a frontend feature extractor and a back-end classifier. We focus on extensive embedding engineering and enhancing the generalization of the back-end classifier model. Specifically, the embedding engineering is based on hand-crafted features and speech representations from a self-supervised model, used for closed and open conditions, respectively. To detect spoof attacks under various adversarial conditions, we trained multiple systems on an augmented training set. Additionally, we used voice conversion technology to synthesize fake audio from genuine audio in the training set to enrich the synthesis algorithms. To leverage the complementary information learned by different model architectures, we employed activation ensemble and fused scores from different systems to obtain the final decision score for spoof detection. During the evaluation phase, the proposed methods achieved 0.3948 minDCF and 14.33% EER in the close condition, and 0.0750 minDCF and 2.59% EER in the open condition, demonstrating the robustness of our submitted systems under adversarial conditions. In Track 2, we continued using the CM system from Track 1 and fused it with a CNN-based ASV system. This approach achieved 0.2814 min-aDCF in the closed condition and 0.0756 min-aDCF in the open condition, showcasing superior performance in the SASV system.",
            "id": "2409.01695",
            "link": "http://arxiv.org/abs/2409.01695v1",
            "published": "2024-09-03T08:28:58+00:00",
            "updated": "2024-09-03T08:28:58+00:00",
            "primary_category": "cs.SD",
            "categories": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ],
            "max_author_hindex": 39
        },
        "2409.01928": {
            "authors": [
                "Imanol Solano",
                "Alejandro Pe\u00f1a",
                "Aythami Morales",
                "Julian Fierrez",
                "Ruben Tolosana",
                "Francisco Zamora-Martinez",
                "Javier San Agustin"
            ],
            "title": "Comprehensive Equity Index (CEI): Definition and Application to Bias Evaluation in Biometrics",
            "abstract": "We present a novel metric designed, among other applications, to quantify biased behaviors of machine learning models. As its core, the metric consists of a new similarity metric between score distributions that balances both their general shapes and tails' probabilities. In that sense, our proposed metric may be useful in many application areas. Here we focus on and apply it to the operational evaluation of face recognition systems, with special attention to quantifying demographic biases; an application where our metric is especially useful. The topic of demographic bias and fairness in biometric recognition systems has gained major attention in recent years. The usage of these systems has spread in society, raising concerns about the extent to which these systems treat different population groups. A relevant step to prevent and mitigate demographic biases is first to detect and quantify them. Traditionally, two approaches have been studied to quantify differences between population groups in machine learning literature: 1) measuring differences in error rates, and 2) measuring differences in recognition score distributions. Our proposed Comprehensive Equity Index (CEI) trade-offs both approaches combining both errors from distribution tails and general distribution shapes. This new metric is well suited to real-world scenarios, as measured on NIST FRVT evaluations, involving high-performance systems and realistic face databases including a wide range of covariates and demographic groups. We first show the limitations of existing metrics to correctly assess the presence of biases in realistic setups and then propose our new metric to tackle these limitations. We tested the proposed metric with two state-of-the-art models and four widely used databases, showing its capacity to overcome the main flaws of previous bias metrics.",
            "id": "2409.01928",
            "link": "http://arxiv.org/abs/2409.01928v1",
            "published": "2024-09-03T14:19:38+00:00",
            "updated": "2024-09-03T14:19:38+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "max_author_hindex": 64
        },
        "2409.01931": {
            "authors": [
                "Yuanqing Wang",
                "Kenichiro Takaba",
                "Michael S. Chen",
                "Marcus Wieder",
                "Yuzhi Xu",
                "Tong Zhu",
                "John Z. H. Zhang",
                "Arnav Nagle",
                "Kuang Yu",
                "Xinyan Wang",
                "Daniel J. Cole",
                "Joshua A. Rackers",
                "Kyunghyun Cho",
                "Joe G. Greener",
                "Peter Eastman",
                "Stefano Martiniani",
                "Mark E. Tuckerman"
            ],
            "title": "On the design space between molecular mechanics and machine learning force fields",
            "abstract": "A force field as accurate as quantum mechanics (QM) and as fast as molecular mechanics (MM), with which one can simulate a biomolecular system efficiently enough and meaningfully enough to get quantitative insights, is among the most ardent dreams of biophysicists -- a dream, nevertheless, not to be fulfilled any time soon. Machine learning force fields (MLFFs) represent a meaningful endeavor towards this direction, where differentiable neural functions are parametrized to fit ab initio energies, and furthermore forces through automatic differentiation. We argue that, as of now, the utility of the MLFF models is no longer bottlenecked by accuracy but primarily by their speed (as well as stability and generalizability), as many recent variants, on limited chemical spaces, have long surpassed the chemical accuracy of $1$ kcal/mol -- the empirical threshold beyond which realistic chemical predictions are possible -- though still magnitudes slower than MM. Hoping to kindle explorations and designs of faster, albeit perhaps slightly less accurate MLFFs, in this review, we focus our attention on the design space (the speed-accuracy tradeoff) between MM and ML force fields. After a brief review of the building blocks of force fields of either kind, we discuss the desired properties and challenges now faced by the force field development community, survey the efforts to make MM force fields more accurate and ML force fields faster, envision what the next generation of MLFF might look like.",
            "id": "2409.01931",
            "link": "http://arxiv.org/abs/2409.01931v2",
            "published": "2024-09-03T14:21:46+00:00",
            "updated": "2024-09-05T13:10:22+00:00",
            "primary_category": "physics.chem-ph",
            "categories": [
                "physics.chem-ph",
                "cs.AI",
                "cs.LG",
                "physics.bio-ph",
                "physics.comp-ph"
            ],
            "max_author_hindex": 70
        },
        "2409.02123": {
            "authors": [
                "Shengchen Zhu",
                "Yiming Chen",
                "Peiying Yu",
                "Xiang Qu",
                "Yuxiao Zhou",
                "Yiming Ma",
                "Zhizhan Zhao",
                "Yukai Liu",
                "Hao Mi",
                "Bin Wang"
            ],
            "title": "PuYun: Medium-Range Global Weather Forecasting Using Large Kernel Attention Convolutional Networks",
            "abstract": "Accurate weather forecasting is essential for understanding and mitigating weather-related impacts. In this paper, we present PuYun, an autoregressive cascade model that leverages large kernel attention convolutional networks. The model's design inherently supports extended weather prediction horizons while broadening the effective receptive field. The integration of large kernel attention mechanisms within the convolutional layers enhances the model's capacity to capture fine-grained spatial details, thereby improving its predictive accuracy for meteorological phenomena.   We introduce PuYun, comprising PuYun-Short for 0-5 day forecasts and PuYun-Medium for 5-10 day predictions. This approach enhances the accuracy of 10-day weather forecasting. Through evaluation, we demonstrate that PuYun-Short alone surpasses the performance of both GraphCast and FuXi-Short in generating accurate 10-day forecasts. Specifically, on the 10th day, PuYun-Short reduces the RMSE for Z500 to 720 $m^2/s^2$, compared to 732 $m^2/s^2$ for GraphCast and 740 $m^2/s^2$ for FuXi-Short. Additionally, the RMSE for T2M is reduced to 2.60 K, compared to 2.63 K for GraphCast and 2.65 K for FuXi-Short. Furthermore, when employing a cascaded approach by integrating PuYun-Short and PuYun-Medium, our method achieves superior results compared to the combined performance of FuXi-Short and FuXi-Medium. On the 10th day, the RMSE for Z500 is further reduced to 638 $m^2/s^2$, compared to 641 $m^2/s^2$ for FuXi. These findings underscore the effectiveness of our model ensemble in advancing medium-range weather prediction. Our training code and model will be open-sourced.",
            "id": "2409.02123",
            "link": "http://arxiv.org/abs/2409.02123v2",
            "published": "2024-09-01T06:25:35+00:00",
            "updated": "2024-09-12T06:08:03+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI",
                "physics.ao-ph"
            ],
            "max_author_hindex": 46
        },
        "2409.02128": {
            "authors": [
                "Muhammad Sonny Abfertiawan",
                "Muchammad Daniyal Kautsar",
                "Faiz Hasan",
                "Yoseph Palinggi",
                "Kris Pranoto"
            ],
            "title": "The Application of Artificial Neural Network Model to Predicting the Acid Mine Drainage from Long-Term Lab Scale Kinetic Test",
            "abstract": "Acid mine drainage (AMD) is one of the common environmental problems in the coal mining industry that was formed by the oxidation of sulfide minerals in the overburden or waste rock. The prediction of acid generation through AMD is important to do in overburden management and planning the post-mining land use. One of the methods used to predict AMD is a lab-scale kinetic test to determine the rate of acid formation over time using representative samples in the field. However, this test requires a long-time procedure and large amount of chemical reagents lead to inefficient cost. On the other hand, there is potential for machine learning to learn the pattern behind the lab-scale kinetic test data. This study describes an approach to use artificial neural network (ANN) modeling to predict the result from lab-scale kinetic tests. Various ANN model is used based on 83 weeks experiments of lab-scale kinetic tests with 100\\% potential acid-forming rock. The model approaches the monitoring of pH, ORP, conductivity, TDS, sulfate, and heavy metals (Fe and Mn). The overall Nash-Sutcliffe Efficiency (NSE) obtained in this study was 0.99 on training and validation data, indicating a strong correlation and accurate prediction compared to the actual lab-scale kinetic tests data. This show the ANN ability to learn patterns, trends, and seasonality from past data for accurate forecasting, thereby highlighting its significant contribution to solving AMD problems. This research is also expected to establish the foundation for a new approach to predict AMD, with time efficient, accurate, and cost-effectiveness in future applications.",
            "id": "2409.02128",
            "link": "http://arxiv.org/abs/2409.02128v1",
            "published": "2024-09-01T16:39:37+00:00",
            "updated": "2024-09-01T16:39:37+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 7
        },
        "2409.02657": {
            "authors": [
                "Jun Ling",
                "Yiwen Wang",
                "Han Xue",
                "Rong Xie",
                "Li Song"
            ],
            "title": "PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation",
            "abstract": "While previous audio-driven talking head generation (THG) methods generate head poses from driving audio, the generated poses or lips cannot match the audio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a THG system that can freely generate lip-synchronized talking head videos with free head poses conditioned on text prompts and audio. The core insight of our method is using head pose to connect visual, linguistic, and audio signals. First, we propose to generate poses from both audio and text prompts, where the audio offers short-term variations and rhythm correspondence of the head movements and the text prompts describe the long-term semantics of head motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to generate motion latent from text prompts and audio cues in a pose latent space. Second, we observe a loss-imbalance problem: the loss for the lip region contributes less than 4\\% of the total reconstruction loss caused by both pose and lip, making optimization lean towards head movements rather than lip shapes. To address this issue, we propose a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce animated images in novel poses and the RefineNet focuses on learning finer lip motions by progressively estimating lip motions from low-to-high resolutions, yielding improved lip-synchronization performance. Experiments demonstrate our pose prediction strategy achieves better pose diversity and realness compared to text-only or audio-only, and our video generator model outperforms state-of-the-art methods in synthesizing talking videos with natural head motions. Project: https://junleen.github.io/projects/posetalk.",
            "id": "2409.02657",
            "link": "http://arxiv.org/abs/2409.02657v1",
            "published": "2024-09-04T12:30:25+00:00",
            "updated": "2024-09-04T12:30:25+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ],
            "max_author_hindex": 26
        },
        "2409.04398": {
            "authors": [
                "Yudi Dai",
                "Zhiyong Wang",
                "Xiping Lin",
                "Chenglu Wen",
                "Lan Xu",
                "Siqi Shen",
                "Yuexin Ma",
                "Cheng Wang"
            ],
            "title": "HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale Space Using Wearable IMUs and LiDAR",
            "abstract": "We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture method, aimed at accurately and efficiently creating a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, rich human-human interactions, and human-environment interactions. By utilizing body-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human motions in unconstrained space without the need for external devices and pre-built maps. This affords great flexibility and accessibility for human-centered interaction and 4D scene capturing in various environments. Taking into account that IMUs can capture human spatially unrestricted poses but are prone to drifting for long-period using, and while LiDAR is stable for global localization but rough for local positions and orientations, HiSC4D employs a joint optimization method, harmonizing all sensors and utilizing environment cues, yielding promising results for long-term capture in large scenes. To promote research of egocentric human interaction in large scenes and facilitate downstream tasks, we also present a dataset, containing 8 sequences in 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D human motions with SMPL annotations and dynamic scenes, 31k frames of cropped human point clouds, and scene mesh of the environment. A variety of scenarios, such as the basketball gym and commercial street, alongside challenging human motions, such as daily greeting, one-on-one basketball playing, and tour guiding, demonstrate the effectiveness and the generalization ability of HiSC4D. The dataset and code will be publicated on www.lidarhumanmotion.net/hisc4d available for research purposes.",
            "id": "2409.04398",
            "link": "http://arxiv.org/abs/2409.04398v2",
            "published": "2024-09-06T16:43:04+00:00",
            "updated": "2024-09-09T15:08:06+00:00",
            "primary_category": "cs.CV",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.GR",
                "cs.MM"
            ],
            "max_author_hindex": 25
        },
        "2409.04600": {
            "authors": [
                "Dmitry Scherbakov",
                "Nina Hubig",
                "Vinita Jansari",
                "Alexander Bakumenko",
                "Leslie A. Lenert"
            ],
            "title": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review",
            "abstract": "Objective: This study aims to summarize the usage of Large Language Models (LLMs) in the process of creating a scientific review. We look at the range of stages in a review that can be automated and assess the current state-of-the-art research projects in the field. Materials and Methods: The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar databases by human reviewers. Screening and extraction process took place in Covidence with the help of LLM add-on which uses OpenAI gpt-4o model. ChatGPT was used to clean extracted data and generate code for figures in this manuscript, ChatGPT and Scite.ai were used in drafting all components of the manuscript, except the methods and discussion sections. Results: 3,788 articles were retrieved, and 172 studies were deemed eligible for the final review. ChatGPT and GPT-based LLM emerged as the most dominant architecture for review automation (n=126, 73.2%). A significant number of review automation projects were found, but only a limited number of papers (n=26, 15.1%) were actual reviews that used LLM during their creation. Most citations focused on automation of a particular stage of review, such as Searching for publications (n=60, 34.9%), and Data extraction (n=54, 31.4%). When comparing pooled performance of GPT-based and BERT-based models, the former were better in data extraction with mean precision 83.0% (SD=10.4), and recall 86.0% (SD=9.8), while being slightly less accurate in title and abstract screening stage (Maccuracy=77.3%, SD=13.0). Discussion/Conclusion: Our LLM-assisted systematic review revealed a significant number of research projects related to review automation using LLMs. The results looked promising, and we anticipate that LLMs will change in the near future the way the scientific reviews are conducted.",
            "id": "2409.04600",
            "link": "http://arxiv.org/abs/2409.04600v1",
            "published": "2024-09-06T20:12:57+00:00",
            "updated": "2024-09-06T20:12:57+00:00",
            "primary_category": "cs.DL",
            "categories": [
                "cs.DL",
                "cs.AI"
            ],
            "max_author_hindex": 41
        },
        "2409.05211": {
            "authors": [
                "Guillermo Bern\u00e1rdez",
                "Lev Telyatnikov",
                "Marco Montagna",
                "Federica Baccini",
                "Mathilde Papillon",
                "Miquel Ferriol-Galm\u00e9s",
                "Mustafa Hajij",
                "Theodore Papamarkou",
                "Maria Sofia Bucarelli",
                "Olga Zaghen",
                "Johan Mathe",
                "Audun Myers",
                "Scott Mahan",
                "Hansen Lillemark",
                "Sharvaree Vadgama",
                "Erik Bekkers",
                "Tim Doster",
                "Tegan Emerson",
                "Henry Kvinge",
                "Katrina Agate",
                "Nesreen K Ahmed",
                "Pengfei Bai",
                "Michael Banf",
                "Claudio Battiloro",
                "Maxim Beketov",
                "Paul Bogdan",
                "Martin Carrasco",
                "Andrea Cavallo",
                "Yun Young Choi",
                "George Dasoulas",
                "Matou\u0161 Elphick",
                "Giordan Escalona",
                "Dominik Filipiak",
                "Halley Fritze",
                "Thomas Gebhart",
                "Manel Gil-Sorribes",
                "Salvish Goomanee",
                "Victor Guallar",
                "Liliya Imasheva",
                "Andrei Irimia",
                "Hongwei Jin",
                "Graham Johnson",
                "Nikos Kanakaris",
                "Boshko Koloski",
                "Veljko Kova\u010d",
                "Manuel Lecha",
                "Minho Lee",
                "Pierrick Leroy",
                "Theodore Long",
                "German Magai",
                "Alvaro Martinez",
                "Marissa Masden",
                "Sebastian Me\u017enar",
                "Bertran Miquel-Oliver",
                "Alexis Molina",
                "Alexander Nikitin",
                "Marco Nurisso",
                "Matt Piekenbrock",
                "Yu Qin",
                "Patryk Rygiel",
                "Alessandro Salatiello",
                "Max Schattauer",
                "Pavel Snopov",
                "Julian Suk",
                "Valentina S\u00e1nchez",
                "Mauricio Tec",
                "Francesco Vaccarino",
                "Jonas Verhellen",
                "Frederic Wantiez",
                "Alexander Weers",
                "Patrik Zajec",
                "Bla\u017e \u0160krlj",
                "Nina Miolane"
            ],
            "title": "ICML Topological Deep Learning Challenge 2024: Beyond the Graph Domain",
            "abstract": "This paper describes the 2nd edition of the ICML Topological Deep Learning Challenge that was hosted within the ICML 2024 ELLIS Workshop on Geometry-grounded Representation Learning and Generative Modeling (GRaM). The challenge focused on the problem of representing data in different discrete topological domains in order to bridge the gap between Topological Deep Learning (TDL) and other types of structured datasets (e.g. point clouds, graphs). Specifically, participants were asked to design and implement topological liftings, i.e. mappings between different data structures and topological domains --like hypergraphs, or simplicial/cell/combinatorial complexes. The challenge received 52 submissions satisfying all the requirements. This paper introduces the main scope of the challenge, and summarizes the main results and findings.",
            "id": "2409.05211",
            "link": "http://arxiv.org/abs/2409.05211v1",
            "published": "2024-09-08T19:59:53+00:00",
            "updated": "2024-09-08T19:59:53+00:00",
            "primary_category": "cs.LG",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "max_author_hindex": 62
        }
    },
    "authors": {
        "Laure Soulier": {
            "authorId": "145159652",
            "name": "L. Soulier",
            "hIndex": 14
        },
        "Li Shen": {
            "name": "Li Shen",
            "hIndex": 0
        },
        "Nedko Savov": {
            "authorId": "1557601824",
            "name": "N. Savov",
            "hIndex": 2
        },
        "Yanrong Hao": {
            "authorId": "5487715",
            "name": "Yanrong Hao",
            "hIndex": 14
        },
        "Yuhki Mitsufuji": {
            "authorId": "139180635",
            "name": "Yuhki Mitsufuji",
            "hIndex": 0
        },
        "Ekaterina Svikhnushina": {
            "authorId": "71187509",
            "name": "Ekaterina Svikhnushina",
            "hIndex": 7
        },
        "Sangji Lee": {
            "authorId": "2108105493",
            "name": "Sangji Lee",
            "hIndex": 4
        },
        "Avinash Madasu": {
            "authorId": "115098946",
            "name": "Avinash Madasu",
            "hIndex": 7
        },
        "Julia Stoyanovich": {
            "authorId": "1682824",
            "name": "Julia Stoyanovich",
            "hIndex": 29
        },
        "Manuel Gil P\u00e9rez": {
            "authorId": "33986887",
            "name": "M. P\u00e9rez",
            "hIndex": 18
        },
        "Joan C. Timoneda": {
            "authorId": "51287923",
            "name": "Joan C. Timoneda",
            "hIndex": 4
        },
        "Taejin Park": {
            "authorId": "50383223",
            "name": "Taejin Park",
            "hIndex": 7
        },
        "Rabih Younes": {
            "authorId": "20537874",
            "name": "Rabih Younes",
            "hIndex": 5
        },
        "Zhipeng Ge": {
            "authorId": "30651147",
            "name": "Zhipeng Ge",
            "hIndex": 7
        },
        "Qihui Wu": {
            "authorId": "145447232",
            "name": "Qi-hui Wu",
            "hIndex": 47
        },
        "Hsin-Wei Wang": {
            "authorId": "49337162",
            "name": "Wei-Hsin Wang",
            "hIndex": 12
        },
        "Chenghua Lin": {
            "authorId": "2146250356",
            "name": "Cheng-Wei Lin",
            "hIndex": 4
        },
        "Yuchang Cheng": {
            "authorId": "145215470",
            "name": "Yu Cheng",
            "hIndex": 52
        },
        "Luiz Eduardo Soares Oliveira": {
            "authorId": "144925520",
            "name": "Luiz Oliveira",
            "hIndex": 42
        },
        "Zhenfeng Ouyang": {
            "authorId": "2266838255",
            "name": "Zhenfeng Ouyang",
            "hIndex": 2
        },
        "Boshi Tang": {
            "authorId": "2257042466",
            "name": "Boshi Tang",
            "hIndex": 2
        },
        "Hakan T. Otal": {
            "authorId": "2279540083",
            "name": "Hakan T. Otal",
            "hIndex": 0
        },
        "Huseyin Inan": {
            "authorId": "3058104",
            "name": "Huseyin A. Inan",
            "hIndex": 15
        },
        "Gyeongcheol Shin": {
            "authorId": "2248271365",
            "name": "Gyeongcheol Shin",
            "hIndex": 1
        },
        "Lei Zhang": {
            "authorId": "2152827362",
            "name": "Lei Zhang",
            "hIndex": 30
        },
        "Yinbo Yu": {
            "authorId": "3417705",
            "name": "Yinbo Yu",
            "hIndex": 7
        },
        "Max W. Y. Lam": {
            "authorId": "49284339",
            "name": "Max W. Y. Lam",
            "hIndex": 17
        },
        "Izzeddin Gur": {
            "authorId": "3737312",
            "name": "Izzeddin Gur",
            "hIndex": 16
        },
        "Won-Yong Shin": {
            "authorId": "145107571",
            "name": "Y. Shin",
            "hIndex": 21
        },
        "Hanyu Zhao": {
            "authorId": "47941070",
            "name": "Hanyu Zhao",
            "hIndex": 7
        },
        "Xiaokang Yang": {
            "authorId": "50031361",
            "name": "Xiaokang Yang",
            "hIndex": 8
        },
        "Allard Oelen": {
            "authorId": "51308420",
            "name": "A. Oelen",
            "hIndex": 10
        },
        "Eleftheria Briakou": {
            "authorId": "40914545",
            "name": "Eleftheria Briakou",
            "hIndex": 8
        },
        "Alexander Lerch": {
            "authorId": "1882886",
            "name": "Alexander Lerch",
            "hIndex": 20
        },
        "Yuanlin Ye": {
            "authorId": "2114059620",
            "name": "Yuanlin Ye",
            "hIndex": 2
        },
        "Yongsu Ahn": {
            "authorId": "151402447",
            "name": "Yongsu Ahn",
            "hIndex": 3
        },
        "Graham Neubig": {
            "authorId": "1700325",
            "name": "Graham Neubig",
            "hIndex": 81
        },
        "Xingfeng Li": {
            "authorId": "39844886",
            "name": "Xingfeng Li",
            "hIndex": 14
        },
        "Andrew Lee": {
            "authorId": "145572455",
            "name": "Andrew H. S. Lee",
            "hIndex": 47
        },
        "Kaushiki Nag": {
            "authorId": "2190948143",
            "name": "Kaushiki Nag",
            "hIndex": 3
        },
        "Sriram Ganapathi Subramanian": {
            "authorId": "40963426",
            "name": "Sriram Ganapathi Subramanian",
            "hIndex": 10
        },
        "Junjie Zhang": {
            "authorId": "123275360",
            "name": "Junjie Zhang",
            "hIndex": 17
        },
        "Meisam Yadollahzadeh-Tabari": {
            "authorId": "9862079",
            "name": "M. Y. Tabari",
            "hIndex": 5
        },
        "Anna Fariha": {
            "authorId": "3071906",
            "name": "Anna Fariha",
            "hIndex": 10
        },
        "Junkai Liu": {
            "authorId": "2108475777",
            "name": "Junkai Liu",
            "hIndex": 28
        },
        "Kinga Szarkowska": {
            "authorId": "2141608197",
            "name": "Kinga Szarkowska",
            "hIndex": 0
        },
        "Francesca Toni": {
            "authorId": "2028960562",
            "name": "F. Toni",
            "hIndex": 3
        },
        "Quinn K Wolter": {
            "authorId": "1397267911",
            "name": "Quinn K. Wolter",
            "hIndex": 1
        },
        "Sara Pohland": {
            "authorId": "1629297026",
            "name": "Sara Pohland",
            "hIndex": 1
        },
        "Hangyu Liu": {
            "authorId": "2118959391",
            "name": "Hangyu Liu",
            "hIndex": 10
        },
        "Qiaochu Huang": {
            "authorId": "2204802757",
            "name": "Qiaochu Huang",
            "hIndex": 3
        },
        "Bojian Li": {
            "authorId": "48218911",
            "name": "B. Li",
            "hIndex": 50
        },
        "Ke Chen": {
            "authorId": "50053475",
            "name": "K. Chen",
            "hIndex": 17
        },
        "Jinzhen Wang": {
            "authorId": "2110118909",
            "name": "Jinzhen Wang",
            "hIndex": 5
        },
        "Pierre Erbacher": {
            "authorId": "2068332934",
            "name": "Pierre Erbacher",
            "hIndex": 4
        },
        "Berlin Chen": {
            "authorId": "2108381400",
            "name": "Berlin Chen",
            "hIndex": 26
        },
        "Abhay Kumar": {
            "authorId": "2143575692",
            "name": "Abhay Kumar",
            "hIndex": 16
        },
        "Jakob Foerster": {
            "name": "Jakob Foerster",
            "hIndex": 0
        },
        "Scott Lowe": {
            "authorId": "1979211",
            "name": "S. Lowe",
            "hIndex": 143
        },
        "Evin Jaff": {
            "authorId": "2316862590",
            "name": "Evin Jaff",
            "hIndex": 0
        },
        "Joseph Keshet": {
            "authorId": "1771345",
            "name": "Joseph Keshet",
            "hIndex": 29
        },
        "Maria Lymperaiou": {
            "authorId": "2184294391",
            "name": "Maria Lymperaiou",
            "hIndex": 3
        },
        "M. Jiang": {
            "authorId": "113079316",
            "name": "M. Jiang",
            "hIndex": 29
        },
        "Linzhan Mou": {
            "authorId": "2205658464",
            "name": "Linzhan Mou",
            "hIndex": 3
        },
        "Edward Choi": {
            "authorId": "3242613",
            "name": "E. Choi",
            "hIndex": 22
        },
        "Linlu Gong": {
            "name": "Linlu Gong",
            "hIndex": 0
        },
        "Qinglong Cao": {
            "authorId": "2146853189",
            "name": "Qinglong Cao",
            "hIndex": 5
        },
        "Jingxiong Liu": {
            "authorId": "2163058569",
            "name": "Jingxiong Liu",
            "hIndex": 9
        },
        "Pu Wang": {
            "authorId": "2108269036",
            "name": "Pu Wang",
            "hIndex": 33
        },
        "Vivi Nastase": {
            "authorId": "2256003",
            "name": "Vivi Nastase",
            "hIndex": 19
        },
        "Mahavir Dabas": {
            "authorId": "2243292345",
            "name": "Mahavir Dabas",
            "hIndex": 0
        },
        "Jingren Zhou": {
            "authorId": "1709595",
            "name": "Jingren Zhou",
            "hIndex": 54
        },
        "Chris Pestano": {
            "name": "Chris Pestano",
            "hIndex": 0
        },
        "Han Hu": {
            "authorId": "4998801",
            "name": "Han-Hwa Hu",
            "hIndex": 27
        },
        "Jingkuan Song": {
            "authorId": "2346105",
            "name": "Jingkuan Song",
            "hIndex": 50
        },
        "Jiun-Ting Li": {
            "authorId": "2136092780",
            "name": "Jiun-Ting Li",
            "hIndex": 1
        },
        "Gabriel Skantze": {
            "authorId": "1711959",
            "name": "Gabriel Skantze",
            "hIndex": 28
        },
        "Nassir Navab": {
            "authorId": "145587210",
            "name": "Nassir Navab",
            "hIndex": 89
        },
        "Albert K. Engstfeld": {
            "authorId": "14210266",
            "name": "A. Engstfeld",
            "hIndex": 14
        },
        "Ross Greer": {
            "authorId": "2024729449",
            "name": "Ross Greer",
            "hIndex": 8
        },
        "Zihan Pan": {
            "authorId": "46225910",
            "name": "Zihan Pan",
            "hIndex": 8
        },
        "Pascal Van Hentenryck": {
            "name": "Pascal Van Hentenryck",
            "hIndex": 0
        },
        "Pingyi Fan": {
            "authorId": "7469995",
            "name": "Pingyi Fan",
            "hIndex": 42
        },
        "Andreas Kipf": {
            "authorId": "1922104",
            "name": "Andreas Kipf",
            "hIndex": 18
        },
        "Farzin Soleymani": {
            "authorId": "2126960554",
            "name": "Farzin Soleymani",
            "hIndex": 3
        },
        "Yuxiang Xu": {
            "name": "Yuxiang Xu",
            "hIndex": 0
        },
        "Mariana Yukari Noguti": {
            "authorId": "1661098153",
            "name": "Mariana Yukari Noguti",
            "hIndex": 0
        },
        "Bernhard Spitzer": {
            "authorId": "2015048",
            "name": "B. Spitzer",
            "hIndex": 24
        },
        "Nicola Bellotto": {
            "authorId": "1709606",
            "name": "N. Bellotto",
            "hIndex": 24
        },
        "Hongyang R. Zhang": {
            "authorId": "40975176",
            "name": "Hongyang Zhang",
            "hIndex": 27
        },
        "Boyuan Zhang": {
            "authorId": "2208118780",
            "name": "Boyuan Zhang",
            "hIndex": 5
        },
        "Kathleen McKeown": {
            "authorId": "145590324",
            "name": "K. McKeown",
            "hIndex": 72
        },
        "Jin Xiongnan": {
            "authorId": "2096526",
            "name": "Xiongnan Jin",
            "hIndex": 10
        },
        "Mohammad Ghafari": {
            "authorId": "90414249",
            "name": "M. Ghafari",
            "hIndex": 23
        },
        "Kazi Injamamul Haque": {
            "authorId": "66382903",
            "name": "Kazi Injamamul Haque",
            "hIndex": 2
        },
        "Georgios Chochlakis": {
            "authorId": "2282533722",
            "name": "Georgios Chochlakis",
            "hIndex": 3
        },
        "Haohua Wang": {
            "authorId": "49528395",
            "name": "Hao-hua Wang",
            "hIndex": 13
        },
        "Qiong Wang": {
            "name": "Qiong Wang",
            "hIndex": 0
        },
        "Haoyu Zhang": {
            "authorId": "2135731500",
            "name": "Haoyu Zhang",
            "hIndex": 5
        },
        "Masayuki Inaba": {
            "authorId": "1749935",
            "name": "M. Inaba",
            "hIndex": 52
        },
        "Daniel Merlo": {
            "authorId": "5426052",
            "name": "D. Merlo",
            "hIndex": 32
        },
        "Ningyuan Xi": {
            "authorId": "2214409986",
            "name": "Ningyuan Xi",
            "hIndex": 2
        },
        "Luc Van Gool": {
            "authorId": "1681236",
            "name": "L. Gool",
            "hIndex": 171
        },
        "Radek Osmulski": {
            "authorId": "2312327686",
            "name": "Radek Osmulski",
            "hIndex": 1
        },
        "Rakif Khan": {
            "name": "Rakif Khan",
            "hIndex": 0
        },
        "Artur Dubrawski": {
            "authorId": "144292541",
            "name": "A. Dubrawski",
            "hIndex": 27
        },
        "Ting Jin": {
            "authorId": "49539679",
            "name": "T. Jin",
            "hIndex": 7
        },
        "William Brannon": {
            "authorId": "50330317",
            "name": "W. Brannon",
            "hIndex": 8
        },
        "Yujian Gan": {
            "authorId": "120624606",
            "name": "Yujian Gan",
            "hIndex": 5
        },
        "Jiancheng Yang": {
            "authorId": "2109720872",
            "name": "Jiancheng Yang",
            "hIndex": 23
        },
        "Funminiyi Olajide": {
            "authorId": "144913746",
            "name": "F. Olajide",
            "hIndex": 8
        },
        "Xiaotong Zhang": {
            "authorId": "2108015472",
            "name": "Xiaotong Zhang",
            "hIndex": 23
        },
        "Haodong Zheng": {
            "authorId": "1706574",
            "name": "Hao Li",
            "hIndex": 74
        },
        "S\u00fcmeyye \u00d6zt\u00fcrk": {
            "authorId": "2143082197",
            "name": "S. \u00d6zt\u00fcrk",
            "hIndex": 0
        },
        "Lin Liu": {
            "authorId": "46458066",
            "name": "Lin-lin Liu",
            "hIndex": 10
        },
        "Chi-Han Lin": {
            "authorId": "2424609",
            "name": "Chi-Han Lin",
            "hIndex": 9
        },
        "Cheston Tan": {
            "authorId": "1694051",
            "name": "Cheston Tan",
            "hIndex": 17
        },
        "Qiang Li": {
            "authorId": "103628338",
            "name": "Qiang Li",
            "hIndex": 51
        },
        "Sourav Batthacharya": {
            "name": "Sourav Batthacharya",
            "hIndex": 0
        },
        "Lantao Hu": {
            "authorId": "2191842",
            "name": "Lantao Hu",
            "hIndex": 4
        },
        "Rogier van Dalen": {
            "authorId": "1757921",
            "name": "Rogier van Dalen",
            "hIndex": 12
        },
        "Bobby Cheng": {
            "authorId": "32857836",
            "name": "B. Cheng",
            "hIndex": 6
        },
        "Jing Xiao": {
            "authorId": "49874498",
            "name": "Jing-jing Xiao",
            "hIndex": 17
        },
        "Luping Zhou": {
            "authorId": "6578587",
            "name": "Luping Zhou",
            "hIndex": 31
        },
        "Xin Wen": {
            "authorId": "14912053",
            "name": "Wenjie Xin",
            "hIndex": 20
        },
        "Toby Jia-Jun Li": {
            "authorId": "34997918",
            "name": "Toby Jia-Jun Li",
            "hIndex": 22
        },
        "Mitsumasa Nakajima": {
            "authorId": "74026917",
            "name": "M. Nakajima",
            "hIndex": 9
        },
        "Yong Li": {
            "authorId": "34117934",
            "name": "Yong Li",
            "hIndex": 44
        },
        "Zhewei Yao": {
            "authorId": "9088433",
            "name": "Z. Yao",
            "hIndex": 31
        },
        "Bing Ma": {
            "authorId": "93333568",
            "name": "B. Ma",
            "hIndex": 11
        },
        "Tetsu Tanabe": {
            "authorId": "73340970",
            "name": "T. Tanabe",
            "hIndex": 36
        },
        "Luk\u00e1\u0161 S\u00fdkora": {
            "authorId": "95599163",
            "name": "Lukas Sykora",
            "hIndex": 1
        },
        "Teng Chen": {
            "authorId": "11573249",
            "name": "Teng-Teng Chen",
            "hIndex": 17
        },
        "Yang Li": {
            "authorId": "2283488844",
            "name": "Y. Li",
            "hIndex": 26
        },
        "Luou Wen": {
            "authorId": "2152129779",
            "name": "Luou Wen",
            "hIndex": 2
        },
        "Anna Kruspe": {
            "authorId": "2728234",
            "name": "Anna Kruspe",
            "hIndex": 11
        },
        "Yifeng Cao": {
            "authorId": "2112867178",
            "name": "Yifeng Cao",
            "hIndex": 8
        },
        "Tongliang Li": {
            "authorId": "47268605",
            "name": "Tongliang Li",
            "hIndex": 4
        },
        "Denis Tarasov": {
            "authorId": "2064312685",
            "name": "Denis Tarasov",
            "hIndex": 5
        },
        "Hiroshi Ishikawa": {
            "authorId": "145322050",
            "name": "H. Ishikawa",
            "hIndex": 14
        },
        "Yingxue Zhang": {
            "authorId": "2885420",
            "name": "Yingxue Zhang",
            "hIndex": 18
        },
        "Mengxuan Zhang": {
            "authorId": "47474217",
            "name": "M. Zhang",
            "hIndex": 13
        },
        "Yuxiao Chen": {
            "authorId": "2115648395",
            "name": "Yuxiao Chen",
            "hIndex": 3
        },
        "Mengyu Wang": {
            "authorId": "2146059602",
            "name": "Meng Wang",
            "hIndex": 7
        },
        "Louis Van Langendonck": {
            "authorId": "2307467422",
            "name": "Louis Van Langendonck",
            "hIndex": 0
        },
        "Ruiqi Gao": {
            "authorId": "14277449",
            "name": "Rui-Ting Gao",
            "hIndex": 20
        },
        "Xiao Zhang": {
            "authorId": "2358097",
            "name": "Xiao Zhang",
            "hIndex": 34
        },
        "Laura Schade": {
            "authorId": "2073422255",
            "name": "L. Schade",
            "hIndex": 4
        },
        "Jianqiang Li": {
            "authorId": "47786516",
            "name": "Jian-qiang Li",
            "hIndex": 10
        },
        "Boris Ginsburg": {
            "authorId": "31963005",
            "name": "Boris Ginsburg",
            "hIndex": 27
        },
        "Zhewen Hu": {
            "authorId": "13643480",
            "name": "Zhewen Hu",
            "hIndex": 6
        },
        "Fubang Zhao": {
            "authorId": "1387821612",
            "name": "Fubang Zhao",
            "hIndex": 5
        },
        "Lijia Lv": {
            "authorId": "1399582528",
            "name": "L. Lv",
            "hIndex": 3
        },
        "Joeran Beel": {
            "authorId": "2065517864",
            "name": "Joeran Beel",
            "hIndex": 9
        },
        "Liang Ding": {
            "authorId": "145394341",
            "name": "L. Ding",
            "hIndex": 30
        },
        "Shiming Ge": {
            "name": "Shiming Ge",
            "hIndex": 0
        },
        "Srijan Sood": {
            "authorId": "46753156",
            "name": "Srijan Sood",
            "hIndex": 4
        },
        "Tiehua Zhang": {
            "authorId": "4329026",
            "name": "Tiehua Zhang",
            "hIndex": 29
        },
        "Liesbet De Vos": {
            "authorId": "2213295036",
            "name": "Liesbet De Vos",
            "hIndex": 1
        },
        "Zi Yang": {
            "authorId": "47087380",
            "name": "Ziyue Yang",
            "hIndex": 13
        },
        "Fei Ye": {
            "authorId": "46594690",
            "name": "F. Ye",
            "hIndex": 30
        },
        "Ziming Zhao": {
            "authorId": "50144607",
            "name": "Ziming Zhao",
            "hIndex": 14
        },
        "Lukas Wegmeth": {
            "authorId": "147383557",
            "name": "Lukas Wegmeth",
            "hIndex": 2
        },
        "Jonathan Rutherford": {
            "authorId": "31348842",
            "name": "Jonathan Rutherford",
            "hIndex": 26
        },
        "Shaswata Mitra": {
            "authorId": "2150699855",
            "name": "Shaswata Mitra",
            "hIndex": 4
        },
        "Ruijie Zhu": {
            "authorId": "7589887",
            "name": "Ruijie Zhu",
            "hIndex": 17
        },
        "Ozgur S. Oguz": {
            "authorId": "32408015",
            "name": "Ozgur S. Oguz",
            "hIndex": 11
        },
        "Arthur Bucker": {
            "authorId": "2008679488",
            "name": "A. Bucker",
            "hIndex": 7
        },
        "Daehee Kim": {
            "authorId": "153586563",
            "name": "Dae-Hee Kim",
            "hIndex": 11
        },
        "Fran\u00e7ois Yvon": {
            "authorId": "1846431",
            "name": "Fran\u00e7ois Yvon",
            "hIndex": 34
        },
        "Sanjiban Choudhury": {
            "authorId": "2487768",
            "name": "Sanjiban Choudhury",
            "hIndex": 22
        },
        "Hans Steege": {
            "authorId": "87996698",
            "name": "H. Steege",
            "hIndex": 40
        },
        "Xiaotong Wei": {
            "authorId": "2219783664",
            "name": "Xiaotong Wei",
            "hIndex": 7
        },
        "Gjergji Kasneci": {
            "authorId": "1686448",
            "name": "Gjergji Kasneci",
            "hIndex": 34
        },
        "Narutatsu Ri": {
            "authorId": "2218040104",
            "name": "Narutatsu Ri",
            "hIndex": 3
        },
        "Wuqiang Zhang": {
            "authorId": "2108318161",
            "name": "Wuqiang Zhang",
            "hIndex": 3
        },
        "Solee Im": {
            "name": "Solee Im",
            "hIndex": 0
        },
        "Ziren Luo": {
            "authorId": "14736224",
            "name": "Ziren Luo",
            "hIndex": 12
        },
        "Atrin Arya": {
            "authorId": "1562116690",
            "name": "Atrin Arya",
            "hIndex": 2
        },
        "Xinshuo Weng": {
            "authorId": "7885067",
            "name": "Xinshuo Weng",
            "hIndex": 20
        },
        "Rami Bahsoon": {
            "authorId": "1804288",
            "name": "Rami Bahsoon",
            "hIndex": 33
        },
        "Lars-Peter Meyer": {
            "authorId": "34886987",
            "name": "Lars-Peter Meyer",
            "hIndex": 5
        },
        "Denys Herasymuk": {
            "authorId": "2204971965",
            "name": "Denys Herasymuk",
            "hIndex": 1
        },
        "Tian Xia": {
            "authorId": "152465261",
            "name": "Tian-dong Xia",
            "hIndex": 25
        },
        "Sophia Katrenko": {
            "authorId": "3153135",
            "name": "S. Katrenko",
            "hIndex": 11
        },
        "Shaoting Zhu": {
            "authorId": "2109907227",
            "name": "S. Zhu",
            "hIndex": 2
        },
        "Sichun Wu": {
            "name": "Sichun Wu",
            "hIndex": 0
        },
        "Rui Ge": {
            "authorId": "83810394",
            "name": "R. Ge",
            "hIndex": 8
        },
        "Raymond H. Cuijpers": {
            "authorId": "143673413",
            "name": "R. Cuijpers",
            "hIndex": 27
        },
        "\u00d6vg\u00fc \u00d6zdemir": {
            "authorId": "2148648882",
            "name": "\u00d6vg\u00fc \u00d6zdemir",
            "hIndex": 1
        },
        "Wenhao Huang": {
            "authorId": "47504473",
            "name": "Wenhao Huang",
            "hIndex": 12
        },
        "Junhyeok Lee": {
            "authorId": "40795415",
            "name": "Junhyeok Lee",
            "hIndex": 6
        },
        "Xiaolan Chen": {
            "authorId": "2109180254",
            "name": "Xiaolan Chen",
            "hIndex": 12
        },
        "Ningmu Zou": {
            "name": "Ningmu Zou",
            "hIndex": 0
        },
        "Sanne Schoenmakers": {
            "authorId": "2353449",
            "name": "S. Schoenmakers",
            "hIndex": 5
        },
        "Fei Richard Yu": {
            "authorId": "1696615",
            "name": "F. Yu",
            "hIndex": 72
        },
        "Kohei Nakajima": {
            "authorId": "1689790",
            "name": "K. Nakajima",
            "hIndex": 27
        },
        "Olivier Ferret": {
            "authorId": "1679133",
            "name": "Olivier Ferret",
            "hIndex": 22
        },
        "Angana Borah": {
            "authorId": "89689961",
            "name": "A. Borah",
            "hIndex": 11
        },
        "Raimund Kammering": {
            "authorId": "31223041",
            "name": "R. Kammering",
            "hIndex": 12
        },
        "Heiga Zen": {
            "authorId": "1691713",
            "name": "H. Zen",
            "hIndex": 49
        },
        "Pavan Seshadri": {
            "authorId": "2122447589",
            "name": "Pavan Seshadri",
            "hIndex": 2
        },
        "Daniel Zhang-Li": {
            "authorId": "2165225735",
            "name": "Daniel Zhang-li",
            "hIndex": 4
        },
        "Xin Zhao": {
            "authorId": "80529150",
            "name": "Xin Zhao",
            "hIndex": 36
        },
        "Dane Morgan": {
            "authorId": "79150910",
            "name": "D. Morgan",
            "hIndex": 64
        },
        "Georgios Th. Papadopoulos": {
            "authorId": "33961149",
            "name": "G. Papadopoulos",
            "hIndex": 17
        },
        "Rohit Jena": {
            "authorId": "108199753",
            "name": "Rohit Jena",
            "hIndex": 5
        },
        "Zhicheng Dou": {
            "authorId": "1897235",
            "name": "Zhicheng Dou",
            "hIndex": 34
        },
        "Haoan Feng": {
            "authorId": "1852514651",
            "name": "Haoan Feng",
            "hIndex": 2
        },
        "Jun Wang": {
            "authorId": "38504611",
            "name": "Jun Wang",
            "hIndex": 12
        },
        "Lars Kai Hansen": {
            "authorId": "49254613",
            "name": "L. Hansen",
            "hIndex": 30
        },
        "Morteza Mousa Pasandi": {
            "authorId": "1482545827",
            "name": "Morteza Mousa Pasandi",
            "hIndex": 2
        },
        "Quanliang Liu": {
            "authorId": "5630451",
            "name": "Quanliang Liu",
            "hIndex": 6
        },
        "Jihye Moon": {
            "authorId": "2087254929",
            "name": "Jihye Moon",
            "hIndex": 5
        },
        "Amirreza Fateh": {
            "authorId": "98815956",
            "name": "A. Fateh",
            "hIndex": 9
        },
        "Kyle Richardson": {
            "authorId": "46666605",
            "name": "Kyle Richardson",
            "hIndex": 21
        },
        "Dylan Hillier": {
            "authorId": "2302793720",
            "name": "Dylan Hillier",
            "hIndex": 1
        },
        "David Alvarez-Melis": {
            "authorId": "1390096054",
            "name": "David Alvarez-Melis",
            "hIndex": 18
        },
        "Gabriel de Souza P. Moreira": {
            "authorId": "35075105",
            "name": "F. Silva",
            "hIndex": 17
        },
        "Lukas Muttenthaler": {
            "authorId": "72543861",
            "name": "Lukas Muttenthaler",
            "hIndex": 8
        },
        "Satoshi Yamane": {
            "authorId": "34989052",
            "name": "S. Yamane",
            "hIndex": 11
        },
        "Qin Dai": {
            "authorId": "7944764",
            "name": "Huanqin Dai",
            "hIndex": 38
        },
        "Tinne Tuytelaars": {
            "authorId": "1704728",
            "name": "T. Tuytelaars",
            "hIndex": 71
        },
        "Qingyang Mao": {
            "authorId": "2142460878",
            "name": "Qingyan Mao",
            "hIndex": 2
        },
        "Zitao Chen": {
            "authorId": "2117097606",
            "name": "Zitao Chen",
            "hIndex": 17
        },
        "Yasuo Kuniyoshi": {
            "authorId": "1744602",
            "name": "Y. Kuniyoshi",
            "hIndex": 48
        },
        "Yi Xu": {
            "authorId": "10032844",
            "name": "Yi-chao Xu",
            "hIndex": 6
        },
        "Zhehui Huang": {
            "authorId": "1753501054",
            "name": "Zhehui Huang",
            "hIndex": 3
        },
        "P\u0131nar Karag\u00f6z": {
            "authorId": "21271012",
            "name": "P\u0131nar Karag\u00f6z",
            "hIndex": 5
        },
        "Hideki Asoh": {
            "authorId": "7142317",
            "name": "H. Asoh",
            "hIndex": 27
        },
        "Babak Damavandi": {
            "authorId": "3057557",
            "name": "Babak Damavandi",
            "hIndex": 6
        },
        "Yunzhi Yao": {
            "authorId": "4841460",
            "name": "Yunzhi Yao",
            "hIndex": 14
        },
        "Patrick Gerard": {
            "authorId": "2839752",
            "name": "P. Gerard",
            "hIndex": 42
        },
        "Makbule Gulcin Ozsoy": {
            "authorId": "40918064",
            "name": "Makbule G\u00fcl\u00e7in \u00d6zsoy",
            "hIndex": 10
        },
        "Stefan Harmeling": {
            "authorId": "1734990",
            "name": "S. Harmeling",
            "hIndex": 35
        },
        "William English": {
            "authorId": "7467153",
            "name": "William B. English",
            "hIndex": 18
        },
        "Ziquan Fu": {
            "authorId": "2165331745",
            "name": "Ziquan Fu",
            "hIndex": 4
        },
        "Bram Willemsen": {
            "authorId": "34785846",
            "name": "Bram Willemsen",
            "hIndex": 7
        },
        "Yan Xia": {
            "authorId": "2111130206",
            "name": "Yan Xia",
            "hIndex": 10
        },
        "Ziwei Yan": {
            "authorId": "81864840",
            "name": "Zi-ming Yan",
            "hIndex": 4
        },
        "Christine Carter": {
            "authorId": "48392055",
            "name": "C. Carter",
            "hIndex": 12
        },
        "Carmelo Sferrazza": {
            "authorId": "47218071",
            "name": "Carmelo Sferrazza",
            "hIndex": 11
        },
        "Pascal Poupart": {
            "authorId": "1807041",
            "name": "P. Poupart",
            "hIndex": 45
        },
        "Cecilia G. Morales": {
            "authorId": "2068084530",
            "name": "E. S\u00e1nchez",
            "hIndex": 8
        },
        "Wataru Kumagai": {
            "authorId": "1896666",
            "name": "Wataru Kumagai",
            "hIndex": 11
        },
        "Alan F. Smeaton": {
            "authorId": "1680223",
            "name": "A. Smeaton",
            "hIndex": 65
        },
        "Fuli Feng": {
            "authorId": "143627571",
            "name": "F. Huang",
            "hIndex": 52
        },
        "Freek Stulp": {
            "authorId": "50707365",
            "name": "F. Stulp",
            "hIndex": 32
        },
        "Tianming Liu": {
            "authorId": "145251056",
            "name": "T. Liu",
            "hIndex": 23
        },
        "Yiwei Li": {
            "authorId": "2154569289",
            "name": "Yi-wei Li",
            "hIndex": 11
        },
        "Donghuo Zeng": {
            "authorId": "8778936",
            "name": "Donghuo Zeng",
            "hIndex": 6
        },
        "Aviv Navon": {
            "authorId": "90227282",
            "name": "Aviv Navon",
            "hIndex": 8
        },
        "Elizabeth Wilson": {
            "authorId": "145653403",
            "name": "Elizabeth B. Wilson",
            "hIndex": 10
        },
        "Sandeep Kumar": {
            "authorId": "1670938478",
            "name": "Sandeep Kumar",
            "hIndex": 42
        },
        "Zuil Pirola": {
            "authorId": "2220761207",
            "name": "Zuil Pirola",
            "hIndex": 0
        },
        "Wenlin Yao": {
            "authorId": "2087264100",
            "name": "Wenlin Yao",
            "hIndex": 10
        },
        "Md Abid Jahan": {
            "name": "Md Abid Jahan",
            "hIndex": 0
        },
        "Niyantha Maruthu Pandiyan": {
            "authorId": "2213496517",
            "name": "Niyantha Maruthu Pandiyan",
            "hIndex": 0
        },
        "Lalitesh Morishetti": {
            "authorId": "117576986",
            "name": "Lalitesh Morishetti",
            "hIndex": 2
        },
        "Mahdi Biparva": {
            "authorId": "24057066",
            "name": "Mahdi Biparva",
            "hIndex": 5
        },
        "Shannon Song": {
            "authorId": "2107567135",
            "name": "S. Song",
            "hIndex": 1
        },
        "Hong Xingyun Hong": {
            "authorId": "1452354498",
            "name": "Xingyun Hong",
            "hIndex": 1
        },
        "Natanael Arndt": {
            "authorId": "2258592",
            "name": "Natanael Arndt",
            "hIndex": 8
        },
        "Jo\u00e3o Silv\u00e9rio": {
            "authorId": "145775871",
            "name": "Jo\u00e3o Silv\u00e9rio",
            "hIndex": 14
        },
        "Mohammad Maftoun": {
            "authorId": "2304559780",
            "name": "Mohammad Maftoun",
            "hIndex": 1
        },
        "Yongsheng Tong": {
            "authorId": "31801830",
            "name": "T. Han",
            "hIndex": 13
        },
        "Wenhao Yu": {
            "authorId": "38767143",
            "name": "W. Yu",
            "hIndex": 19
        },
        "Rongwei Yang": {
            "authorId": "3370845",
            "name": "Rongwei Yang",
            "hIndex": 16
        },
        "Tatsunori Hashimoto": {
            "authorId": "3056528",
            "name": "Tatsunori B. Hashimoto",
            "hIndex": 26
        },
        "Mulin Chen": {
            "authorId": "3381638",
            "name": "Mulin Chen",
            "hIndex": 15
        },
        "Wei Wei": {
            "authorId": "145509487",
            "name": "Wei Wei",
            "hIndex": 39
        },
        "Thomas Trappenberg": {
            "authorId": "1741671",
            "name": "T. Trappenberg",
            "hIndex": 25
        },
        "Shridhar Kumar": {
            "authorId": "50812160",
            "name": "K. Shridhar",
            "hIndex": 15
        },
        "Ilias Siniosoglou": {
            "authorId": "1387155143",
            "name": "Ilias Siniosoglou",
            "hIndex": 7
        },
        "Aditi Krishana": {
            "name": "Aditi Krishana",
            "hIndex": 0
        },
        "Nasredine Semmar": {
            "authorId": "1817556",
            "name": "N. Semmar",
            "hIndex": 13
        },
        "Siqi Chen": {
            "authorId": "2027384677",
            "name": "Si-Yu Chen",
            "hIndex": 6
        },
        "Alvaro Alonso": {
            "authorId": "2072861538",
            "name": "\u00c1lvaro Alonso",
            "hIndex": 10
        },
        "Chang Cai": {
            "authorId": "49829599",
            "name": "C. Cai",
            "hIndex": 14
        },
        "Chang Sun": {
            "authorId": "1769625",
            "name": "S. Chang",
            "hIndex": 17
        },
        "Kang Zhu": {
            "authorId": "145898531",
            "name": "Dekang Zhu",
            "hIndex": 32
        },
        "Mohsen Azarmi": {
            "authorId": "1908498313",
            "name": "Mohsen Azarmi",
            "hIndex": 3
        },
        "Changyue Hu": {
            "authorId": "102492039",
            "name": "Chang-Ru Hu",
            "hIndex": 5
        },
        "Ali Arabian": {
            "authorId": "83369794",
            "name": "Alipour Farshid",
            "hIndex": 4
        },
        "Nicola Bertelli": {
            "authorId": "31029794",
            "name": "N. Bertelli",
            "hIndex": 16
        },
        "Fenno F. Heath III": {
            "name": "Fenno F. Heath III",
            "hIndex": 0
        },
        "Bowen Zheng": {
            "authorId": "1400222554",
            "name": "Bowen Zheng",
            "hIndex": 17
        },
        "Zachary Liu": {
            "authorId": "49292882",
            "name": "Zongzhi Z. Liu",
            "hIndex": 29
        },
        "Laura Hollink": {
            "authorId": "1749071",
            "name": "L. Hollink",
            "hIndex": 26
        },
        "Jan Elfes": {
            "authorId": "2167942993",
            "name": "Jan Elfes",
            "hIndex": 1
        },
        "Hongyang Li": {
            "authorId": "2117929187",
            "name": "Hongyan Li",
            "hIndex": 8
        },
        "Matthew Purver": {
            "authorId": "1701461",
            "name": "Matthew Purver",
            "hIndex": 33
        },
        "Anh-Triet Do": {
            "name": "Anh-Triet Do",
            "hIndex": 0
        },
        "Xiaoyu Du": {
            "authorId": "46565773",
            "name": "Du Xiaoyu",
            "hIndex": 4
        },
        "Firoj Alam": {
            "authorId": "37784060",
            "name": "Firoj Alam",
            "hIndex": 33
        },
        "Sherry Thomas": {
            "name": "Sherry Thomas",
            "hIndex": 0
        },
        "Run Luo": {
            "authorId": "50052183",
            "name": "H. Luo",
            "hIndex": 34
        },
        "Changlong Sun": {
            "authorId": "2118133333",
            "name": "Chang-Woo Sun",
            "hIndex": 3
        },
        "He Huang": {
            "authorId": "49696865",
            "name": "H. Huang",
            "hIndex": 24
        },
        "Thorsten Hellert": {
            "authorId": "73134625",
            "name": "T. Hellert",
            "hIndex": 6
        },
        "Christina Walker": {
            "authorId": "32593174",
            "name": "Christina J Walker",
            "hIndex": 6
        },
        "Daniel Jones": {
            "authorId": "38709990",
            "name": "Daniel T. Jones",
            "hIndex": 17
        },
        "Min Chen": {
            "authorId": "50133589",
            "name": "Min Chen",
            "hIndex": 46
        },
        "Sakshi Deo Shukla": {
            "name": "Sakshi Deo Shukla",
            "hIndex": 0
        },
        "Michael Kampouridis": {
            "authorId": "2615683",
            "name": "Michael Kampouridis",
            "hIndex": 14
        },
        "Wei-Hsiang Liao": {
            "authorId": "40186991",
            "name": "Wei-Hsiang Liao",
            "hIndex": 8
        },
        "Timo Breuer": {
            "name": "Timo Breuer",
            "hIndex": 0
        },
        "Zhong-Yi Lu": {
            "authorId": "9651810",
            "name": "Yizhong Lu",
            "hIndex": 44
        },
        "Julian R\u00fcth": {
            "authorId": "104195410",
            "name": "J. R\u00fcth",
            "hIndex": 1
        },
        "Yifeng Wang": {
            "authorId": "2115568471",
            "name": "Yifeng Wang",
            "hIndex": 21
        },
        "Daniel Sadoc Menasch\u00e9": {
            "authorId": "1735876",
            "name": "D. Menasch\u00e9",
            "hIndex": 21
        },
        "Mahdi Rezaei": {
            "authorId": "145254542",
            "name": "M. Rezaei",
            "hIndex": 10
        },
        "Vinayak Goyal": {
            "authorId": "2143528895",
            "name": "Vinayak Goyal",
            "hIndex": 1
        },
        "Melissa Gresle": {
            "authorId": "5030572",
            "name": "M. Gresle",
            "hIndex": 21
        },
        "Tadachika Ozono": {
            "authorId": "2843264",
            "name": "Tadachika Ozono",
            "hIndex": 9
        },
        "David D. McManus": {
            "authorId": "4577010",
            "name": "D. McManus",
            "hIndex": 65
        },
        "Matthieu Dubois": {
            "authorId": "50185565",
            "name": "Matthieu Dubois",
            "hIndex": 5
        },
        "Joshua B. Tenenbaum": {
            "authorId": "1763295",
            "name": "J. Tenenbaum",
            "hIndex": 126
        },
        "Richard Altendorfer": {
            "authorId": "2162953",
            "name": "R. Altendorfer",
            "hIndex": 12
        },
        "Yossi Gandelsman": {
            "authorId": "52164591",
            "name": "Yossi Gandelsman",
            "hIndex": 10
        },
        "Bilel Omrani": {
            "authorId": "1486608754",
            "name": "Bilel Omrani",
            "hIndex": 3
        },
        "Stefan Stojanov": {
            "authorId": "94958383",
            "name": "Stefan Stojanov",
            "hIndex": 9
        },
        "Yingshu Li": {
            "authorId": "2155508655",
            "name": "Yingshu Li",
            "hIndex": 3
        },
        "Anneke Van Der Walt": {
            "authorId": "3729226",
            "name": "A. van der Walt",
            "hIndex": 24
        },
        "Xingwei Qu": {
            "authorId": "1720750202",
            "name": "Xingwei Qu",
            "hIndex": 5
        },
        "Yichuan Mo": {
            "authorId": "2058273994",
            "name": "Yi Mo",
            "hIndex": 3
        },
        "Joana Ribeiro de Faria": {
            "authorId": "1583444960",
            "name": "J. Ferreira",
            "hIndex": 7
        },
        "Tugtekin Turan": {
            "authorId": "143650512",
            "name": "M. A. T. Turan",
            "hIndex": 8
        },
        "Anastasios Lytos": {
            "authorId": "46231049",
            "name": "Anastasios Lytos",
            "hIndex": 5
        },
        "Wei Ji": {
            "authorId": "4088867",
            "name": "Wei-xiao Ji",
            "hIndex": 27
        },
        "Pavel Denisov": {
            "authorId": "51151648",
            "name": "Pavel Denisov",
            "hIndex": 11
        },
        "David R. Glowacki": {
            "authorId": "2293307",
            "name": "D. Glowacki",
            "hIndex": 34
        },
        "Jianli Xiao": {
            "authorId": "49874548",
            "name": "Jianli Xiao",
            "hIndex": 10
        },
        "Kai Li": {
            "authorId": "47731358",
            "name": "Kaikai Li",
            "hIndex": 19
        },
        "Lei Hou": {
            "authorId": "2055765001",
            "name": "L. Hou",
            "hIndex": 7
        },
        "Sixuan Du": {
            "authorId": "2090048824",
            "name": "Sixuan Du",
            "hIndex": 3
        },
        "Seyed Mohammad Azimi-Abarghouyi": {
            "authorId": "1399498190",
            "name": "Seyed Mohammad Azimi-Abarghouyi",
            "hIndex": 7
        },
        "Miao Yin": {
            "authorId": "143869661",
            "name": "Yinbin Miao",
            "hIndex": 27
        },
        "Shuichiro Haruta": {
            "authorId": "2339614",
            "name": "Shuichiro Haruta",
            "hIndex": 5
        },
        "Jui-Nan Yen": {
            "authorId": "2132855533",
            "name": "Jui-Nan Yen",
            "hIndex": 2
        },
        "Geyu Lin": {
            "authorId": "51041760",
            "name": "Geyu Lin",
            "hIndex": 6
        },
        "Le Zhang": {
            "authorId": "33145463",
            "name": "Zhang-Gao Le",
            "hIndex": 19
        },
        "Xia Hou": {
            "authorId": "92263905",
            "name": "Z. Hou",
            "hIndex": 11
        },
        "Marco Pavone": {
            "authorId": "1696085",
            "name": "M. Pavone",
            "hIndex": 61
        },
        "Lu Zhang": {
            "authorId": "50081801",
            "name": "Lulu Zhang",
            "hIndex": 11
        },
        "Heng Tao Shen": {
            "authorId": "1724393",
            "name": "Heng Tao Shen",
            "hIndex": 78
        },
        "Devdatt Dubhashi": {
            "name": "Devdatt Dubhashi",
            "hIndex": 0
        },
        "Alexander L. Gaunt": {
            "authorId": "35058304",
            "name": "Alexander L. Gaunt",
            "hIndex": 23
        },
        "Dami\u00e1n Furman": {
            "authorId": "2121898803",
            "name": "D. Furman",
            "hIndex": 4
        },
        "Shumin Deng": {
            "authorId": "6063732",
            "name": "Shumin Deng",
            "hIndex": 12
        },
        "Dianbo Ma": {
            "authorId": "66086972",
            "name": "Dianbo Ma",
            "hIndex": 1
        },
        "Zechen Hu": {
            "authorId": "2110750470",
            "name": "Zechen Hu",
            "hIndex": 8
        },
        "Yuya Fujisaki": {
            "name": "Yuya Fujisaki",
            "hIndex": 0
        },
        "Chaoqun Yang": {
            "authorId": "102756756",
            "name": "C. Yang",
            "hIndex": 29
        },
        "Ray Johns": {
            "authorId": "2368834",
            "name": "C. Johns",
            "hIndex": 8
        },
        "Jing Sun": {
            "authorId": "46969341",
            "name": "Jing Sun",
            "hIndex": 47
        },
        "Sebastian Scherer": {
            "authorId": "32634992",
            "name": "S. Scherer",
            "hIndex": 45
        },
        "Mohamed elShehaby": {
            "authorId": "90872009",
            "name": "M. Elshehaby",
            "hIndex": 2
        },
        "Igor Mordatch": {
            "authorId": "2316241372",
            "name": "Igor Mordatch",
            "hIndex": 15
        },
        "Muhammad Usman": {
            "authorId": "48798471",
            "name": "M. Usman",
            "hIndex": 13
        },
        "Xiaobo Xia": {
            "authorId": "48202992",
            "name": "Xiaobo Shi",
            "hIndex": 25
        },
        "Bowen Jiang": {
            "authorId": "47198175",
            "name": "Bowen Jiang",
            "hIndex": 6
        },
        "Nicolas G. H\u00f6rmann": {
            "authorId": "66788583",
            "name": "N. H\u00f6rmann",
            "hIndex": 8
        },
        "Kei Okada": {
            "authorId": "1683608",
            "name": "K. Okada",
            "hIndex": 38
        },
        "Shangqing Tu": {
            "authorId": "2116520118",
            "name": "Shangqing Tu",
            "hIndex": 5
        },
        "Ant\u00f3nio Loison": {
            "name": "Ant\u00f3nio Loison",
            "hIndex": 0
        },
        "Sana Ayromlou": {
            "authorId": "2055028830",
            "name": "Sana Ayromlou",
            "hIndex": 2
        },
        "Michael Roeder": {
            "authorId": "115888163",
            "name": "M. Roeder",
            "hIndex": 10
        },
        "Ajay Patel": {
            "authorId": "49486062",
            "name": "A. Patel",
            "hIndex": 16
        },
        "Emre Akbas": {
            "authorId": "15090700",
            "name": "H. Akba\u015f",
            "hIndex": 10
        },
        "Edduardo Vellasques": {
            "name": "Edduardo Vellasques",
            "hIndex": 0
        },
        "Haolin Liu": {
            "authorId": "2143856384",
            "name": "Hao Liu",
            "hIndex": 15
        },
        "Chengyuan Liu": {
            "authorId": "50846802",
            "name": "Cheng-Yuan Liu",
            "hIndex": 11
        },
        "Camilo Thorne": {
            "authorId": "2093331",
            "name": "Camilo Thorne",
            "hIndex": 11
        },
        "Aly Lidayan": {
            "name": "Aly Lidayan",
            "hIndex": 0
        },
        "Hang Yu": {
            "authorId": "93094159",
            "name": "Hang Z. Yu",
            "hIndex": 17
        },
        "Nicholas J. Pioch": {
            "authorId": "2399430",
            "name": "N. Pioch",
            "hIndex": 8
        },
        "Gao Liu": {
            "authorId": "98232015",
            "name": "Gao-lian Liu",
            "hIndex": 11
        },
        "Axel Martinez": {
            "authorId": "1402562691",
            "name": "A. Martinez-M\u00f6ller",
            "hIndex": 17
        },
        "R. Harald Baayen": {
            "authorId": "2918948",
            "name": "R. Baayen",
            "hIndex": 57
        },
        "Kazuki Yamauchi": {
            "authorId": "3289588",
            "name": "K. Yamauchi",
            "hIndex": 9
        },
        "Ashwini Gundappa": {
            "name": "Ashwini Gundappa",
            "hIndex": 0
        },
        "Benjamin Heinzerling": {
            "authorId": "2266692",
            "name": "Benjamin Heinzerling",
            "hIndex": 9
        },
        "Ehsan Adeli": {
            "name": "Ehsan Adeli",
            "hIndex": 0
        },
        "M. Yaqoob Wani": {
            "authorId": "34549740",
            "name": "M. Wani",
            "hIndex": 13
        },
        "Timos Sellis": {
            "authorId": "144302930",
            "name": "T. Sellis",
            "hIndex": 55
        },
        "Minsuk Choi": {
            "authorId": "8486223",
            "name": "Min-Je Choi",
            "hIndex": 8
        },
        "TaeWoong Seo": {
            "authorId": "2302010999",
            "name": "Taewoong Seo",
            "hIndex": 0
        },
        "Thanh Tam Nguyen": {
            "authorId": "2117824517",
            "name": "T. Nguyen",
            "hIndex": 10
        },
        "Yixue Hao": {
            "name": "Yixue Hao",
            "hIndex": 0
        },
        "Weimin Tan": {
            "authorId": "2974027",
            "name": "Weimin Tan",
            "hIndex": 16
        },
        "Nancy F. Chen": {
            "authorId": "2185019",
            "name": "Nancy F. Chen",
            "hIndex": 24
        },
        "Jiaxuan Li": {
            "authorId": "2109003773",
            "name": "Jiaxuan Li",
            "hIndex": 6
        },
        "Ewan Dunbar": {
            "authorId": "46967555",
            "name": "Ewan Dunbar",
            "hIndex": 14
        },
        "Teresa Dorszewski": {
            "authorId": "2316635027",
            "name": "Teresa Dorszewski",
            "hIndex": 0
        },
        "Abdarahmane Traor\u00e9": {
            "authorId": "133675420",
            "name": "Abdarahmane Traor\u00e9",
            "hIndex": 3
        },
        "Louis Thomson": {
            "authorId": "3422486",
            "name": "L. Thomson",
            "hIndex": 46
        },
        "Yujie Tong": {
            "authorId": "2119289777",
            "name": "Yu He",
            "hIndex": 4
        },
        "Yurii Halychansky": {
            "name": "Yurii Halychansky",
            "hIndex": 0
        },
        "Zhuohan Liu": {
            "authorId": "2175613084",
            "name": "Zhuohan Liu",
            "hIndex": 2
        },
        "Andrew Zhu": {
            "authorId": "1910496",
            "name": "A. Zhu",
            "hIndex": 103
        },
        "Ruslan Salakhutdinov": {
            "authorId": "145124475",
            "name": "R. Salakhutdinov",
            "hIndex": 113
        },
        "Ze-Feng Gao": {
            "name": "Ze-Feng Gao",
            "hIndex": 0
        },
        "Kilian M. Pohl": {
            "authorId": "2916954",
            "name": "K. Pohl",
            "hIndex": 34
        },
        "Rui Ye": {
            "authorId": "37264038",
            "name": "Ruisong Ye",
            "hIndex": 15
        },
        "Huijin Wang": {
            "authorId": "2098915799",
            "name": "Wang Huijin",
            "hIndex": 4
        },
        "Alireza Ghafarollahi": {
            "authorId": "103778332",
            "name": "A. Ghafarollahi",
            "hIndex": 8
        },
        "Jiaqi Li": {
            "authorId": "2108991836",
            "name": "Jiaqing Li",
            "hIndex": 13
        },
        "Kiran Raja": {
            "authorId": "2065764061",
            "name": "K. Raja",
            "hIndex": 10
        },
        "David Bani-Harouni": {
            "authorId": "1667759086",
            "name": "David Bani-Harouni",
            "hIndex": 2
        },
        "seung-won hwang": {
            "authorId": "1716415",
            "name": "Seung-won Hwang",
            "hIndex": 33
        },
        "Devansh Dhrafani": {
            "authorId": "2273982741",
            "name": "Devansh Dhrafani",
            "hIndex": 1
        },
        "Youssef Drissi": {
            "authorId": "1839053",
            "name": "Y. Drissi",
            "hIndex": 8
        },
        "Xudong Gao": {
            "authorId": "49779664",
            "name": "Xudong Gao",
            "hIndex": 24
        },
        "Vagrant Gautam": {
            "authorId": "2047602319",
            "name": "Vagrant Gautam",
            "hIndex": 3
        },
        "Antonio Mastropietro": {
            "authorId": "2022374828",
            "name": "Antonio Mastropietro",
            "hIndex": 3
        },
        "John D. Kelleher": {
            "authorId": "34967075",
            "name": "John D. Kelleher",
            "hIndex": 24
        },
        "Wijnand IJsselsteijn": {
            "authorId": "1679478",
            "name": "W. Ijsselsteijn",
            "hIndex": 54
        },
        "Bhiksha Raj": {
            "authorId": "1681921",
            "name": "B. Raj",
            "hIndex": 54
        },
        "Tony Scerri": {
            "authorId": "1729291592",
            "name": "Tony Scerri",
            "hIndex": 3
        },
        "Juhwan Choi": {
            "authorId": "2478014",
            "name": "Juhwan Choi",
            "hIndex": 7
        },
        "Jie Huang": {
            "authorId": "50535644",
            "name": "Jiejie Huang",
            "hIndex": 27
        },
        "Hong Guo": {
            "authorId": "2111300704",
            "name": "Hong L. Guo",
            "hIndex": 14
        },
        "Guojiang Zhao": {
            "authorId": "2109903145",
            "name": "John Zhao",
            "hIndex": 9
        },
        "Yu Zhang": {
            "authorId": "49889909",
            "name": "Yu Zhang",
            "hIndex": 14
        },
        "Jiahui Yang": {
            "authorId": "29782448",
            "name": "A. Yang",
            "hIndex": 4
        },
        "Jun Chen": {
            "authorId": "2108208690",
            "name": "Jun-zhu Chen",
            "hIndex": 19
        },
        "Se-Young Yun": {
            "authorId": "145317736",
            "name": "Seyoung Yun",
            "hIndex": 16
        },
        "Peng-Jie Guo": {
            "authorId": "38654394",
            "name": "Shenmin Zhang",
            "hIndex": 68
        },
        "Shiyuan He": {
            "authorId": "2183584",
            "name": "Shiyuan He",
            "hIndex": 8
        },
        "Shuo Chen": {
            "authorId": "115436064",
            "name": "I. Chen",
            "hIndex": 20
        },
        "Sebastian M\u00f6ller": {
            "authorId": "145733288",
            "name": "Sebastian M\u00f6ller",
            "hIndex": 38
        },
        "Dale Schuurmans": {
            "authorId": "1714772",
            "name": "Dale Schuurmans",
            "hIndex": 52
        },
        "Francesco Bonacci": {
            "authorId": "101133483",
            "name": "F. Bonacci",
            "hIndex": 4
        },
        "Zubair Shafiq": {
            "authorId": "1777650",
            "name": "M. Shafiq",
            "hIndex": 25
        },
        "Bryan Renard": {
            "name": "Bryan Renard",
            "hIndex": 0
        },
        "Xianglong Liu": {
            "authorId": "6820648",
            "name": "Xianglong Liu",
            "hIndex": 53
        },
        "Turlan Kuzhagaliyev": {
            "name": "Turlan Kuzhagaliyev",
            "hIndex": 0
        },
        "Dhruv Shah": {
            "authorId": "114265333",
            "name": "Dhruv R Shah",
            "hIndex": 6
        },
        "J. H. Liu": {
            "authorId": "120809991",
            "name": "J. Liu",
            "hIndex": 31
        },
        "Hongjin Qian": {
            "authorId": "1839918",
            "name": "Hongjin He",
            "hIndex": 15
        },
        "Axel-Cyrille Ngonga Ngomo": {
            "authorId": "1712107",
            "name": "A. N. Ngomo",
            "hIndex": 49
        },
        "Shunming Liu": {
            "authorId": "2166423929",
            "name": "Shunming Liu",
            "hIndex": 5
        },
        "Jaeseong Lee": {
            "authorId": "2144702137",
            "name": "Jae-Seong Lee",
            "hIndex": 5
        },
        "Tackeun Kim": {
            "authorId": "35121689",
            "name": "Tackeun Kim",
            "hIndex": 21
        },
        "Uzair Shah": {
            "authorId": "144784083",
            "name": "S. U. A. Shah",
            "hIndex": 9
        },
        "Jiaqi Kang": {
            "authorId": "32494117",
            "name": "Ji\u2010Sue Kang",
            "hIndex": 5
        },
        "Jingwen Tong": {
            "authorId": "3257888",
            "name": "Jingwen Tong",
            "hIndex": 4
        },
        "Xin Hu": {
            "authorId": "102805155",
            "name": "Xinyang Hu",
            "hIndex": 41
        },
        "Yixuan Zhou": {
            "authorId": "47943260",
            "name": "Yixuan Zhou",
            "hIndex": 7
        },
        "Yi Xin": {
            "authorId": "3144527",
            "name": "Xinyi Wu",
            "hIndex": 28
        },
        "Abdur R. Shahid": {
            "authorId": "9614768",
            "name": "A. Shahid",
            "hIndex": 7
        },
        "Dongyue Li": {
            "authorId": "2257089345",
            "name": "Dongyue Li",
            "hIndex": 3
        },
        "Junzheng Zhang": {
            "authorId": "2144124130",
            "name": "Junzheng Zhang",
            "hIndex": 12
        },
        "Paul Weng": {
            "name": "Paul Weng",
            "hIndex": 0
        },
        "Hyunjong Ok": {
            "authorId": "2216442555",
            "name": "Hyunjong Ok",
            "hIndex": 2
        },
        "Xinyong Zhou": {
            "authorId": "2148927363",
            "name": "Xinyong Zhou",
            "hIndex": 4
        },
        "Shrestha Mohanty": {
            "authorId": "31788008",
            "name": "Shrestha Mohanty",
            "hIndex": 8
        },
        "Kuang-Huei Lee": {
            "name": "Kuang-Huei Lee",
            "hIndex": 0
        },
        "Kai Ma": {
            "authorId": "2075321035",
            "name": "K. Ma",
            "hIndex": 7
        },
        "Yuanyuan He": {
            "authorId": "46968224",
            "name": "Yuanyuan He",
            "hIndex": 10
        },
        "Lianghao Xia": {
            "authorId": "1830455155",
            "name": "Lianghao Xia",
            "hIndex": 26
        },
        "Muratahan Aykol": {
            "authorId": "7995028",
            "name": "Muratahan Aykol",
            "hIndex": 37
        },
        "Aishan Liu": {
            "authorId": "153152072",
            "name": "Aishan Liu",
            "hIndex": 19
        },
        "Yan Ting Chok": {
            "authorId": "2102138238",
            "name": "Y. T. Chok",
            "hIndex": 0
        },
        "Weimin Lyu": {
            "authorId": "39533001",
            "name": "Wei Wu",
            "hIndex": 92
        },
        "Michael C. Mozer": {
            "authorId": "144473519",
            "name": "M. Mozer",
            "hIndex": 57
        },
        "Ai Ti Aw": {
            "authorId": "2113601787",
            "name": "A. Aw",
            "hIndex": 8
        },
        "Shidong Shang": {
            "authorId": "2052100290",
            "name": "Shidong Shang",
            "hIndex": 7
        },
        "Romaric Besan\u00e7on": {
            "authorId": "1740190",
            "name": "Romaric Besan\u00e7on",
            "hIndex": 16
        },
        "Bozhong Tian": {
            "authorId": "2064522174",
            "name": "Bo Tian",
            "hIndex": 6
        },
        "Riccardo De Monte": {
            "name": "Riccardo De Monte",
            "hIndex": 0
        },
        "Shen Li": {
            "authorId": "2121271401",
            "name": "Li Shen",
            "hIndex": 44
        },
        "Chao Ma": {
            "authorId": "2109467242",
            "name": "C. Ma",
            "hIndex": 15
        },
        "Suyash Fulay": {
            "authorId": "13659288",
            "name": "Suyash Fulay",
            "hIndex": 2
        },
        "Zhou Zhao": {
            "authorId": "12268912",
            "name": "Zhaolu Zhou",
            "hIndex": 15
        },
        "Vandana P. Janeja": {
            "authorId": "1730181",
            "name": "V. P Janeja",
            "hIndex": 18
        },
        "Vincent Stragier": {
            "authorId": "2150505236",
            "name": "V. Stragier",
            "hIndex": 1
        },
        "Muraleekrishna Gopinathan": {
            "authorId": "2123449716",
            "name": "Muraleekrishna Gopinathan",
            "hIndex": 1
        },
        "M. AbuGhanem": {
            "authorId": "104063826",
            "name": "M. AbuGhanem",
            "hIndex": 3
        },
        "Gy\u00f6rgy Fazekas": {
            "authorId": "145131681",
            "name": "Gy\u00f6rgy Fazekas",
            "hIndex": 27
        },
        "Davide Buffelli": {
            "authorId": "150964467",
            "name": "Davide Buffelli",
            "hIndex": 5
        },
        "Joymallya Chakraborty": {
            "authorId": "103817340",
            "name": "Joymallya Chakraborty",
            "hIndex": 9
        },
        "Daniel Rose": {
            "authorId": "22225944",
            "name": "D. I. Rose",
            "hIndex": 3
        },
        "Krishna C. Puvvada": {
            "authorId": "2121516",
            "name": "Krishna C. Puvvada",
            "hIndex": 4
        },
        "Frederik Wiehr": {
            "authorId": "2722786",
            "name": "Frederik Wiehr",
            "hIndex": 10
        },
        "Adrian Weller": {
            "authorId": "145689461",
            "name": "Adrian Weller",
            "hIndex": 41
        },
        "Haau-Sing Li": {
            "authorId": "1993902967",
            "name": "Haau-Sing Li",
            "hIndex": 4
        },
        "Alan Akbik": {
            "authorId": "2403712",
            "name": "A. Akbik",
            "hIndex": 15
        },
        "Raunaq Bhirangi": {
            "authorId": "152466940",
            "name": "Raunaq M. Bhirangi",
            "hIndex": 4
        },
        "Bo Liu": {
            "authorId": "6174708",
            "name": "Bo-Tau Liu",
            "hIndex": 17
        },
        "Jin Sob Kim": {
            "authorId": "2157943035",
            "name": "Jin Sob Kim",
            "hIndex": 4
        },
        "Petar Veli\u010dkovi\u0107": {
            "authorId": "3444569",
            "name": "Petar Velickovic",
            "hIndex": 23
        },
        "Lansheng Han": {
            "authorId": "2256602",
            "name": "Lansheng Han",
            "hIndex": 9
        },
        "Xi Chen": {
            "authorId": "48283704",
            "name": "Xi Chen",
            "hIndex": 9
        },
        "Mengyao Xu": {
            "authorId": "92816652",
            "name": "Mengyao Xu",
            "hIndex": 6
        },
        "Yongbin Li": {
            "authorId": "2135299408",
            "name": "Yongbing Li",
            "hIndex": 12
        },
        "Juan Castorena": {
            "authorId": "1780793",
            "name": "J. Castorena",
            "hIndex": 10
        },
        "Marcus R\u00fcb": {
            "authorId": "2090563537",
            "name": "Marcus R\u00fcb",
            "hIndex": 1
        },
        "Janet Dick": {
            "authorId": "145781330",
            "name": "J. Dick",
            "hIndex": 33
        },
        "Bikram Boote": {
            "authorId": "2023707382",
            "name": "Bikram Boote",
            "hIndex": 2
        },
        "YoungBin Kim": {
            "name": "YoungBin Kim",
            "hIndex": 0
        },
        "Ilya Gusev": {
            "authorId": "145281628",
            "name": "I. Gusev",
            "hIndex": 6
        },
        "Cheng Tan": {
            "authorId": "103477696",
            "name": "Chengpeng Tan",
            "hIndex": 9
        },
        "Simon Kornblith": {
            "authorId": "40464924",
            "name": "Simon Kornblith",
            "hIndex": 30
        },
        "Kai Jin": {
            "authorId": "18187631",
            "name": "K. Jin",
            "hIndex": 10
        },
        "Chunlin Tian": {
            "authorId": "2072927378",
            "name": "C. Tian",
            "hIndex": 4
        },
        "Ga\u00ebl de Chalendar": {
            "authorId": "2621219",
            "name": "Ga\u00ebl de Chalendar",
            "hIndex": 11
        },
        "Vil\u00e9m Sklen\u00e1k": {
            "authorId": "50975551",
            "name": "Vil\u00e9m Sklen\u00e1k",
            "hIndex": 4
        },
        "Renxia Xue": {
            "name": "Renxia Xue",
            "hIndex": 0
        },
        "Wei Liu": {
            "authorId": "48152323",
            "name": "Wei Liu",
            "hIndex": 29
        },
        "Mehak Singal": {
            "name": "Mehak Singal",
            "hIndex": 0
        },
        "Suyan Li": {
            "authorId": "92273624",
            "name": "Liu Suyan",
            "hIndex": 2
        },
        "Rajesh Sharma": {
            "authorId": "120509754",
            "name": "Rajesh K Sharma",
            "hIndex": 17
        },
        "Aaron": {
            "authorId": "145737565",
            "name": "Joann Aaron",
            "hIndex": 11
        },
        "Sherry Yang": {
            "authorId": "2724590",
            "name": "Sherry X. Yang",
            "hIndex": 25
        },
        "Stefan Sch\u00fctte": {
            "authorId": "46191051",
            "name": "S. Sch\u00fctte",
            "hIndex": 19
        },
        "Bin Jiang": {
            "authorId": "38441019",
            "name": "Binbin Jiang",
            "hIndex": 24
        },
        "Arvind Krishna Sridhar": {
            "authorId": "2064327462",
            "name": "A. Sridhar",
            "hIndex": 5
        },
        "Molly": {
            "authorId": "14741731",
            "name": "L. Molly",
            "hIndex": 8
        },
        "Feng Liu": {
            "authorId": "50677962",
            "name": "Bifeng Liu",
            "hIndex": 43
        },
        "Rui Cao": {
            "authorId": "144991324",
            "name": "R. Cao",
            "hIndex": 12
        },
        "Abdulhamid Aldoobi": {
            "name": "Abdulhamid Aldoobi",
            "hIndex": 0
        },
        "Zeqing Qin": {
            "authorId": "2106562545",
            "name": "Zeqing Qin",
            "hIndex": 1
        },
        "Wei Bi": {
            "authorId": "4406036",
            "name": "J. Bi",
            "hIndex": 17
        },
        "Yu-Ru Lin": {
            "authorId": "34821764",
            "name": "Y. Lin",
            "hIndex": 35
        },
        "Jiayuan Mao": {
            "name": "Jiayuan Mao",
            "hIndex": 0
        },
        "Wei Shen": {
            "authorId": "92122138",
            "name": "W. Shen",
            "hIndex": 26
        },
        "Ting Long": {
            "authorId": "47903389",
            "name": "Ting Long",
            "hIndex": 10
        },
        "Hangyu Chen": {
            "authorId": "49178041",
            "name": "Hangyu Chen",
            "hIndex": 6
        },
        "Zakaria Patel": {
            "authorId": "2048000050",
            "name": "Zakaria Patel",
            "hIndex": 2
        },
        "Bradley Malin": {
            "authorId": "31717144",
            "name": "B. Malin",
            "hIndex": 52
        },
        "Karim Lekadir": {
            "authorId": "2294860177",
            "name": "K. Lekadir",
            "hIndex": 14
        },
        "Bo Zhao": {
            "authorId": "50728597",
            "name": "Bohan Zhao",
            "hIndex": 9
        },
        "Ali Taghibakhshi": {
            "authorId": "113435906",
            "name": "Ali Taghibakhshi",
            "hIndex": 4
        },
        "Zheng He": {
            "authorId": "145866253",
            "name": "Heyong Zheng",
            "hIndex": 11
        },
        "Jianmei Jiang": {
            "authorId": "11324581",
            "name": "Jianmei Jiang",
            "hIndex": 4
        },
        "Xinyi Chen": {
            "authorId": "2109080902",
            "name": "Xinyi Chen",
            "hIndex": 25
        },
        "Gregory Gay": {
            "authorId": "38996708",
            "name": "Gregory Gay",
            "hIndex": 16
        },
        "Boian Alexandrov": {
            "authorId": "2025666",
            "name": "B. Alexandrov",
            "hIndex": 22
        },
        "Xinyou Wang": {
            "authorId": "122024354",
            "name": "Xinyou Wang",
            "hIndex": 7
        },
        "Julie A. Shah": {
            "authorId": "143873972",
            "name": "J. Shah",
            "hIndex": 41
        },
        "Praveen Srinivasa Varadhan": {
            "authorId": "1658800462",
            "name": "Praveena Varadhan",
            "hIndex": 1
        },
        "Moa Johansson": {
            "authorId": "145006848",
            "name": "Moa Johansson",
            "hIndex": 13
        },
        "Maram Hasanain": {
            "authorId": "2905745",
            "name": "Maram Hasanain",
            "hIndex": 20
        },
        "Ahmed Imteaj": {
            "authorId": "9200592",
            "name": "Ahmed Imteaj",
            "hIndex": 14
        },
        "Tianyu Zheng": {
            "authorId": "15523767",
            "name": "Tianyue Zheng",
            "hIndex": 17
        },
        "Toshiaki Koike-Akino": {
            "authorId": "1401636364",
            "name": "T. Koike-Akino",
            "hIndex": 28
        },
        "Xiao-Ming Wu": {
            "authorId": "145066048",
            "name": "Xiao-ming Wu",
            "hIndex": 30
        },
        "See-Kiong Ng": {
            "authorId": "1794527",
            "name": "See-Kiong Ng",
            "hIndex": 36
        },
        "Xuanhan Wang": {
            "authorId": "9764377",
            "name": "Xuanhan Wang",
            "hIndex": 10
        },
        "Yahya Jabary": {
            "name": "Yahya Jabary",
            "hIndex": 0
        },
        "Weiyi Zhang": {
            "authorId": "49039542",
            "name": "WeiYi Zhang",
            "hIndex": 8
        },
        "Xiaowei Zhou": {
            "authorId": "47177386",
            "name": "Zhou Xiaowei",
            "hIndex": 3
        },
        "Xiao-Qi Han": {
            "authorId": "93747581",
            "name": "Xiaoduo Qi",
            "hIndex": 8
        },
        "Naomi Nagy": {
            "authorId": "2519441",
            "name": "N. Nagy",
            "hIndex": 20
        },
        "Stuart Russell": {
            "authorId": "29687567",
            "name": "S. Russell",
            "hIndex": 9
        },
        "Shaolei Zhang": {
            "authorId": "122200516",
            "name": "Shao-lei Zhang",
            "hIndex": 4
        },
        "Joy Hsu": {
            "authorId": "47317790",
            "name": "J. Hsu",
            "hIndex": 10
        },
        "Yibing Zhan": {
            "authorId": "1895813",
            "name": "Yibing Zhan",
            "hIndex": 22
        },
        "Ivan Medennikov": {
            "authorId": "2139008",
            "name": "I. Medennikov",
            "hIndex": 12
        },
        "Fuxiao Liu": {
            "authorId": "52220309",
            "name": "Fuxiao Liu",
            "hIndex": 12
        },
        "Lei Meng": {
            "authorId": "147340840",
            "name": "Leilei Meng",
            "hIndex": 23
        },
        "Sacha Muller": {
            "authorId": "1751496271",
            "name": "Sacha Muller-Botti",
            "hIndex": 1
        },
        "Elinor Poole-Dayan": {
            "authorId": "2133330526",
            "name": "Elinor Poole-Dayan",
            "hIndex": 2
        },
        "Seung-jun Lee": {
            "authorId": "2108206202",
            "name": "Seung J. Lee",
            "hIndex": 26
        },
        "Shuai Wang": {
            "authorId": "2118512341",
            "name": "Shuai-shuai Wang",
            "hIndex": 11
        },
        "Nhi Nguyen": {
            "authorId": "79948055",
            "name": "N. Nguyen",
            "hIndex": 8
        },
        "Qing Zhao": {
            "authorId": "73301330",
            "name": "Qing\u2010Li Zhao",
            "hIndex": 38
        },
        "Lucile Ter-Minassian": {
            "authorId": "1432801716",
            "name": "Lucile Ter-Minassian",
            "hIndex": 3
        },
        "Xiaowei Zhu": {
            "authorId": "46875161",
            "name": "Xiaowei Zhu",
            "hIndex": 14
        },
        "Xinya Du": {
            "authorId": "2114712267",
            "name": "Xinya Du",
            "hIndex": 6
        },
        "Zihang Peng": {
            "authorId": "2148404065",
            "name": "Zihang Peng",
            "hIndex": 18
        },
        "Smaranda Muresan": {
            "authorId": "2295928",
            "name": "S. Muresan",
            "hIndex": 31
        },
        "Minh Vu": {
            "authorId": "2261390984",
            "name": "M. Vu",
            "hIndex": 3
        },
        "Seungheun Baek": {
            "authorId": "2220877899",
            "name": "Seungheun Baek",
            "hIndex": 1
        },
        "Jiwoo Hong": {
            "authorId": "6819828",
            "name": "Jiwoo Hong",
            "hIndex": 15
        },
        "Jaewoo Kim": {
            "authorId": "1590807480",
            "name": "JaeWoo Kim",
            "hIndex": 10
        },
        "Yelong Shen": {
            "authorId": "1752875",
            "name": "Yelong Shen",
            "hIndex": 35
        },
        "Yanzhi Chen": {
            "authorId": "2109259317",
            "name": "Yanzhi Chen",
            "hIndex": 8
        },
        "Purang Abolmaesumi": {
            "authorId": "2427371",
            "name": "P. Abolmaesumi",
            "hIndex": 46
        },
        "Jeremie Bogaert": {
            "authorId": "2098835202",
            "name": "J\u00e9r\u00e9mie Bogaert",
            "hIndex": 1
        },
        "Peizhi Wu": {
            "authorId": "2111193903",
            "name": "Peizhi Wu",
            "hIndex": 7
        },
        "Zihan Liao": {
            "authorId": "114025926",
            "name": "Z. Liao",
            "hIndex": 3
        },
        "Christian Druckenbrodt": {
            "authorId": "150055420",
            "name": "Christian Druckenbrodt",
            "hIndex": 11
        },
        "Jiaju Ma": {
            "authorId": "40117281",
            "name": "Jiaju Ma",
            "hIndex": 14
        },
        "Justin Wagle": {
            "authorId": "49710938",
            "name": "Justin Wagle",
            "hIndex": 0
        },
        "Yang Bai": {
            "authorId": "12786419",
            "name": "Baixue Yang",
            "hIndex": 12
        },
        "Richard J. Antonello": {
            "authorId": "1676759117",
            "name": "Richard Antonello",
            "hIndex": 5
        },
        "Siwei Wu": {
            "authorId": "6604172",
            "name": "Siwei Wu",
            "hIndex": 12
        },
        "Shahrzad Shashaani": {
            "authorId": "2133182089",
            "name": "S. Shashaani",
            "hIndex": 2
        },
        "Hamdi Kavak": {
            "authorId": "1905917",
            "name": "Hamdi Kavak",
            "hIndex": 11
        },
        "Marianna Apidianaki": {
            "authorId": "2817917",
            "name": "Marianna Apidianaki",
            "hIndex": 18
        },
        "Hang Zhao": {
            "authorId": "3425717",
            "name": "Hanghang Zhao",
            "hIndex": 17
        },
        "Jonathan Li": {
            "authorId": "1920933346",
            "name": "Jonathan Z. Li",
            "hIndex": 48
        },
        "Kai Sauerwald": {
            "authorId": "13092666",
            "name": "Kai Sauerwald",
            "hIndex": 5
        },
        "Weixuan Li": {
            "authorId": "2051230113",
            "name": "Wei-Xuan Li",
            "hIndex": 5
        },
        "Danda Pani Paudel": {
            "authorId": "35268081",
            "name": "D. Paudel",
            "hIndex": 23
        },
        "Greg Wallace": {
            "authorId": "3029175",
            "name": "Gregory S. Wallace",
            "hIndex": 23
        },
        "Javad Hassannataj Joloudari": {
            "authorId": "104687259",
            "name": "Javad Hassannataj Joloudari",
            "hIndex": 15
        },
        "Lovkush Agarwal": {
            "authorId": "1931404",
            "name": "Lovkush Agarwal",
            "hIndex": 2
        },
        "Thomas Unterthiner": {
            "authorId": "2465270",
            "name": "Thomas Unterthiner",
            "hIndex": 27
        },
        "Xiang Yin": {
            "authorId": "93257687",
            "name": "Xianglu Yin",
            "hIndex": 14
        },
        "Christine Mallinson": {
            "authorId": "145469075",
            "name": "Christine Mallinson",
            "hIndex": 15
        },
        "Yining Chen": {
            "authorId": "2108965431",
            "name": "Yining Chen",
            "hIndex": 17
        },
        "Yuhang Qin": {
            "authorId": "2107504930",
            "name": "Y. Qin",
            "hIndex": 3
        },
        "Meng Zhou": {
            "authorId": "143849611",
            "name": "Mengchu Zhou",
            "hIndex": 107
        },
        "Li Du": {
            "authorId": "1443777317",
            "name": "Lili Du",
            "hIndex": 23
        },
        "Mirjam Ernestus": {
            "authorId": "1801087",
            "name": "M. Ernestus",
            "hIndex": 36
        },
        "Saika Zaman": {
            "authorId": "9347864",
            "name": "Saika Zaman",
            "hIndex": 2
        },
        "Shun Long": {
            "authorId": "19247995",
            "name": "Shunqin Long",
            "hIndex": 10
        },
        "He Wang": {
            "authorId": "39483421",
            "name": "H. Wang",
            "hIndex": 10
        },
        "Ye Wang": {
            "authorId": "119917588",
            "name": "Yetong Wang",
            "hIndex": 6
        },
        "Max Ploner": {
            "authorId": "2291364423",
            "name": "Max Ploner",
            "hIndex": 1
        },
        "Zongyuan Ge": {
            "authorId": "1808390",
            "name": "ZongYuan Ge",
            "hIndex": 28
        },
        "Bing Han": {
            "authorId": "144506274",
            "name": "Bingnan Han",
            "hIndex": 20
        },
        "Savvina Daniil": {
            "authorId": "2179337706",
            "name": "Savvina Daniil",
            "hIndex": 2
        },
        "Sihang Liu": {
            "authorId": "5262019",
            "name": "Sihang Liu",
            "hIndex": 10
        },
        "Chris Callison-Burch": {
            "authorId": "1763608",
            "name": "Chris Callison-Burch",
            "hIndex": 62
        },
        "Sarah R. Kingsbury": {
            "authorId": "3574715",
            "name": "S. Kingsbury",
            "hIndex": 24
        },
        "Matthias Nie\u00dfner": {
            "authorId": "2209612",
            "name": "M. Nie\u00dfner",
            "hIndex": 73
        },
        "Ming He": {
            "authorId": "98754313",
            "name": "M. He",
            "hIndex": 18
        },
        "Syun'ichi Shiraiwa": {
            "authorId": "47606319",
            "name": "S. Shiraiwa",
            "hIndex": 27
        },
        "Jason St. John": {
            "authorId": "73408711",
            "name": "J. S. John",
            "hIndex": 95
        },
        "Tathagata Raha": {
            "authorId": "1840492875",
            "name": "Tathagata Raha",
            "hIndex": 5
        },
        "Zhihong Zhu": {
            "authorId": "120628478",
            "name": "Zhihong Zhu",
            "hIndex": 33
        },
        "Yanan Wang": {
            "authorId": "30333347",
            "name": "Yan\u2019an Wang",
            "hIndex": 7
        },
        "Tomas Bosschieter": {
            "authorId": "2175781312",
            "name": "Tomas M. Bosschieter",
            "hIndex": 3
        },
        "Fengrun Zhang": {
            "authorId": "2204997310",
            "name": "Fengrun Zhang",
            "hIndex": 1
        },
        "Zhaohu Xing": {
            "authorId": "153107262",
            "name": "Zhaohu Xing",
            "hIndex": 6
        },
        "Saber Fallah": {
            "authorId": "34929667",
            "name": "Saber Fallah",
            "hIndex": 22
        },
        "Jing Yuan": {
            "authorId": "1390958297",
            "name": "Jing Yuan",
            "hIndex": 31
        },
        "Wonjun Lee": {
            "authorId": "46605489",
            "name": "Wonjun Lee",
            "hIndex": 6
        },
        "Tristan Thrush": {
            "authorId": "1500242049",
            "name": "Tristan Thrush",
            "hIndex": 14
        },
        "Yanan Hu": {
            "authorId": "9318659",
            "name": "H. Yanan",
            "hIndex": 5
        },
        "Stav Cohen": {
            "authorId": "2107578370",
            "name": "S. Cohen",
            "hIndex": 1
        },
        "Matheus Martins": {
            "authorId": "1580619712",
            "name": "M. Martins",
            "hIndex": 1
        },
        "Hyongon Ryu": {
            "name": "Hyongon Ryu",
            "hIndex": 0
        },
        "Buck Shlegeris": {
            "authorId": "79384063",
            "name": "Buck Shlegeris",
            "hIndex": 7
        },
        "Fernando Calamante": {
            "authorId": "1706917",
            "name": "F. Calamante",
            "hIndex": 60
        },
        "Nicholas Diakopoulos": {
            "authorId": "2943892",
            "name": "N. Diakopoulos",
            "hIndex": 35
        },
        "Xinjie Zhou": {
            "authorId": "152482253",
            "name": "Xinjie Zhou",
            "hIndex": 14
        },
        "Bu Jin": {
            "authorId": "1442311294",
            "name": "Jin-bu Xu",
            "hIndex": 5
        },
        "Yuan-Fang Li": {
            "authorId": "10718292",
            "name": "Fangli Yuan",
            "hIndex": 37
        },
        "Mao Yan": {
            "authorId": "145793751",
            "name": "Y. Mao",
            "hIndex": 7
        },
        "Han Liu": {
            "authorId": "2140162920",
            "name": "Hanwu Liu",
            "hIndex": 9
        },
        "Jingzhi Gong": {
            "authorId": "2112952788",
            "name": "Jing Gong",
            "hIndex": 7
        },
        "Xiaojun Chen": {
            "authorId": "7438748",
            "name": "C. Xiaojun",
            "hIndex": 9
        },
        "Muhammad Shahid Muneer": {
            "authorId": "2188536976",
            "name": "Muhammad Shahid Muneer",
            "hIndex": 1
        },
        "Lisa Tabor Connor": {
            "authorId": "6600192",
            "name": "L. Connor",
            "hIndex": 28
        },
        "Weijie Yu": {
            "authorId": "3313676",
            "name": "Wei-jie Yu",
            "hIndex": 14
        },
        "Hao Tang": {
            "authorId": "2088976",
            "name": "Haoyang Tang",
            "hIndex": 14
        },
        "Freda Shi": {
            "authorId": "8815141",
            "name": "Freda Shi",
            "hIndex": 12
        },
        "Victor M. Campello": {
            "authorId": "1412401614",
            "name": "V\u00edctor M. Campello",
            "hIndex": 10
        },
        "Hongyang Lei": {
            "authorId": "50841851",
            "name": "H. Wang",
            "hIndex": 60
        },
        "Minghong Fang": {
            "authorId": "1732755",
            "name": "Minghong Fang",
            "hIndex": 10
        },
        "Aparajita Saraf": {
            "authorId": "51912276",
            "name": "Aparajita Saraf",
            "hIndex": 3
        },
        "Zhiyu Chen": {
            "authorId": "2111630774",
            "name": "Zhiyun Chen",
            "hIndex": 6
        },
        "Sandra Pieraccini": {
            "authorId": "29794331",
            "name": "S. Pieraccini",
            "hIndex": 19
        },
        "Guangming Zhao": {
            "authorId": "47153482",
            "name": "Guang Zhao",
            "hIndex": 6
        },
        "Sigmund Slang": {
            "authorId": "1453614173",
            "name": "S. Slang",
            "hIndex": 3
        },
        "Shili Zhou": {
            "authorId": "16271156",
            "name": "Shilin Zhou",
            "hIndex": 11
        },
        "Ivana Dusparic": {
            "authorId": "2219844",
            "name": "Ivana Dusparic",
            "hIndex": 19
        },
        "Vinal Asodia": {
            "authorId": "2315507424",
            "name": "Vinal Asodia",
            "hIndex": 0
        },
        "Hira Dhamyal": {
            "authorId": "1397306698",
            "name": "Hira Dhamyal",
            "hIndex": 5
        },
        "Rohit Raj Rai": {
            "authorId": "2277212212",
            "name": "Rohit Raj Rai",
            "hIndex": 0
        },
        "Masatsugu Ono": {
            "authorId": "2054340566",
            "name": "M. Ono",
            "hIndex": 2
        },
        "Fuchen Zheng": {
            "authorId": "2319449329",
            "name": "Fuchen Zheng",
            "hIndex": 0
        },
        "Bruno D. Ferreira-Saraiva": {
            "authorId": "31222481",
            "name": "D. Silva",
            "hIndex": 12
        },
        "B. Sankar": {
            "authorId": "41124816",
            "name": "B. Sankar",
            "hIndex": 39
        },
        "Youssef Khaky": {
            "name": "Youssef Khaky",
            "hIndex": 0
        },
        "Marco AF Pimentel": {
            "authorId": "144669267",
            "name": "M. Pimentel",
            "hIndex": 13
        },
        "Yan Zhao": {
            "authorId": "2109923848",
            "name": "Yan-Yan Zhao",
            "hIndex": 18
        },
        "Zhiyong Wu": {
            "authorId": "47039425",
            "name": "Zhiyong Wu",
            "hIndex": 29
        },
        "Jiawei Shao": {
            "authorId": "2111877589",
            "name": "Jiawei Shao",
            "hIndex": 13
        },
        "Cl\u00e9ment Christophe": {
            "authorId": "69928993",
            "name": "C. Clement",
            "hIndex": 88
        },
        "Jo\u00e3o P. Matos-Carvalho": {
            "authorId": "143615304",
            "name": "J. Silva",
            "hIndex": 13
        },
        "Yebo Wu": {
            "authorId": "2316430547",
            "name": "Wu Yebo",
            "hIndex": 1
        },
        "Qiujing Lu": {
            "authorId": "2151674403",
            "name": "Qiujing Lu",
            "hIndex": 4
        },
        "Daniel F Campos": {
            "authorId": "11922334",
            "name": "D. F. de Campos Mazo",
            "hIndex": 6
        },
        "Mingyue Cheng": {
            "authorId": "1491233507",
            "name": "Mingyue Cheng",
            "hIndex": 11
        },
        "Yujie Luo": {
            "authorId": "2157855325",
            "name": "Yue Luo",
            "hIndex": 3
        },
        "Cho-Jui Hsieh": {
            "authorId": "1793529",
            "name": "Cho-Jui Hsieh",
            "hIndex": 74
        },
        "Edanur Demir": {
            "authorId": "2139163064",
            "name": "E. Demir",
            "hIndex": 1
        },
        "Cormac Herley": {
            "authorId": "1679146",
            "name": "Cormac Herley",
            "hIndex": 39
        },
        "Junkun Chen": {
            "authorId": "2260819592",
            "name": "Jun-Kun Chen",
            "hIndex": 3
        },
        "Vojt\u011bch Balek": {
            "name": "Vojt\u011bch Balek",
            "hIndex": 0
        },
        "Fu-Peng Li": {
            "authorId": "49149704",
            "name": "Liye Fu",
            "hIndex": 21
        },
        "M. Abdullah Canbaz": {
            "authorId": "3456930",
            "name": "M. A. Canbaz",
            "hIndex": 6
        },
        "Jintian Zhang": {
            "authorId": "2253784578",
            "name": "Jintian Zhang",
            "hIndex": 6
        },
        "Ming Jin": {
            "authorId": "48870288",
            "name": "M. Jin",
            "hIndex": 14
        },
        "Pranita Marajan": {
            "name": "Pranita Marajan",
            "hIndex": 0
        },
        "Alberto Carlo Maria Mancino": {
            "authorId": "2121913506",
            "name": "Alberto Carlo Maria Mancino",
            "hIndex": 3
        },
        "Philip G. Conaghan": {
            "authorId": "145866331",
            "name": "P. Conaghan",
            "hIndex": 107
        },
        "Massimo Poesio": {
            "authorId": "1678591",
            "name": "Massimo Poesio",
            "hIndex": 50
        },
        "Fangzhou Zhao": {
            "authorId": "3656113",
            "name": "Fangzhou Zhao",
            "hIndex": 10
        },
        "Haiyue Feng": {
            "authorId": "121272466",
            "name": "H. Feng",
            "hIndex": 5
        },
        "Yuwei Chen": {
            "authorId": "92709239",
            "name": "Yuwei Chen",
            "hIndex": 29
        },
        "Reza Zandehshahvar": {
            "name": "Reza Zandehshahvar",
            "hIndex": 0
        },
        "Weijia Guo": {
            "authorId": "47032954",
            "name": "Weijia Guo",
            "hIndex": 10
        },
        "Mohammad Reza Mohammadi": {
            "authorId": "2057894748",
            "name": "M. Mohammadi",
            "hIndex": 10
        },
        "Zhuohang Li": {
            "authorId": "2118389371",
            "name": "Zhuo Li",
            "hIndex": 20
        },
        "Javiera Castillo-Navarro": {
            "authorId": "1412125303",
            "name": "J. Castillo-Navarro",
            "hIndex": 4
        },
        "Mariano Cabezas": {
            "authorId": "144511905",
            "name": "M. Cabezas",
            "hIndex": 22
        },
        "Katsuma Inoue": {
            "name": "Katsuma Inoue",
            "hIndex": 0
        },
        "Haohan Wang": {
            "authorId": "3669925",
            "name": "Haohan Wang",
            "hIndex": 20
        },
        "Kairui Ding": {
            "authorId": "2294856557",
            "name": "Kairui Ding",
            "hIndex": 1
        },
        "Jeongsoo Han": {
            "authorId": "47180714",
            "name": "J. Han",
            "hIndex": 6
        },
        "Dan Wang": {
            "authorId": "1712231",
            "name": "Dan-dan Wang",
            "hIndex": 20
        },
        "Ming Chen": {
            "authorId": "151490576",
            "name": "Mingming Chen",
            "hIndex": 32
        },
        "Lifu Huang": {
            "authorId": "2273197501",
            "name": "Lifu Huang",
            "hIndex": 3
        },
        "Mateus Nogueira": {
            "authorId": "3483290",
            "name": "M. Nogueira",
            "hIndex": 9
        },
        "Jueon Park": {
            "authorId": "2116009930",
            "name": "Jueon Park",
            "hIndex": 3
        },
        "Junhyun Lee": {
            "authorId": "2108454762",
            "name": "Junhyun Lee",
            "hIndex": 14
        },
        "Antonio Loquercio": {
            "authorId": "20580939",
            "name": "Antonio Loquercio",
            "hIndex": 21
        },
        "Ekin D. Cubuk": {
            "authorId": "8132903",
            "name": "E. D. Cubuk",
            "hIndex": 51
        },
        "Kamal Youcef-Toumi": {
            "authorId": "1396509677",
            "name": "K. Youcef-Toumi",
            "hIndex": 40
        },
        "Long Hu": {
            "authorId": "6153821",
            "name": "Longbo Hu",
            "hIndex": 10
        },
        "Herman Kamper": {
            "authorId": "2308553",
            "name": "H. Kamper",
            "hIndex": 27
        },
        "Xu": {
            "authorId": "23706436",
            "name": "Xu-dong Xu",
            "hIndex": 8
        },
        "Haipeng Liu": {
            "authorId": "2629841",
            "name": "Liu Haipeng",
            "hIndex": 6
        },
        "Ning Xie": {
            "authorId": "33810469",
            "name": "Hongning Xie",
            "hIndex": 16
        },
        "Daniel Mueller-Gritschneder": {
            "authorId": "1397424763",
            "name": "Daniel Mueller-Gritschneder",
            "hIndex": 16
        },
        "Shen Huang": {
            "authorId": "87499891",
            "name": "Bing-Shen Huang",
            "hIndex": 17
        },
        "Nasser Mohammadiha": {
            "authorId": "3058699",
            "name": "N. Mohammadiha",
            "hIndex": 18
        },
        "Georgios Mikros": {
            "authorId": "2084392201",
            "name": "Georgios K. Mikros",
            "hIndex": 2
        },
        "Peng Yang": {
            "authorId": "48220441",
            "name": "Peng Yang",
            "hIndex": 28
        },
        "Richard Futrell": {
            "authorId": "2585394",
            "name": "Richard Futrell",
            "hIndex": 30
        },
        "Xinwei Zhang": {
            "authorId": "2108000901",
            "name": "X. Zhang",
            "hIndex": 7
        },
        "Lei Liang": {
            "authorId": "47930450",
            "name": "L. Lei",
            "hIndex": 17
        },
        "Nima Tajbakhsh": {
            "authorId": "1930128",
            "name": "Nima Tajbakhsh",
            "hIndex": 22
        },
        "Menachem Pirchi": {
            "authorId": "1402733628",
            "name": "Menachem Pirchi",
            "hIndex": 1
        },
        "Alin Albu-Sch\u00e4ffer": {
            "authorId": "1397935382",
            "name": "A. Albu-Sch\u00e4ffer",
            "hIndex": 67
        },
        "Jian Xu": {
            "authorId": "47883226",
            "name": "Jian Xu",
            "hIndex": 23
        },
        "Arash Eshghi": {
            "authorId": "2634217",
            "name": "Arash Eshghi",
            "hIndex": 19
        },
        "Alisia Lupidi": {
            "authorId": "2223643390",
            "name": "A. Lupidi",
            "hIndex": 1
        },
        "Hayato Tsukagoshi": {
            "authorId": "2090616568",
            "name": "Hayato Tsukagoshi",
            "hIndex": 2
        },
        "Tatiana Anikina": {
            "authorId": "4464232",
            "name": "T. A. Anikina",
            "hIndex": 5
        },
        "Rania Saber": {
            "authorId": "91226366",
            "name": "R. Saber",
            "hIndex": 2
        },
        "Ming Tao": {
            "authorId": "48209797",
            "name": "M. Tao",
            "hIndex": 17
        },
        "Dobrik Georgiev": {
            "authorId": "1713791843",
            "name": "Dobrik Georgiev",
            "hIndex": 7
        },
        "Atharva Gundawar": {
            "authorId": "2224613953",
            "name": "Atharva Gundawar",
            "hIndex": 1
        },
        "Tengfei Pan": {
            "authorId": "8801925",
            "name": "T. Pan",
            "hIndex": 6
        },
        "Ming Lu": {
            "authorId": "144654220",
            "name": "Ming Lu",
            "hIndex": 19
        },
        "Qingyu Zhao": {
            "authorId": "1390640437",
            "name": "Qingyun Zhao",
            "hIndex": 7
        },
        "Lynda Tamine": {
            "authorId": "1737705",
            "name": "L. Tamine",
            "hIndex": 22
        },
        "Zhen Guo": {
            "authorId": "2109324140",
            "name": "Zhenyou Guo",
            "hIndex": 6
        },
        "Mingzhen Li": {
            "authorId": "2017954921",
            "name": "Mingzheng Li",
            "hIndex": 6
        },
        "Chengyu Pan": {
            "authorId": "87647905",
            "name": "Chengyu Pan",
            "hIndex": 5
        },
        "Lianli Gao": {
            "authorId": "2671321",
            "name": "Lianli Gao",
            "hIndex": 41
        },
        "Yuchen Su": {
            "authorId": "121873739",
            "name": "Yuchen Su",
            "hIndex": 6
        },
        "Haozhi Qi": {
            "authorId": "7217794",
            "name": "Haozhi Qi",
            "hIndex": 12
        },
        "Xinying Lu": {
            "authorId": "32095229",
            "name": "Xinying Lu",
            "hIndex": 10
        },
        "Ruichu Cai": {
            "authorId": "1856374",
            "name": "Ruichu Cai",
            "hIndex": 20
        },
        "Sheng Xu": {
            "authorId": "73094390",
            "name": "Sheng Xu",
            "hIndex": 27
        },
        "Shian Jia": {
            "authorId": "51175959",
            "name": "S. Jia",
            "hIndex": 9
        },
        "Manel Slokom": {
            "authorId": "2465834",
            "name": "Manel Slokom",
            "hIndex": 5
        },
        "Toshioki Soga": {
            "authorId": "70564388",
            "name": "Toshioki Soga",
            "hIndex": 2
        },
        "Philipp Tuchel": {
            "authorId": "2087435839",
            "name": "Philipp Tuchel",
            "hIndex": 2
        },
        "Kento Kawaharazuka": {
            "authorId": "8308607",
            "name": "Kento Kawaharazuka",
            "hIndex": 11
        },
        "Emmanuel Cand\u00e8s": {
            "authorId": "2006869",
            "name": "E. Cand\u00e8s",
            "hIndex": 99
        },
        "Zhishu Shen": {
            "authorId": "144256495",
            "name": "Zhishu Shen",
            "hIndex": 7
        },
        "Stefan Uhlich": {
            "authorId": "2504395",
            "name": "S. Uhlich",
            "hIndex": 13
        },
        "Jie Wen": {
            "authorId": "2274006",
            "name": "Wenjie Zhao",
            "hIndex": 26
        },
        "Markus Knauer": {
            "authorId": "4798600",
            "name": "Markus Wendelin Knauer",
            "hIndex": 11
        },
        "Muhammad Akhtar Munir": {
            "authorId": "153286823",
            "name": "Muhammad Akhtar Munir",
            "hIndex": 6
        },
        "Inzamamul Alam": {
            "authorId": "2283979266",
            "name": "Inzamamul Alam",
            "hIndex": 1
        },
        "Robert Graubohm": {
            "authorId": "66267909",
            "name": "Robert Graubohm",
            "hIndex": 5
        },
        "Dongyu Xue": {
            "authorId": "30126064",
            "name": "X. Dongyu",
            "hIndex": 10
        },
        "Himanshu Gaurav Singh": {
            "authorId": "2190751709",
            "name": "H. Singh",
            "hIndex": 2
        },
        "Ilia Kuznetsov": {
            "authorId": "145775250",
            "name": "Ilia Kuznetsov",
            "hIndex": 9
        },
        "Haolun Li": {
            "authorId": "2145537404",
            "name": "Haolun Li",
            "hIndex": 7
        },
        "Fuhui Zhou": {
            "authorId": "3311360",
            "name": "Fuhui Zhou",
            "hIndex": 41
        },
        "Yasaman Etesam": {
            "authorId": "84184549",
            "name": "Yasaman Etesam",
            "hIndex": 2
        },
        "Jason Cong": {
            "authorId": "2259796",
            "name": "J. Cong",
            "hIndex": 80
        },
        "Robert Laganiere": {
            "name": "Robert Laganiere",
            "hIndex": 0
        },
        "Philipp Kr\u00e4henb\u00fchl": {
            "authorId": "2562966",
            "name": "Philipp Kr\u00e4henb\u00fchl",
            "hIndex": 40
        },
        "Tucker Balch": {
            "authorId": "1826964",
            "name": "T. Balch",
            "hIndex": 43
        },
        "Gill Hetz": {
            "authorId": "104342583",
            "name": "Gill Hetz",
            "hIndex": 7
        },
        "Jihyun Lee": {
            "authorId": "2108585711",
            "name": "Jihyun Lee",
            "hIndex": 17
        },
        "Danda Paudel": {
            "authorId": "35268081",
            "name": "D. Paudel",
            "hIndex": 23
        },
        "Hanane Djeddal": {
            "authorId": "2122691048",
            "name": "Hanane Djeddal",
            "hIndex": 1
        },
        "Xiaodan Zhu": {
            "authorId": "2130251012",
            "name": "Xiaodan Zhu",
            "hIndex": 5
        },
        "Yung-Chang Hsu": {
            "authorId": "3311470",
            "name": "Yung-Chang Hsu",
            "hIndex": 6
        },
        "Ji-Rong Wen": {
            "authorId": "153693432",
            "name": "Ji-rong Wen",
            "hIndex": 43
        },
        "Martin Kr\u00fcger": {
            "authorId": "143754766",
            "name": "M. Kr\u00fcger",
            "hIndex": 41
        },
        "Thomas Larsen Greiner": {
            "authorId": "144020895",
            "name": "Thomas Greiner",
            "hIndex": 10
        },
        "Chengwei Wu": {
            "authorId": "49762689",
            "name": "Chengwei Wu",
            "hIndex": 7
        },
        "Russell Mendonca": {
            "authorId": "35509365",
            "name": "Russell Mendonca",
            "hIndex": 9
        },
        "Zhanhui Kang": {
            "authorId": "2705857",
            "name": "Zhanhui Kang",
            "hIndex": 5
        },
        "Yanning Zhang": {
            "authorId": "49890870",
            "name": "Yan-Ning Zhang",
            "hIndex": 6
        },
        "Tal Baumel": {
            "authorId": "3171055",
            "name": "Tal Baumel",
            "hIndex": 7
        },
        "Issa Khalil": {
            "authorId": "1783739",
            "name": "Issa M. Khalil",
            "hIndex": 29
        },
        "Kaizhe Fan": {
            "authorId": "2317257525",
            "name": "Kaizhe Fan",
            "hIndex": 0
        },
        "Dan Zhao": {
            "authorId": "2110814690",
            "name": "Dandan Zhao",
            "hIndex": 18
        },
        "Lingqiao Liu": {
            "authorId": "120095791",
            "name": "Ling-ling Liu",
            "hIndex": 7
        },
        "Christopher M. Ackerman": {
            "authorId": "22448248",
            "name": "C. Ackerman",
            "hIndex": 1
        },
        "Barkin Dagda": {
            "authorId": "2302794666",
            "name": "Barkin Dagda",
            "hIndex": 0
        },
        "Anja Surina": {
            "authorId": "83843132",
            "name": "A. Surina",
            "hIndex": 2
        },
        "John C. Wright": {
            "authorId": "144366936",
            "name": "J. C. Wright",
            "hIndex": 37
        },
        "Qiongxiu Li": {
            "authorId": "2108144668",
            "name": "Qiongxiu Li",
            "hIndex": 8
        },
        "Beatrix Suess": {
            "authorId": "6929135",
            "name": "B. Suess",
            "hIndex": 36
        },
        "Zitong Yang": {
            "authorId": "9966584",
            "name": "Z. Pang",
            "hIndex": 5
        },
        "Falaah Arif Khan": {
            "authorId": "1410017982",
            "name": "Falaah Arif Khan",
            "hIndex": 4
        },
        "Tuan-Hy Le": {
            "name": "Tuan-Hy Le",
            "hIndex": 0
        },
        "Jiabin Tang": {
            "authorId": "5365864",
            "name": "Jiabin Tang",
            "hIndex": 33
        },
        "Chenfeng Gao": {
            "authorId": "2114086495",
            "name": "Chen Gao",
            "hIndex": 7
        },
        "Qifeng Li": {
            "authorId": "2108494777",
            "name": "Qifeng Li",
            "hIndex": 16
        },
        "Wei Peng": {
            "authorId": "145439284",
            "name": "Wei Peng",
            "hIndex": 41
        },
        "Yutaka Matsuo": {
            "authorId": "145733486",
            "name": "Y. Matsuo",
            "hIndex": 52
        },
        "Leanne Nortje": {
            "authorId": "102286895",
            "name": "Leanne Nortje",
            "hIndex": 5
        },
        "Elias Stengel-Eskin": {
            "authorId": "1405407255",
            "name": "Elias Stengel-Eskin",
            "hIndex": 9
        },
        "Torsten Bertram": {
            "authorId": "145492266",
            "name": "T. Bertram",
            "hIndex": 27
        },
        "Zeyu Ma": {
            "authorId": "152986997",
            "name": "Ze-ming Ma",
            "hIndex": 5
        },
        "Mohammadmehdi Ataei": {
            "authorId": "145069424",
            "name": "M. Ataei",
            "hIndex": 42
        },
        "Yonggang Wen": {
            "name": "Yonggang Wen",
            "hIndex": 0
        },
        "Lei Chen": {
            "authorId": "47818234",
            "name": "Leilei Chen",
            "hIndex": 14
        },
        "M. Tu\u011fberk \u0130\u015fyapar": {
            "name": "M. Tu\u011fberk \u0130\u015fyapar",
            "hIndex": 0
        },
        "Nils Dycke": {
            "authorId": "1740602147",
            "name": "Nils Dycke",
            "hIndex": 4
        },
        "Mohammad Mehdi Rastikerdar": {
            "authorId": "2264182093",
            "name": "Mohammad Mehdi Rastikerdar",
            "hIndex": 0
        },
        "Elham Khabiri": {
            "authorId": "1840408",
            "name": "Elham Khabiri",
            "hIndex": 9
        },
        "Ga\u00ebl Varoquaux": {
            "authorId": "3025780",
            "name": "G. Varoquaux",
            "hIndex": 58
        },
        "Aoting Hu": {
            "authorId": "120897492",
            "name": "Aoting Hu",
            "hIndex": 2
        },
        "Erin Bransom": {
            "name": "Erin Bransom",
            "hIndex": 0
        },
        "Mingda Li": {
            "authorId": "8679877",
            "name": "Mingda Li",
            "hIndex": 26
        },
        "Patrick Palmer": {
            "authorId": "145029704",
            "name": "P. Palmer",
            "hIndex": 28
        },
        "Aurick Qiao": {
            "authorId": "1762117",
            "name": "Aurick Qiao",
            "hIndex": 7
        },
        "Xingwu Sun": {
            "authorId": "143900241",
            "name": "Xing Sun",
            "hIndex": 30
        },
        "Aviv Shamsian": {
            "authorId": "1587627172",
            "name": "Aviv Shamsian",
            "hIndex": 8
        },
        "Sandro M. Reia": {
            "authorId": "3407230",
            "name": "Sandro M. Reia",
            "hIndex": 7
        },
        "Corey Harper": {
            "authorId": "5249381",
            "name": "Corey D. Harper",
            "hIndex": 9
        },
        "Manuel Marques-Pita": {
            "authorId": "1403109128",
            "name": "Manuel Marques-Pita",
            "hIndex": 9
        },
        "James Thorne": {
            "authorId": "36154603",
            "name": "J. Thorne",
            "hIndex": 35
        },
        "Hao Sun": {
            "authorId": "3145120",
            "name": "Haopeng Sun",
            "hIndex": 33
        },
        "Aleksej Gaj": {
            "authorId": "2186498397",
            "name": "Aleksej Gaj",
            "hIndex": 1
        },
        "Suho Yoo": {
            "authorId": "83718584",
            "name": "S. Yoo",
            "hIndex": 2
        },
        "Fateh A. Tipu": {
            "authorId": "1993991",
            "name": "F. Tipu",
            "hIndex": 7
        },
        "Quanquan Gu": {
            "authorId": "9937103",
            "name": "Quanquan Gu",
            "hIndex": 58
        },
        "Deepika Goyal": {
            "authorId": "37949340",
            "name": "D. Goyal",
            "hIndex": 19
        },
        "Yuning Shen": {
            "authorId": "11693055",
            "name": "Yuning Shen",
            "hIndex": 12
        },
        "Han Zhou": {
            "authorId": "120347355",
            "name": "Hanxiao Zhou",
            "hIndex": 14
        },
        "Weidong Cai": {
            "authorId": "122905659",
            "name": "Weidong (Tom) Cai",
            "hIndex": 46
        },
        "Sebastian J. Wetzel": {
            "authorId": "2094890160",
            "name": "S. Wetzel",
            "hIndex": 9
        },
        "Li Li": {
            "authorId": "2156055147",
            "name": "Li Li",
            "hIndex": 9
        },
        "Minha Kim": {
            "authorId": "105016488",
            "name": "Min-Ha Kim",
            "hIndex": 10
        },
        "Sangwon Ryu": {
            "authorId": "10337739",
            "name": "Sangwon Ryu",
            "hIndex": 7
        },
        "Jinwei He": {
            "authorId": "8516672",
            "name": "Jinwei He",
            "hIndex": 29
        },
        "Lingkai Kong": {
            "authorId": "1557295765",
            "name": "Lingkai Kong",
            "hIndex": 11
        },
        "Shao Yan Shao": {
            "authorId": "144418159",
            "name": "Y. Shao",
            "hIndex": 15
        },
        "Jiazheng Li": {
            "authorId": "2041741238",
            "name": "Jiazheng Li",
            "hIndex": 7
        },
        "Tobias Vente": {
            "authorId": "2239566946",
            "name": "Tobias Vente",
            "hIndex": 2
        },
        "Shuyuan Zheng": {
            "authorId": "2111071258",
            "name": "Shuyuan Zheng",
            "hIndex": 4
        },
        "Fan Zhuo": {
            "authorId": "2113847070",
            "name": "Zhu Fan",
            "hIndex": 2
        },
        "Lukas Schmitz": {
            "authorId": "2122610922",
            "name": "L. Schmitz",
            "hIndex": 2
        },
        "Milad Alshomary": {
            "authorId": "2300829",
            "name": "Milad Alshomary",
            "hIndex": 13
        },
        "Jules Wulms": {
            "authorId": "2809579",
            "name": "J. Wulms",
            "hIndex": 6
        },
        "Mustafa Yildirim": {
            "authorId": "37629610",
            "name": "M. Y\u0131ld\u0131r\u0131m",
            "hIndex": 8
        },
        "Dhruv Srikanth": {
            "authorId": "2154616380",
            "name": "Dhruv Srikanth",
            "hIndex": 1
        },
        "Panagiotis Fouliras": {
            "authorId": "2091623486",
            "name": "Panagiotis E. Fouliras",
            "hIndex": 6
        },
        "Jaewoo Song": {
            "authorId": "116206590",
            "name": "Jae-Gun Song",
            "hIndex": 3
        },
        "T. Aleksandra Ma": {
            "name": "T. Aleksandra Ma",
            "hIndex": 0
        },
        "Caiyan Jia": {
            "authorId": "35654754",
            "name": "Caiyan Jia",
            "hIndex": 17
        },
        "Kai Wang": {
            "authorId": "2150030988",
            "name": "Kai\u2010Kai Wang",
            "hIndex": 9
        },
        "Sian Jin": {
            "authorId": "3393245",
            "name": "Sian Jin",
            "hIndex": 14
        },
        "Mustapha Hemis": {
            "authorId": "2205523",
            "name": "Mustapha Hemis",
            "hIndex": 5
        },
        "Nan Cheng": {
            "authorId": "2202256222",
            "name": "C. Chang",
            "hIndex": 24
        },
        "Michael Barnett": {
            "authorId": "144026377",
            "name": "M. Barnett",
            "hIndex": 40
        },
        "Klaus Werner Schmidt": {
            "authorId": "21449091",
            "name": "K. Schmidt",
            "hIndex": 18
        },
        "Alessandro Suglia": {
            "authorId": "3444866",
            "name": "Alessandro Suglia",
            "hIndex": 9
        },
        "Andrea Silvi": {
            "authorId": "93533387",
            "name": "A. Silvi",
            "hIndex": 2
        },
        "Shurong Cao": {
            "authorId": "49174532",
            "name": "Ju Liang",
            "hIndex": 6
        },
        "Shreyas S R": {
            "authorId": "41210145",
            "name": "S. Managave",
            "hIndex": 12
        },
        "Syed S. Naqvi": {
            "authorId": "32278120",
            "name": "S. Naqvi",
            "hIndex": 23
        },
        "Md. Rafiul Biswas": {
            "authorId": "32267310",
            "name": "Md. Rafiul Biswas",
            "hIndex": 7
        },
        "Wang Zhilin Wang": {
            "authorId": "50218224",
            "name": "Zhiling Wang",
            "hIndex": 10
        },
        "Sahil Jain": {
            "authorId": "1995911",
            "name": "Sahil Jain",
            "hIndex": 12
        },
        "Lu\u00eds Roberto Mercado D\u00edaz": {
            "authorId": "2162695279",
            "name": "Lu\u00eds Roberto Mercado D\u00edaz",
            "hIndex": 1
        },
        "Devis Tuia": {
            "authorId": "2977931",
            "name": "D. Tuia",
            "hIndex": 63
        },
        "Helmut Butzkueven": {
            "authorId": "2184228",
            "name": "H. Butzkueven",
            "hIndex": 66
        },
        "Yanghua Xiao": {
            "authorId": "3011950",
            "name": "Yanghua Xiao",
            "hIndex": 28
        },
        "Ruian He": {
            "authorId": "2118180466",
            "name": "Ruian He",
            "hIndex": 3
        },
        "Giuseppe Samo": {
            "authorId": "73118979",
            "name": "Giuseppe Samo",
            "hIndex": 5
        },
        "Jingjing Qu": {
            "authorId": "11234813",
            "name": "J. Qu",
            "hIndex": 13
        },
        "Riccardo Simionato": {
            "authorId": "9615742",
            "name": "Riccardo Simionato",
            "hIndex": 1
        },
        "Condy Bao": {
            "authorId": "2303269966",
            "name": "Condy Bao",
            "hIndex": 0
        },
        "Chenyu Wang": {
            "authorId": "2109501922",
            "name": "Chenyu Wang",
            "hIndex": 3
        },
        "Soyon Park": {
            "authorId": "2115590681",
            "name": "Soyon Park",
            "hIndex": 7
        },
        "Salman Khan": {
            "authorId": "152973423",
            "name": "Salman Hameed Khan",
            "hIndex": 50
        },
        "Henrique F. de Arruda": {
            "authorId": "1903282",
            "name": "H. F. D. Arruda",
            "hIndex": 10
        },
        "Mehrdad Zakershahrak": {
            "authorId": "51887973",
            "name": "Mehrdad Zakershahrak",
            "hIndex": 6
        },
        "Amit Roth": {
            "authorId": "2310611793",
            "name": "Amit Roth",
            "hIndex": 1
        },
        "Markus Haug": {
            "authorId": "37104114",
            "name": "M. Haug",
            "hIndex": 16
        },
        "Rashik Shahriar Akash": {
            "name": "Rashik Shahriar Akash",
            "hIndex": 0
        },
        "Woojin Chung": {
            "authorId": "2068133612",
            "name": "Woojin Chung",
            "hIndex": 8
        },
        "Mahmoud Nazzal": {
            "authorId": "3644909",
            "name": "S. Nazzal",
            "hIndex": 31
        },
        "Jitendra Malik": {
            "authorId": "143751119",
            "name": "Jitendra Malik",
            "hIndex": 136
        },
        "Shahram Rahimi": {
            "authorId": "145548298",
            "name": "S. Rahimi",
            "hIndex": 17
        },
        "Shiri Dori-Hacohen": {
            "authorId": "1403034406",
            "name": "Shiri Dori-Hacohen",
            "hIndex": 10
        },
        "Kazuhito Koishida": {
            "authorId": "145733034",
            "name": "K. Koishida",
            "hIndex": 16
        },
        "Jia Liu": {
            "authorId": "2108771657",
            "name": "J. Liu",
            "hIndex": 28
        },
        "Meng-Ting Tsai": {
            "authorId": "47331880",
            "name": "Meng-Ting Tsai",
            "hIndex": 7
        },
        "Wenbo Ding": {
            "authorId": "49248419",
            "name": "W. Ding",
            "hIndex": 38
        },
        "Haider Ali": {
            "authorId": "49287236",
            "name": "A. Haider",
            "hIndex": 13
        },
        "Hanyu Zhang": {
            "authorId": "48213255",
            "name": "Hanyu Zhang",
            "hIndex": 12
        },
        "Massa Baali": {
            "authorId": "1380273855",
            "name": "Massa Baali",
            "hIndex": 4
        },
        "Nada Saadi": {
            "authorId": "2071622141",
            "name": "S. Nada",
            "hIndex": 2
        },
        "Iryna Gurevych": {
            "authorId": "1730400",
            "name": "Iryna Gurevych",
            "hIndex": 76
        },
        "Jizhan Fang": {
            "name": "Jizhan Fang",
            "hIndex": 0
        },
        "Georg Kucsko": {
            "authorId": "4301543",
            "name": "G. Kucsko",
            "hIndex": 10
        },
        "Samuel Yen-Chi Chen": {
            "authorId": "2107968379",
            "name": "Samuel Yen-Chi Chen",
            "hIndex": 17
        },
        "Yikai Han": {
            "authorId": "2108813267",
            "name": "Yi Han",
            "hIndex": 7
        },
        "Axel Sikora": {
            "authorId": "1727859",
            "name": "A. Sikora",
            "hIndex": 17
        },
        "Hamza Kheddar": {
            "authorId": "70335909",
            "name": "Hamza Kheddar",
            "hIndex": 8
        },
        "Chonghao Sima": {
            "authorId": "2144553163",
            "name": "Chonghao Sima",
            "hIndex": 10
        },
        "Bolun Wang": {
            "authorId": "2153206874",
            "name": "Bo Wang",
            "hIndex": 7
        },
        "Andrew Lowy": {
            "authorId": "3760359",
            "name": "A. Lowy",
            "hIndex": 70
        },
        "Yetao Wu": {
            "authorId": "4092737",
            "name": "Yetao Wu",
            "hIndex": 5
        },
        "Naoyasu Ubayashi": {
            "authorId": "1806862",
            "name": "Naoyasu Ubayashi",
            "hIndex": 19
        },
        "Jianping Fan": {
            "authorId": "1692580",
            "name": "Jianping Fan",
            "hIndex": 18
        },
        "Peng Xu": {
            "authorId": "50590957",
            "name": "P. Xu",
            "hIndex": 28
        },
        "Jane Wu": {
            "authorId": "152327375",
            "name": "Jane Y. Wu",
            "hIndex": 61
        },
        "Lerrel Pinto": {
            "authorId": "34026610",
            "name": "Lerrel Pinto",
            "hIndex": 36
        },
        "Paola Merlo": {
            "name": "Paola Merlo",
            "hIndex": 0
        },
        "Zijian Li": {
            "authorId": "46947383",
            "name": "Zijian Li",
            "hIndex": 18
        },
        "Edgar Altszyler": {
            "name": "Edgar Altszyler",
            "hIndex": 0
        },
        "Vianna Cramer": {
            "name": "Vianna Cramer",
            "hIndex": 0
        },
        "Akash Saravanan": {
            "authorId": "2113903742",
            "name": "Akash Saravanan",
            "hIndex": 3
        },
        "Yingying Hua": {
            "authorId": "6373228",
            "name": "H. Yingying",
            "hIndex": 10
        },
        "Shucong Zhang": {
            "authorId": "2116136004",
            "name": "Shucong Zhang",
            "hIndex": 7
        },
        "Aneesh Sharma": {
            "authorId": "2109669217",
            "name": "Aneesh Sharma",
            "hIndex": 16
        },
        "Shahzaib Iqbal": {
            "authorId": "25652272",
            "name": "Shahzaib Iqbal",
            "hIndex": 7
        },
        "Musab Alsheikh": {
            "authorId": "98421997",
            "name": "Musab Alsheikh",
            "hIndex": 1
        },
        "Kieran Parsons": {
            "authorId": "2429521",
            "name": "K. Parsons",
            "hIndex": 23
        },
        "Yizhi Li": {
            "authorId": "2051696260",
            "name": "Yizhi Li",
            "hIndex": 3
        },
        "J. Yin": {
            "authorId": "2107807680",
            "name": "J. Yin",
            "hIndex": 38
        },
        "Hyeongju Kim": {
            "authorId": "2109894594",
            "name": "Hyeongju Kim",
            "hIndex": 8
        },
        "Haiming Wang": {
            "authorId": "46490972",
            "name": "Wang Haiming",
            "hIndex": 6
        },
        "Jaydeep Sen": {
            "authorId": "2450381",
            "name": "Jaydeep Sen",
            "hIndex": 8
        },
        "Weigang Zhang": {
            "authorId": "2108142456",
            "name": "Weigang Zhang",
            "hIndex": 23
        },
        "Thomas Seidel": {
            "authorId": "32209840",
            "name": "T. Seidel",
            "hIndex": 18
        },
        "Mirco Musolesi": {
            "authorId": "1806767",
            "name": "Mirco Musolesi",
            "hIndex": 53
        },
        "Tianyi Zhang": {
            "authorId": "81427786",
            "name": "Tian-Yi Zhang",
            "hIndex": 9
        },
        "Junzhou Huang": {
            "authorId": "1768190",
            "name": "Junzhou Huang",
            "hIndex": 76
        },
        "Asim Naveed": {
            "authorId": "144176639",
            "name": "M. Ikram",
            "hIndex": 39
        },
        "Lothar Narins": {
            "authorId": "49948981",
            "name": "Lothar Narins",
            "hIndex": 5
        },
        "Matthew Guzdial": {
            "authorId": "144599741",
            "name": "Matthew J. Guzdial",
            "hIndex": 14
        },
        "Oleksii Hrinchuk": {
            "authorId": "29891116",
            "name": "Oleksii Hrinchuk",
            "hIndex": 12
        },
        "Karol Bot": {
            "authorId": "23708418",
            "name": "K. Bot",
            "hIndex": 8
        },
        "Sung Won Han": {
            "authorId": "153114402",
            "name": "S. Han",
            "hIndex": 16
        },
        "Kangjun Lee": {
            "authorId": "3234544",
            "name": "Kangjun Lee",
            "hIndex": 2
        },
        "Wenyu Zhang": {
            "name": "Wenyu Zhang",
            "hIndex": 0
        },
        "Willmar Sosa Diaz": {
            "authorId": "2311698710",
            "name": "Willmar Sosa Diaz",
            "hIndex": 0
        },
        "Zhaoyue Sun": {
            "authorId": "2130315926",
            "name": "Zhao-Yan Sun",
            "hIndex": 19
        },
        "Jiajun Wu": {
            "authorId": "2110436206",
            "name": "Jia-jing Wu",
            "hIndex": 7
        },
        "Julius Steuer": {
            "name": "Julius Steuer",
            "hIndex": 0
        },
        "Lorenz Linhardt": {
            "authorId": "4271465",
            "name": "Lorenz Linhardt",
            "hIndex": 6
        },
        "Jibin Jia": {
            "authorId": "2005255211",
            "name": "Jibin Jia",
            "hIndex": 3
        },
        "Ronay Ak": {
            "authorId": "50309003",
            "name": "Ronay Ak",
            "hIndex": 12
        },
        "Joy Lim Jia Yin": {
            "name": "Joy Lim Jia Yin",
            "hIndex": 0
        },
        "Claire Liang": {
            "authorId": "34881338",
            "name": "Claire Y. C. Liang",
            "hIndex": 5
        },
        "Linnea Wahlgren": {
            "authorId": "80390679",
            "name": "Linnea Wahlgren",
            "hIndex": 0
        },
        "Shize Su": {
            "authorId": "1816464",
            "name": "Shize Su",
            "hIndex": 8
        },
        "Marina Ceccon": {
            "authorId": "82986186",
            "name": "M. Ceccon",
            "hIndex": 1
        },
        "Peiqi Gao": {
            "name": "Peiqi Gao",
            "hIndex": 0
        },
        "Guoheng Huang": {
            "authorId": "2376100",
            "name": "Guoheng Huang",
            "hIndex": 8
        },
        "Yingjie Li": {
            "authorId": "2201603277",
            "name": "Yingjie Li",
            "hIndex": 39
        },
        "Ronnie Rajan": {
            "authorId": "150282268",
            "name": "Ronnie Rajan",
            "hIndex": 2
        },
        "Rongfeng Lu": {
            "authorId": "117031481",
            "name": "R. Lu",
            "hIndex": 12
        },
        "Minghao Fu": {
            "authorId": "2158818865",
            "name": "Minghao Fu",
            "hIndex": 4
        },
        "Runji Lin": {
            "authorId": "2167032295",
            "name": "Runji Lin",
            "hIndex": 5
        },
        "Emily Cheng": {
            "authorId": "144435687",
            "name": "E. Cheng",
            "hIndex": 55
        },
        "Lu Wang": {
            "authorId": "114310654",
            "name": "Lulu Wang",
            "hIndex": 15
        },
        "Achille Fokoue": {
            "authorId": "2297836",
            "name": "Achille Fokoue",
            "hIndex": 26
        },
        "Yong Luo": {
            "authorId": "50984269",
            "name": "Yong Luo",
            "hIndex": 25
        },
        "Sami Bourouis": {
            "authorId": "2772755",
            "name": "Sami Bourouis",
            "hIndex": 29
        },
        "Yanbin Wei": {
            "authorId": "2157966819",
            "name": "Yanbing Wei",
            "hIndex": 6
        },
        "Daniele Malitesta": {
            "authorId": "1768656900",
            "name": "Daniele Malitesta",
            "hIndex": 7
        },
        "Vishwajeet Kumar": {
            "authorId": "2112128933",
            "name": "Vishwajeet Kumar",
            "hIndex": 11
        },
        "Giri Raju": {
            "authorId": "7406740",
            "name": "Giri R Sura",
            "hIndex": 4
        },
        "Leon Guertler": {
            "authorId": "2302797460",
            "name": "Leon Guertler",
            "hIndex": 1
        },
        "Haowei Cheng": {
            "authorId": "2086922",
            "name": "Hao-Chun Cheng",
            "hIndex": 18
        },
        "Stan Z. Li": {
            "authorId": "1390908654",
            "name": "Stan Z. Li",
            "hIndex": 34
        },
        "Devan Williams": {
            "authorId": "2115972125",
            "name": "Devan Williams",
            "hIndex": 2
        },
        "Jovan Jeromela": {
            "authorId": "2174371952",
            "name": "Jovan Jeromela",
            "hIndex": 2
        },
        "Stefan Wagner": {
            "authorId": "144085154",
            "name": "S. Wagner",
            "hIndex": 15
        },
        "Dibakar Sen": {
            "authorId": "1890363",
            "name": "Dibakar Sen",
            "hIndex": 8
        },
        "Steven McDonald": {
            "authorId": "2232312",
            "name": "Steve McDonald",
            "hIndex": 26
        },
        "Yanfeng Wang": {
            "authorId": "48665289",
            "name": "W. Yanfeng",
            "hIndex": 9
        },
        "Mohammed Alsaafin": {
            "name": "Mohammed Alsaafin",
            "hIndex": 0
        },
        "Fuxiang Huang": {
            "authorId": "48677449",
            "name": "F. Huang",
            "hIndex": 9
        },
        "Chieh-Hsin Lai": {
            "authorId": "153892533",
            "name": "Chieh-Hsin Lai",
            "hIndex": 9
        },
        "Jennifer D'Souza": {
            "authorId": "1409093271",
            "name": "Jennifer D\u2019Souza",
            "hIndex": 14
        },
        "Stavros Kalafatis": {
            "name": "Stavros Kalafatis",
            "hIndex": 0
        },
        "Ahmed Sabir": {
            "authorId": "94575167",
            "name": "A. Sabir",
            "hIndex": 17
        },
        "Ni Li": {
            "name": "Ni Li",
            "hIndex": 0
        },
        "Mirek Riedewald": {
            "authorId": "1722086",
            "name": "Mirek Riedewald",
            "hIndex": 30
        },
        "Haroon Ahmed Khan": {
            "authorId": "2056139476",
            "name": "H. A. Khan",
            "hIndex": 5
        },
        "Bin Wang": {
            "authorId": "2152593027",
            "name": "B. Wang",
            "hIndex": 9
        },
        "Yuki Saito": {
            "authorId": "72416123",
            "name": "Yu Saito",
            "hIndex": 19
        },
        "Denis Kulandin": {
            "name": "Denis Kulandin",
            "hIndex": 0
        },
        "Tianyuan Zhang": {
            "authorId": "49105027",
            "name": "Tian-Ming Zhang",
            "hIndex": 14
        },
        "Wolfgang Gatterbauer": {
            "authorId": "2000882",
            "name": "Wolfgang Gatterbauer",
            "hIndex": 26
        },
        "Simon Batzner": {
            "authorId": "96427180",
            "name": "Simon L. Batzner",
            "hIndex": 11
        },
        "Gaozheng Zhang": {
            "authorId": "1826307",
            "name": "Gaozheng Zhang",
            "hIndex": 1
        },
        "Deovrat Mehendale": {
            "authorId": "2290017914",
            "name": "Deovrat Mehendale",
            "hIndex": 1
        },
        "Antonin Sulc": {
            "authorId": "2643868",
            "name": "Antonin Sulc",
            "hIndex": 7
        },
        "Fugen Zhou": {
            "authorId": "145592139",
            "name": "Fugen Zhou",
            "hIndex": 5
        },
        "Ralph Peeters": {
            "authorId": "2055462710",
            "name": "R. Peeters",
            "hIndex": 9
        },
        "Peicheng Wu": {
            "authorId": "84282825",
            "name": "Peicheng Wu",
            "hIndex": 7
        },
        "Jaewoo Kang": {
            "authorId": "144323862",
            "name": "Jaewoo Kang",
            "hIndex": 43
        },
        "Laura Fleig": {
            "authorId": "2183059791",
            "name": "Laura Fleig",
            "hIndex": 0
        },
        "Samuel D. Relton": {
            "authorId": "2643233",
            "name": "Samuel D. Relton",
            "hIndex": 17
        },
        "Muhammad Saleem": {
            "authorId": "152893067",
            "name": "M. Saleem",
            "hIndex": 13
        },
        "Lav R. Varshney": {
            "authorId": "1697944",
            "name": "L. Varshney",
            "hIndex": 31
        },
        "Yingling Lu": {
            "authorId": "48518263",
            "name": "Yinglin Lu",
            "hIndex": 8
        },
        "Yuhou Xia": {
            "authorId": "9689501",
            "name": "Yuhou Xia",
            "hIndex": 6
        },
        "Hui Huang": {
            "authorId": "48186626",
            "name": "Hui Huang",
            "hIndex": 71
        },
        "Viviana Cotik": {
            "authorId": "2292216",
            "name": "Viviana Cotik",
            "hIndex": 9
        },
        "Yongbo Zhang": {
            "authorId": "7550767",
            "name": "Yongbo Zhang",
            "hIndex": 13
        },
        "Peter Knees": {
            "authorId": "1729484",
            "name": "Peter Knees",
            "hIndex": 28
        },
        "Ken Nakagaki": {
            "authorId": "40059027",
            "name": "Ken Nakagaki",
            "hIndex": 14
        },
        "Saihao Yan": {
            "name": "Saihao Yan",
            "hIndex": 0
        },
        "Yidan Xing": {
            "name": "Yidan Xing",
            "hIndex": 0
        },
        "Eugene Bykovets": {
            "authorId": "2026327923",
            "name": "Eugene Bykovets",
            "hIndex": 1
        },
        "Longhao Zhang": {
            "authorId": "2108280634",
            "name": "Longhao Zhang",
            "hIndex": 11
        },
        "Haoning Wu": {
            "authorId": "120155330",
            "name": "Haoning Wu",
            "hIndex": 18
        },
        "Yi-Cheng Wang": {
            "authorId": "47074311",
            "name": "Cheng-Yi Wang",
            "hIndex": 35
        },
        "Jing Liu": {
            "authorId": "144299696",
            "name": "J. Liu",
            "hIndex": 8
        },
        "Minghui Du": {
            "authorId": "31866339",
            "name": "Minghui Du",
            "hIndex": 11
        },
        "Georgios Pantazopoulos": {
            "authorId": "152644479",
            "name": "G. Pantazopoulos",
            "hIndex": 3
        },
        "Ant\u00f3nio Farinhas": {
            "authorId": "1748971692",
            "name": "Ant\u00f3nio Farinhas",
            "hIndex": 6
        },
        "Zhen Yu": {
            "name": "Zhen Yu",
            "hIndex": 0
        },
        "Peihao Dong": {
            "authorId": "2558626",
            "name": "Peihao Dong",
            "hIndex": 13
        },
        "Weiming Lu": {
            "authorId": "1776903",
            "name": "Weiming Lu",
            "hIndex": 21
        },
        "Pedro Beltr\u00e1n L\u00f3pez": {
            "authorId": "2107165734",
            "name": "A. Hern\u00e2ndez",
            "hIndex": 6
        },
        "Kenny Chirino Ampudia": {
            "name": "Kenny Chirino Ampudia",
            "hIndex": 0
        },
        "Francesco Vaccarino": {
            "authorId": "6042194",
            "name": "F. Vaccarino",
            "hIndex": 16
        },
        "MunYong Yi": {
            "authorId": "50750445",
            "name": "Mun-gyu Yi",
            "hIndex": 1
        },
        "Zehong Lin": {
            "authorId": "2025909",
            "name": "Li Zehong",
            "hIndex": 6
        },
        "Alexander del Rio": {
            "authorId": "2107274143",
            "name": "A. Fern\u00e1ndez",
            "hIndex": 8
        },
        "Qi Qin": {
            "authorId": "50086659",
            "name": "Qi Qin",
            "hIndex": 13
        },
        "Ji\u0159\u00ed Wiedermann": {
            "name": "Ji\u0159\u00ed Wiedermann",
            "hIndex": 0
        },
        "Shengwei Zhao": {
            "authorId": "152836873",
            "name": "Shengwei Zhao",
            "hIndex": 11
        },
        "Yuchuan Wu": {
            "authorId": "2142637404",
            "name": "Yuchuan Wu",
            "hIndex": 10
        },
        "Hironori Washizaki": {
            "authorId": "1794290",
            "name": "H. Washizaki",
            "hIndex": 25
        },
        "Ted Briscoe": {
            "authorId": "145693410",
            "name": "Ted Briscoe",
            "hIndex": 48
        },
        "Yisen Wang": {
            "authorId": "2115869684",
            "name": "Yisen Wang",
            "hIndex": 22
        },
        "Kuiyun Chen": {
            "name": "Kuiyun Chen",
            "hIndex": 0
        },
        "Tuba Gokhan": {
            "authorId": "2135089057",
            "name": "Tuba Gokhan",
            "hIndex": 2
        },
        "Manuel V. Loureiro": {
            "authorId": "40493203",
            "name": "A. Silva",
            "hIndex": 15
        },
        "Armin Saadat": {
            "authorId": "2143245639",
            "name": "Armin Saadat",
            "hIndex": 2
        },
        "Yanjie Zhao": {
            "authorId": "1661293968",
            "name": "Yan-Jie Zhao",
            "hIndex": 17
        },
        "Joseph A. Gallego-Mejia": {
            "authorId": "2042087198",
            "name": "Joseph A. Gallego-Mejia",
            "hIndex": 4
        },
        "Demet Demir": {
            "authorId": "7982961",
            "name": "D. Demir",
            "hIndex": 13
        },
        "Thierry Langer": {
            "authorId": "35167984",
            "name": "T. Langer",
            "hIndex": 52
        },
        "Mori Kurokawa": {
            "authorId": "1689172",
            "name": "M. Kurokawa",
            "hIndex": 7
        },
        "Robert Kaufman": {
            "authorId": "49671593",
            "name": "R. Kaufman",
            "hIndex": 32
        },
        "Jiahao Nick Li": {
            "authorId": "2289777040",
            "name": "Jiahao Nick Li",
            "hIndex": 5
        },
        "Mohamed Bayan Kmainasi": {
            "name": "Mohamed Bayan Kmainasi",
            "hIndex": 0
        },
        "Tariq M. Khan": {
            "authorId": "20688889",
            "name": "T. Khan",
            "hIndex": 24
        },
        "S. M. Saiful Islam Badhon": {
            "authorId": "1581954936",
            "name": "S. M. S. I. Badhon",
            "hIndex": 2
        },
        "Zora Zhiruo Wang": {
            "authorId": "2307569999",
            "name": "Zora Zhiruo Wang",
            "hIndex": 1
        },
        "Yufei Zhou": {
            "authorId": "1518113329",
            "name": "Yufei Zhou",
            "hIndex": 16
        },
        "Marco A. Mart\u00ednez-Ram\u00edrez": {
            "authorId": "152378090",
            "name": "M. Mu\u00f1oz",
            "hIndex": 9
        },
        "Tien-Hong Lo": {
            "authorId": "31773575",
            "name": "Tien-Hong Lo",
            "hIndex": 8
        },
        "Deokhyung Kang": {
            "authorId": "2275466066",
            "name": "Deokhyung Kang",
            "hIndex": 0
        },
        "Shoutao Guo": {
            "authorId": "2188348159",
            "name": "Shoutao Guo",
            "hIndex": 6
        },
        "Tirthankar Ghosal": {
            "authorId": "13427974",
            "name": "Tirthankar Ghosal",
            "hIndex": 14
        },
        "Erick Arias": {
            "authorId": "1403783179",
            "name": "Erick Ram\u00edrez-\u00c1rias",
            "hIndex": 5
        },
        "Nikolaos Tziavelis": {
            "authorId": "1396775510",
            "name": "Nikolaos Tziavelis",
            "hIndex": 7
        },
        "Tom\u00e1\u0161 Kliegr": {
            "authorId": "2005670",
            "name": "Tom\u00e1\u0161 Kliegr",
            "hIndex": 15
        },
        "Zheng Liu": {
            "authorId": "2145976054",
            "name": "Zheng Liu",
            "hIndex": 27
        },
        "Tushar Khot": {
            "authorId": "2236429",
            "name": "Tushar Khot",
            "hIndex": 37
        },
        "Asen Nachkov": {
            "authorId": "2226782786",
            "name": "Asen Nachkov",
            "hIndex": 1
        },
        "Tomoya Iwakura": {
            "authorId": "34758951",
            "name": "Tomoya Iwakura",
            "hIndex": 7
        },
        "Manjia Liang": {
            "name": "Manjia Liang",
            "hIndex": 0
        },
        "Michele Mancusi": {
            "authorId": "16987772",
            "name": "Michele Mancusi",
            "hIndex": 4
        },
        "Junu Kim": {
            "authorId": "2109160165",
            "name": "Junuk Kim",
            "hIndex": 13
        },
        "Eleanor Chodroff": {
            "authorId": "3448883",
            "name": "Eleanor Chodroff",
            "hIndex": 12
        },
        "He Li": {
            "authorId": "153175263",
            "name": "Hehe Li",
            "hIndex": 30
        },
        "Shuyue Guo": {
            "authorId": "2279398873",
            "name": "Shuyue Guo",
            "hIndex": 3
        },
        "Johannes Frey": {
            "authorId": "21114794",
            "name": "D. Frey",
            "hIndex": 15
        },
        "Jun Zhang": {
            "authorId": "47539882",
            "name": "Jun Zhang",
            "hIndex": 12
        },
        "Xinyang Shao": {
            "authorId": "47403728",
            "name": "X. Shao",
            "hIndex": 4
        },
        "Fei Wu": {
            "authorId": "48092886",
            "name": "Feixiang Wu",
            "hIndex": 22
        },
        "Saeed Anwar": {
            "authorId": "144455127",
            "name": "A. Saeed",
            "hIndex": 31
        },
        "Yan Zhou": {
            "authorId": "88991223",
            "name": "Yan-Ting Zhou",
            "hIndex": 21
        },
        "Deval Mehta": {
            "authorId": "2363364",
            "name": "D. Mehta",
            "hIndex": 8
        },
        "Anders \u00c5gren Thun\u00e9": {
            "authorId": "2296605951",
            "name": "Anders \u00c5gren Thun\u00e9",
            "hIndex": 0
        },
        "Jonathan K. Kummerfeld": {
            "authorId": "1727211",
            "name": "Jonathan K. Kummerfeld",
            "hIndex": 23
        },
        "Roberta Raileanu": {
            "authorId": "48647153",
            "name": "Roberta Raileanu",
            "hIndex": 19
        },
        "Francesco Pasti": {
            "authorId": "2316947494",
            "name": "Francesco Pasti",
            "hIndex": 0
        },
        "Panagiotis Sarigiannidis": {
            "authorId": "1802117",
            "name": "P. Sarigiannidis",
            "hIndex": 35
        },
        "Bryan Kian Hsiang Low": {
            "authorId": "40222533",
            "name": "B. Low",
            "hIndex": 12
        },
        "Leyang Cui": {
            "authorId": "152496687",
            "name": "Leyang Cui",
            "hIndex": 16
        },
        "Na Min An": {
            "authorId": "2109751732",
            "name": "Na An",
            "hIndex": 2
        },
        "Qiong Wu": {
            "authorId": "2108719008",
            "name": "Q. Wu",
            "hIndex": 6
        },
        "Xiong Liu": {
            "authorId": "92041676",
            "name": "X. Liu",
            "hIndex": 34
        },
        "Sait Cakmak": {
            "authorId": "1811232061",
            "name": "Sait Cakmak",
            "hIndex": 4
        },
        "Wei-Liang Qian": {
            "authorId": "153289382",
            "name": "W. Qian",
            "hIndex": 4
        },
        "Bumsub Ham": {
            "authorId": "38723538",
            "name": "Bumsub Ham",
            "hIndex": 30
        },
        "Muyang Fang": {
            "name": "Muyang Fang",
            "hIndex": 0
        },
        "Francesco Santini": {
            "name": "Francesco Santini",
            "hIndex": 0
        },
        "Zhengrui Ma": {
            "authorId": "2125040671",
            "name": "Zhengrui Ma",
            "hIndex": 7
        },
        "Lirong Wu": {
            "authorId": "50789930",
            "name": "Li-rong Wu",
            "hIndex": 7
        },
        "Gautier Viaud": {
            "authorId": "46760349",
            "name": "Gautier Viaud",
            "hIndex": 6
        },
        "Ashish Gaurav": {
            "authorId": "1413791825",
            "name": "G. Kamat",
            "hIndex": 11
        },
        "Jaeho Lee": {
            "authorId": "2108805324",
            "name": "Jaeho Lee",
            "hIndex": 11
        },
        "Qianli Wang": {
            "authorId": "5939230",
            "name": "Qianli Wang",
            "hIndex": 14
        },
        "Yao He": {
            "authorId": "2734257",
            "name": "Yaoyao He",
            "hIndex": 20
        },
        "\u015eule G\u00fcnd\u00fcz \u00d6\u011f\u00fcd\u00fcc\u00fc": {
            "authorId": "7782730",
            "name": "\u015e. \u00d6\u011f\u00fcd\u00fcc\u00fc",
            "hIndex": 19
        },
        "Xiaohui Li": {
            "authorId": "50391474",
            "name": "L. Xiaohui",
            "hIndex": 16
        },
        "Berk Cicek": {
            "authorId": "2268319998",
            "name": "Berk Cicek",
            "hIndex": 0
        },
        "Muqing Li": {
            "authorId": "2000235330",
            "name": "Mu Li",
            "hIndex": 20
        },
        "Bo Li": {
            "authorId": "39064211",
            "name": "B. Li",
            "hIndex": 20
        },
        "Umm-e- Habiba": {
            "authorId": "34342537",
            "name": "U. Habiba",
            "hIndex": 17
        },
        "Boyi Li": {
            "authorId": "2132473300",
            "name": "Boyi Li",
            "hIndex": 10
        },
        "Archiki Prasad": {
            "authorId": "1677896557",
            "name": "Archiki Prasad",
            "hIndex": 7
        },
        "Xiangxu Meng": {
            "authorId": "2190054540",
            "name": "XiangXu Meng",
            "hIndex": 2
        },
        "Lingxi Xiao": {
            "authorId": "2308421811",
            "name": "Lingxi Xiao",
            "hIndex": 4
        },
        "Deyu Meng": {
            "authorId": "1803714",
            "name": "Deyu Meng",
            "hIndex": 69
        },
        "Vera Schmitt": {
            "authorId": "5760009",
            "name": "V. L. Schmitt",
            "hIndex": 6
        },
        "Arda Sarp Yenicesu": {
            "authorId": "2261397330",
            "name": "Arda Sarp Yenicesu",
            "hIndex": 0
        },
        "Bornstein": {
            "authorId": "22206430",
            "name": "B. Bornstein",
            "hIndex": 21
        },
        "Khanh-Van Tran": {
            "authorId": "1903848",
            "name": "V. Tran",
            "hIndex": 8
        },
        "Zhiqiang Zhang": {
            "authorId": "48813297",
            "name": "Zhang Zhiqiang",
            "hIndex": 13
        },
        "Gabriele Pergola": {
            "authorId": "46922295",
            "name": "Gabriele Pergola",
            "hIndex": 10
        },
        "Kannan Achan": {
            "authorId": "1684085",
            "name": "Kannan Achan",
            "hIndex": 16
        },
        "Nasir Saleem": {
            "authorId": "144751765",
            "name": "S. Nasir",
            "hIndex": 21
        },
        "Chun Wang": {
            "authorId": "2108895713",
            "name": "C. Wang",
            "hIndex": 8
        },
        "Qijiong Liu": {
            "authorId": "150270469",
            "name": "Qijiong Liu",
            "hIndex": 5
        },
        "Bernardo Gon\u00e7alves": {
            "authorId": "39601837",
            "name": "V. Bernardo",
            "hIndex": 9
        },
        "H. Zhang": {
            "authorId": "40259946",
            "name": "H. Zhang",
            "hIndex": 7
        },
        "Mayinuer Yusufu": {
            "authorId": "17362261",
            "name": "Mayinuer Yusufu",
            "hIndex": 13
        },
        "Ryan Marcus": {
            "authorId": "40412858",
            "name": "Ryan Marcus",
            "hIndex": 19
        },
        "Zhuqing Liu": {
            "authorId": "46271268",
            "name": "Zhuqing Liu",
            "hIndex": 7
        },
        "Hui Zou": {
            "authorId": "32272466",
            "name": "Huifang Zou",
            "hIndex": 7
        },
        "Bo Liang": {
            "authorId": "144824690",
            "name": "Weibo Liang",
            "hIndex": 32
        },
        "Martin N\u00f6llenburg": {
            "name": "Martin N\u00f6llenburg",
            "hIndex": 0
        },
        "Francesco Della Santa": {
            "authorId": "2088329461",
            "name": "F. D. Santa",
            "hIndex": 4
        },
        "Yiming Shi": {
            "name": "Yiming Shi",
            "hIndex": 0
        },
        "Haojian Huang": {
            "authorId": "2296857672",
            "name": "Haojian Huang",
            "hIndex": 3
        },
        "Jianwu Wang": {
            "authorId": "1926440628",
            "name": "Jianwu Wang",
            "hIndex": 6
        },
        "Chang liu": {
            "authorId": "2117036314",
            "name": "Chang\u2010jun Liu",
            "hIndex": 51
        },
        "Yong Yuan": {
            "authorId": "51093237",
            "name": "Yong Yuan",
            "hIndex": 41
        },
        "Thomas Elboth": {
            "authorId": "2996263",
            "name": "T. Elboth",
            "hIndex": 11
        },
        "Pere Barlet-Ros": {
            "authorId": "1398658366",
            "name": "P. Barlet-Ros",
            "hIndex": 27
        },
        "Yinyi Guo": {
            "authorId": "3264207",
            "name": "Yinyi Guo",
            "hIndex": 3
        },
        "Carolyn M. Baum": {
            "authorId": "145743776",
            "name": "C. Baum",
            "hIndex": 36
        },
        "Xing Li": {
            "authorId": "47029878",
            "name": "L. Xing",
            "hIndex": 24
        },
        "Nirmalya Thakur": {
            "authorId": "35948332",
            "name": "Nirmalya Thakur",
            "hIndex": 14
        },
        "Minzheng Wang": {
            "authorId": "6923107",
            "name": "Minzheng Wang",
            "hIndex": 10
        },
        "Changling Li": {
            "authorId": "2619051",
            "name": "Changlin Li",
            "hIndex": 8
        },
        "Yossi Adi": {
            "authorId": "2727584",
            "name": "Yossi Adi",
            "hIndex": 30
        },
        "Travis Bartley": {
            "authorId": "2042094",
            "name": "T. Bartley",
            "hIndex": 5
        },
        "Yuyang Zhang": {
            "authorId": "121310862",
            "name": "Yuyang Zhang",
            "hIndex": 14
        },
        "Deepak Pathak": {
            "authorId": "38236002",
            "name": "Deepak Pathak",
            "hIndex": 30
        },
        "Caglar Gulcehre": {
            "authorId": "146372255",
            "name": "Caglar Gulcehre",
            "hIndex": 13
        },
        "Ruoxi Jia": {
            "authorId": "39823639",
            "name": "R. Jia",
            "hIndex": 28
        },
        "James Geller": {
            "authorId": "47432972",
            "name": "R. Geller",
            "hIndex": 36
        },
        "Yucong Luo": {
            "name": "Yucong Luo",
            "hIndex": 0
        },
        "Quanjun Li": {
            "authorId": "4953361",
            "name": "Quanjun Li",
            "hIndex": 28
        },
        "Zachary G. Ives": {
            "authorId": "1804315",
            "name": "Z. Ives",
            "hIndex": 42
        },
        "Chi-Man Pun": {
            "authorId": "145956876",
            "name": "Chi-Man Pun",
            "hIndex": 38
        },
        "Longze Chen": {
            "name": "Longze Chen",
            "hIndex": 0
        },
        "Mingyang Yin": {
            "authorId": "47842382",
            "name": "Mingyan Liu",
            "hIndex": 24
        },
        "Wei Xia": {
            "authorId": "153664340",
            "name": "Xia-Wei Wei",
            "hIndex": 21
        },
        "Charlie Griffin": {
            "authorId": "96523875",
            "name": "C. Griffin",
            "hIndex": 4
        },
        "Alba Jorquera Jimenez de Aberasturi": {
            "name": "Alba Jorquera Jimenez de Aberasturi",
            "hIndex": 0
        },
        "Lavon Davis": {
            "authorId": "2264003242",
            "name": "Lavon Davis",
            "hIndex": 1
        },
        "Phillip Howard": {
            "authorId": "48575315",
            "name": "Phillip Howard",
            "hIndex": 7
        },
        "Chengxin Gao": {
            "authorId": "14618134",
            "name": "C. Gao",
            "hIndex": 3
        },
        "Azal Ahmad Khan": {
            "authorId": "2163674593",
            "name": "A. Khan",
            "hIndex": 2
        },
        "Yuxuan Liu": {
            "authorId": "2108135769",
            "name": "Yuxuan Liu",
            "hIndex": 12
        },
        "Khiem Ton": {
            "name": "Khiem Ton",
            "hIndex": 0
        },
        "Jianshu Hu": {
            "authorId": "14603101",
            "name": "Jianshu Hu",
            "hIndex": 8
        },
        "Nico Potyka": {
            "authorId": "2716132",
            "name": "Nico Potyka",
            "hIndex": 14
        },
        "Fahad Shahbaz Khan": {
            "authorId": "2358803",
            "name": "F. Khan",
            "hIndex": 78
        },
        "Keith A. Dufendach": {
            "authorId": "11303818",
            "name": "K. Dufendach",
            "hIndex": 10
        },
        "Leandros Tassiulas": {
            "authorId": "1705536",
            "name": "L. Tassiulas",
            "hIndex": 73
        },
        "Neil Band": {
            "authorId": "1726096678",
            "name": "Neil Band",
            "hIndex": 6
        },
        "MD Al Amin Shuvo": {
            "authorId": "2232355194",
            "name": "Md. Al-Amin Bhuiyan Shuvo",
            "hIndex": 1
        },
        "Shiwen Ni": {
            "name": "Shiwen Ni",
            "hIndex": 0
        },
        "Kohei Tsuji": {
            "authorId": "1869310",
            "name": "Kohei Tsuji",
            "hIndex": 13
        },
        "Mike Ebrahimi": {
            "name": "Mike Ebrahimi",
            "hIndex": 0
        },
        "Mohamed Hassan": {
            "authorId": "2005694238",
            "name": "Mohamed M. Hassan",
            "hIndex": 21
        },
        "Peng Zhou": {
            "authorId": "48354970",
            "name": "P. Zhou",
            "hIndex": 11
        },
        "Yaoyu Hu": {
            "authorId": "2113665545",
            "name": "Yaoyu Hu",
            "hIndex": 7
        },
        "Tatiana V. Guy": {
            "authorId": "145768994",
            "name": "Tatiana V. Guy",
            "hIndex": 12
        },
        "Taimur Kashif": {
            "name": "Taimur Kashif",
            "hIndex": 0
        },
        "Johannes M. Hermann": {
            "authorId": "2150018412",
            "name": "J. Wei\u00df",
            "hIndex": 18
        },
        "David Darby": {
            "authorId": "74768284",
            "name": "D. Darby",
            "hIndex": 43
        },
        "Wei-Qiang Zhang": {
            "authorId": "50549927",
            "name": "Weiqiang Zhang",
            "hIndex": 16
        },
        "Jake Street": {
            "authorId": "2162808490",
            "name": "Jake Street",
            "hIndex": 0
        },
        "Tingyu Fan": {
            "authorId": "50015189",
            "name": "Tingyu Fan",
            "hIndex": 10
        },
        "Xiangxin Zhou": {
            "authorId": "122128795",
            "name": "Xiangxin Zhou",
            "hIndex": 5
        },
        "Jumpei Takami": {
            "authorId": "2121027873",
            "name": "Jumpei Takami",
            "hIndex": 1
        },
        "Andreas Plesner": {
            "authorId": "2166731979",
            "name": "Andreas Plesner",
            "hIndex": 2
        },
        "Sieun Hyeon": {
            "authorId": "2316050814",
            "name": "Sieun Hyeon",
            "hIndex": 0
        },
        "Yihang Xu": {
            "authorId": "1914605192",
            "name": "Yihang Xu",
            "hIndex": 6
        },
        "Jumana Abu-Khalaf": {
            "authorId": "1413436861",
            "name": "J. Abu-Khalaf",
            "hIndex": 6
        },
        "Xiao Guang Gao": {
            "authorId": "1815605",
            "name": "Xiao-guang Gao",
            "hIndex": 14
        },
        "Peng Yu": {
            "authorId": "2114103795",
            "name": "Pengyao Yu",
            "hIndex": 24
        },
        "Xingyi Yang": {
            "authorId": "6514166",
            "name": "Xingyi Yang",
            "hIndex": 8
        },
        "Zehao Wang": {
            "authorId": "2108727131",
            "name": "Ze-Nian Wang",
            "hIndex": 6
        },
        "Naveed Janvekar": {
            "authorId": "2228929658",
            "name": "Naveed Janvekar",
            "hIndex": 0
        },
        "Deepak Ganesan": {
            "authorId": "1742299",
            "name": "Deepak Ganesan",
            "hIndex": 49
        },
        "Carlos Gemmell": {
            "authorId": "1796270950",
            "name": "Carlos Gemmell",
            "hIndex": 5
        },
        "Wei Yuan": {
            "authorId": "144786547",
            "name": "W. Yuan",
            "hIndex": 10
        },
        "Vinh Thinh Ho": {
            "authorId": "40401855",
            "name": "Vinh Thinh Ho",
            "hIndex": 7
        },
        "Qingqing Gu": {
            "authorId": "1384639858",
            "name": "Q. Gu",
            "hIndex": 29
        },
        "Alessandro Abate": {
            "authorId": "144938187",
            "name": "A. Abate",
            "hIndex": 41
        },
        "Yuan Tian": {
            "authorId": "49902369",
            "name": "Yuan Tian",
            "hIndex": 22
        },
        "Rita Singh": {
            "authorId": "47609362",
            "name": "Rita Singh",
            "hIndex": 18
        },
        "Boris Ivanovic": {
            "authorId": "145156173",
            "name": "B. Ivanovic",
            "hIndex": 21
        },
        "Xiaofang Zhou": {
            "authorId": "48667278",
            "name": "Xiaofang Zhou",
            "hIndex": 64
        },
        "Giorgos Stamou": {
            "authorId": "1719165",
            "name": "G. Stamou",
            "hIndex": 29
        },
        "Stefan Woltran": {
            "authorId": "1743085",
            "name": "S. Woltran",
            "hIndex": 37
        },
        "Hongxia Yang": {
            "authorId": "5789049",
            "name": "Hongxi Yang",
            "hIndex": 18
        },
        "Danilo J. Rezende": {
            "authorId": "1748523",
            "name": "Danilo Jimenez Rezende",
            "hIndex": 43
        },
        "Ziyan Gao": {
            "authorId": "51114810",
            "name": "Ziyan Gao",
            "hIndex": 15
        },
        "Francesco Paissan": {
            "authorId": "151505853",
            "name": "F. Paissan",
            "hIndex": 6
        },
        "Sudip Mittal": {
            "authorId": "2736774",
            "name": "Sudip Mittal",
            "hIndex": 24
        },
        "Weihong Deng": {
            "authorId": "1774956",
            "name": "Weihong Deng",
            "hIndex": 45
        },
        "Andrei Jalba": {
            "authorId": "49758039",
            "name": "A. Jalba",
            "hIndex": 17
        },
        "Bahareh Nakisa": {
            "authorId": "20510630",
            "name": "Bahareh Nakisa",
            "hIndex": 13
        },
        "Anton Klenitskiy": {
            "authorId": "2240533116",
            "name": "Anton Klenitskiy",
            "hIndex": 2
        },
        "Ningyu Zhang": {
            "authorId": "49410859",
            "name": "Ning Zhang",
            "hIndex": 25
        },
        "Shicheng Liu": {
            "authorId": "50151875",
            "name": "Shi-ting Liu",
            "hIndex": 4
        },
        "David Broman": {
            "authorId": "1715714",
            "name": "David Broman",
            "hIndex": 27
        },
        "Uehwan Kim": {
            "authorId": "120630672",
            "name": "U-Chang Kim",
            "hIndex": 0
        },
        "Siqing Li": {
            "authorId": "66464523",
            "name": "Liu Siqing",
            "hIndex": 4
        },
        "Hayden Houscher": {
            "name": "Hayden Houscher",
            "hIndex": 0
        },
        "Angelo Benoit": {
            "name": "Angelo Benoit",
            "hIndex": 0
        },
        "Xuezhi Cao": {
            "authorId": "2562591",
            "name": "Xuezhi Cao",
            "hIndex": 14
        },
        "Mirjam Cuper": {
            "authorId": "2126081069",
            "name": "Mirjam Cuper",
            "hIndex": 2
        },
        "Zijian Yi": {
            "authorId": "48806019",
            "name": "Z. Zhang",
            "hIndex": 4
        },
        "Jean-Philippe Thiran": {
            "authorId": "1710257",
            "name": "J. Thiran",
            "hIndex": 68
        },
        "Nils Feldhus": {
            "authorId": "1641658310",
            "name": "Nils Feldhus",
            "hIndex": 8
        },
        "Cheng Ling": {
            "authorId": "71205387",
            "name": "Cheng-Ling Tai",
            "hIndex": 5
        },
        "Yinheng Li": {
            "authorId": "115621744",
            "name": "Yinhe Li",
            "hIndex": 2
        },
        "Wassim Bouachir": {
            "authorId": "2047002",
            "name": "W. Bouachir",
            "hIndex": 12
        },
        "Zhengkai Jiang": {
            "authorId": "143646284",
            "name": "Zheng Feng",
            "hIndex": 43
        },
        "Dongkun Huo": {
            "authorId": "2162253567",
            "name": "Dongkun Huo",
            "hIndex": 1
        },
        "Viktor Palmkvist": {
            "authorId": "66390850",
            "name": "Viktor Palmkvist",
            "hIndex": 2
        },
        "Tianshu Hu": {
            "authorId": "93901895",
            "name": "Tianshu Hu",
            "hIndex": 4
        },
        "Arkadeep Acharya": {
            "authorId": "2273560136",
            "name": "Arkadeep Acharya",
            "hIndex": 3
        },
        "Rudra Murthy": {
            "authorId": "2124507712",
            "name": "B. Rudra Murthy",
            "hIndex": 2
        },
        "Ting-En Lin": {
            "name": "Ting-En Lin",
            "hIndex": 0
        },
        "Ivan Ovinnikov": {
            "authorId": "66443344",
            "name": "Ivan Ovinnikov",
            "hIndex": 2
        },
        "Ron Bitton": {
            "authorId": "50616846",
            "name": "Ron Bitton",
            "hIndex": 13
        },
        "Hongzhi Shu": {
            "name": "Hongzhi Shu",
            "hIndex": 0
        },
        "Pegah Emdad": {
            "name": "Pegah Emdad",
            "hIndex": 0
        },
        "Guiliang Liu": {
            "authorId": "50084567",
            "name": "Guiliang Liu",
            "hIndex": 15
        },
        "Mingjie Li": {
            "name": "Mingjie Li",
            "hIndex": 0
        },
        "Anke Xue": {
            "authorId": "145878136",
            "name": "A. Xue",
            "hIndex": 33
        },
        "Shuangping Chen": {
            "authorId": "1403091894",
            "name": "Chen Shuang-ping",
            "hIndex": 2
        },
        "Minye Wu": {
            "authorId": "2128656980",
            "name": "Minye Wu",
            "hIndex": 12
        },
        "Yiwei Jiang": {
            "authorId": "31569810",
            "name": "Yiwei Jiang",
            "hIndex": 12
        },
        "Pietro Li\u00f2": {
            "authorId": "2075355155",
            "name": "Pietro Lio'",
            "hIndex": 20
        },
        "Ge Zhang": {
            "authorId": "51454312",
            "name": "Ge Zhang",
            "hIndex": 6
        },
        "Ashwin Sankar": {
            "authorId": "40416142",
            "name": "A. Sankar",
            "hIndex": 7
        },
        "Lin Gui": {
            "authorId": "25374714",
            "name": "L. Gui",
            "hIndex": 18
        },
        "Jingyu Liu": {
            "authorId": "2108507287",
            "name": "Jingyu Liu",
            "hIndex": 18
        },
        "Feiteng Fang": {
            "authorId": "2150629064",
            "name": "Feiteng Fang",
            "hIndex": 2
        },
        "Duan Manni Duan": {
            "authorId": "2448387",
            "name": "Manni Duan",
            "hIndex": 5
        },
        "Haonan Zhang": {
            "authorId": "1466509637",
            "name": "Haonan Zhang",
            "hIndex": 4
        },
        "Han Li": {
            "authorId": "30059904",
            "name": "L. Han",
            "hIndex": 10
        },
        "Hoang Anh Just": {
            "authorId": "1412591482",
            "name": "H. Just",
            "hIndex": 5
        },
        "Anu Bhamidipaty": {
            "name": "Anu Bhamidipaty",
            "hIndex": 0
        },
        "Hambisa Keno": {
            "authorId": "66079149",
            "name": "Hambisa Keno",
            "hIndex": 1
        },
        "Svitlana Volkova": {
            "authorId": "143683394",
            "name": "Svitlana Volkova",
            "hIndex": 22
        },
        "Anna Rapberger": {
            "authorId": "1394218378",
            "name": "Anna Rapberger",
            "hIndex": 8
        },
        "Ruya Jiang": {
            "authorId": "66750289",
            "name": "Jiang Ruya",
            "hIndex": 1
        },
        "Pablo Piantanida": {
            "authorId": "1743922",
            "name": "P. Piantanida",
            "hIndex": 26
        },
        "Radiful Islam": {
            "name": "Radiful Islam",
            "hIndex": 0
        },
        "Ryan Cotterell": {
            "authorId": "1750769",
            "name": "Ryan Cotterell",
            "hIndex": 41
        },
        "Aaron Steiner": {
            "authorId": "49074838",
            "name": "Aaron B. Steiner",
            "hIndex": 6
        },
        "Guanghui Fu": {
            "authorId": "9103187",
            "name": "G. Fu",
            "hIndex": 8
        },
        "Markus J. Buehler": {
            "authorId": "2273480",
            "name": "M. Buehler",
            "hIndex": 107
        },
        "Cristian Trout": {
            "authorId": "150959853",
            "name": "C. Trout",
            "hIndex": 0
        },
        "Hyeok-jae Lee": {
            "authorId": "2108805304",
            "name": "Jae\u2010Hyeok Lee",
            "hIndex": 16
        },
        "Christopher Potts": {
            "name": "Christopher Potts",
            "hIndex": 0
        },
        "Yiqiao Wang": {
            "authorId": "2108941522",
            "name": "Yiqiao Wang",
            "hIndex": 13
        },
        "Hao Frank Yang": {
            "authorId": "102320235",
            "name": "Hao Yang",
            "hIndex": 30
        },
        "Wei Niu": {
            "authorId": "145168790",
            "name": "W. Niu",
            "hIndex": 22
        },
        "Yong Chen": {
            "authorId": "2142745665",
            "name": "Yongsheng Chen",
            "hIndex": 16
        },
        "Ziying Song": {
            "authorId": "2091830970",
            "name": "Ziying Song",
            "hIndex": 8
        },
        "Mingxuan Yuan": {
            "authorId": "1688812",
            "name": "Mingxuan Yuan",
            "hIndex": 18
        },
        "Leman Akoglu": {
            "authorId": "3255268",
            "name": "L. Akoglu",
            "hIndex": 46
        },
        "Martin Masek": {
            "authorId": "1807869",
            "name": "M. Ma\u0161ek",
            "hIndex": 17
        },
        "David Suter": {
            "authorId": "50592201",
            "name": "D. Suter",
            "hIndex": 47
        },
        "Runhan Huang": {
            "authorId": "2125447255",
            "name": "R. Huang",
            "hIndex": 4
        },
        "Bill Yuchen Lin": {
            "authorId": "51583409",
            "name": "Bill Yuchen Lin",
            "hIndex": 30
        },
        "Hu Wang": {
            "authorId": "2109012704",
            "name": "Huhu Wang",
            "hIndex": 30
        },
        "Benedikt Schifferer": {
            "authorId": "2080538157",
            "name": "Benedikt D. Schifferer",
            "hIndex": 5
        },
        "Jingang Wang": {
            "authorId": "2132039409",
            "name": "Jin-gang Wang",
            "hIndex": 18
        },
        "Rohan Bhambhoria": {
            "authorId": "2008160154",
            "name": "R. Bhambhoria",
            "hIndex": 5
        },
        "Rui Wang": {
            "authorId": "145490534",
            "name": "Rui Wang",
            "hIndex": 29
        },
        "Lu Fan": {
            "authorId": "47961244",
            "name": "Lu-lu Fan",
            "hIndex": 20
        },
        "Jonilyn Dick": {
            "name": "Jonilyn Dick",
            "hIndex": 0
        },
        "Rajat Khanda": {
            "authorId": "69923048",
            "name": "Rajat Khanda",
            "hIndex": 1
        },
        "Feng Lyu": {
            "authorId": "2265656",
            "name": "Q. Lyu",
            "hIndex": 27
        },
        "Elias Castegren": {
            "authorId": "3263254",
            "name": "Elias Castegren",
            "hIndex": 6
        },
        "Hyunmin Cheong": {
            "authorId": "2617295",
            "name": "Hyunmin Cheong",
            "hIndex": 16
        },
        "Xiaoxiao Li": {
            "authorId": "2108537091",
            "name": "Xiao-xiao Li",
            "hIndex": 11
        },
        "Xinran Wu": {
            "authorId": "2582581",
            "name": "Xin-Wen Wu",
            "hIndex": 6
        },
        "Ahmed Burak Ercan": {
            "name": "Ahmed Burak Ercan",
            "hIndex": 0
        },
        "Hao Wen": {
            "authorId": "2075348817",
            "name": "Hao Wen",
            "hIndex": 9
        },
        "Isaac Xu": {
            "authorId": "1411822279",
            "name": "I. Xu",
            "hIndex": 4
        },
        "Zhineng Chen": {
            "authorId": "8157255",
            "name": "Zhineng Chen",
            "hIndex": 23
        },
        "Luca La Fisca": {
            "authorId": "2087223917",
            "name": "L. L. Fisca",
            "hIndex": 1
        },
        "Yujia Wu": {
            "authorId": "2115663148",
            "name": "Yujia Wu",
            "hIndex": 5
        },
        "Chuheng Zhang": {
            "authorId": "144114271",
            "name": "Chuheng Zhang",
            "hIndex": 8
        },
        "Joachim M. Buhmann": {
            "authorId": "1682548",
            "name": "J. Buhmann",
            "hIndex": 74
        },
        "Fatemeh Askari": {
            "authorId": "83442403",
            "name": "F. Askari",
            "hIndex": 15
        },
        "Nasir Hayat": {
            "authorId": "35505612",
            "name": "N. Hayat",
            "hIndex": 23
        },
        "Chenggang Yan": {
            "name": "Chenggang Yan",
            "hIndex": 0
        },
        "Lukas Garbas": {
            "name": "Lukas Garbas",
            "hIndex": 0
        },
        "Daria Stepanova": {
            "authorId": "144075663",
            "name": "D. Stepanova",
            "hIndex": 15
        },
        "Wenyang Hu": {
            "authorId": "50105380",
            "name": "Wenyang Hu",
            "hIndex": 11
        },
        "Yiwei Wu": {
            "name": "Yiwei Wu",
            "hIndex": 0
        },
        "Yang Yang": {
            "authorId": "2119900961",
            "name": "Yangyang Yang",
            "hIndex": 10
        },
        "Ki H. Chon": {
            "authorId": "145400573",
            "name": "K. Chon",
            "hIndex": 47
        },
        "Songlin Hu": {
            "authorId": "2115190434",
            "name": "Song Hu",
            "hIndex": 6
        },
        "Wonkyung Lee": {
            "authorId": "49627237",
            "name": "Wonkyun Lee",
            "hIndex": 3
        },
        "Yihang Dong": {
            "authorId": "2051449393",
            "name": "Yihang Dong",
            "hIndex": 5
        },
        "Satoru Watanabe": {
            "authorId": "2107366602",
            "name": "S. Watanabe",
            "hIndex": 20
        },
        "Zhihao Dou": {
            "authorId": "2067204168",
            "name": "Zhihao Dou",
            "hIndex": 1
        },
        "Khuzaima Daudjee": {
            "authorId": "2176381",
            "name": "Khuzaima S. Daudjee",
            "hIndex": 17
        },
        "Harry J. Stroud": {
            "authorId": "40651272",
            "name": "H. Stroud",
            "hIndex": 3
        },
        "Huan-ang Gao": {
            "name": "Huan-ang Gao",
            "hIndex": 0
        },
        "Xinhu Zheng": {
            "authorId": "2481151",
            "name": "Xinhu Zheng",
            "hIndex": 11
        },
        "Talita Perciano": {
            "authorId": "2200720",
            "name": "T. Perciano",
            "hIndex": 13
        },
        "Tianyu Zhao": {
            "authorId": "2024772601",
            "name": "Tian Zhao",
            "hIndex": 14
        },
        "Makoto Onizuka": {
            "authorId": "2179564",
            "name": "M. Onizuka",
            "hIndex": 28
        },
        "Jilin Mei": {
            "authorId": "32576697",
            "name": "Jilin Mei",
            "hIndex": 6
        },
        "Paula Rodriguez-Diaz": {
            "authorId": "2058111606",
            "name": "Paula Rodr\u00edguez D\u00edaz",
            "hIndex": 2
        },
        "Till Menzel": {
            "authorId": "35403066",
            "name": "Till Menzel",
            "hIndex": 6
        },
        "Helen Meng": {
            "authorId": "2057833292",
            "name": "Helen M. Meng",
            "hIndex": 11
        },
        "Zichen Wang": {
            "authorId": "66194549",
            "name": "Wang Zichen",
            "hIndex": 6
        },
        "Chenglei Shen": {
            "authorId": "1775502",
            "name": "Chenglei Shen",
            "hIndex": 3
        },
        "Martin Balfroid": {
            "authorId": "2218628696",
            "name": "Martin Balfroid",
            "hIndex": 1
        },
        "Huiqin Liu": {
            "authorId": "49958380",
            "name": "Hui-xia Liu",
            "hIndex": 6
        },
        "Alexandros Koulakos": {
            "name": "Alexandros Koulakos",
            "hIndex": 0
        },
        "Jaemin Seo": {
            "authorId": "51254875",
            "name": "Jaemin Seo",
            "hIndex": 12
        },
        "Masato Akagi": {
            "authorId": "2969364",
            "name": "M. Akagi",
            "hIndex": 23
        },
        "Emil Carlsson": {
            "authorId": "39523985",
            "name": "Emil Carlsson",
            "hIndex": 6
        },
        "Dieter Pfoser": {
            "authorId": "2258036",
            "name": "D. Pfoser",
            "hIndex": 31
        },
        "Ashish Sabharwal": {
            "authorId": "48229640",
            "name": "Ashish Sabharwal",
            "hIndex": 50
        },
        "Nadir Weibel": {
            "authorId": "1752711",
            "name": "Nadir Weibel",
            "hIndex": 24
        },
        "Jati H. Husen": {
            "authorId": "2086494762",
            "name": "Jati H. Husen",
            "hIndex": 4
        },
        "Shiyin Kang": {
            "authorId": "2220070414",
            "name": "Shiyin Kang",
            "hIndex": 19
        },
        "Noah D. Goodman": {
            "authorId": "144002017",
            "name": "Noah D. Goodman",
            "hIndex": 69
        },
        "Thorsten Holz": {
            "authorId": "144227650",
            "name": "Thorsten Holz",
            "hIndex": 67
        },
        "Jethro Lee": {
            "name": "Jethro Lee",
            "hIndex": 0
        },
        "Lei Wang": {
            "authorId": "2152506293",
            "name": "Lei Wang",
            "hIndex": 31
        },
        "Gary Geunbae Lee": {
            "authorId": "2219028748",
            "name": "G. G. Lee",
            "hIndex": 28
        },
        "Andre Manoel": {
            "authorId": "145580422",
            "name": "Andre Manoel",
            "hIndex": 14
        },
        "Ivan Bercovich": {
            "authorId": "113451215",
            "name": "I. Bercovich",
            "hIndex": 1
        },
        "Faiz Ali Shah": {
            "authorId": "2837724",
            "name": "Faiz Ali Shah",
            "hIndex": 9
        },
        "Gian Antonio Susto": {
            "authorId": "3126083",
            "name": "Gian Antonio Susto",
            "hIndex": 24
        },
        "Yangyang Kang": {
            "authorId": "38753454",
            "name": "Yangyang Kang",
            "hIndex": 8
        },
        "Bi-Cheng Yan": {
            "authorId": "50461900",
            "name": "Bin Chen",
            "hIndex": 96
        },
        "Philipp Schaer": {
            "authorId": "34588911",
            "name": "Philipp Schaer",
            "hIndex": 14
        },
        "Ruizhen Hu": {
            "authorId": "2154334",
            "name": "Ruizhen Hu",
            "hIndex": 19
        },
        "Yuchi Fengting": {
            "name": "Yuchi Fengting",
            "hIndex": 0
        },
        "Xiang Yue": {
            "authorId": "3085488",
            "name": "Yue Xiang",
            "hIndex": 28
        },
        "Samuel Dahan": {
            "authorId": "119208875",
            "name": "Samuel Dahan",
            "hIndex": 7
        },
        "Siyuan Liu": {
            "authorId": "1399920746",
            "name": "S. Liu",
            "hIndex": 15
        },
        "Zaixiang Zheng": {
            "authorId": "24018493",
            "name": "Zaixiang Zheng",
            "hIndex": 12
        },
        "Jasmina Gajcin": {
            "name": "Jasmina Gajcin",
            "hIndex": 0
        },
        "Anbin QI": {
            "authorId": "102626496",
            "name": "Anbin Qi",
            "hIndex": 2
        },
        "Tatiana Bysheva": {
            "name": "Tatiana Bysheva",
            "hIndex": 0
        },
        "Ruochen Cao": {
            "authorId": "2048831095",
            "name": "R. Cao",
            "hIndex": 9
        },
        "Svetlana Maslenkova": {
            "name": "Svetlana Maslenkova",
            "hIndex": 0
        },
        "Zhengliang Liu": {
            "authorId": "2145977326",
            "name": "Zheng Liu",
            "hIndex": 22
        },
        "Damodar Panigrahi": {
            "authorId": "2154608320",
            "name": "Damodar Panigrahi",
            "hIndex": 1
        },
        "Iman Soltani": {
            "authorId": "31292611",
            "name": "I. S. Bozchalooi",
            "hIndex": 15
        },
        "Xunliang Cai": {
            "authorId": "2111683220",
            "name": "Xunliang Cai",
            "hIndex": 5
        },
        "Alina Fastowski": {
            "authorId": "2302795616",
            "name": "Alina Fastowski",
            "hIndex": 1
        },
        "Lihao Wang": {
            "authorId": "9453176",
            "name": "Wang Lihao",
            "hIndex": 5
        },
        "Shihang Wang": {
            "authorId": "2109511579",
            "name": "Shih-Chieh Wang",
            "hIndex": 7
        },
        "Mohamed Dhouioui": {
            "name": "Mohamed Dhouioui",
            "hIndex": 0
        },
        "Jiashu Zhang": {
            "authorId": "103203778",
            "name": "Jia-shu Zhang",
            "hIndex": 13
        },
        "Yuchen Shen": {
            "authorId": "2112809252",
            "name": "Yu Fang",
            "hIndex": 4
        },
        "Siyuan Liang": {
            "authorId": "1387433264",
            "name": "Si-yuan Liang",
            "hIndex": 7
        },
        "Moulay A. Akhloufi": {
            "authorId": "2629166",
            "name": "M. Akhloufi",
            "hIndex": 27
        },
        "Anna Volodkevich": {
            "authorId": "51940816",
            "name": "A. Volodkevich",
            "hIndex": 1
        },
        "Xinchao Wang": {
            "authorId": "48631088",
            "name": "Xinchao Wang",
            "hIndex": 52
        },
        "Tianran Liu": {
            "authorId": "2122309639",
            "name": "Tianran Liu",
            "hIndex": 11
        },
        "Mario Fritz": {
            "authorId": "1739548",
            "name": "Mario Fritz",
            "hIndex": 65
        },
        "Xin Cao": {
            "authorId": "2149214614",
            "name": "Xinrui Cao",
            "hIndex": 4
        },
        "Mohammed Eunus Ali": {
            "authorId": "2111130873",
            "name": "Mohammed Eunus Ali",
            "hIndex": 4
        },
        "Ben Bogin": {
            "authorId": "50757607",
            "name": "Ben Bogin",
            "hIndex": 18
        },
        "Ian Chuang": {
            "authorId": "144069493",
            "name": "I. Chuang",
            "hIndex": 1
        },
        "Kejuan Yang": {
            "authorId": "2218347104",
            "name": "Kejuan Yang",
            "hIndex": 1
        },
        "Hongyu Lu": {
            "authorId": "2307103",
            "name": "Hongyuan Lu",
            "hIndex": 6
        },
        "Resul Tugay": {
            "authorId": "31184921",
            "name": "Resul Tugay",
            "hIndex": 3
        },
        "Yingxu He": {
            "authorId": "2118918022",
            "name": "Ying He",
            "hIndex": 3
        },
        "Mitesh Khapra": {
            "authorId": "2361078",
            "name": "Mitesh M. Khapra",
            "hIndex": 36
        },
        "Mingyue Ma": {
            "authorId": "50657932",
            "name": "M. Ma",
            "hIndex": 9
        },
        "Ganesh Adam": {
            "authorId": "153381125",
            "name": "Ganesh Adam",
            "hIndex": 3
        },
        "Leiv-J Gelius": {
            "authorId": "31315801",
            "name": "L. Gelius",
            "hIndex": 22
        },
        "Qiuhong Shen": {
            "authorId": "46417162",
            "name": "Qiuhong Shen",
            "hIndex": 4
        },
        "Long-Gang Pang": {
            "authorId": "8330675",
            "name": "L. Pang",
            "hIndex": 28
        },
        "Tolulope Ale": {
            "authorId": "2160930036",
            "name": "T. A. Ale",
            "hIndex": 2
        },
        "Khalid Zaman": {
            "authorId": "46608225",
            "name": "K. Zaman",
            "hIndex": 60
        },
        "Chenyi Lei": {
            "authorId": "2954826",
            "name": "Chenyi Lei",
            "hIndex": 11
        },
        "Joris Veerbeek": {
            "authorId": "2135030284",
            "name": "Joris Veerbeek",
            "hIndex": 1
        },
        "Dan Ma": {
            "authorId": "34304593",
            "name": "Dan-wei Ma",
            "hIndex": 8
        },
        "Xiaoyang Wang": {
            "authorId": "2118774785",
            "name": "X. Wang",
            "hIndex": 39
        },
        "Louis Penafiel": {
            "authorId": "114184480",
            "name": "Louis Pe\u00f1afiel",
            "hIndex": 1
        },
        "Rongxiang Weng": {
            "authorId": "24009282",
            "name": "Rongxiang Weng",
            "hIndex": 11
        },
        "Zerrin Yumak": {
            "authorId": "1730934",
            "name": "Zerrin Yumak",
            "hIndex": 11
        },
        "Kaijia Xu": {
            "authorId": "12962689",
            "name": "Kaijia Xu",
            "hIndex": 21
        },
        "Jingcheng Wu": {
            "authorId": "47875898",
            "name": "Jingcheng Wu",
            "hIndex": 10
        },
        "Haokai Ma": {
            "authorId": "2152191425",
            "name": "Haokai Ma",
            "hIndex": 7
        },
        "Chuan Xiao": {
            "authorId": "121433833",
            "name": "Xiaoguang Guo",
            "hIndex": 20
        },
        "Mathieu Tanneau": {
            "authorId": "2253610610",
            "name": "Mathieu Tanneau",
            "hIndex": 5
        },
        "Roohallah Alizadehsani": {
            "authorId": "150271813",
            "name": "R. Alizadehsani",
            "hIndex": 37
        },
        "Jean Oh": {
            "authorId": "143904954",
            "name": "Jean Oh",
            "hIndex": 23
        },
        "Junghyun Koo": {
            "authorId": "151486141",
            "name": "Junghyun Koo",
            "hIndex": 6
        },
        "Anuj Kumar": {
            "authorId": "145573265",
            "name": "Anuj Kumar",
            "hIndex": 14
        },
        "Bailin Wang": {
            "authorId": "7425507",
            "name": "Bai-lin Wang",
            "hIndex": 8
        },
        "Milind Tambe": {
            "authorId": "143736701",
            "name": "Milind Tambe",
            "hIndex": 84
        },
        "Zeno Kujawa": {
            "authorId": "2265752952",
            "name": "Zeno Kujawa",
            "hIndex": 1
        },
        "Toshikazu Hashimoto": {
            "authorId": "32256564",
            "name": "T. Hashimoto",
            "hIndex": 23
        },
        "Chenyang Lu": {
            "authorId": "46655354",
            "name": "Chenyang Lu",
            "hIndex": 20
        },
        "Wei Zhang": {
            "authorId": "30771513",
            "name": "Wei Zhang",
            "hIndex": 29
        },
        "Alessandra Russo": {
            "authorId": "145277911",
            "name": "A. Russo",
            "hIndex": 37
        },
        "Hainan Xu": {
            "authorId": "2094929",
            "name": "Hainan Xu",
            "hIndex": 17
        },
        "Deb Roy": {
            "authorId": "145699534",
            "name": "P. D. Roy",
            "hIndex": 7
        },
        "Haitao Lin": {
            "authorId": "46933284",
            "name": "Hai-tao Lin",
            "hIndex": 9
        },
        "Eileen Bingert": {
            "authorId": "2294878412",
            "name": "Eileen Bingert",
            "hIndex": 1
        },
        "Wei Guo": {
            "authorId": "2149239106",
            "name": "Wei Guo",
            "hIndex": 26
        },
        "Sung-Lin Yeh": {
            "authorId": "51264527",
            "name": "Sung-Lin Yeh",
            "hIndex": 8
        },
        "Karthik Pattabiraman": {
            "authorId": "1715185",
            "name": "K. Pattabiraman",
            "hIndex": 39
        },
        "Xin Jin": {
            "authorId": "144898068",
            "name": "Xin Jin",
            "hIndex": 23
        },
        "Hiroshi Saruwatari": {
            "authorId": "1685827",
            "name": "H. Saruwatari",
            "hIndex": 38
        },
        "Hamza Javed": {
            "authorId": "2075815415",
            "name": "Hamza A. Javed",
            "hIndex": 5
        },
        "Hossein Hajipour": {
            "authorId": "3021793",
            "name": "Hossein Hajipour",
            "hIndex": 5
        },
        "Ruiqi Wang": {
            "authorId": "9198320",
            "name": "W. Ruiqi",
            "hIndex": 7
        },
        "Naren Khatwani": {
            "authorId": "2302746673",
            "name": "Naren Khatwani",
            "hIndex": 0
        },
        "Satvik Verma": {
            "authorId": "2302525622",
            "name": "Satvik Verma",
            "hIndex": 0
        },
        "Srija Anand": {
            "authorId": "2146683905",
            "name": "Srija Anand",
            "hIndex": 0
        },
        "Victor Bocharov": {
            "authorId": "144442324",
            "name": "V. Bocharov",
            "hIndex": 4
        },
        "David Eriksson": {
            "authorId": "1699330",
            "name": "D. Eriksson",
            "hIndex": 14
        },
        "Feng Lu": {
            "authorId": "48006807",
            "name": "Feng H. Lu",
            "hIndex": 10
        },
        "Simon S. Woo": {
            "authorId": "35154708",
            "name": "Simon S. Woo",
            "hIndex": 33
        },
        "Tim Weninger": {
            "authorId": "1714166",
            "name": "Tim Weninger",
            "hIndex": 25
        },
        "Kajal Patel": {
            "authorId": "2113271557",
            "name": "Kajal S Patel",
            "hIndex": 4
        },
        "Christian Bizer": {
            "authorId": "1729154",
            "name": "Christian Bizer",
            "hIndex": 61
        },
        "Felix Brei": {
            "authorId": "2236686278",
            "name": "Felix Brei",
            "hIndex": 2
        },
        "Haibo Yang": {
            "authorId": "3284258",
            "name": "Y. Haibo",
            "hIndex": 7
        },
        "Lawrence Jang": {
            "authorId": "2280901917",
            "name": "Lawrence Jang",
            "hIndex": 1
        },
        "George Rayment": {
            "authorId": "47261718",
            "name": "G. Rayment",
            "hIndex": 15
        },
        "Shashank Gupta": {
            "authorId": "2991916",
            "name": "Shashank Gupta",
            "hIndex": 26
        },
        "Neha Prakriya": {
            "authorId": "2090207616",
            "name": "Neha Prakriya",
            "hIndex": 2
        },
        "Rogerio Bonatti": {
            "authorId": "3444893",
            "name": "Rogerio Bonatti",
            "hIndex": 14
        },
        "Andrew Clegg": {
            "authorId": "153530642",
            "name": "A. Clegg",
            "hIndex": 40
        },
        "Umair Qudus": {
            "authorId": "2063800112",
            "name": "Umair Qudus",
            "hIndex": 2
        },
        "Jonathan D. Thomas": {
            "authorId": "3492256",
            "name": "J. Thomas",
            "hIndex": 18
        },
        "Shuhan Tan": {
            "authorId": "117435761",
            "name": "Shuhan Tan",
            "hIndex": 10
        },
        "Sepehr Nourmohammadi": {
            "authorId": "2276429055",
            "name": "Sepehr Nourmohammadi",
            "hIndex": 0
        },
        "Yuchao Li": {
            "name": "Yuchao Li",
            "hIndex": 0
        },
        "Xiaoyi Liu": {
            "authorId": "2111037000",
            "name": "Xiaoyi Liu",
            "hIndex": 11
        },
        "Daniel Fried": {
            "authorId": "48582444",
            "name": "D. Fried",
            "hIndex": 52
        },
        "Xuhang Chen": {
            "authorId": "2193108626",
            "name": "Xuhang Chen",
            "hIndex": 8
        },
        "Hrishikesh Shridhar Deodhar": {
            "authorId": "2214804138",
            "name": "H. Deodhar",
            "hIndex": 1
        },
        "He wang": {
            "authorId": "39483421",
            "name": "H. Wang",
            "hIndex": 10
        },
        "Zoe Hancox": {
            "authorId": "150356601",
            "name": "Zoe Hancox",
            "hIndex": 4
        },
        "Jifan Yu": {
            "authorId": "2116034394",
            "name": "Jifan Yu",
            "hIndex": 9
        },
        "Yunshui Li": {
            "authorId": "2204767592",
            "name": "Yunshui Li",
            "hIndex": 3
        },
        "Abdallah Khreishah": {
            "authorId": "1750196",
            "name": "Abdallah Khreishah",
            "hIndex": 33
        },
        "Juanzi Li": {
            "authorId": "8549842",
            "name": "Juan-Zi Li",
            "hIndex": 51
        },
        "Hongyu Jiang": {
            "authorId": "32767934",
            "name": "Hongyu Jiang",
            "hIndex": 28
        },
        "Mogan Gim": {
            "authorId": "2187857050",
            "name": "Mogan Gim",
            "hIndex": 2
        },
        "Lenka T\u011btkov\u00e1": {
            "authorId": "2215406780",
            "name": "Lenka T\u011btkov\u00e1",
            "hIndex": 1
        },
        "Xinyi Wang": {
            "authorId": "2115554124",
            "name": "Xinyi Wang",
            "hIndex": 10
        },
        "Kun Fan": {
            "authorId": "87239192",
            "name": "Kunkun Fan",
            "hIndex": 16
        },
        "Jiwei Wei": {
            "name": "Jiwei Wei",
            "hIndex": 0
        },
        "Miriam Doh": {
            "authorId": "2219246241",
            "name": "Miriam Doh",
            "hIndex": 1
        },
        "Chengwei Sun": {
            "authorId": "3423107",
            "name": "Chengwei Sun",
            "hIndex": 16
        },
        "Vera Demberg": {
            "authorId": "2869436",
            "name": "Vera Demberg",
            "hIndex": 30
        },
        "Tatsuya Hiraoka": {
            "authorId": "114789302",
            "name": "Tatsuya Hiraoka",
            "hIndex": 6
        },
        "Sangmin Lee": {
            "authorId": "2108151539",
            "name": "S. Lee",
            "hIndex": 14
        },
        "Gerald Shen": {
            "authorId": "145960335",
            "name": "G. Tan",
            "hIndex": 6
        },
        "Brendan McMorrow": {
            "authorId": "2144195324",
            "name": "Brendan McMorrow",
            "hIndex": 2
        },
        "Xiaolong Cheng": {
            "authorId": "48683942",
            "name": "Xiao-Bin Cheng",
            "hIndex": 10
        },
        "Luo Ji": {
            "authorId": "11768808",
            "name": "Ji-sheng Luo",
            "hIndex": 13
        },
        "Surajsinh Parmar": {
            "authorId": "2220404761",
            "name": "Surajsinh Parmar",
            "hIndex": 1
        },
        "Kun Kuang": {
            "authorId": "145129032",
            "name": "Kun Huang",
            "hIndex": 10
        },
        "Naser Kazemi": {
            "authorId": "2221389692",
            "name": "Naser Kazemi",
            "hIndex": 1
        },
        "Ethan Gotlieb Wilcox": {
            "authorId": "51445267",
            "name": "Ethan Gotlieb Wilcox",
            "hIndex": 16
        },
        "Cristian Borcea": {
            "authorId": "144888146",
            "name": "C. Borcea",
            "hIndex": 29
        },
        "Yan Wang": {
            "authorId": "38491490",
            "name": "Yan Wang",
            "hIndex": 25
        },
        "Xiaojiao Guo": {
            "authorId": "2518237",
            "name": "Xiaojiao Guo",
            "hIndex": 16
        },
        "Nicola Cancedda": {
            "authorId": "1683776",
            "name": "Nicola Cancedda",
            "hIndex": 14
        },
        "Xin Xu": {
            "authorId": "2152776608",
            "name": "X. Xu",
            "hIndex": 6
        },
        "Anup Sathya": {
            "authorId": "2089549430",
            "name": "Anup Sathya",
            "hIndex": 3
        },
        "Guansong Pang": {
            "authorId": "3224619",
            "name": "Guansong Pang",
            "hIndex": 31
        },
        "Robert Sim": {
            "authorId": "2665945",
            "name": "Robert B Sim",
            "hIndex": 90
        },
        "Enes Erciyes": {
            "name": "Enes Erciyes",
            "hIndex": 0
        },
        "Muyun Yang": {
            "authorId": "2105775",
            "name": "Muyun Yang",
            "hIndex": 12
        },
        "Huazhen Huang": {
            "authorId": "82563776",
            "name": "Huazhen Huang",
            "hIndex": 6
        },
        "Huateng Zhang": {
            "name": "Huateng Zhang",
            "hIndex": 0
        },
        "Xiaoming Fu": {
            "authorId": "70384311",
            "name": "Xiaoming Fu",
            "hIndex": 8
        },
        "Pantaleone Nespoli": {
            "authorId": "46200403",
            "name": "P. Nespoli",
            "hIndex": 13
        },
        "Stefan Sylvius Wagner": {
            "authorId": "2114116887",
            "name": "Stefan Wagner",
            "hIndex": 3
        },
        "Jaehwan Jeong": {
            "authorId": "3415445",
            "name": "Jaehwan Jeong",
            "hIndex": 6
        },
        "Yulong Cao": {
            "authorId": "2112823206",
            "name": "Yulong Cao",
            "hIndex": 10
        },
        "Xudong Chen": {
            "authorId": "33207280",
            "name": "Xudong Chen",
            "hIndex": 34
        },
        "Ang Li": {
            "authorId": "2112839155",
            "name": "A. Li",
            "hIndex": 8
        },
        "Paula Arkhangorodsky": {
            "name": "Paula Arkhangorodsky",
            "hIndex": 0
        },
        "Linfeng Zhang": {
            "authorId": "2125538501",
            "name": "Linfeng Zhang",
            "hIndex": 31
        },
        "Rex Ying": {
            "authorId": "83539859",
            "name": "Rex Ying",
            "hIndex": 25
        },
        "Zhongliang Liu": {
            "authorId": "46270994",
            "name": "Zhongliang Liu",
            "hIndex": 32
        },
        "Shichen Zhan": {
            "name": "Shichen Zhan",
            "hIndex": 0
        },
        "Danli Shi": {
            "authorId": "2064971829",
            "name": "Danli Shi",
            "hIndex": 10
        },
        "Mark Hallap": {
            "authorId": "2189306524",
            "name": "Mark Hallap",
            "hIndex": 1
        },
        "Nayel Fabian Salem": {
            "authorId": "2177333857",
            "name": "Nayel Fabian Salem",
            "hIndex": 2
        },
        "Kentaro Inui": {
            "authorId": "3040648",
            "name": "Kentaro Inui",
            "hIndex": 40
        },
        "Andr\u00e9 F. T. Martins": {
            "authorId": "145644643",
            "name": "Andr\u00e9 F. T. Martins",
            "hIndex": 37
        },
        "Chunyang Jiang": {
            "authorId": "5066613",
            "name": "Chunyang Jiang",
            "hIndex": 13
        },
        "Tyler Harp": {
            "authorId": "49288292",
            "name": "T. Harp",
            "hIndex": 5
        },
        "Hujun Bao": {
            "authorId": "1679542",
            "name": "H. Bao",
            "hIndex": 64
        },
        "Jun-Jie Zhang": {
            "authorId": "47538828",
            "name": "Junjie Zhang",
            "hIndex": 34
        },
        "Zhanyu Wang": {
            "authorId": "2108405619",
            "name": "Zhanyu Wang",
            "hIndex": 6
        },
        "Zhiyuan Tang": {
            "authorId": "122700762",
            "name": "Zhi-Qun Tang",
            "hIndex": 18
        },
        "E. Wes Bethel": {
            "authorId": "143874684",
            "name": "E. .. Bethel",
            "hIndex": 31
        },
        "Pinar Donmez": {
            "authorId": "2046500",
            "name": "Pinar E. Donmez",
            "hIndex": 17
        },
        "Youngho Jun": {
            "name": "Youngho Jun",
            "hIndex": 0
        },
        "Benoit Dufumier": {
            "authorId": "89653117",
            "name": "Benoit Dufumier",
            "hIndex": 6
        },
        "Frieda Born": {
            "name": "Frieda Born",
            "hIndex": 0
        },
        "Jieyun Bai": {
            "authorId": "144166300",
            "name": "Jieyun Bai",
            "hIndex": 12
        },
        "Maciej P. Polak": {
            "authorId": "143720509",
            "name": "M. Polak",
            "hIndex": 12
        },
        "Simon Ott": {
            "authorId": "119994729",
            "name": "Simon Ott",
            "hIndex": 9
        },
        "Xiu-Cheng Wang": {
            "authorId": "12142683",
            "name": "Xiu-Cheng Wang",
            "hIndex": 4
        },
        "Jin Huang": {
            "authorId": "24348221",
            "name": "Jin H. Huang",
            "hIndex": 18
        },
        "Victor-Alexandru Darvariu": {
            "authorId": "41031873",
            "name": "Victor-Alexandru Darvariu",
            "hIndex": 5
        },
        "Bo Yan": {
            "authorId": "51109011",
            "name": "B. Yan",
            "hIndex": 15
        },
        "Juan Manuel P\u00e9rez": {
            "authorId": "74377824",
            "name": "J. P\u00e9rez",
            "hIndex": 7
        },
        "Julio Vizcarra": {
            "authorId": "2719546",
            "name": "Julio Vizcarra",
            "hIndex": 3
        },
        "Chanho Eom": {
            "authorId": "9366027",
            "name": "Chanho Eom",
            "hIndex": 5
        },
        "Yongfeng Huang": {
            "authorId": "1731776",
            "name": "Yongfeng Huang",
            "hIndex": 49
        },
        "Yiming Ma": {
            "authorId": "48521384",
            "name": "YiMing Ma",
            "hIndex": 5
        },
        "Tess Hellebrekers": {
            "authorId": "2576308",
            "name": "T. Hellebrekers",
            "hIndex": 12
        },
        "Ruilin Xu": {
            "authorId": "2115800396",
            "name": "Rui Xu",
            "hIndex": 11
        },
        "Ahmad Lotfi": {
            "authorId": "144005237",
            "name": "Ahmad Lotfi",
            "hIndex": 21
        },
        "Mangesh Pujari": {
            "authorId": "2059086328",
            "name": "Mangesh D. Pujari",
            "hIndex": 1
        },
        "Chao Zhu": {
            "authorId": "145258833",
            "name": "Chao Zhu",
            "hIndex": 19
        },
        "Claire Tomlin": {
            "authorId": "1693894",
            "name": "C. Tomlin",
            "hIndex": 79
        },
        "Zhaolin Ren": {
            "authorId": "2087734879",
            "name": "Zhao Ren",
            "hIndex": 21
        },
        "Zehong Shen": {
            "authorId": "1384523019",
            "name": "Zehong Shen",
            "hIndex": 5
        },
        "J\u00fcri Keller": {
            "authorId": "2149598395",
            "name": "J\u00fcri Keller",
            "hIndex": 2
        },
        "Yu Chen": {
            "authorId": "49778677",
            "name": "Chen Yu",
            "hIndex": 13
        },
        "Bohan Wang": {
            "authorId": "2153211564",
            "name": "Bohan Wang",
            "hIndex": 9
        },
        "Dacheng Tao": {
            "authorId": "143719920",
            "name": "D. Tao",
            "hIndex": 153
        },
        "Matthew B. Blaschko": {
            "authorId": "1758219",
            "name": "Matthew B. Blaschko",
            "hIndex": 38
        },
        "Maike Behrendt": {
            "authorId": "113725315",
            "name": "Maike Behrendt",
            "hIndex": 3
        },
        "Manuela Veloso": {
            "authorId": "1956361",
            "name": "M. Veloso",
            "hIndex": 77
        },
        "Ryohei Sasano": {
            "authorId": "2293543",
            "name": "Ryohei Sasano",
            "hIndex": 14
        },
        "Hai Li": {
            "authorId": "2115210686",
            "name": "H. Li",
            "hIndex": 5
        },
        "Christian Himpe": {
            "authorId": "3223382",
            "name": "Christian Himpe",
            "hIndex": 14
        },
        "K. S. M. Tozammel Hossain": {
            "authorId": "144022002",
            "name": "K. T. Hossain",
            "hIndex": 5
        },
        "Jeffrey O. Kephart": {
            "authorId": "1721327",
            "name": "J. Kephart",
            "hIndex": 46
        },
        "Shuo Sun": {
            "authorId": "50414877",
            "name": "Shuo Sun",
            "hIndex": 10
        },
        "Yulan He": {
            "authorId": "1423462132",
            "name": "Yulan He",
            "hIndex": 3
        },
        "Youyuan Liu": {
            "authorId": "2895728",
            "name": "You-Xing Liu",
            "hIndex": 4
        },
        "Michele Laurelli": {
            "name": "Michele Laurelli",
            "hIndex": 0
        },
        "Anbai Jiang": {
            "authorId": "2192056711",
            "name": "Anbai Jiang",
            "hIndex": 2
        },
        "Qianbo Zang": {
            "authorId": "2142954198",
            "name": "Qianbo Zang",
            "hIndex": 0
        },
        "Geon Lee": {
            "authorId": "144182095",
            "name": "G. Lee",
            "hIndex": 7
        },
        "Darren Chen": {
            "authorId": "4834609",
            "name": "Darren B. Chen",
            "hIndex": 18
        },
        "Shengxin Hong": {
            "authorId": "2279932265",
            "name": "Shengxin Hong",
            "hIndex": 2
        },
        "Kristian Kersting": {
            "authorId": "1746871",
            "name": "K. Kersting",
            "hIndex": 58
        },
        "Yuting Yan": {
            "authorId": "6740696",
            "name": "Yuting Yan",
            "hIndex": 16
        },
        "Yejin Lee": {
            "authorId": "93053402",
            "name": "Lee\uff0c Yejin",
            "hIndex": 3
        },
        "Shuo Feng": {
            "authorId": "5173740",
            "name": "Shuofeng Zhang",
            "hIndex": 11
        },
        "Fei Huang": {
            "authorId": "12195113",
            "name": "F. Huang",
            "hIndex": 14
        },
        "Jacco van Ossenbruggen": {
            "authorId": "1708506",
            "name": "J. V. Ossenbruggen",
            "hIndex": 30
        },
        "Vasudev Lal": {
            "authorId": "95340164",
            "name": "Vasudev Lal",
            "hIndex": 9
        },
        "Malek Mouhoub": {
            "authorId": "1727269",
            "name": "Malek Mouhoub",
            "hIndex": 17
        },
        "Zunjie Zhu": {
            "authorId": "32450931",
            "name": "Zunjie Zhu",
            "hIndex": 4
        },
        "Xunlong Zou": {
            "authorId": "2308672717",
            "name": "Xunlong Zou",
            "hIndex": 0
        },
        "Gabor Fodor": {
            "name": "Gabor Fodor",
            "hIndex": 0
        },
        "Safwen Naimi": {
            "authorId": "2139566438",
            "name": "Safwen Naimi",
            "hIndex": 1
        },
        "Dong Wang": {
            "authorId": "2152687313",
            "name": "Dong Wang",
            "hIndex": 21
        },
        "Tianwu Lei": {
            "authorId": "1450640957",
            "name": "Lei Tianwu",
            "hIndex": 0
        },
        "Masato Kikuchi": {
            "authorId": "4793002",
            "name": "M. Kikuchi",
            "hIndex": 8
        },
        "Goran Glava\u0161": {
            "authorId": "2472657",
            "name": "Goran Glavas",
            "hIndex": 30
        },
        "Oleksandra Zolotarevych": {
            "name": "Oleksandra Zolotarevych",
            "hIndex": 0
        },
        "Haoyu Wang": {
            "authorId": "2109571805",
            "name": "Haoyu Wang",
            "hIndex": 7
        },
        "Leo Peckham": {
            "name": "Leo Peckham",
            "hIndex": 0
        },
        "Rickard Ewetz": {
            "authorId": "2828546",
            "name": "Rickard Ewetz",
            "hIndex": 10
        },
        "Renjie Wu": {
            "authorId": "150287468",
            "name": "R. Wu",
            "hIndex": 8
        },
        "Raghavendra Ramachandra": {
            "authorId": "144781317",
            "name": "Ramachandra Raghavendra",
            "hIndex": 37
        },
        "Jinghua Yue": {
            "authorId": "151068701",
            "name": "Jinghua Yue",
            "hIndex": 2
        },
        "Feiyang Jia": {
            "authorId": "2163184299",
            "name": "Feiyang Jia",
            "hIndex": 4
        },
        "Veronica Haber": {
            "authorId": "2107691351",
            "name": "L. Garcia",
            "hIndex": 8
        },
        "Qi Gan": {
            "authorId": "2068514966",
            "name": "Qi Gan",
            "hIndex": 12
        },
        "Javier Conde": {
            "authorId": "83519270",
            "name": "J. Garcia-conde",
            "hIndex": 39
        },
        "Gallil Maimon": {
            "name": "Gallil Maimon",
            "hIndex": 0
        },
        "Yanmin Qian": {
            "authorId": "2480051",
            "name": "Y. Qian",
            "hIndex": 40
        },
        "Peng Wu": {
            "authorId": "8327090",
            "name": "Peng-Fei Wu",
            "hIndex": 26
        },
        "Faezeh Faez": {
            "authorId": "2083552598",
            "name": "F. Fathi",
            "hIndex": 5
        },
        "Ruixin Shi": {
            "authorId": "66947025",
            "name": "Ruixin Shi",
            "hIndex": 7
        },
        "Jingyi Chai": {
            "authorId": "2273563877",
            "name": "Jingyi Chai",
            "hIndex": 2
        },
        "Alexei Pisacane": {
            "name": "Alexei Pisacane",
            "hIndex": 0
        },
        "Tianhe Lu": {
            "authorId": "2113488632",
            "name": "Tian-yu Lu",
            "hIndex": 2
        },
        "Shervin Ghasemlou": {
            "authorId": "2709180",
            "name": "S. Ghasemlou",
            "hIndex": 5
        },
        "Alejandro Pozo": {
            "authorId": "145924980",
            "name": "A. Pozo",
            "hIndex": 28
        },
        "Quoc-Bao Nguyen-Le": {
            "authorId": "47742694",
            "name": "Khoa Nguyen",
            "hIndex": 12
        },
        "Samira Ghodratnama": {
            "authorId": "69932254",
            "name": "Samira Ghodratnama",
            "hIndex": 8
        },
        "Rhoslyn Roebuck Williams": {
            "authorId": "1687991690",
            "name": "Rhoslyn Roebuck Williams",
            "hIndex": 3
        },
        "Taylor Anderson": {
            "authorId": "2003642221",
            "name": "T. Anderson",
            "hIndex": 9
        },
        "Christian Pirker": {
            "authorId": "2064485548",
            "name": "Christian Pirker",
            "hIndex": 1
        },
        "Yutong Ban": {
            "authorId": "8422060",
            "name": "Yutong Ban",
            "hIndex": 17
        },
        "Giorgio Fabbro": {
            "name": "Giorgio Fabbro",
            "hIndex": 0
        },
        "Giorgos Filandrianos": {
            "authorId": "2080432906",
            "name": "Giorgos Filandrianos",
            "hIndex": 4
        },
        "Kaixin Ma": {
            "authorId": "22244290",
            "name": "Kaixin Ma",
            "hIndex": 13
        },
        "Chengcheng Han": {
            "authorId": "2152125182",
            "name": "Chengcheng Han",
            "hIndex": 9
        },
        "Aryan Deshwal": {
            "authorId": "151474453",
            "name": "Aryan Deshwal",
            "hIndex": 13
        },
        "Tommaso Di Noia": {
            "authorId": "1737962",
            "name": "T. D. Noia",
            "hIndex": 39
        },
        "Nam-Joon Kim": {
            "authorId": "145858850",
            "name": "Nam Kim",
            "hIndex": 27
        },
        "Xin Guo": {
            "authorId": "2026460423",
            "name": "Xin Guo",
            "hIndex": 12
        },
        "Titouan Parcollet": {
            "authorId": "9033417",
            "name": "Titouan Parcollet",
            "hIndex": 20
        },
        "Lennart Keller": {
            "authorId": "1678318324",
            "name": "G. Keller",
            "hIndex": 7
        },
        "Jiaming Luo": {
            "authorId": "80141596",
            "name": "Jiaming Luo",
            "hIndex": 2
        },
        "Gil Ayache": {
            "name": "Gil Ayache",
            "hIndex": 0
        },
        "Fabio De Sousa Ribeiro": {
            "authorId": "144055526",
            "name": "F. Silva",
            "hIndex": 17
        },
        "Shoujun Zhou": {
            "authorId": "49219180",
            "name": "Shou-jun Zhou",
            "hIndex": 12
        },
        "Enrico Lovat": {
            "authorId": "3207610",
            "name": "Enrico Lovat",
            "hIndex": 7
        },
        "Qingyun Sun": {
            "authorId": "2019468998",
            "name": "Qingyun Sun",
            "hIndex": 14
        },
        "Gollam Rabby": {
            "authorId": "75164225",
            "name": "Gollam Rabby",
            "hIndex": 5
        },
        "Zahra Khanjani": {
            "authorId": "2141499163",
            "name": "Zahra Khanjani",
            "hIndex": 4
        },
        "Dong Yu": {
            "authorId": "98907842",
            "name": "Y. Dong",
            "hIndex": 10
        },
        "Ruoming Jin": {
            "authorId": "1740308",
            "name": "R. Jin",
            "hIndex": 42
        },
        "Qing Zhang": {
            "authorId": "2059749641",
            "name": "Zhang Qing",
            "hIndex": 7
        },
        "Raouf Toukal": {
            "name": "Raouf Toukal",
            "hIndex": 0
        },
        "Geigh Zollicoffer": {
            "authorId": "2146822967",
            "name": "Geigh Zollicoffer",
            "hIndex": 0
        },
        "Zhiyuan Liu": {
            "authorId": "2020151597",
            "name": "L. Zhiyuan",
            "hIndex": 6
        },
        "Jinba Xiao": {
            "authorId": "1503673740",
            "name": "Jinba Xiao",
            "hIndex": 3
        },
        "Shuangping Li": {
            "authorId": "31023771",
            "name": "Shuangping Li",
            "hIndex": 5
        },
        "Robin Huo": {
            "name": "Robin Huo",
            "hIndex": 0
        },
        "Daniel Adam": {
            "authorId": "84565716",
            "name": "Adam Etinson",
            "hIndex": 6
        },
        "Nikolaos Vastardis": {
            "authorId": "2385808",
            "name": "Nikolaos Vastardis",
            "hIndex": 5
        },
        "Yongxin Guo": {
            "authorId": "49813771",
            "name": "Yongxin Guo",
            "hIndex": 10
        },
        "Yu Hu": {
            "authorId": "2109330322",
            "name": "Y. Hu",
            "hIndex": 20
        },
        "Jan van Leeuwen": {
            "authorId": "143817739",
            "name": "J. Leeuwen",
            "hIndex": 54
        },
        "Jane Dwivedi-Yu": {
            "authorId": "2173509991",
            "name": "Jane Dwivedi-Yu",
            "hIndex": 9
        },
        "Zhi Cen": {
            "authorId": "69461810",
            "name": "Z. Cen",
            "hIndex": 21
        },
        "Hui Guan": {
            "authorId": "11613838",
            "name": "X. Guan",
            "hIndex": 20
        },
        "Yue Zhang": {
            "authorId": "2145912775",
            "name": "Yueming Zhang",
            "hIndex": 15
        },
        "James M. Rehg": {
            "authorId": "144177248",
            "name": "James M. Rehg",
            "hIndex": 80
        },
        "Eduardo Cueto-Mendoza": {
            "authorId": "2112435108",
            "name": "E. Gonz\u00e1lez",
            "hIndex": 7
        },
        "Shrikanth Narayanan": {
            "authorId": "152434613",
            "name": "Shrikanth S. Narayanan",
            "hIndex": 13
        },
        "S\u00f6ren Auer": {
            "authorId": "145044578",
            "name": "S. Auer",
            "hIndex": 53
        },
        "Julien Albert": {
            "authorId": "3721034",
            "name": "J. Sebag",
            "hIndex": 22
        },
        "Shiro Takagi": {
            "authorId": "12194878",
            "name": "S. Takagi",
            "hIndex": 10
        },
        "Qingbin Liu": {
            "authorId": "31074234",
            "name": "L. Qingbin",
            "hIndex": 7
        },
        "\u00c1lvaro S\u00e1nchez Villar": {
            "authorId": "79880528",
            "name": "A. G\u00f3mez",
            "hIndex": 7
        },
        "Francisco Valentini": {
            "authorId": "153541579",
            "name": "Francisco Valentini",
            "hIndex": 2
        },
        "Jinxian Qu": {
            "authorId": "30536624",
            "name": "Jinxiang Qu",
            "hIndex": 0
        },
        "Lei Li": {
            "authorId": "2151528694",
            "name": "Lei\u2010shi Li",
            "hIndex": 35
        },
        "Leila De Floriani": {
            "authorId": "1715008",
            "name": "L. Floriani",
            "hIndex": 41
        },
        "Anirban Majumder": {
            "authorId": "48883499",
            "name": "A. Majumder",
            "hIndex": 13
        },
        "Aman Sakhardande": {
            "name": "Aman Sakhardande",
            "hIndex": 0
        },
        "Ahmad Faraz Khan": {
            "authorId": "48673156",
            "name": "A. Faraz",
            "hIndex": 17
        },
        "Songlin Yang": {
            "authorId": "5328864",
            "name": "S. Yang",
            "hIndex": 10
        },
        "Dingcheng Huang": {
            "authorId": "4951173",
            "name": "Dingcheng Huang",
            "hIndex": 6
        },
        "Jack H. Good": {
            "authorId": "50810772",
            "name": "Jack H. Good",
            "hIndex": 1
        },
        "Ozgur Kara": {
            "authorId": "35464370",
            "name": "O. Kara",
            "hIndex": 16
        },
        "Justus Bogner": {
            "authorId": "48174976",
            "name": "J. Bogner",
            "hIndex": 18
        },
        "Yanru Wu": {
            "authorId": "2134151058",
            "name": "Yan-ru Wu",
            "hIndex": 4
        },
        "Felix Steffek": {
            "authorId": "2076573173",
            "name": "Felix Steffek",
            "hIndex": 5
        },
        "Yifei Liu": {
            "authorId": "2145514615",
            "name": "Y. Liu",
            "hIndex": 6
        },
        "Jianyi Zhang": {
            "authorId": "2052992573",
            "name": "Jianyi(Jay) Zhang",
            "hIndex": 55
        },
        "Even Oldridge": {
            "authorId": "79722739",
            "name": "Even Oldridge",
            "hIndex": 7
        },
        "Markus Maurer": {
            "authorId": "50363217",
            "name": "M. Maurer",
            "hIndex": 30
        },
        "Pradeep Kumar Jayaraman": {
            "authorId": "2514027",
            "name": "P. Jayaraman",
            "hIndex": 14
        },
        "Yunting Song": {
            "authorId": "10054799",
            "name": "Yunting Song",
            "hIndex": 9
        },
        "Wayne Xin Zhao": {
            "authorId": "2542603",
            "name": "Wayne Xin Zhao",
            "hIndex": 57
        },
        "Yiran Chen": {
            "authorId": "49794415",
            "name": "Xiaobin Chen",
            "hIndex": 16
        },
        "Chiman Salavati": {
            "authorId": "51219074",
            "name": "Chiman Salavati",
            "hIndex": 6
        },
        "Sheng Chen": {
            "authorId": "83819161",
            "name": "Shengdi Chen",
            "hIndex": 37
        },
        "Xuyang Ge": {
            "authorId": "144322422",
            "name": "Xuyang Ge",
            "hIndex": 14
        },
        "Yiming Ju": {
            "authorId": "2143876071",
            "name": "Yiming Ju",
            "hIndex": 4
        },
        "Hyun Joon Park": {
            "authorId": "11987928",
            "name": "Joon-hyun Park",
            "hIndex": 11
        },
        "Mohit Bansal": {
            "authorId": "143977268",
            "name": "Mohit Bansal",
            "hIndex": 72
        },
        "Siyu Huang": {
            "authorId": "2110060223",
            "name": "Si-Yu Huang",
            "hIndex": 16
        },
        "Soham Palande": {
            "authorId": "2137374667",
            "name": "Soham Palande",
            "hIndex": 0
        },
        "Cynthia C. S. Liem": {
            "authorId": "1968667",
            "name": "Cynthia C. S. Liem",
            "hIndex": 15
        },
        "Malvina Nikandrou": {
            "authorId": "2133187991",
            "name": "Malvina Nikandrou",
            "hIndex": 3
        },
        "Guohong Fu": {
            "authorId": "2059275",
            "name": "G. Fu",
            "hIndex": 20
        },
        "Kin Wai Cheuk": {
            "authorId": "93894852",
            "name": "K. Cheuk",
            "hIndex": 11
        },
        "Boyuan Chen": {
            "authorId": "48951550",
            "name": "Boyuan Chen",
            "hIndex": 13
        },
        "Dietrich Klakow": {
            "authorId": "2561225",
            "name": "D. Klakow",
            "hIndex": 32
        },
        "Wajdi Zaghouani": {
            "authorId": "2034351",
            "name": "W. Zaghouani",
            "hIndex": 27
        },
        "Min Yang": {
            "authorId": "71368689",
            "name": "Jung\u2010Min Yang",
            "hIndex": 21
        },
        "Jin-Duk Park": {
            "authorId": "2151177676",
            "name": "Jin-Duk Park",
            "hIndex": 4
        },
        "Sara Abdali": {
            "authorId": "47473650",
            "name": "S. Abdali",
            "hIndex": 4
        },
        "Carlos Arriaga": {
            "authorId": "1410571637",
            "name": "C. Calles-Arriaga",
            "hIndex": 6
        },
        "Yang Feng": {
            "authorId": "76997653",
            "name": "Feng-Chun Yang",
            "hIndex": 42
        },
        "Stefano Fasciani": {
            "authorId": "3231055",
            "name": "Stefano Fasciani",
            "hIndex": 6
        },
        "Ronie Salgado": {
            "authorId": "3080877",
            "name": "R. Salgado",
            "hIndex": 2
        },
        "Xiuyi Fan": {
            "authorId": "33853958",
            "name": "Xiuyi Fan",
            "hIndex": 17
        },
        "Zack Hui": {
            "authorId": "2086808192",
            "name": "Zheng Zack Hui",
            "hIndex": 1
        },
        "Dingxin Cheng": {
            "authorId": "96049373",
            "name": "DingXin Cheng",
            "hIndex": 10
        },
        "Xinyu Li": {
            "authorId": "13998894",
            "name": "L. Xinyu",
            "hIndex": 15
        },
        "Prithwish Dan": {
            "authorId": "2228665381",
            "name": "Prithwish Dan",
            "hIndex": 2
        },
        "Jianguo Li": {
            "authorId": "2015313659",
            "name": "Li Jianguo",
            "hIndex": 5
        },
        "Xin Zhang": {
            "authorId": "2149171782",
            "name": "Xin A. Zhang",
            "hIndex": 31
        },
        "Kunal Dhawan": {
            "name": "Kunal Dhawan",
            "hIndex": 0
        },
        "Hyunseok Oh": {
            "authorId": "2105614483",
            "name": "Hyun Oh",
            "hIndex": 4
        },
        "Qingkai Fang": {
            "authorId": "2159548678",
            "name": "Qingkai Fang",
            "hIndex": 7
        },
        "Quan Meng": {
            "authorId": "5914391",
            "name": "Q. Meng",
            "hIndex": 14
        },
        "Sien Reeve Peralta": {
            "authorId": "2219056614",
            "name": "Sien Reeve Ordonez Peralta",
            "hIndex": 1
        },
        "So Yeon Kim": {
            "authorId": "2109609637",
            "name": "So-Yeon Kim",
            "hIndex": 20
        },
        "Srideepika Jayaraman": {
            "authorId": "1988817745",
            "name": "Srideepika Jayaraman",
            "hIndex": 3
        },
        "Wanli Qian": {
            "authorId": "2100518701",
            "name": "Wanli Qian",
            "hIndex": 3
        },
        "Tao Chen": {
            "authorId": "46618513",
            "name": "Taotao Chen",
            "hIndex": 16
        },
        "Jizhong Han": {
            "authorId": "2710247",
            "name": "Jizhong Han",
            "hIndex": 20
        },
        "Mohammad Baqar": {
            "authorId": "10738155",
            "name": "Sumbal Zaidi",
            "hIndex": 10
        },
        "Na Li": {
            "authorId": "6486454",
            "name": "Nana Li",
            "hIndex": 17
        },
        "Lea Sch\u00f6nherr": {
            "authorId": "6474643",
            "name": "Lea Sch\u00f6nherr",
            "hIndex": 9
        },
        "Matej Ze\u010devi\u0107": {
            "authorId": "35745805",
            "name": "M. Zecevic",
            "hIndex": 5
        },
        "JongWoo Kim": {
            "authorId": "2144162978",
            "name": "Jongwoo Kim",
            "hIndex": 6
        },
        "Klaus Greff": {
            "name": "Klaus Greff",
            "hIndex": 0
        },
        "Grace Colverd": {
            "authorId": "2265495230",
            "name": "Grace Colverd",
            "hIndex": 1
        },
        "Nobukazu Yoshioka": {
            "name": "Nobukazu Yoshioka",
            "hIndex": 0
        },
        "Lingxiao Wei": {
            "authorId": "2337781",
            "name": "Lingxiao Wei",
            "hIndex": 10
        },
        "Guillaume-Alexandre Bilodeau": {
            "authorId": "1705256",
            "name": "Guillaume-Alexandre Bilodeau",
            "hIndex": 23
        },
        "Shadab Khan": {
            "authorId": "8197155",
            "name": "S. Khan",
            "hIndex": 14
        },
        "Ali Anwar": {
            "authorId": "2208074797",
            "name": "A. Ali",
            "hIndex": 32
        },
        "Ganapathy Mani": {
            "authorId": "37936665",
            "name": "G. Mani",
            "hIndex": 10
        },
        "Ukcheol Shin": {
            "authorId": "151357911",
            "name": "Ukcheol Shin",
            "hIndex": 9
        },
        "Phil Bates": {
            "authorId": "50494192",
            "name": "P. Bates",
            "hIndex": 20
        },
        "Zitao Liu": {
            "authorId": "2117940912",
            "name": "Zitao Liu",
            "hIndex": 20
        },
        "Amit Awekar": {
            "authorId": "1702207",
            "name": "Amit Awekar",
            "hIndex": 7
        },
        "Xichen Tang": {
            "authorId": "2287916487",
            "name": "Xichen Tang",
            "hIndex": 1
        },
        "Jad Kabbara": {
            "authorId": "2631301",
            "name": "Jad Kabbara",
            "hIndex": 7
        },
        "Michael Dennis": {
            "authorId": "89407018",
            "name": "M. Dennis",
            "hIndex": 12
        },
        "Peng Wang": {
            "authorId": "2296405360",
            "name": "P. Wang",
            "hIndex": 54
        },
        "Qitao Qin": {
            "authorId": "2112703642",
            "name": "Q. Song",
            "hIndex": 5
        },
        "Shaojie Tang": {
            "authorId": "1682309",
            "name": "Shaojie Tang",
            "hIndex": 47
        },
        "Haoshu Xu": {
            "authorId": "93084931",
            "name": "Haoshu Xu",
            "hIndex": 3
        },
        "Emilia Ellsiepen": {
            "authorId": "3361437",
            "name": "E. Ellsiepen",
            "hIndex": 4
        },
        "Yao Shu": {
            "authorId": "144343033",
            "name": "Y. Shu",
            "hIndex": 18
        },
        "Walid Chaabene": {
            "authorId": "13366781",
            "name": "W. Chaabene",
            "hIndex": 0
        },
        "Sean Robertson": {
            "authorId": "2072941418",
            "name": "S. Robertson",
            "hIndex": 8
        },
        "Seungwhan Moon": {
            "authorId": "29072828",
            "name": "Seungwhan Moon",
            "hIndex": 21
        },
        "Zekun Xi": {
            "authorId": "2229632309",
            "name": "Zekun Xi",
            "hIndex": 3
        },
        "Mihail Stoian": {
            "authorId": "1438301263",
            "name": "Mihail Stoian",
            "hIndex": 5
        },
        "Kousuke Imamura": {
            "authorId": "1791576",
            "name": "K. Imamura",
            "hIndex": 8
        },
        "Md Hasebul Hasan": {
            "authorId": "2223385090",
            "name": "Md. Hasebul Hasan Niloy",
            "hIndex": 1
        },
        "Sida Peng": {
            "authorId": "12582170",
            "name": "Sida Peng",
            "hIndex": 4
        },
        "Ismael Castell-Uroz": {
            "authorId": "1409093363",
            "name": "Ismael Castell-Uroz",
            "hIndex": 4
        },
        "Marco Baioletti": {
            "name": "Marco Baioletti",
            "hIndex": 0
        },
        "Yifeng Niu": {
            "authorId": "1939702",
            "name": "Yifeng Niu",
            "hIndex": 12
        },
        "Sumit Jha": {
            "authorId": "2809164",
            "name": "S. Jha",
            "hIndex": 27
        },
        "Robert J. Baseman": {
            "authorId": "91612695",
            "name": "R. Baseman",
            "hIndex": 12
        },
        "Ehsan Firouzi": {
            "authorId": "82318229",
            "name": "Ehsan Firouzi",
            "hIndex": 2
        },
        "Yunyi Liu": {
            "authorId": "46399692",
            "name": "Yunyi Liu",
            "hIndex": 12
        },
        "Raika Karimi": {
            "name": "Raika Karimi",
            "hIndex": 0
        },
        "Simon Ostermann": {
            "authorId": "3101311",
            "name": "S. Ostermann",
            "hIndex": 15
        },
        "Yijun Yang": {
            "authorId": "49308230",
            "name": "Yijun Yang",
            "hIndex": 29
        },
        "Davide Dalle Pezze": {
            "authorId": "2088018191",
            "name": "Davide Dalle Pezze",
            "hIndex": 4
        },
        "Jason Weston": {
            "authorId": "145183709",
            "name": "J. Weston",
            "hIndex": 105
        },
        "Matthias Keicher": {
            "authorId": "2372357",
            "name": "Matthias Keicher",
            "hIndex": 7
        },
        "Pearl Pu": {
            "authorId": "1781996",
            "name": "P. Pu",
            "hIndex": 44
        },
        "Gavin Butts": {
            "name": "Gavin Butts",
            "hIndex": 0
        },
        "Yuxiong He": {
            "authorId": "2145020341",
            "name": "Yuxiong He",
            "hIndex": 38
        },
        "Xuefeng Su": {
            "authorId": "34291687",
            "name": "X. Su",
            "hIndex": 11
        },
        "Changwei Song": {
            "authorId": "49153056",
            "name": "Changwei Song",
            "hIndex": 11
        },
        "Yuntian Chen": {
            "authorId": "47557829",
            "name": "Yuntian Chen",
            "hIndex": 11
        },
        "Heike Adel": {
            "authorId": "145793834",
            "name": "Heike Adel",
            "hIndex": 21
        },
        "Lihu Chen": {
            "authorId": "9081258",
            "name": "Lihu Chen",
            "hIndex": 9
        },
        "Jagadeesh Balam": {
            "authorId": "2494510",
            "name": "Jagadeesh Balam",
            "hIndex": 6
        },
        "Yoonji Nam": {
            "name": "Yoonji Nam",
            "hIndex": 0
        },
        "Anne Lauscher": {
            "name": "Anne Lauscher",
            "hIndex": 0
        },
        "Enze Liu": {
            "authorId": "48813454",
            "name": "Enze Liu",
            "hIndex": 7
        },
        "Karen Pinel-Sauvagnat": {
            "name": "Karen Pinel-Sauvagnat",
            "hIndex": 0
        },
        "Hiroki Furuta": {
            "authorId": "49157633",
            "name": "H. Furuta",
            "hIndex": 9
        },
        "Maxime Kawawa-Beaudan": {
            "authorId": "2103462088",
            "name": "Maxime Kawawa-Beaudan",
            "hIndex": 1
        },
        "Qi Jia": {
            "authorId": "51213868",
            "name": "Q. Jia",
            "hIndex": 16
        },
        "Yinwei Wu": {
            "authorId": "12116410",
            "name": "Yin-Wei Wu",
            "hIndex": 5
        },
        "Wei Huang": {
            "authorId": "47504583",
            "name": "Wei Huang",
            "hIndex": 24
        },
        "Zhiyu Quan": {
            "authorId": "152517200",
            "name": "Zhiyu Quan",
            "hIndex": 3
        },
        "Shuochen Gao": {
            "authorId": "2023865201",
            "name": "Shuochen Gao",
            "hIndex": 4
        },
        "Ben Nassi": {
            "authorId": "7665773",
            "name": "Ben Nassi",
            "hIndex": 13
        },
        "NhatHai Phan": {
            "authorId": "11032760",
            "name": "Nhathai Phan",
            "hIndex": 17
        },
        "Fabricio Murai": {
            "authorId": "1781883",
            "name": "Fabricio Murai",
            "hIndex": 12
        },
        "Klaus-Robert M\u00fcller": {
            "authorId": "145034054",
            "name": "K. M\u00fcller",
            "hIndex": 128
        },
        "Wei Yu": {
            "authorId": "144323104",
            "name": "Wei Yu",
            "hIndex": 39
        },
        "Michael Ong": {
            "authorId": "50122294",
            "name": "M. Ong",
            "hIndex": 13
        },
        "Kelong Mao": {
            "authorId": "1580228663",
            "name": "Kelong Mao",
            "hIndex": 11
        },
        "Arkiev D'Souza": {
            "authorId": "1401651307",
            "name": "A. D'Souza",
            "hIndex": 8
        },
        "Kangkai Zhang": {
            "authorId": "1557328669",
            "name": "Kangkai Zhang",
            "hIndex": 6
        },
        "Marcus Nolte": {
            "authorId": "152948655",
            "name": "Marcus Nolte",
            "hIndex": 11
        },
        "Shan Lin": {
            "authorId": "1409851507",
            "name": "Shan C. Lin",
            "hIndex": 47
        },
        "Kristina Lerman": {
            "authorId": "1782658",
            "name": "Kristina Lerman",
            "hIndex": 60
        },
        "Huiyuan Xie": {
            "name": "Huiyuan Xie",
            "hIndex": 0
        },
        "Peter Clark": {
            "authorId": "121145962",
            "name": "P. Clark",
            "hIndex": 17
        },
        "Andreas Opedal": {
            "authorId": "2023300626",
            "name": "Andreas Opedal",
            "hIndex": 3
        },
        "Ashton Woiwood": {
            "authorId": "2230100040",
            "name": "Ashton Woiwood",
            "hIndex": 1
        },
        "Zhiqiang Xu": {
            "authorId": "2144376706",
            "name": "Zhiqiang Xu",
            "hIndex": 12
        },
        "Pieter Abbeel": {
            "authorId": "1689992",
            "name": "P. Abbeel",
            "hIndex": 150
        },
        "Lei Zhu": {
            "authorId": "49835934",
            "name": "Leilei Zhu",
            "hIndex": 23
        },
        "C. Su": {
            "authorId": "77613849",
            "name": "C. Su",
            "hIndex": 10
        },
        "Murtaza Dalal": {
            "authorId": "35904540",
            "name": "Murtaza Dalal",
            "hIndex": 10
        },
        "Emilio Hernandez": {
            "authorId": "144902280",
            "name": "E. Hern\u00e1ndez",
            "hIndex": 5
        },
        "Jia Rong": {
            "authorId": "48265810",
            "name": "R. Jia",
            "hIndex": 21
        },
        "Ryo Suzuki": {
            "authorId": "1665179454",
            "name": "R. Suzuki",
            "hIndex": 14
        },
        "Jun Zhou": {
            "authorId": "1728391",
            "name": "Jun Zhou",
            "hIndex": 35
        },
        "Herik Evangelinelis": {
            "name": "Herik Evangelinelis",
            "hIndex": 0
        },
        "Jian Yang": {
            "authorId": "2146237053",
            "name": "Jian Yang",
            "hIndex": 11
        },
        "Han Wang": {
            "authorId": "2144395181",
            "name": "Hanyang Wang",
            "hIndex": 4
        },
        "Chenxi Liu": {
            "authorId": "2116898052",
            "name": "Chenxi Liu",
            "hIndex": 11
        },
        "Oliver Lemon": {
            "authorId": "1782798",
            "name": "Oliver Lemon",
            "hIndex": 44
        },
        "Shun Lei": {
            "authorId": "7201237",
            "name": "Shunlei Duan",
            "hIndex": 7
        },
        "Timothy H. Chung": {
            "authorId": "1756109",
            "name": "Timothy H. Chung",
            "hIndex": 22
        },
        "Yuxin Liang": {
            "authorId": "4058030",
            "name": "Yeru Liang",
            "hIndex": 36
        },
        "Hao Zhao": {
            "authorId": "2184872",
            "name": "Zhaopeng Hao",
            "hIndex": 17
        },
        "JaeEun Im": {
            "name": "JaeEun Im",
            "hIndex": 0
        },
        "Oliver Wieder": {
            "authorId": "1755478229",
            "name": "O. Wieder",
            "hIndex": 6
        },
        "Christoph Busch": {
            "authorId": "46347050",
            "name": "C. Busch",
            "hIndex": 54
        },
        "Andrew Jong": {
            "authorId": "38387026",
            "name": "A. D. de Jong",
            "hIndex": 12
        },
        "Chao Huang": {
            "authorId": "3435866",
            "name": "Chao-yuan Huang",
            "hIndex": 34
        },
        "Ben Glocker": {
            "authorId": "1709824",
            "name": "Ben Glocker",
            "hIndex": 57
        },
        "Christopher Guagliano": {
            "authorId": "2130864177",
            "name": "Christopher Guagliano",
            "hIndex": 1
        },
        "Xianpan Zhou": {
            "authorId": "2213893862",
            "name": "Xianpan Zhou",
            "hIndex": 2
        },
        "David Kirsh": {
            "authorId": "145891494",
            "name": "D. Kirsh",
            "hIndex": 33
        },
        "Zhuohao Jerry Zhang": {
            "authorId": "83723816",
            "name": "Zhuohao Zhang",
            "hIndex": 5
        },
        "Shixiang Shane Gu": {
            "authorId": "2046135",
            "name": "S. Gu",
            "hIndex": 38
        },
        "Kexin Wang": {
            "authorId": "12436992",
            "name": "Kexing Wang",
            "hIndex": 8
        },
        "Ali Ezzat Shahroor": {
            "name": "Ali Ezzat Shahroor",
            "hIndex": 0
        },
        "Kuldip S. Atwal": {
            "authorId": "46511611",
            "name": "K. S. Atwal",
            "hIndex": 4
        },
        "Pasquale Minervini": {
            "authorId": "3051815",
            "name": "Pasquale Minervini",
            "hIndex": 21
        },
        "Eric Y. Ding": {
            "authorId": "47064831",
            "name": "E. Ding",
            "hIndex": 16
        },
        "Yihong Wang": {
            "authorId": "2120300622",
            "name": "Yihong Wang",
            "hIndex": 15
        },
        "Boushra Bendou": {
            "name": "Boushra Bendou",
            "hIndex": 0
        },
        "Shlomo Dubnov": {
            "authorId": "2204186",
            "name": "S. Dubnov",
            "hIndex": 34
        },
        "Edoardo D'Amico": {
            "authorId": "2070591052",
            "name": "Edoardo D'Amico",
            "hIndex": 3
        },
        "Chaowei He": {
            "authorId": "2115068671",
            "name": "Chaowei He",
            "hIndex": 5
        },
        "Markus Freitag": {
            "authorId": "1659014307",
            "name": "M. Freitag",
            "hIndex": 31
        },
        "Jimmy Lin": {
            "authorId": "145580839",
            "name": "Jimmy J. Lin",
            "hIndex": 80
        },
        "Arash Vahdat": {
            "authorId": "3214848",
            "name": "Arash Vahdat",
            "hIndex": 29
        },
        "Huaijin Pi": {
            "authorId": "1477287936",
            "name": "Huaijin Pi",
            "hIndex": 4
        },
        "Md Zarif Hossain": {
            "authorId": "2197739642",
            "name": "Md. Zarif Hossain",
            "hIndex": 3
        },
        "Jinxia Xie": {
            "authorId": "48086550",
            "name": "Jinxia Xie",
            "hIndex": 5
        },
        "Asif Ekbal": {
            "authorId": "1734904",
            "name": "Asif Ekbal",
            "hIndex": 47
        },
        "Manish Bhattarai": {
            "authorId": "38065704",
            "name": "Manish Bhattarai",
            "hIndex": 8
        },
        "Haomin Wen": {
            "authorId": "36620472",
            "name": "Hao Chen",
            "hIndex": 53
        },
        "Paul Barsch": {
            "authorId": "115528064",
            "name": "Paul Barsch",
            "hIndex": 1
        },
        "Shiyang Ruan": {
            "authorId": "1689528911",
            "name": "S. Ruan",
            "hIndex": 5
        },
        "Mengshu Sun": {
            "authorId": "2273904059",
            "name": "Mengshu Sun",
            "hIndex": 3
        },
        "Nithin Rao Koluguri": {
            "authorId": "17909597",
            "name": "N. Koluguri",
            "hIndex": 6
        },
        "Jiahao Lai": {
            "authorId": "9725046",
            "name": "Jiahao Lai",
            "hIndex": 8
        },
        "George Fragulis": {
            "authorId": "2760613",
            "name": "G. Fragulis",
            "hIndex": 11
        },
        "Angela Dai": {
            "authorId": "2208531",
            "name": "Angela Dai",
            "hIndex": 36
        },
        "Zheyuan Zhang": {
            "authorId": "2133195231",
            "name": "Zheyu Zhang",
            "hIndex": 6
        },
        "Dominic Simon": {
            "authorId": "33192403",
            "name": "Dominic Simon",
            "hIndex": 11
        },
        "Sheng Shen": {
            "authorId": "46495140",
            "name": "S. Shen",
            "hIndex": 18
        },
        "Dong Han": {
            "name": "Dong Han",
            "hIndex": 0
        },
        "Jinsu Kim": {
            "authorId": "89021225",
            "name": "Jinsu Kim",
            "hIndex": 14
        },
        "Jitka Homolov\u00e1": {
            "authorId": "2393061",
            "name": "J. Homolov\u00e1",
            "hIndex": 3
        },
        "Masashi Unoki": {
            "authorId": "2196412",
            "name": "M. Unoki",
            "hIndex": 20
        },
        "Quoc-Huy Trinh": {
            "authorId": "4003007",
            "name": "V. Trinh",
            "hIndex": 11
        },
        "Li-Ting Pai": {
            "name": "Li-Ting Pai",
            "hIndex": 0
        },
        "Ashraf Matrawy": {
            "name": "Ashraf Matrawy",
            "hIndex": 0
        },
        "Aaryan Panda": {
            "name": "Aaryan Panda",
            "hIndex": 0
        },
        "Gustavo Olague": {
            "authorId": "1721596",
            "name": "Gustavo Olague",
            "hIndex": 25
        },
        "Liang Qu": {
            "authorId": "145080435",
            "name": "Lianghuan Qu",
            "hIndex": 10
        },
        "Andrew K. Lampinen": {
            "authorId": "32322945",
            "name": "Andrew Kyle Lampinen",
            "hIndex": 21
        },
        "Shanu Vashishtha": {
            "authorId": "2203108051",
            "name": "Shanu Vashishtha",
            "hIndex": 1
        },
        "Dillon Dupont": {
            "name": "Dillon Dupont",
            "hIndex": 0
        },
        "Hsiang-Ting Chen": {
            "authorId": "49178414",
            "name": "Hsiang-Ting Chen",
            "hIndex": 12
        },
        "Ruobing Xie": {
            "authorId": "3360722",
            "name": "Ruobing Xie",
            "hIndex": 33
        },
        "Thomas Ball": {
            "authorId": "144434028",
            "name": "T. Ball",
            "hIndex": 54
        },
        "Kyudan Jung": {
            "authorId": "2292352183",
            "name": "Kyudan Jung",
            "hIndex": 1
        },
        "Nazar Protsiv": {
            "name": "Nazar Protsiv",
            "hIndex": 0
        },
        "Isibor Ihianle": {
            "authorId": "3456360",
            "name": "I. Ihianle",
            "hIndex": 5
        },
        "Peitian Zhang": {
            "authorId": "2153419738",
            "name": "Peitian Zhang",
            "hIndex": 8
        },
        "Silin Chen": {
            "authorId": "47336066",
            "name": "Silin Chen",
            "hIndex": 6
        },
        "Pengpeng Zeng": {
            "authorId": "31081539",
            "name": "Pengpeng Zeng",
            "hIndex": 10
        },
        "Wenchao Zhao": {
            "authorId": "6708703",
            "name": "Wen-Chao Zhao",
            "hIndex": 9
        },
        "Siheng Chen": {
            "authorId": "145552439",
            "name": "Siheng Chen",
            "hIndex": 40
        },
        "Yuanhaur Chang": {
            "name": "Yuanhaur Chang",
            "hIndex": 0
        },
        "Weiqing Wang": {
            "authorId": "50292668",
            "name": "Wang Weiqing",
            "hIndex": 6
        },
        "Wooseok Shin": {
            "authorId": "48189779",
            "name": "Woo-seok Shin",
            "hIndex": 8
        },
        "Aleksandra Faust": {
            "authorId": "145520045",
            "name": "Aleksandra Faust",
            "hIndex": 27
        },
        "Zhendong Zhao": {
            "authorId": "2435606",
            "name": "Zhendong Zhao",
            "hIndex": 22
        },
        "Tri Kurniawan Wijaya": {
            "authorId": "3174548",
            "name": "Tri Kurniawan Wijaya",
            "hIndex": 16
        },
        "Renjie Xie": {
            "authorId": "2143721582",
            "name": "Renjie Xie",
            "hIndex": 3
        },
        "Bochao Liu": {
            "authorId": "2156640145",
            "name": "Bo Liu",
            "hIndex": 13
        },
        "Roger Wattenhofer": {
            "authorId": "1716440",
            "name": "Roger Wattenhofer",
            "hIndex": 79
        },
        "SeongYeub Chu": {
            "authorId": "2313730766",
            "name": "SeongYeub Chu",
            "hIndex": 0
        },
        "Shervin Ardeshir": {
            "authorId": "2599451",
            "name": "Shervin Ardeshir",
            "hIndex": 9
        },
        "Maria Lomeli": {
            "authorId": "3376175",
            "name": "M. Lomeli",
            "hIndex": 10
        },
        "Erik Visser": {
            "authorId": "39298280",
            "name": "E. Visser",
            "hIndex": 9
        },
        "Alexey Vasilev": {
            "authorId": "2059063077",
            "name": "A. Vasilev",
            "hIndex": 4
        },
        "Aaron Broukhim": {
            "authorId": "2227561675",
            "name": "Aaron Broukhim",
            "hIndex": 1
        },
        "Anh Thai": {
            "authorId": "8360691",
            "name": "Anh Thai",
            "hIndex": 7
        },
        "Ashish Katiyar": {
            "authorId": "3412585",
            "name": "A. Katiyar",
            "hIndex": 5
        },
        "John Poole": {
            "authorId": "145138118",
            "name": "J. Poole",
            "hIndex": 27
        },
        "Zihao Tang": {
            "authorId": "2112518515",
            "name": "Zihao Tang",
            "hIndex": 5
        },
        "Jonathan Barnoud": {
            "authorId": "1867309",
            "name": "J. Barnoud",
            "hIndex": 21
        },
        "Weicong Qin": {
            "authorId": "2274936473",
            "name": "Weicong Qin",
            "hIndex": 1
        },
        "Emmanuel Jean": {
            "authorId": "92470547",
            "name": "E. Jean",
            "hIndex": 9
        },
        "Jiajia Liu": {
            "authorId": "2136211621",
            "name": "Jiajia Liu",
            "hIndex": 10
        },
        "Praveen K Kanithi": {
            "authorId": "8135229",
            "name": "P. Kanithi",
            "hIndex": 3
        },
        "Dimitri Bertsekas": {
            "authorId": "1786249",
            "name": "D. Bertsekas",
            "hIndex": 95
        },
        "N. Alpay Karag\u00f6z": {
            "authorId": "144689905",
            "name": "N. A. Karag\u00f6z",
            "hIndex": 4
        },
        "Liang Chen": {
            "authorId": "102252477",
            "name": "Liangxu Chen",
            "hIndex": 8
        },
        "Ben Nebgen": {
            "authorId": "16253773",
            "name": "B. Nebgen",
            "hIndex": 20
        },
        "Xinglin Li": {
            "authorId": "31275716",
            "name": "Liu Xinglin",
            "hIndex": 4
        },
        "Guimin Hu": {
            "authorId": "1992682487",
            "name": "Guimin Hu",
            "hIndex": 5
        },
        "Xuehai Tang": {
            "authorId": "102993876",
            "name": "Xuehai Tang",
            "hIndex": 7
        },
        "Jian-Nan Chen": {
            "authorId": "49251547",
            "name": "Jiannan Chen",
            "hIndex": 29
        },
        "Vijay Somanath": {
            "name": "Vijay Somanath",
            "hIndex": 0
        },
        "Jieming Zhu": {
            "authorId": "2142300313",
            "name": "Jieming Zhu",
            "hIndex": 7
        },
        "Vasileios Argyriou": {
            "authorId": "1689047",
            "name": "Vasileios Argyriou",
            "hIndex": 27
        },
        "Venkatesh Pattabiraman": {
            "authorId": "2284217920",
            "name": "Venkatesh Pattabiraman",
            "hIndex": 1
        },
        "Kushal Kedia": {
            "authorId": "34904511",
            "name": "K. Kedia",
            "hIndex": 4
        },
        "Ludvig Lemner": {
            "name": "Ludvig Lemner",
            "hIndex": 0
        },
        "Jiangjie Chen": {
            "name": "Jiangjie Chen",
            "hIndex": 0
        },
        "Ali Maatouk": {
            "authorId": "13147876",
            "name": "A. Maatouk",
            "hIndex": 13
        },
        "Huajun Chen": {
            "authorId": "2138727955",
            "name": "Huajun Chen",
            "hIndex": 20
        },
        "Shuang Liang": {
            "authorId": "103016255",
            "name": "S. Liang",
            "hIndex": 20
        },
        "Jianzhi Lu": {
            "authorId": "150152433",
            "name": "Jianzhi Lu",
            "hIndex": 2
        },
        "Tiejun Ma": {
            "authorId": "66789525",
            "name": "Tiejun Ma",
            "hIndex": 8
        },
        "Ning Zhang": {
            "authorId": "6283900",
            "name": "Ningyan Zhang",
            "hIndex": 36
        },
        "Hongming Zhang": {
            "authorId": "1492086327",
            "name": "Hongming Zhang",
            "hIndex": 29
        },
        "Elisabetta Farella": {
            "authorId": "1777488",
            "name": "Elisabetta Farella",
            "hIndex": 34
        },
        "Colin Cherry": {
            "authorId": "144507724",
            "name": "Colin Cherry",
            "hIndex": 39
        },
        "Zainab Ali Majid": {
            "authorId": "2090383009",
            "name": "Z. Ali",
            "hIndex": 0
        },
        "Anubhav Bhatti": {
            "authorId": "14987805",
            "name": "Anubha Bhatti",
            "hIndex": 6
        },
        "Xiaoyun Jin": {
            "authorId": "16077543",
            "name": "Xiaoyun Jin",
            "hIndex": 5
        },
        "Xiangjie Wang": {
            "authorId": "1818193337",
            "name": "Xiang-zhi Wang",
            "hIndex": 8
        },
        "Aili Chen": {
            "authorId": "144394467",
            "name": "Aili Chen",
            "hIndex": 10
        },
        "Geraint Wiggins": {
            "authorId": "1808338",
            "name": "Geraint A. Wiggins",
            "hIndex": 43
        },
        "Cassandra Overney": {
            "authorId": "115250823",
            "name": "Cassandra Overney",
            "hIndex": 4
        },
        "Joakim Wennerberg": {
            "name": "Joakim Wennerberg",
            "hIndex": 0
        },
        "Mingguang He": {
            "authorId": "2116974703",
            "name": "M. He",
            "hIndex": 9
        },
        "Nicholas Pochinkov": {
            "authorId": "2289832527",
            "name": "Nicholas Pochinkov",
            "hIndex": 1
        },
        "Liqiang Jing": {
            "authorId": "2468898",
            "name": "L. Jing",
            "hIndex": 64
        },
        "Dan Oneata": {
            "authorId": "3095774",
            "name": "Dan Onea\u0163\u0103",
            "hIndex": 10
        },
        "Danilo Numeroso": {
            "authorId": "2008258460",
            "name": "Danilo Numeroso",
            "hIndex": 5
        },
        "Aymen Ben Said": {
            "authorId": "104070077",
            "name": "A. Said",
            "hIndex": 5
        },
        "Ling-Yuan Chen": {
            "authorId": "123331696",
            "name": "Yuanling Chen",
            "hIndex": 18
        },
        "Jun Xu": {
            "authorId": "144381632",
            "name": "Jun Xu",
            "hIndex": 34
        },
        "Zhonglin Jiang": {
            "authorId": "20685675",
            "name": "Zhonglin Jiang",
            "hIndex": 7
        },
        "Wenqi Jia": {
            "authorId": "14485983",
            "name": "Wen\u2010Liang Jia",
            "hIndex": 9
        },
        "Daeun Kyung": {
            "authorId": "2167780446",
            "name": "Daeun Kyung",
            "hIndex": 4
        },
        "Bastian Rieck": {
            "authorId": "2208337",
            "name": "Bastian Alexander Rieck",
            "hIndex": 25
        },
        "Chenjie Xie": {
            "authorId": "25153282",
            "name": "Chenjie Xie",
            "hIndex": 9
        },
        "Zhen Huang": {
            "name": "Zhen Huang",
            "hIndex": 0
        },
        "Xiaoyu Shen": {
            "authorId": "3377433",
            "name": "Xiao-ji Shen",
            "hIndex": 5
        },
        "Shusen Zhang": {
            "authorId": "3552990",
            "name": "Shusen Zhang",
            "hIndex": 10
        },
        "Shichao Song": {
            "authorId": "2631267",
            "name": "Shichao Song",
            "hIndex": 15
        },
        "Yiwen Wang": {
            "authorId": "2108841643",
            "name": "Yiwen Wang",
            "hIndex": 10
        },
        "Chinmay Rao": {
            "authorId": "38349046",
            "name": "C. Rao",
            "hIndex": 7
        },
        "Hyeonwoo Kim": {
            "authorId": "2109895313",
            "name": "Hyeonwoo Kim",
            "hIndex": 7
        },
        "Harman Singh": {
            "authorId": "32444039",
            "name": "Harman Preet Singh",
            "hIndex": 7
        },
        "Yongkang Li": {
            "authorId": "2110412090",
            "name": "Yongkang Li",
            "hIndex": 10
        },
        "Minjoon Seo": {
            "authorId": "4418074",
            "name": "Minjoon Seo",
            "hIndex": 30
        },
        "Simon Jenni": {
            "authorId": "46361084",
            "name": "S. Jenni",
            "hIndex": 24
        },
        "Xianpei Han": {
            "authorId": "3194601",
            "name": "Xianpei Han",
            "hIndex": 25
        },
        "Yuxiao Dong": {
            "authorId": "2047998",
            "name": "Yuxiao Dong",
            "hIndex": 42
        },
        "Michael Cafarella": {
            "authorId": "1725561",
            "name": "Michael J. Cafarella",
            "hIndex": 40
        },
        "Changsheng Xu": {
            "authorId": "145194969",
            "name": "Changsheng Xu",
            "hIndex": 71
        },
        "Akash Maharaj": {
            "authorId": "4927719",
            "name": "Akash V. Maharaj",
            "hIndex": 10
        },
        "Zhe Xu": {
            "authorId": "98220552",
            "name": "Zhenliang Xu",
            "hIndex": 10
        },
        "Yu Qin": {
            "authorId": "9907021",
            "name": "Yu-ling Qin",
            "hIndex": 10
        },
        "Kaidi Wang": {
            "authorId": "48884732",
            "name": "Ka Wang",
            "hIndex": 6
        },
        "Hongcheng Guo": {
            "authorId": "2234806",
            "name": "Hongcheng Guo",
            "hIndex": 9
        },
        "Anoushka Harit": {
            "authorId": "2084552907",
            "name": "Anoushka Harit",
            "hIndex": 6
        },
        "Bo Jiang": {
            "authorId": "2008287200",
            "name": "Jiang Bo",
            "hIndex": 12
        },
        "Randy Buchanan": {
            "authorId": "26027341",
            "name": "Randy K. Buchanan",
            "hIndex": 10
        },
        "Nienke Hoekstra": {
            "authorId": "38379587",
            "name": "N. Hoekstra",
            "hIndex": 6
        },
        "Khalid Al-Khatib": {
            "authorId": "2248209",
            "name": "Khalid Al Khatib",
            "hIndex": 16
        },
        "Lina Yao": {
            "authorId": "2082966",
            "name": "Lina Yao",
            "hIndex": 45
        },
        "Yizhou Sun": {
            "authorId": "2109461904",
            "name": "Yizhou Sun",
            "hIndex": 57
        },
        "Florentin D Hildebrandt": {
            "authorId": "2051712726",
            "name": "F. D. Hildebrandt",
            "hIndex": 3
        },
        "Wen-Yen Chen": {
            "authorId": "2118386551",
            "name": "Yen-Wen Chen",
            "hIndex": 26
        },
        "Michael Achmann-Denkler": {
            "authorId": "2319416553",
            "name": "Michael Achmann-Denkler",
            "hIndex": 0
        },
        "Qingyang Li": {
            "authorId": "2108421323",
            "name": "Qingyan Li",
            "hIndex": 8
        },
        "Marco Lorenzi": {
            "authorId": "50397633",
            "name": "Marco Lorenzi",
            "hIndex": 25
        },
        "Yunwen Lei": {
            "authorId": "2365013",
            "name": "Yunwen Lei",
            "hIndex": 20
        },
        "Yoshiki Fukada": {
            "authorId": "91187957",
            "name": "Yoshiki Fukada",
            "hIndex": 6
        },
        "Yan Shvartzshnaider": {
            "authorId": "3324413",
            "name": "Yan Shvartzshnaider",
            "hIndex": 9
        },
        "Meng Wang": {
            "authorId": "2146059475",
            "name": "Meng Wang",
            "hIndex": 6
        },
        "Hao Peng": {
            "authorId": "3480163",
            "name": "Pengyu Hao",
            "hIndex": 21
        },
        "Achille Felicetti": {
            "authorId": "2828455",
            "name": "A. Felicetti",
            "hIndex": 11
        },
        "Weilin Xie": {
            "authorId": "46520587",
            "name": "W. Xie",
            "hIndex": 15
        },
        "Guohui Zhang": {
            "authorId": "46266173",
            "name": "Guohui Zhang",
            "hIndex": 25
        },
        "Chenyang Yu": {
            "authorId": "2050182903",
            "name": "Chenyang Yu",
            "hIndex": 7
        },
        "Yan Zheng": {
            "authorId": "2111090722",
            "name": "Yan Zheng",
            "hIndex": 27
        },
        "Zhe Liu": {
            "authorId": "2116746413",
            "name": "Zhe J. Liu",
            "hIndex": 26
        },
        "Filippo Aglietti": {
            "authorId": "145939060",
            "name": "F. Aglietti",
            "hIndex": 3
        },
        "Haoxuan Li": {
            "authorId": "3105865",
            "name": "Haoxuan Li",
            "hIndex": 5
        },
        "Ishan Rajendrakumar Dave": {
            "authorId": "2160630204",
            "name": "I. Dave",
            "hIndex": 6
        },
        "Jonas Stein": {
            "authorId": "48693624",
            "name": "Jonas Stein",
            "hIndex": 8
        },
        "Conor Houghton": {
            "authorId": "2517285",
            "name": "Conor J. Houghton",
            "hIndex": 15
        },
        "Mahefa Ratsisetraina Ravelonanosy": {
            "authorId": "2186053513",
            "name": "Mahefa Ratsisetraina Ravelonanosy",
            "hIndex": 1
        },
        "Lingwei Meng": {
            "authorId": "48153638",
            "name": "L. Meng",
            "hIndex": 8
        },
        "Gavin Jager": {
            "name": "Gavin Jager",
            "hIndex": 0
        },
        "Abbas Abdolmaleki": {
            "authorId": "2799799",
            "name": "A. Abdolmaleki",
            "hIndex": 26
        },
        "Vincent Wilmet": {
            "authorId": "91613022",
            "name": "Vincent Wilmet",
            "hIndex": 1
        },
        "Mehul A. Shah": {
            "authorId": "1757638",
            "name": "Mehul A. Shah",
            "hIndex": 26
        },
        "Depeng Jin": {
            "authorId": "49953590",
            "name": "Depeng Jin",
            "hIndex": 54
        },
        "Jinjoo Lee": {
            "authorId": "2108513096",
            "name": "Jinjoo Lee",
            "hIndex": 20
        },
        "Helan Hu": {
            "authorId": "2266695201",
            "name": "Helan Hu",
            "hIndex": 2
        },
        "Kristen Brent Venable": {
            "authorId": "1712010",
            "name": "K. Venable",
            "hIndex": 29
        },
        "Zhiyi Shi": {
            "authorId": "33422429",
            "name": "Zhiyi Shi",
            "hIndex": 9
        },
        "Yeqi Bai": {
            "authorId": "115358722",
            "name": "Yeqi Bai",
            "hIndex": 6
        },
        "Zhou Xu": {
            "authorId": "104855071",
            "name": "Xuhua Zhou",
            "hIndex": 10
        },
        "Jun Li": {
            "authorId": "2152747603",
            "name": "Jun Li",
            "hIndex": 23
        },
        "John Wickerson": {
            "authorId": "2970471",
            "name": "John Wickerson",
            "hIndex": 16
        },
        "Zhiwei Liu": {
            "authorId": "2130357223",
            "name": "Zhiwei Liu",
            "hIndex": 37
        },
        "Le Lu": {
            "authorId": "144931928",
            "name": "Le-wu Lu",
            "hIndex": 23
        },
        "Mohammad Reshadati": {
            "authorId": "2319604353",
            "name": "Mohammad Reshadati",
            "hIndex": 0
        },
        "Majed Bamardouf": {
            "authorId": "2320153663",
            "name": "Majed Bamardouf",
            "hIndex": 0
        },
        "Ben Fei": {
            "authorId": "1764671",
            "name": "B. Fei",
            "hIndex": 41
        },
        "Chris Lu": {
            "authorId": "11614724",
            "name": "Chris Xiaoxuan Lu",
            "hIndex": 23
        },
        "Han Fang": {
            "authorId": "2478039",
            "name": "Hanlin Fang",
            "hIndex": 11
        },
        "Yuansheng Ni": {
            "authorId": "2268493966",
            "name": "Yuansheng Ni",
            "hIndex": 4
        },
        "Jing Pan": {
            "name": "Jing Pan",
            "hIndex": 0
        },
        "Zakaria Mhammedi": {
            "authorId": "8272223",
            "name": "Zakaria Mhammedi",
            "hIndex": 12
        },
        "Rohan Dutta": {
            "authorId": "1910934995",
            "name": "R. Dutta",
            "hIndex": 9
        },
        "Silvio Savarese": {
            "name": "Silvio Savarese",
            "hIndex": 0
        },
        "Hongbo Wang": {
            "authorId": "49527762",
            "name": "Hongbo Wang",
            "hIndex": 29
        },
        "Xiaoxuan Li": {
            "authorId": "47056754",
            "name": "Xiaoxuan Li",
            "hIndex": 17
        },
        "Shijie Wang": {
            "authorId": "2283200175",
            "name": "Shijie Wang",
            "hIndex": 13
        },
        "Hongrui Liang": {
            "authorId": "2109102389",
            "name": "Hong Hu",
            "hIndex": 10
        },
        "Piotr Stolarski": {
            "name": "Piotr Stolarski",
            "hIndex": 0
        },
        "Christiaan Viviers": {
            "authorId": "2158734266",
            "name": "Christiaan G. A. Viviers",
            "hIndex": 4
        },
        "Dongdong Ge": {
            "authorId": "92066138",
            "name": "Dongdong Ge",
            "hIndex": 31
        },
        "Liang-Yan Gui": {
            "authorId": "2587808",
            "name": "Liangyan Gui",
            "hIndex": 14
        },
        "Asaf Yehudai": {
            "name": "Asaf Yehudai",
            "hIndex": 0
        },
        "Lorenzo Servadei": {
            "authorId": "35318914",
            "name": "Lorenzo Servadei",
            "hIndex": 8
        },
        "Jong Chul Ye": {
            "authorId": "2998762",
            "name": "J. C. Ye",
            "hIndex": 63
        },
        "Mohammad Marufuzzaman": {
            "authorId": "2804367",
            "name": "M. Marufuzzaman",
            "hIndex": 32
        },
        "Yannick Est\u00e8ve": {
            "authorId": "1736665",
            "name": "Y. Est\u00e8ve",
            "hIndex": 30
        },
        "Aizier Abulaiti": {
            "name": "Aizier Abulaiti",
            "hIndex": 0
        },
        "Mohammadamin Tarighatpayma": {
            "authorId": "2319605279",
            "name": "Mohammadamin Tarighatpayma",
            "hIndex": 0
        },
        "Joschka B\u00f6decker": {
            "authorId": "1762354",
            "name": "Joschka B\u00f6decker",
            "hIndex": 3
        },
        "Zifan Zhao": {
            "authorId": "1520084016",
            "name": "Zifan Zhao",
            "hIndex": 11
        },
        "Nan Jiang": {
            "authorId": "48272787",
            "name": "N. Jiang",
            "hIndex": 38
        },
        "Xiao Yu": {
            "authorId": "2139766323",
            "name": "X. Yu",
            "hIndex": 15
        },
        "Mark Santolucito": {
            "authorId": "2465596",
            "name": "Mark Santolucito",
            "hIndex": 9
        },
        "Ce Zheng": {
            "authorId": "2158130",
            "name": "C. Zheng",
            "hIndex": 22
        },
        "Chih-Cheng Lee": {
            "authorId": "2109465339",
            "name": "C. Lee",
            "hIndex": 7
        },
        "Li-Wei Chen": {
            "authorId": "2157154881",
            "name": "Wei-Li Chen",
            "hIndex": 28
        },
        "Omid Taheri": {
            "authorId": "2819168",
            "name": "O. Taheri",
            "hIndex": 7
        },
        "Sihang Jiang": {
            "authorId": "1999030240",
            "name": "Sihang Jiang",
            "hIndex": 7
        },
        "Frank Diermeyer": {
            "authorId": "3443013",
            "name": "Frank Diermeyer",
            "hIndex": 15
        },
        "Haicheng Liao": {
            "authorId": "2269958997",
            "name": "Haicheng Liao",
            "hIndex": 6
        },
        "Ehsan Shareghi": {
            "authorId": "2888926",
            "name": "Ehsan Shareghi",
            "hIndex": 14
        },
        "Matthieu Geist": {
            "authorId": "1737555",
            "name": "M. Geist",
            "hIndex": 38
        },
        "Mike Cook": {
            "authorId": "18005808",
            "name": "Michael A. Cook",
            "hIndex": 11
        },
        "Dhabaleswar K. Panda": {
            "authorId": "1731654",
            "name": "D. Panda",
            "hIndex": 57
        },
        "Ramon Tavares": {
            "authorId": "81649632",
            "name": "P. D. Silva",
            "hIndex": 13
        },
        "Zhanwen Liu": {
            "authorId": "2734674",
            "name": "Zhanwen Liu",
            "hIndex": 8
        },
        "Lameck Mbangula Amugongo": {
            "authorId": "40977304",
            "name": "L. M. Amugongo",
            "hIndex": 6
        },
        "Wenhao Guo": {
            "authorId": "2112740805",
            "name": "Wen-Hao Guo",
            "hIndex": 4
        },
        "Guanwen Xie": {
            "authorId": "2242634396",
            "name": "Guanwen Xie",
            "hIndex": 2
        },
        "Ezequiel Lopez-Lopez": {
            "authorId": "123170618",
            "name": "Ezequiel L\u00f3pez-Rubio",
            "hIndex": 20
        },
        "Anna Lukina": {
            "authorId": "144997964",
            "name": "Anna Lukina",
            "hIndex": 8
        },
        "Junhan Liu": {
            "authorId": "49722758",
            "name": "Junhan Liu",
            "hIndex": 9
        },
        "Dallas Sacca": {
            "authorId": "2299023280",
            "name": "Dallas Sacca",
            "hIndex": 0
        },
        "Hassan Shakil": {
            "authorId": "2134584075",
            "name": "Mohammad Hassan Shakil",
            "hIndex": 13
        },
        "Zhicheng Ren": {
            "authorId": "2113995826",
            "name": "Zhi-Jian Ren",
            "hIndex": 3
        },
        "Dawei Dai": {
            "authorId": "8412808",
            "name": "D. Dai",
            "hIndex": 13
        },
        "Giovanni Pollo": {
            "authorId": "2242918869",
            "name": "Giovanni Pollo",
            "hIndex": 0
        },
        "Fatma Yasmine Loumachi": {
            "authorId": "2319609108",
            "name": "Fatma Yasmine Loumachi",
            "hIndex": 0
        },
        "Sophie Dubois": {
            "authorId": "49531602",
            "name": "S. Dubois",
            "hIndex": 13
        },
        "Qiguang Chen": {
            "authorId": "3798166",
            "name": "Qiguang Chen",
            "hIndex": 7
        },
        "Haojie Hao": {
            "authorId": "153608800",
            "name": "Haojie Zeng",
            "hIndex": 18
        },
        "Hongkai Wen": {
            "authorId": "144169294",
            "name": "Hongkai Wen",
            "hIndex": 25
        },
        "Ayal Taitler": {
            "authorId": "9538926",
            "name": "Ayal Taitler",
            "hIndex": 4
        },
        "Fuqiang Niu": {
            "authorId": "1845885703",
            "name": "Fuqiang Niu",
            "hIndex": 5
        },
        "Yuhan Ji": {
            "authorId": "102452297",
            "name": "Yu-Ji Shi",
            "hIndex": 11
        },
        "Jost Tobias Springenberg": {
            "authorId": "2060551",
            "name": "Jost Tobias Springenberg",
            "hIndex": 38
        },
        "Sabrina Di Lorenzo": {
            "authorId": "40299187",
            "name": "Sabrina Di Lorenzo",
            "hIndex": 1
        },
        "Zeming Wei": {
            "authorId": "2191808925",
            "name": "Zeming Wei",
            "hIndex": 7
        },
        "Zhuo Li": {
            "authorId": "2118393168",
            "name": "Zhuo Li",
            "hIndex": 10
        },
        "Jia Zhu": {
            "authorId": "2144872680",
            "name": "Jiajia Zhu",
            "hIndex": 10
        },
        "Kiet Pham": {
            "authorId": "107614994",
            "name": "Kiet Tuan Huy Pham",
            "hIndex": 10
        },
        "Glen Berseth": {
            "authorId": "2994035",
            "name": "G. Berseth",
            "hIndex": 20
        },
        "Xichou Zhu": {
            "authorId": "2319618691",
            "name": "Xichou Zhu",
            "hIndex": 0
        },
        "Mingze Ni": {
            "authorId": "2144594837",
            "name": "Mingze Ni",
            "hIndex": 3
        },
        "Kayla Kahn": {
            "authorId": "2136604314",
            "name": "Kayla Kahn",
            "hIndex": 2
        },
        "Jian Jun Zhang": {
            "authorId": "2144230989",
            "name": "Jian-Jun Zhang",
            "hIndex": 16
        },
        "Hongwei Zheng": {
            "authorId": "49170696",
            "name": "Hongwei Zheng",
            "hIndex": 13
        },
        "Emmanuel Coquery": {
            "authorId": "1718479",
            "name": "E. Coquery",
            "hIndex": 9
        },
        "Jeremy Gow": {
            "authorId": "2419877",
            "name": "J. Gow",
            "hIndex": 19
        },
        "Juan A. Berrios Moya": {
            "authorId": "2308028764",
            "name": "Juan A. Berrios Moya",
            "hIndex": 0
        },
        "Weidong Wen": {
            "authorId": "66587295",
            "name": "Wei-Wen Hu",
            "hIndex": 17
        },
        "Jianguo Wang": {
            "authorId": "49606010",
            "name": "Jianguo Wang",
            "hIndex": 21
        },
        "Jianyang Gao": {
            "authorId": "93573097",
            "name": "Jianyang Gao",
            "hIndex": 13
        },
        "Yiwei Liu": {
            "authorId": "2108146022",
            "name": "Yiwei Liu",
            "hIndex": 25
        },
        "Song Gao": {
            "authorId": "143828529",
            "name": "Song Gao",
            "hIndex": 31
        },
        "Tao Shan": {
            "authorId": "1936522",
            "name": "T. Shan",
            "hIndex": 21
        },
        "Ismael Benito-Altamirano": {
            "authorId": "1404109296",
            "name": "I. Benito-Altamirano",
            "hIndex": 4
        },
        "Ritik Shrivastava": {
            "authorId": "2319830241",
            "name": "Ritik Shrivastava",
            "hIndex": 0
        },
        "Yuxiao Zhou": {
            "authorId": "2311570154",
            "name": "Yuxiao Zhou",
            "hIndex": 4
        },
        "Zainab Iftikhar": {
            "authorId": "51519359",
            "name": "Zainab Iftikhar",
            "hIndex": 10
        },
        "Yunpeng Chai": {
            "authorId": "2332105",
            "name": "Yunpeng Chai",
            "hIndex": 12
        },
        "Guang Li": {
            "authorId": "1872310",
            "name": "Guangping Li",
            "hIndex": 40
        },
        "Yong Liu": {
            "name": "Yong Liu",
            "hIndex": 0
        },
        "Paul Bogdan": {
            "authorId": "1698286",
            "name": "P. Bogdan",
            "hIndex": 38
        },
        "Xiaofei Xie": {
            "authorId": "49419199",
            "name": "Xiaofei Xie",
            "hIndex": 35
        },
        "Jongmin Yu": {
            "authorId": "2199479",
            "name": "Jongmin Yu",
            "hIndex": 11
        },
        "Martha White": {
            "authorId": "144492244",
            "name": "Martha White",
            "hIndex": 42
        },
        "Xiaolin Wang": {
            "authorId": "48093492",
            "name": "Wang Xiaolin",
            "hIndex": 15
        },
        "Caiming Xiong": {
            "name": "Caiming Xiong",
            "hIndex": 0
        },
        "Yun-hui Liu": {
            "authorId": "119924603",
            "name": "Yunhui Liu",
            "hIndex": 53
        },
        "Krishna Manoorkar": {
            "authorId": "52037492",
            "name": "Krishna Manoorkar",
            "hIndex": 5
        },
        "Robert Wille": {
            "authorId": "144385184",
            "name": "R. Wille",
            "hIndex": 46
        },
        "Dandan Tu": {
            "authorId": "2929196",
            "name": "Dandan Tu",
            "hIndex": 11
        },
        "Stanis\u0142aw Dro\u017cd\u017c": {
            "authorId": "3183845",
            "name": "S. Dro\u017cd\u017c",
            "hIndex": 36
        },
        "Zengyi Gao": {
            "authorId": "2319809105",
            "name": "Zengyi Gao",
            "hIndex": 0
        },
        "Huan Wang": {
            "authorId": "2113268879",
            "name": "H. Wang",
            "hIndex": 15
        },
        "Tengfei Xue": {
            "authorId": "26657302",
            "name": "Teng-fei Xue",
            "hIndex": 11
        },
        "Felix Wu": {
            "authorId": "2297390759",
            "name": "Felix F. Wu",
            "hIndex": 54
        },
        "Jasper Dekoninck": {
            "authorId": "2268310707",
            "name": "Jasper Dekoninck",
            "hIndex": 2
        },
        "Thomas Lemberger": {
            "authorId": "40307239",
            "name": "T. Lemberger",
            "hIndex": 34
        },
        "Mohammad Farazi": {
            "authorId": "144525772",
            "name": "Mohammad Farazi",
            "hIndex": 5
        },
        "Stephane Marchand-Maillet": {
            "authorId": "1398659765",
            "name": "S. Marchand-Maillet",
            "hIndex": 25
        },
        "Ernesto William De Luca": {
            "authorId": "1692708",
            "name": "E. W. D. Luca",
            "hIndex": 18
        },
        "Maciej A. Mazurowski": {
            "authorId": "1786177",
            "name": "M. Mazurowski",
            "hIndex": 35
        },
        "Pedro Ramoneda": {
            "authorId": "2140581928",
            "name": "Pedro Ramoneda",
            "hIndex": 4
        },
        "Jianpeng Cheng": {
            "authorId": "1941442",
            "name": "Jianpeng Cheng",
            "hIndex": 13
        },
        "Ahmad Abdellatif": {
            "authorId": "145141548",
            "name": "Ahmad Abdellatif",
            "hIndex": 6
        },
        "Yan Ru Pei": {
            "authorId": "39872583",
            "name": "M. Wang",
            "hIndex": 89
        },
        "Shuxian Bi": {
            "authorId": "152264711",
            "name": "Shuxian Bi",
            "hIndex": 13
        },
        "Daniel Otero": {
            "authorId": "17220870",
            "name": "L. D. Otero",
            "hIndex": 13
        },
        "Chongxin Fan": {
            "authorId": "16066477",
            "name": "Chongxin Fan",
            "hIndex": 1
        },
        "Francesco Osborne": {
            "authorId": "2052329",
            "name": "Francesco Osborne",
            "hIndex": 23
        },
        "Alex Bewley": {
            "authorId": "2457683",
            "name": "A. Bewley",
            "hIndex": 20
        },
        "Hao Xu": {
            "authorId": "2108834656",
            "name": "Haoting Xu",
            "hIndex": 27
        },
        "Xinyan Wang": {
            "authorId": "51010714",
            "name": "Xinyan Wang",
            "hIndex": 18
        },
        "Sookbun Lee": {
            "authorId": "2305177434",
            "name": "Sookbun Lee",
            "hIndex": 1
        },
        "Shun Zhang": {
            "authorId": "2108060170",
            "name": "Shunian Zhang",
            "hIndex": 12
        },
        "Marco Dinarelli": {
            "authorId": "3003979",
            "name": "Marco Dinarelli",
            "hIndex": 17
        },
        "Pavel Chizhov": {
            "authorId": "84498931",
            "name": "P. Chizhov",
            "hIndex": 9
        },
        "Tomoyoshi Utsumi": {
            "authorId": "143709200",
            "name": "T. Utsumi",
            "hIndex": 2
        },
        "Jiyeong Bae": {
            "authorId": "4347077",
            "name": "Jiyeong Bae",
            "hIndex": 3
        },
        "Xiaolong Wang": {
            "authorId": "1519966623",
            "name": "Xiaolong Wang",
            "hIndex": 22
        },
        "Cathy Wu": {
            "authorId": "1744726",
            "name": "Cathy H. Wu",
            "hIndex": 61
        },
        "Nojun Kwak": {
            "authorId": "3160425",
            "name": "Nojun Kwak",
            "hIndex": 36
        },
        "Parth Parmar": {
            "authorId": "2060311554",
            "name": "Parth Parmar",
            "hIndex": 2
        },
        "Riccardo Taiello": {
            "authorId": "2241571176",
            "name": "Riccardo Taiello",
            "hIndex": 2
        },
        "Chuang Hu": {
            "authorId": "144879963",
            "name": "Chuan Hu",
            "hIndex": 6
        },
        "Alexander Nikitin": {
            "authorId": "33462752",
            "name": "A. Nikitin",
            "hIndex": 8
        },
        "Chengbing Wang": {
            "authorId": "48586555",
            "name": "Chengbing Wang",
            "hIndex": 30
        },
        "Erik Bekkers": {
            "authorId": "2231179",
            "name": "E. Bekkers",
            "hIndex": 23
        },
        "Anh Tuan Nguyen": {
            "authorId": "1398541475",
            "name": "A. Nguyen",
            "hIndex": 17
        },
        "Mark Lindblad": {
            "authorId": "48968801",
            "name": "M. Lindblad",
            "hIndex": 14
        },
        "Oktie Hassanzadeh": {
            "authorId": "1728091",
            "name": "Oktie Hassanzadeh",
            "hIndex": 27
        },
        "Qiang Yang": {
            "authorId": "143760307",
            "name": "Qiang Yang",
            "hIndex": 24
        },
        "Wenqing Zhang": {
            "authorId": "2108125938",
            "name": "Wenqing Zhang",
            "hIndex": 5
        },
        "Ruiming Tang": {
            "authorId": "2824766",
            "name": "Ruiming Tang",
            "hIndex": 31
        },
        "Qianyi Xu": {
            "authorId": "2279843769",
            "name": "Qianyi Xu",
            "hIndex": 3
        },
        "Jonas Weiss": {
            "authorId": "2150018338",
            "name": "J. Weiss",
            "hIndex": 13
        },
        "Martin Hirzel": {
            "authorId": "1728836",
            "name": "Martin Hirzel",
            "hIndex": 28
        },
        "Hyungkeun Park": {
            "authorId": "10382983",
            "name": "Hyungkeun Park",
            "hIndex": 9
        },
        "Jin Jiang": {
            "authorId": "46400923",
            "name": "Jinjin Jiang",
            "hIndex": 9
        },
        "Shi Yu": {
            "authorId": "47421588",
            "name": "Shicheng Yu",
            "hIndex": 27
        },
        "Lisa Christine Adams": {
            "authorId": "39429177",
            "name": "L. Adams",
            "hIndex": 18
        },
        "Tianci Bu": {
            "authorId": "51128189",
            "name": "Tianci Bu",
            "hIndex": 2
        },
        "Husam Yasin": {
            "authorId": "115500555",
            "name": "Husam N. Yasin",
            "hIndex": 2
        },
        "Assem Zhunis": {
            "authorId": "2106505094",
            "name": "Assem Zhunis",
            "hIndex": 4
        },
        "Eduard I. Vorobyov": {
            "authorId": "87669674",
            "name": "E. Vorobyov",
            "hIndex": 33
        },
        "Wanxiang Che": {
            "authorId": "2256319",
            "name": "Wanxiang Che",
            "hIndex": 50
        },
        "Zhyar Rzgar K Rostam": {
            "authorId": "1470666105",
            "name": "Z. R. K. Rostam",
            "hIndex": 1
        },
        "Qingwen Fu": {
            "authorId": "1710487066",
            "name": "Qingwen Fu",
            "hIndex": 2
        },
        "Anibely Torres": {
            "authorId": "2319627549",
            "name": "Anibely Torres",
            "hIndex": 0
        },
        "Gerasimos Spanakis": {
            "authorId": "3266578",
            "name": "Gerasimos Spanakis",
            "hIndex": 17
        },
        "Ping Wang": {
            "authorId": "2172855177",
            "name": "Ping Wang",
            "hIndex": 36
        },
        "Tao Hu": {
            "authorId": "2000471212",
            "name": "T. Hu",
            "hIndex": 9
        },
        "Shane Luke": {
            "authorId": "2068672626",
            "name": "Shaneise Ellis",
            "hIndex": 2
        },
        "Ruihan Wu": {
            "authorId": "9380962",
            "name": "Ruihan Wu",
            "hIndex": 14
        },
        "Junjie Dong": {
            "authorId": "2146158156",
            "name": "Jun Yang",
            "hIndex": 4
        },
        "Marco Montagna": {
            "authorId": "2171349",
            "name": "M. Montagna",
            "hIndex": 53
        },
        "Rattanaphon Chaisaen": {
            "authorId": "1620503913",
            "name": "Rattanaphon Chaisaen",
            "hIndex": 5
        },
        "Karthikeyan Natesan Ramamurthy": {
            "authorId": "1704263",
            "name": "K. Ramamurthy",
            "hIndex": 29
        },
        "Jillian Cwycyshyn": {
            "authorId": "2276515844",
            "name": "Jillian Cwycyshyn",
            "hIndex": 1
        },
        "Alessio Ansuini": {
            "name": "Alessio Ansuini",
            "hIndex": 0
        },
        "Lewen Wang": {
            "authorId": "2136183165",
            "name": "Lewen Wang",
            "hIndex": 6
        },
        "Xingxing Wei": {
            "authorId": "2769710",
            "name": "Xingxing Wei",
            "hIndex": 22
        },
        "Andrew Blinn": {
            "authorId": "2113663024",
            "name": "Andrew Blinn",
            "hIndex": 3
        },
        "Kaiqun Fu": {
            "authorId": "3097079",
            "name": "Kaiqun Fu",
            "hIndex": 10
        },
        "Shuo Wang": {
            "authorId": "2052328381",
            "name": "S. Wang",
            "hIndex": 6
        },
        "Yujia Fu": {
            "name": "Yujia Fu",
            "hIndex": 0
        },
        "Viktor Schlegel": {
            "authorId": "71034258",
            "name": "Viktor Schlegel",
            "hIndex": 7
        },
        "Zekang Yang": {
            "authorId": "150358653",
            "name": "Zekang Yang",
            "hIndex": 10
        },
        "Geyong Min": {
            "authorId": "145896559",
            "name": "G. Min",
            "hIndex": 55
        },
        "Kanthimathi S": {
            "authorId": "4866630",
            "name": "M. Kanthimathi",
            "hIndex": 25
        },
        "Yinghui Xia": {
            "authorId": "2111057170",
            "name": "Yinghui Xia",
            "hIndex": 5
        },
        "Citlalli Grijalva": {
            "authorId": "2319335701",
            "name": "Citlalli Grijalva",
            "hIndex": 0
        },
        "Guoliang Li": {
            "authorId": "91990478",
            "name": "Guoliang Li",
            "hIndex": 32
        },
        "Jacek Wasilewski": {
            "authorId": "40591655",
            "name": "J. Wasilewski",
            "hIndex": 13
        },
        "Ivan Koychev": {
            "authorId": "52553663",
            "name": "Ivan Koychev",
            "hIndex": 24
        },
        "Tony Garnock-Jones": {
            "authorId": "1403894496",
            "name": "Tony Garnock-Jones",
            "hIndex": 4
        },
        "Yaemi Teramoto": {
            "authorId": "31638538",
            "name": "Yaemi Teramoto",
            "hIndex": 3
        },
        "Junjie Li": {
            "authorId": "46281799",
            "name": "L. Junjie",
            "hIndex": 5
        },
        "Keshi Zhao": {
            "authorId": "2320126993",
            "name": "Keshi Zhao",
            "hIndex": 0
        },
        "Nimra": {
            "authorId": "101741256",
            "name": "A. Nimra",
            "hIndex": 3
        },
        "Jim Goddard": {
            "authorId": "34460174",
            "name": "J. Goddard",
            "hIndex": 10
        },
        "Yixiao Ge": {
            "authorId": "152988335",
            "name": "Yixiao Ge",
            "hIndex": 26
        },
        "Avanti Bhandarkar": {
            "authorId": "49182411",
            "name": "AV Bhandarkar",
            "hIndex": 2
        },
        "Meng Sun": {
            "authorId": "2115371685",
            "name": "Meng Sun",
            "hIndex": 17
        },
        "Ali Merali": {
            "authorId": "1394153404",
            "name": "Ali-Reza Merali",
            "hIndex": 1
        },
        "Jianping Zhang": {
            "authorId": "2144136701",
            "name": "Jian-Ping Zhang",
            "hIndex": 12
        },
        "Xueqi Bao": {
            "authorId": "152153143",
            "name": "Xueqi Bao",
            "hIndex": 3
        },
        "Yingwei Luo": {
            "authorId": "36644172",
            "name": "Yingwei Luo",
            "hIndex": 17
        },
        "Yang Zhou": {
            "authorId": "2628827",
            "name": "Yangxin Zhou",
            "hIndex": 12
        },
        "Varun Prakash Rajamohan": {
            "authorId": "2261174056",
            "name": "Varun Prakash Rajamohan",
            "hIndex": 0
        },
        "Hyojin Chin": {
            "authorId": "32721006",
            "name": "Hyojin Chin",
            "hIndex": 7
        },
        "Thomas Savage": {
            "authorId": "49391141",
            "name": "T. Savage",
            "hIndex": 13
        },
        "Mingze Wang": {
            "authorId": "2316671901",
            "name": "Mingze Wang",
            "hIndex": 5
        },
        "Nanyun Peng": {
            "authorId": "3157053",
            "name": "Nanyun Peng",
            "hIndex": 44
        },
        "Wei Hu": {
            "authorId": "145303919",
            "name": "Wei Hu",
            "hIndex": 38
        },
        "Rebecca Wray": {
            "authorId": "48141584",
            "name": "E. Wray",
            "hIndex": 4
        },
        "Zhongtian Sun": {
            "authorId": "1446912194",
            "name": "Zhongtian Sun",
            "hIndex": 6
        },
        "Susan A. Murphy": {
            "authorId": "144180010",
            "name": "S. Murphy",
            "hIndex": 54
        },
        "Gianluca Carloni": {
            "authorId": "2189584480",
            "name": "Gianluca Carloni",
            "hIndex": 4
        },
        "An Dinh Ngoc": {
            "authorId": "144530049",
            "name": "A. Dinh",
            "hIndex": 3
        },
        "Lutfi Eren Erdogan": {
            "authorId": "2311119790",
            "name": "Lutfi Eren Erdogan",
            "hIndex": 0
        },
        "Junyu Gao": {
            "authorId": "2065757129",
            "name": "Junyu Gao",
            "hIndex": 20
        },
        "Tan Yu": {
            "authorId": "145279310",
            "name": "Yu Tan",
            "hIndex": 9
        },
        "Xuan Wang": {
            "authorId": "104030739",
            "name": "Xuanhua Wang",
            "hIndex": 8
        },
        "Chengzhong Xu": {
            "authorId": "2153076221",
            "name": "Chengzhong Xu",
            "hIndex": 11
        },
        "Juan Wachs": {
            "authorId": "1768610",
            "name": "J. Wachs",
            "hIndex": 31
        },
        "Krishna R. Kalari": {
            "authorId": "2090320",
            "name": "Krishna R. Kalari",
            "hIndex": 42
        },
        "Arjun Roy": {
            "authorId": "9294315",
            "name": "A. Kar-Roy",
            "hIndex": 10
        },
        "Lei Fang": {
            "authorId": "1491729397",
            "name": "F. Lei",
            "hIndex": 20
        },
        "Vincent Gripon": {
            "authorId": "144916029",
            "name": "Vincent Gripon",
            "hIndex": 25
        },
        "Dingjie Song": {
            "authorId": "1610927763",
            "name": "Dingjie Song",
            "hIndex": 4
        },
        "Yuto Kondo": {
            "authorId": "1733256955",
            "name": "Yuto Kondo",
            "hIndex": 4
        },
        "Wenxiao Wan": {
            "name": "Wenxiao Wan",
            "hIndex": 0
        },
        "Xinlong Hao": {
            "authorId": "2218547568",
            "name": "Xin Yu",
            "hIndex": 6
        },
        "Mustafa Co\u015fkun": {
            "authorId": "144093408",
            "name": "M. Coskun",
            "hIndex": 9
        },
        "Ran Xu": {
            "authorId": "143668233",
            "name": "Xu Ran",
            "hIndex": 11
        },
        "Ren Togo": {
            "authorId": "3470264",
            "name": "Ren Togo",
            "hIndex": 13
        },
        "A. Sanchez-Cuadrado": {
            "authorId": "2319413191",
            "name": "A. Sanchez-Cuadrado",
            "hIndex": 0
        },
        "Hongbin Liu": {
            "authorId": "2048414985",
            "name": "Hongbin Liu",
            "hIndex": 38
        },
        "Soo Ye Kim": {
            "authorId": "5168807",
            "name": "S. Kim",
            "hIndex": 12
        },
        "Sreedath Panat": {
            "authorId": "37249193",
            "name": "S. Panat",
            "hIndex": 3
        },
        "Nathan Lambert": {
            "authorId": "2052363815",
            "name": "Nathan Lambert",
            "hIndex": 14
        },
        "Khalid Benabdeslem": {
            "authorId": "1776295",
            "name": "K. Benabdeslem",
            "hIndex": 11
        },
        "Xinyi Bai": {
            "authorId": "152742134",
            "name": "X. Bai",
            "hIndex": 4
        },
        "Vivian Ding": {
            "authorId": "91408223",
            "name": "V. Ding",
            "hIndex": 1
        },
        "Mark Xu": {
            "authorId": "50746241",
            "name": "Mark Xu",
            "hIndex": 12
        },
        "Junyang Zhong": {
            "authorId": "46179758",
            "name": "Junyang Zhong",
            "hIndex": 5
        },
        "Thomas Gresavage": {
            "authorId": "72203546",
            "name": "Thomas J. Gresavage",
            "hIndex": 0
        },
        "Cheol Jun Cho": {
            "authorId": "7004920",
            "name": "Jun-Cheol Cho",
            "hIndex": 11
        },
        "Andrea Mannocci": {
            "authorId": "2043406",
            "name": "Andrea Mannocci",
            "hIndex": 12
        },
        "Wei Gao": {
            "authorId": "2153710865",
            "name": "Wei-Qiang Gao",
            "hIndex": 37
        },
        "Kris Pranoto": {
            "authorId": "2104965470",
            "name": "K. Pranoto",
            "hIndex": 3
        },
        "Guillermo Bern\u00e1rdez": {
            "authorId": "2158186200",
            "name": "J. M. L\u00f3pez",
            "hIndex": 6
        },
        "Yonatan Belinkov": {
            "authorId": "2083259",
            "name": "Yonatan Belinkov",
            "hIndex": 47
        },
        "Haoqiong Bian": {
            "authorId": "2667024",
            "name": "Haoqiong Bian",
            "hIndex": 5
        },
        "Lui Sha": {
            "authorId": "2919287",
            "name": "L. Sha",
            "hIndex": 61
        },
        "Jiantong Jiang": {
            "authorId": "22171652",
            "name": "Jiantong Jiang",
            "hIndex": 4
        },
        "Zhuo Liu": {
            "name": "Zhuo Liu",
            "hIndex": 0
        },
        "Huan Sun": {
            "authorId": "2115410952",
            "name": "H. Sun",
            "hIndex": 6
        },
        "Robert Klein": {
            "authorId": "144760496",
            "name": "R. Klein",
            "hIndex": 9
        },
        "Daniel Zhang-li": {
            "authorId": "2165225735",
            "name": "Daniel Zhang-li",
            "hIndex": 4
        },
        "Daoguang Zan": {
            "authorId": "2134434187",
            "name": "Daoguang Zan",
            "hIndex": 7
        },
        "Ryan Whetten": {
            "authorId": "2209395914",
            "name": "Ryan Whetten",
            "hIndex": 2
        },
        "Peyman Najafirad": {
            "authorId": "71756373",
            "name": "Peyman Najafirad",
            "hIndex": 10
        },
        "Corey Miller": {
            "authorId": "32360429",
            "name": "Corey N. Miller",
            "hIndex": 16
        },
        "Kejun Zhang": {
            "authorId": "151470175",
            "name": "Ke-jun Zhang",
            "hIndex": 16
        },
        "Christoforos Spyretos": {
            "authorId": "2319410578",
            "name": "Christoforos Spyretos",
            "hIndex": 0
        },
        "Alberto Sinigaglia": {
            "authorId": "2045939975",
            "name": "A. Sinigaglia",
            "hIndex": 1
        },
        "Jean-Marc Ogier": {
            "authorId": "1695766",
            "name": "J. Ogier",
            "hIndex": 29
        },
        "Qicheng Lao": {
            "authorId": "40522277",
            "name": "Qicheng Lao",
            "hIndex": 12
        },
        "Moe Kayali": {
            "authorId": "1619938751",
            "name": "Moe Kayali",
            "hIndex": 3
        },
        "Mayank Dave": {
            "authorId": "144695890",
            "name": "M. Dave",
            "hIndex": 31
        },
        "Shijie Li": {
            "authorId": "73626710",
            "name": "Li Shijie",
            "hIndex": 11
        },
        "Youwei Feng": {
            "authorId": "13446742",
            "name": "Youwei Feng",
            "hIndex": 6
        },
        "Daphne Ippolito": {
            "name": "Daphne Ippolito",
            "hIndex": 0
        },
        "Seyed Mohammad Ali Jafari": {
            "authorId": "2066114448",
            "name": "Seyed Mohammad Ali Jafari",
            "hIndex": 7
        },
        "Hanming Li": {
            "authorId": "2118384711",
            "name": "Han-ming Li",
            "hIndex": 2
        },
        "Li Wang": {
            "authorId": "2152716401",
            "name": "Li Wang",
            "hIndex": 6
        },
        "Silas Workman": {
            "authorId": "2319605555",
            "name": "Silas Workman",
            "hIndex": 0
        },
        "Tianyu Shi": {
            "authorId": "2068916513",
            "name": "Tianyu Shi",
            "hIndex": 7
        },
        "Yunkai Jia": {
            "name": "Yunkai Jia",
            "hIndex": 0
        },
        "Larissa Pusch": {
            "authorId": "4773489",
            "name": "L. Pusch",
            "hIndex": 6
        },
        "Chaozheng Wang": {
            "authorId": "2144446583",
            "name": "Chao Wang",
            "hIndex": 25
        },
        "Jongho Nang": {
            "authorId": "2135810",
            "name": "J. Nang",
            "hIndex": 9
        },
        "Han Xiao": {
            "authorId": "143718069",
            "name": "X. Han",
            "hIndex": 33
        },
        "Marco Nurisso": {
            "authorId": "2159553501",
            "name": "Marco Nurisso",
            "hIndex": 2
        },
        "Alessandro T. Gifford": {
            "authorId": "2159349255",
            "name": "A. T. Gifford",
            "hIndex": 3
        },
        "Ze Wang": {
            "authorId": "2108725730",
            "name": "Zehua Wang",
            "hIndex": 8
        },
        "Jianlong Wu": {
            "authorId": "2720873",
            "name": "Jianlong Wu",
            "hIndex": 19
        },
        "Patrik Zajec": {
            "authorId": "103709243",
            "name": "Patrik Zajec",
            "hIndex": 6
        },
        "Jialin Li": {
            "authorId": "2108961257",
            "name": "Jialin Li",
            "hIndex": 21
        },
        "Shubham Jain": {
            "authorId": "1995907244",
            "name": "Shubham Jain",
            "hIndex": 4
        },
        "Arjen P. de Vries": {
            "authorId": "144509504",
            "name": "A. D. Vries",
            "hIndex": 44
        },
        "Abdelmalek Mouazer": {
            "name": "Abdelmalek Mouazer",
            "hIndex": 0
        },
        "Hiromu Yakura": {
            "authorId": "33688371",
            "name": "Hiromu Yakura",
            "hIndex": 10
        },
        "Frank Nielsen": {
            "authorId": "144863930",
            "name": "F. Nielsen",
            "hIndex": 44
        },
        "Xiaoyu Tan": {
            "authorId": "5584527",
            "name": "X. Tan",
            "hIndex": 17
        },
        "Peter Wu": {
            "authorId": "2111192945",
            "name": "Peter K. Wu",
            "hIndex": 12
        },
        "Yongxin Deng": {
            "authorId": "2108849928",
            "name": "Yongxin Deng",
            "hIndex": 10
        },
        "June Hyung Kim": {
            "authorId": "2109240375",
            "name": "Junehyung Kim",
            "hIndex": 20
        },
        "Federico Ruggeri": {
            "authorId": "39921439",
            "name": "Federico Ruggeri",
            "hIndex": 8
        },
        "Yubo Zhang": {
            "authorId": "47302553",
            "name": "Zhang Yubo",
            "hIndex": 10
        },
        "Lev Telyatnikov": {
            "authorId": "1396779962",
            "name": "Lev Telyatnikov",
            "hIndex": 4
        },
        "Kun Zhu": {
            "authorId": "11052451",
            "name": "K. Zhu",
            "hIndex": 21
        },
        "Rui Wen": {
            "authorId": "48359020",
            "name": "R. Wen",
            "hIndex": 34
        },
        "Yiyang Luo": {
            "authorId": "4351951",
            "name": "Yi-Jyun Luo",
            "hIndex": 12
        },
        "Senthil Kumar Jagatheesaperumal": {
            "authorId": "2067322880",
            "name": "S. Jagatheesaperumal",
            "hIndex": 9
        },
        "Asterios Katsifodimos": {
            "authorId": "1680579",
            "name": "Asterios Katsifodimos",
            "hIndex": 21
        },
        "Fu Lin": {
            "authorId": "1734244",
            "name": "Fu-Ren Lin",
            "hIndex": 27
        },
        "Zohreh Tajabadi": {
            "authorId": "67198272",
            "name": "Zohreh Tajabadi",
            "hIndex": 3
        },
        "Christian Bartelt": {
            "authorId": "1768219",
            "name": "Christian Bartelt",
            "hIndex": 11
        },
        "Yunli Wang": {
            "authorId": "2107934107",
            "name": "Yunling Wang",
            "hIndex": 17
        },
        "Yaxin Xiao": {
            "authorId": "46671860",
            "name": "Yaxin Xiao",
            "hIndex": 4
        },
        "Gopala Krishna Anumanchipalli": {
            "authorId": "1692246",
            "name": "G. Anumanchipalli",
            "hIndex": 17
        },
        "Fei Ma": {
            "authorId": "47865205",
            "name": "M. Fei",
            "hIndex": 7
        },
        "Chengming Li": {
            "authorId": "2128670258",
            "name": "Chengming Li",
            "hIndex": 13
        },
        "Chinmay Maheshwari": {
            "authorId": "27675535",
            "name": "C. Maheshwari",
            "hIndex": 9
        },
        "Julien Grosjean": {
            "authorId": "3310323",
            "name": "J. Grosjean",
            "hIndex": 11
        },
        "Mick Grierson": {
            "authorId": "1691146",
            "name": "M. Grierson",
            "hIndex": 13
        },
        "Fabian Diet": {
            "authorId": "65731550",
            "name": "Fabian Diet",
            "hIndex": 2
        },
        "Xin Zou": {
            "authorId": "30459435",
            "name": "Xin-Yao Zou",
            "hIndex": 6
        },
        "Ponnurangam Kumaraguru": {
            "authorId": "1734731",
            "name": "P. Kumaraguru",
            "hIndex": 41
        },
        "Jong Moon": {
            "authorId": "1780754",
            "name": "J. Moon",
            "hIndex": 46
        },
        "Jiaming Yin": {
            "authorId": "3275420",
            "name": "J. Yin",
            "hIndex": 12
        },
        "Chang-yu Hsieh": {
            "authorId": "33397943",
            "name": "Chang-Yu Hsieh",
            "hIndex": 26
        },
        "Baskar Ganapathysubramanian": {
            "authorId": "3042938",
            "name": "B. Ganapathysubramanian",
            "hIndex": 44
        },
        "Ahmad Halwani": {
            "authorId": "49841891",
            "name": "A. Halwani",
            "hIndex": 19
        },
        "Thomas Brox": {
            "authorId": "1710872",
            "name": "T. Brox",
            "hIndex": 94
        },
        "Mutian He": {
            "authorId": "51056597",
            "name": "Mutian He",
            "hIndex": 8
        },
        "S. Kevin Zhou": {
            "authorId": "2107323185",
            "name": "S. K. Zhou",
            "hIndex": 21
        },
        "Santosh Kumar Vishvakarma": {
            "authorId": "3052594",
            "name": "S. Vishvakarma",
            "hIndex": 20
        },
        "Atsushi Otsuka": {
            "authorId": "144346567",
            "name": "A. Otsuka",
            "hIndex": 45
        },
        "Esther Lagemann": {
            "authorId": "2258963033",
            "name": "Esther Lagemann",
            "hIndex": 1
        },
        "Alberto Barr\u00f3n-Cede\u00f1o": {
            "authorId": "1397442049",
            "name": "Alberto Barr\u00f3n-Cede\u00f1o",
            "hIndex": 41
        },
        "Xuanzhao Dong": {
            "authorId": "2319406104",
            "name": "Xuanzhao Dong",
            "hIndex": 0
        },
        "Anil Surisetty": {
            "authorId": "2163311863",
            "name": "Anil Surisetty",
            "hIndex": 2
        },
        "Frank Joublin": {
            "authorId": "1759554",
            "name": "F. Joublin",
            "hIndex": 17
        },
        "Stefanos Nikolaidis": {
            "authorId": "37586303",
            "name": "S. Nikolaidis",
            "hIndex": 26
        },
        "Radha R. Gulhane": {
            "authorId": "71849671",
            "name": "Radha Gulhane",
            "hIndex": 1
        },
        "Yuhao Wu": {
            "authorId": "1920125152",
            "name": "Yuhao Wu",
            "hIndex": 5
        },
        "Hannaneh Hajishirzi": {
            "authorId": "2548384",
            "name": "Hannaneh Hajishirzi",
            "hIndex": 71
        },
        "Tong Yu": {
            "authorId": "48429043",
            "name": "Y. Tong",
            "hIndex": 22
        },
        "Jiang Bian": {
            "authorId": "143901037",
            "name": "J. Bian",
            "hIndex": 32
        },
        "Procheta Sen": {
            "authorId": "50650338",
            "name": "Procheta Sen",
            "hIndex": 14
        },
        "Varun Gangal": {
            "authorId": "3375999",
            "name": "Varun Gangal",
            "hIndex": 11
        },
        "Sajib Acharjee Dip": {
            "authorId": "2293313831",
            "name": "Sajib Acharjee Dip",
            "hIndex": 1
        },
        "Jesse Roberts": {
            "authorId": "145916413",
            "name": "Jesse D. Roberts",
            "hIndex": 20
        },
        "Ke Qin": {
            "name": "Ke Qin",
            "hIndex": 0
        },
        "Ryo Wakizaka": {
            "authorId": "2203954593",
            "name": "Ryo Wakizaka",
            "hIndex": 1
        },
        "Matthias A\u00dfenmacher": {
            "authorId": "7809845",
            "name": "M. A\u00dfenmacher",
            "hIndex": 6
        },
        "Wan Li": {
            "authorId": "48624868",
            "name": "Wanwan Li",
            "hIndex": 32
        },
        "Jinze Li": {
            "authorId": "2125648310",
            "name": "Jinze Li",
            "hIndex": 4
        },
        "Johanna Sandgren": {
            "authorId": "2163414",
            "name": "J. Sandgren",
            "hIndex": 15
        },
        "Ruoyu Wen": {
            "authorId": "36008567",
            "name": "R. Wen",
            "hIndex": 3
        },
        "Abdurahman Maarouf": {
            "authorId": "2188779058",
            "name": "Abdurahman Maarouf",
            "hIndex": 2
        },
        "Wen-Huang Cheng": {
            "authorId": "3036610",
            "name": "Wen-Cheng Huang",
            "hIndex": 33
        },
        "Ge Yang": {
            "authorId": "49982506",
            "name": "Y. Ge",
            "hIndex": 14
        },
        "Ridong Han": {
            "authorId": "2093920100",
            "name": "Ridong Han",
            "hIndex": 6
        },
        "Liyuan Ma": {
            "authorId": "50709913",
            "name": "Li-Yuan Ma",
            "hIndex": 9
        },
        "Michael R. Lyu": {
            "authorId": "1785083",
            "name": "Michael R. Lyu",
            "hIndex": 98
        },
        "Vitor Guizilini": {
            "authorId": "9370721",
            "name": "V. Guizilini",
            "hIndex": 23
        },
        "Olga Kolesnikova": {
            "authorId": "39913047",
            "name": "O. Kolesnikova",
            "hIndex": 15
        },
        "Antonello Ceravola": {
            "authorId": "2832005",
            "name": "A. Ceravola",
            "hIndex": 6
        },
        "Chao Ban": {
            "authorId": "35605621",
            "name": "Chaoyi Ban",
            "hIndex": 12
        },
        "Soumajyoti Sarkar": {
            "authorId": "3081847",
            "name": "Soumajyoti Sarkar",
            "hIndex": 8
        },
        "Jinlin Wu": {
            "authorId": "2300558",
            "name": "Jin-lin Wu",
            "hIndex": 13
        },
        "Manish Nagireddy": {
            "authorId": "2273660627",
            "name": "Manish Nagireddy",
            "hIndex": 3
        },
        "Yuan Liu": {
            "authorId": "2143860110",
            "name": "Yuan Liu",
            "hIndex": 15
        },
        "Ahsan Saadat": {
            "authorId": "2452334",
            "name": "Ahsan Saadat",
            "hIndex": 3
        },
        "Mohamed Rahouti": {
            "authorId": "145202596",
            "name": "M. Rahouti",
            "hIndex": 13
        },
        "Anna L. Trella": {
            "authorId": "2113964440",
            "name": "Anna L. Trella",
            "hIndex": 3
        },
        "Jiangtao Xie": {
            "authorId": "144418234",
            "name": "Jiangtao Xie",
            "hIndex": 10
        },
        "Mengxi Jia": {
            "authorId": "72986043",
            "name": "Mengxi Jia",
            "hIndex": 8
        },
        "Jiyan Yang": {
            "authorId": "2791531",
            "name": "Jiyan Yang",
            "hIndex": 20
        },
        "J. Boal": {
            "authorId": "2829203",
            "name": "J. Boal",
            "hIndex": 25
        },
        "Yiheng Wang": {
            "authorId": "50874737",
            "name": "Yiheng Wang",
            "hIndex": 10
        },
        "Gemma Canet Tarr\u00e9s": {
            "authorId": "17121809",
            "name": "Gemma Canet Tarr\u00e9s",
            "hIndex": 2
        },
        "Nikos Kanakaris": {
            "authorId": "1471828829",
            "name": "Nikos Kanakaris",
            "hIndex": 7
        },
        "Peter Baile Chen": {
            "authorId": "2302770964",
            "name": "Peter Baille Chen",
            "hIndex": 1
        },
        "Fuqiang Liu": {
            "authorId": "1742846",
            "name": "Fuqiang Liu",
            "hIndex": 26
        },
        "Albert Zhai": {
            "authorId": "23537797",
            "name": "A. Zhai",
            "hIndex": 4
        },
        "Pierre Cl\u00e9au": {
            "name": "Pierre Cl\u00e9au",
            "hIndex": 0
        },
        "Anurudh Peduri": {
            "name": "Anurudh Peduri",
            "hIndex": 0
        },
        "Sarika Jain": {
            "authorId": "2247936",
            "name": "Sarika Jain",
            "hIndex": 24
        },
        "Aditya Parameswaran": {
            "authorId": "145592539",
            "name": "Aditya G. Parameswaran",
            "hIndex": 41
        },
        "Gaurav Pandey": {
            "authorId": "144530583",
            "name": "G. Pandey",
            "hIndex": 17
        },
        "Yuge Tu": {
            "authorId": "121894366",
            "name": "Yu-Hui Tu",
            "hIndex": 1
        },
        "Fuyang Cui": {
            "authorId": "2319357254",
            "name": "Fuyang Cui",
            "hIndex": 0
        },
        "Chanyoung Park": {
            "authorId": "2109120259",
            "name": "Chanyoung Park",
            "hIndex": 18
        },
        "Aamir Shafi": {
            "name": "Aamir Shafi",
            "hIndex": 0
        },
        "Yingfa Chen": {
            "authorId": "2109274417",
            "name": "Yingfa Chen",
            "hIndex": 4
        },
        "Pheng Ann Heng": {
            "authorId": "1714602",
            "name": "P. Heng",
            "hIndex": 101
        },
        "Takehiro Sato": {
            "authorId": "2110904655",
            "name": "Takehiro Sato",
            "hIndex": 13
        },
        "Zhipeng Teng": {
            "authorId": "8806188",
            "name": "Zhipeng Teng",
            "hIndex": 10
        },
        "Vinita Jansari": {
            "authorId": "2007176690",
            "name": "V. Jansari",
            "hIndex": 1
        },
        "Oyvind Tafjord": {
            "authorId": "3385516",
            "name": "Oyvind Tafjord",
            "hIndex": 30
        },
        "Zedong Wang": {
            "authorId": "87934196",
            "name": "Zedong Wang",
            "hIndex": 25
        },
        "Shunian Chen": {
            "authorId": "2108762505",
            "name": "Shunian Chen",
            "hIndex": 8
        },
        "Jiaxing Wu": {
            "name": "Jiaxing Wu",
            "hIndex": 0
        },
        "Hadis Shabanipour": {
            "authorId": "2319616036",
            "name": "Hadis Shabanipour",
            "hIndex": 0
        },
        "Zhiyong Wang": {
            "authorId": "144450448",
            "name": "Zhiyong Wang",
            "hIndex": 25
        },
        "Andrew Rosenberg": {
            "authorId": "46453660",
            "name": "A. Rosenberg",
            "hIndex": 46
        },
        "Jingbo Shang": {
            "authorId": "2254284383",
            "name": "Jingbo Shang",
            "hIndex": 6
        },
        "Lipika Dey": {
            "authorId": "1724193",
            "name": "Lipika Dey",
            "hIndex": 21
        },
        "Benjamin Roth": {
            "authorId": "48462153",
            "name": "Benjamin J. Roth",
            "hIndex": 12
        },
        "Lei Sang": {
            "authorId": "46864445",
            "name": "L. Sang",
            "hIndex": 11
        },
        "Shulin Cao": {
            "authorId": "1712738522",
            "name": "S. Cao",
            "hIndex": 11
        },
        "Sayeed Sajjad Razin": {
            "authorId": "2319384983",
            "name": "Sayeed Sajjad Razin",
            "hIndex": 0
        },
        "Bobby Khaleque": {
            "authorId": "2016515409",
            "name": "Bobby Khaleque",
            "hIndex": 1
        },
        "Yuhao Zhu": {
            "authorId": "2109412755",
            "name": "Yuhao Zhu",
            "hIndex": 10
        },
        "Abu Bakar Siddiqur Rahman": {
            "authorId": "2115323701",
            "name": "Abu Bakar Siddiqur Rahman",
            "hIndex": 4
        },
        "Wen Sun": {
            "authorId": "1808085",
            "name": "Wen-feng Sun",
            "hIndex": 25
        },
        "Andrew Gilbert": {
            "authorId": "143847519",
            "name": "A. Gilbert",
            "hIndex": 112
        },
        "Momchil Hardalov": {
            "authorId": "3255454",
            "name": "Momchil Hardalov",
            "hIndex": 13
        },
        "Shravan Venkatraman": {
            "authorId": "2210066816",
            "name": "Shravan Venkatraman",
            "hIndex": 2
        },
        "Xinmei Tian": {
            "authorId": "40434674",
            "name": "Xinmei Tian",
            "hIndex": 36
        },
        "Zhiqing Hu": {
            "authorId": "28113167",
            "name": "Zhiqing Hu",
            "hIndex": 15
        },
        "Anil Bas": {
            "name": "Anil Bas",
            "hIndex": 0
        },
        "Harsha Kalutarage": {
            "authorId": "2387461",
            "name": "H. Kalutarage",
            "hIndex": 8
        },
        "Leshem Chosen": {
            "authorId": "2319605710",
            "name": "Leshem Chosen",
            "hIndex": 0
        },
        "Michael-Andrei Panaitescu-Liess": {
            "authorId": "1648623943",
            "name": "Michael-Andrei Panaitescu-Liess",
            "hIndex": 3
        },
        "Mathieu La\u00ef-king": {
            "name": "Mathieu La\u00ef-king",
            "hIndex": 0
        },
        "Jack Krolik": {
            "authorId": "2319407180",
            "name": "Jack Krolik",
            "hIndex": 0
        },
        "Sai Sree Harsha": {
            "authorId": "2154622177",
            "name": "Sai Sree Harsha",
            "hIndex": 1
        },
        "Maimaiti Nuliqiman": {
            "authorId": "2267575896",
            "name": "Maimaiti Nuliqiman",
            "hIndex": 2
        },
        "Wenlin Li": {
            "authorId": "9242550",
            "name": "L. Wenlin",
            "hIndex": 4
        },
        "Holger Bronger": {
            "authorId": "3507019",
            "name": "H. Bronger",
            "hIndex": 20
        },
        "Yu Xiang Tan": {
            "authorId": "4656194",
            "name": "Xiangping Tan",
            "hIndex": 16
        },
        "Sijie Zhao": {
            "authorId": "2124493792",
            "name": "Sijie Zhao",
            "hIndex": 4
        },
        "Yuexuan Xu": {
            "authorId": "2179739041",
            "name": "Yuexuan Xu",
            "hIndex": 3
        },
        "Irwin King": {
            "authorId": "145310659",
            "name": "I. King",
            "hIndex": 12
        },
        "Kang Liu": {
            "authorId": "2218821786",
            "name": "Kangkang Liu",
            "hIndex": 19
        },
        "Aaron Elkins": {
            "authorId": "3329640",
            "name": "Aaron C. Elkins",
            "hIndex": 16
        },
        "Angela Yao": {
            "authorId": "46759569",
            "name": "Jiayun Yao",
            "hIndex": 12
        },
        "Tajuddeen Gwadabe": {
            "authorId": "2352354",
            "name": "T. Gwadabe",
            "hIndex": 8
        },
        "Liang Yang": {
            "authorId": "2143921147",
            "name": "Liangru Yang",
            "hIndex": 8
        },
        "Veronica Kecki": {
            "authorId": "2320153222",
            "name": "Veronica Kecki",
            "hIndex": 0
        },
        "Jin Song Dong": {
            "authorId": "92450679",
            "name": "J. Song",
            "hIndex": 25
        },
        "Mireia Crispin-Ortuzar": {
            "authorId": "103783571",
            "name": "M. C. Ortuzar",
            "hIndex": 42
        },
        "Zhixiang Shen": {
            "authorId": "2523717",
            "name": "Zhixiang Shen",
            "hIndex": 18
        },
        "Praneet Rathi": {
            "authorId": "2277601605",
            "name": "Praneet Rathi",
            "hIndex": 0
        },
        "Andrew Pavlo": {
            "authorId": "1774210",
            "name": "Andrew Pavlo",
            "hIndex": 35
        },
        "Hari Subramoni": {
            "authorId": "1802958",
            "name": "H. Subramoni",
            "hIndex": 27
        },
        "Aneta Pawelec": {
            "name": "Aneta Pawelec",
            "hIndex": 0
        },
        "Max Schattauer": {
            "name": "Max Schattauer",
            "hIndex": 0
        },
        "Andrew Smart": {
            "authorId": "40800194",
            "name": "A. Smart",
            "hIndex": 18
        },
        "Christoph Lassner": {
            "authorId": "3266545",
            "name": "Christoph Lassner",
            "hIndex": 20
        },
        "Akihiro Kubo": {
            "authorId": "119725518",
            "name": "A. Kubo",
            "hIndex": 35
        },
        "Shu-Kuo Zhao": {
            "authorId": "92736451",
            "name": "Shukuo Zhao",
            "hIndex": 1
        },
        "Marco Cal\u00ec": {
            "authorId": "2056948700",
            "name": "Marco Cal\u00ec",
            "hIndex": 1
        },
        "Pranay Thangeda": {
            "authorId": "103751119",
            "name": "Pranay Thangeda",
            "hIndex": 4
        },
        "Kou Tanaka": {
            "authorId": "2109245369",
            "name": "Kou Tanaka",
            "hIndex": 18
        },
        "G\u00e1bor Kert\u00e9sz": {
            "authorId": "9717627",
            "name": "G\u00e1bor Kert\u00e9sz",
            "hIndex": 7
        },
        "Andrew J. Gerber": {
            "authorId": "33736689",
            "name": "A. Gerber",
            "hIndex": 21
        },
        "Arti Singh": {
            "authorId": "152395042",
            "name": "Arti Singh",
            "hIndex": 19
        },
        "Wonseok Hwang": {
            "authorId": "48913039",
            "name": "Wonseok Hwang",
            "hIndex": 15
        },
        "Cheng Qian": {
            "authorId": "2523358",
            "name": "Qian-Fu Cheng",
            "hIndex": 11
        },
        "Pranava Madhyastha": {
            "authorId": "3238408",
            "name": "P. Madhyastha",
            "hIndex": 14
        },
        "Rossella Arcucci": {
            "authorId": "2058854",
            "name": "Rossella Arcucci",
            "hIndex": 21
        },
        "Gopal Raut": {
            "authorId": "48215276",
            "name": "Rathy Shankar",
            "hIndex": 12
        },
        "Nicola Bartolo": {
            "authorId": "6310782",
            "name": "N. Bartolo",
            "hIndex": 90
        },
        "Jianwen Jiang": {
            "authorId": "2219058012",
            "name": "Jianwen Jiang",
            "hIndex": 19
        },
        "Jacob Eisenstein": {
            "authorId": "144154709",
            "name": "Jacob Eisenstein",
            "hIndex": 52
        },
        "Andrea Stor\u00e5s": {
            "authorId": "2129233472",
            "name": "A. Stor\u00e5s",
            "hIndex": 4
        },
        "Yixuan Tang": {
            "authorId": "49578084",
            "name": "Yixuan Tang",
            "hIndex": 6
        },
        "Debaditya Shome": {
            "authorId": "2005862970",
            "name": "Debaditya Shome",
            "hIndex": 4
        },
        "Merlin Montag": {
            "authorId": "2319605029",
            "name": "Merlin Montag",
            "hIndex": 0
        },
        "Yizhe Zhang": {
            "authorId": "1591116845",
            "name": "YI-ZHE Zhang",
            "hIndex": 9
        },
        "Fuzheng Zhang": {
            "authorId": "2642200",
            "name": "Fuzheng Zhang",
            "hIndex": 34
        },
        "Zheyu Chen": {
            "authorId": "48354245",
            "name": "Zhe-yu Chen",
            "hIndex": 19
        },
        "Guanyu Lin": {
            "authorId": "49901289",
            "name": "Guanyu Chen",
            "hIndex": 12
        },
        "Desmond Elliott": {
            "authorId": "50369944",
            "name": "Desmond Elliott",
            "hIndex": 28
        },
        "Hiroaki Sugiyama": {
            "authorId": "101863242",
            "name": "H. Sugiyama",
            "hIndex": 22
        },
        "Wentao Zhang": {
            "authorId": "50539586",
            "name": "Zhang Wentao",
            "hIndex": 7
        },
        "Kunal Gupta": {
            "authorId": "1693335277",
            "name": "Kunal Gupta",
            "hIndex": 9
        },
        "Nima Mesgarani": {
            "authorId": "1686269",
            "name": "N. Mesgarani",
            "hIndex": 39
        },
        "Na Meng": {
            "authorId": "2000661177",
            "name": "N. Meng",
            "hIndex": 13
        },
        "Bipin Koirala": {
            "authorId": "2091582483",
            "name": "Bipin Koirala",
            "hIndex": 2
        },
        "Arash Sadrieh": {
            "authorId": "39987255",
            "name": "Arash Sadrieh",
            "hIndex": 6
        },
        "Maokun Li": {
            "authorId": "47628788",
            "name": "Maokun Li",
            "hIndex": 37
        },
        "Minjeon Jeon": {
            "authorId": "2319412814",
            "name": "Minjeong Jeon",
            "hIndex": 0
        },
        "Malika Meghjani": {
            "authorId": "2376514",
            "name": "Malika Meghjani",
            "hIndex": 13
        },
        "Dengcheng Hu": {
            "authorId": "71538747",
            "name": "Dengcheng Hu",
            "hIndex": 1
        },
        "Xike Xie": {
            "authorId": "1678927",
            "name": "Xike Xie",
            "hIndex": 20
        },
        "Umut Yildirim": {
            "authorId": "13139358",
            "name": "U. Y\u0131ld\u0131r\u0131m",
            "hIndex": 10
        },
        "K Roth": {
            "authorId": "2055741736",
            "name": "K. Roth",
            "hIndex": 10
        },
        "Edward Y. Chang": {
            "authorId": "33794424",
            "name": "E. Chang",
            "hIndex": 45
        },
        "Anjana Wijekoon": {
            "authorId": "4198744",
            "name": "A. Wijekoon",
            "hIndex": 6
        },
        "Yu Wang": {
            "authorId": "2153605350",
            "name": "Yu Wang",
            "hIndex": 14
        },
        "Robert Babu\u0161ka": {
            "authorId": "1705222",
            "name": "Robert Babu\u0161ka",
            "hIndex": 58
        },
        "Fei Qin": {
            "authorId": "145037027",
            "name": "F. Qin",
            "hIndex": 19
        },
        "Ryan Tabrizi": {
            "authorId": "2270943053",
            "name": "Ryan Tabrizi",
            "hIndex": 1
        },
        "Roman Hauksson": {
            "authorId": "2305680177",
            "name": "Roman Hauksson",
            "hIndex": 1
        },
        "Andrea Piano": {
            "authorId": "152507630",
            "name": "A. Piano",
            "hIndex": 12
        },
        "Nina Miolane": {
            "authorId": "2765850",
            "name": "Nina Miolane",
            "hIndex": 13
        },
        "Adrian Ulges": {
            "authorId": "1782440",
            "name": "A. Ulges",
            "hIndex": 19
        },
        "Linke Ouyang": {
            "authorId": "2161162356",
            "name": "Linke Ouyang",
            "hIndex": 7
        },
        "Shivam Pande": {
            "authorId": "151216887",
            "name": "Shivam Pande",
            "hIndex": 7
        },
        "Bo Hong": {
            "authorId": "1417466246",
            "name": "B. Hong",
            "hIndex": 10
        },
        "Riya Samanta": {
            "authorId": "46232380",
            "name": "Riya Samanta",
            "hIndex": 4
        },
        "Bo Zhang": {
            "authorId": "2208119218",
            "name": "Bo-Heng Zhang",
            "hIndex": 21
        },
        "Paulo Soares": {
            "authorId": "2141450098",
            "name": "P. Soares",
            "hIndex": 4
        },
        "Alexander Glaser": {
            "authorId": "46694525",
            "name": "A. Glaser",
            "hIndex": 16
        },
        "Mehrtash Harandi": {
            "authorId": "1686714",
            "name": "M. Harandi",
            "hIndex": 38
        },
        "Hong Liu": {
            "authorId": "2118902608",
            "name": "Hong Liu",
            "hIndex": 16
        },
        "Sally Fang": {
            "authorId": "13808514",
            "name": "S. Fang",
            "hIndex": 3
        },
        "Bhavna Agrawal": {
            "authorId": "33437371",
            "name": "B. Agrawal",
            "hIndex": 6
        },
        "Yuqiao Tan": {
            "authorId": "114699033",
            "name": "Yu Tan",
            "hIndex": 15
        },
        "Tomasz Stanisz": {
            "authorId": "3393250",
            "name": "Tomasz Stanisz",
            "hIndex": 5
        },
        "Hyeoncheol Kim": {
            "authorId": "2109893449",
            "name": "Hyeon-Cheol Kim",
            "hIndex": 12
        },
        "M. J. P\u00e9rez-Jim\u00e9nez": {
            "authorId": "145404662",
            "name": "M. J. P. Jim\u00e9nez",
            "hIndex": 16
        },
        "Anirudh Sureshan": {
            "authorId": "2320152477",
            "name": "Anirudh Sureshan",
            "hIndex": 0
        },
        "Qiwen Cui": {
            "authorId": "1993999973",
            "name": "Qiwen Cui",
            "hIndex": 10
        },
        "Leyi Pan": {
            "authorId": "2226138943",
            "name": "Leyi Pan",
            "hIndex": 4
        },
        "Benno Weck": {
            "authorId": "3254122",
            "name": "Benno Weck",
            "hIndex": 5
        },
        "Chenglei Si": {
            "authorId": "152358188",
            "name": "Chenglei Si",
            "hIndex": 12
        },
        "Rachel X. Peng": {
            "authorId": "151191233",
            "name": "Rachel X. Peng",
            "hIndex": 5
        },
        "Guang Yang": {
            "authorId": "122037078",
            "name": "Guang\u2010Yang Yang",
            "hIndex": 10
        },
        "Jo\u00e3o F. Henriques": {
            "authorId": "143848064",
            "name": "Jo\u00e3o F. Henriques",
            "hIndex": 28
        },
        "Krishna Rani Kalari": {
            "authorId": "2090320",
            "name": "Krishna R. Kalari",
            "hIndex": 42
        },
        "Abdur Rahman": {
            "authorId": "2148760752",
            "name": "A. Rahman",
            "hIndex": 16
        },
        "Yankai Lin": {
            "authorId": "2427350",
            "name": "Yankai Lin",
            "hIndex": 27
        },
        "Yanchen Wang": {
            "authorId": "2125062570",
            "name": "Yanchen Wang",
            "hIndex": 6
        },
        "D. Orellana-Mart\u00edn": {
            "authorId": "145494327",
            "name": "D. Mart\u00edn",
            "hIndex": 3
        },
        "Lan Xu": {
            "authorId": "8540844",
            "name": "Lan-lan Xu",
            "hIndex": 7
        },
        "Alex Meyer": {
            "authorId": "30110923",
            "name": "Alex M Meyer",
            "hIndex": 7
        },
        "Bowen Qu": {
            "authorId": "153249713",
            "name": "Bowen Qu",
            "hIndex": 4
        },
        "Han Zhang": {
            "authorId": "1400978947",
            "name": "H. Han\u2010Zhang",
            "hIndex": 18
        },
        "Serghei Mihailov": {
            "authorId": "2319829644",
            "name": "Serghei Mihailov",
            "hIndex": 0
        },
        "Xueting Lin": {
            "authorId": "2136243",
            "name": "Lin Xueting",
            "hIndex": 3
        },
        "Jashwanth R": {
            "authorId": "2319333793",
            "name": "R. Jashwanth",
            "hIndex": 0
        },
        "Melek \u00d6nen": {
            "authorId": "3249805",
            "name": "Melek \u00d6nen",
            "hIndex": 22
        },
        "Jiaqi Wang": {
            "authorId": "9108891",
            "name": "W. Jiaqi",
            "hIndex": 6
        },
        "Mahdi Ghaffarzadeh-Esfahani": {
            "authorId": "2319605263",
            "name": "Mahdi Ghaffarzadeh-Esfahani",
            "hIndex": 0
        },
        "Pavel Snopov": {
            "authorId": "2308031662",
            "name": "Pavel Snopov",
            "hIndex": 0
        },
        "Haotian Guo": {
            "authorId": "2110877689",
            "name": "Hao Guo",
            "hIndex": 6
        },
        "Emilia Parada-Cabaleiro": {
            "authorId": "1410317787",
            "name": "Emilia Parada-Cabaleiro",
            "hIndex": 13
        },
        "Xuanjing Huang": {
            "authorId": "1790227",
            "name": "Xuanjing Huang",
            "hIndex": 61
        },
        "Chulayuth Asawaroengchai": {
            "authorId": "50844587",
            "name": "Chulayuth Asawaroengchai",
            "hIndex": 3
        },
        "Young-Koo Lee": {
            "authorId": "2806926",
            "name": "Young-Koo Lee",
            "hIndex": 41
        },
        "Abdulrahman Almalki": {
            "authorId": "1753392809",
            "name": "Abdulrahman Almalki",
            "hIndex": 4
        },
        "Zuzanna B\u0105czek": {
            "name": "Zuzanna B\u0105czek",
            "hIndex": 0
        },
        "Wai Keen Vong": {
            "authorId": "3379322",
            "name": "Wai Keen Vong",
            "hIndex": 11
        },
        "Tanay Aggarwal": {
            "authorId": "2191153188",
            "name": "Tanay Aggarwal",
            "hIndex": 1
        },
        "Fulei Chu": {
            "authorId": "145555571",
            "name": "F. Chu",
            "hIndex": 63
        },
        "Ming Li": {
            "authorId": "47605188",
            "name": "Li Ming",
            "hIndex": 7
        },
        "Ben wiesel": {
            "authorId": "2144946110",
            "name": "Ben Wiesel",
            "hIndex": 2
        },
        "Juan Carlos Niebles": {
            "authorId": "9200530",
            "name": "Juan Carlos Niebles",
            "hIndex": 54
        },
        "Wenyu Yu": {
            "authorId": "118367141",
            "name": "Wenyu Yang",
            "hIndex": 23
        },
        "Ge Wang": {
            "authorId": "50248838",
            "name": "Ge Wang",
            "hIndex": 37
        },
        "Ritwik Sinha": {
            "authorId": "2703103",
            "name": "R. Sinha",
            "hIndex": 10
        },
        "Jiakai Wang": {
            "authorId": "2109618469",
            "name": "Jiakai Wang",
            "hIndex": 11
        },
        "Gabriela Prochazka": {
            "authorId": "11698044",
            "name": "Gabriela Prochazka",
            "hIndex": 8
        },
        "Linlin Li": {
            "authorId": "49808638",
            "name": "Li Linlin",
            "hIndex": 9
        },
        "Joshua Pickard": {
            "authorId": "2894213",
            "name": "J. Pickard",
            "hIndex": 5
        },
        "Disheng Wu": {
            "authorId": "2118290049",
            "name": "Di Wu",
            "hIndex": 5
        },
        "Yao Mu": {
            "authorId": "11317540",
            "name": "Mudi Yao",
            "hIndex": 17
        },
        "J\u00fanior R. Lima": {
            "authorId": "1402293867",
            "name": "R. Lima-J\u00fanior",
            "hIndex": 26
        },
        "Ajmal Mian": {
            "authorId": "1747500",
            "name": "A. Mian",
            "hIndex": 60
        },
        "Lingzhe Zhang": {
            "authorId": "2145399076",
            "name": "Lingzhen Zhang",
            "hIndex": 4
        },
        "Xun Yu Zhou": {
            "authorId": "1706721",
            "name": "X. Zhou",
            "hIndex": 55
        },
        "Zuoquan Lin": {
            "authorId": "1783654",
            "name": "Zuoquan Lin",
            "hIndex": 16
        },
        "Jef Wijsen": {
            "authorId": "1693316",
            "name": "Jef Wijsen",
            "hIndex": 24
        },
        "Matthew Carroll": {
            "authorId": "82541444",
            "name": "M. Carroll",
            "hIndex": 14
        },
        "Zilin Huang": {
            "authorId": "5323196",
            "name": "Zilin Huang",
            "hIndex": 17
        },
        "Roy Ka-Wei Lee": {
            "authorId": "38656724",
            "name": "R. Lee",
            "hIndex": 18
        },
        "Danda B. Rawat": {
            "name": "Danda B. Rawat",
            "hIndex": 0
        },
        "Zhuangcheng Gu": {
            "authorId": "2298573645",
            "name": "Zhuangcheng Gu",
            "hIndex": 1
        },
        "Yue Shen": {
            "authorId": "5671391",
            "name": "Yue-xiao Shen",
            "hIndex": 31
        },
        "Mahshad Sarikhani": {
            "authorId": "2217704322",
            "name": "Mahshad Sarikhani",
            "hIndex": 0
        },
        "Hongfei Rong": {
            "authorId": "2213676691",
            "name": "Hongfei Rong",
            "hIndex": 1
        },
        "Alex Reneau": {
            "authorId": "2033360293",
            "name": "Alex Reneau",
            "hIndex": 2
        },
        "Taijie Chen": {
            "authorId": "7934368",
            "name": "Taijie Chen",
            "hIndex": 5
        },
        "Ronghua Li": {
            "authorId": "50392199",
            "name": "Ronghua Li",
            "hIndex": 58
        },
        "Joe G. Greener": {
            "authorId": "36526409",
            "name": "Joe G. Greener",
            "hIndex": 12
        },
        "Marcos Abel Zuzu\u00e1rregui": {
            "name": "Marcos Abel Zuzu\u00e1rregui",
            "hIndex": 0
        },
        "Mauricio Vargas Sepulveda": {
            "authorId": "2319385015",
            "name": "Mauricio Vargas Sepulveda",
            "hIndex": 0
        },
        "Jiyoung Han": {
            "authorId": "1796260814",
            "name": "Jiyoung Han",
            "hIndex": 5
        },
        "Aliakbar Nafar": {
            "authorId": "2124879956",
            "name": "Aliakbar Nafar",
            "hIndex": 2
        },
        "Merve Astekin": {
            "authorId": "1978167",
            "name": "Merve Astekin",
            "hIndex": 6
        },
        "Hyunjun Kim": {
            "authorId": "2155095345",
            "name": "Hyunjung Kim",
            "hIndex": 4
        },
        "Yizhou Shan": {
            "authorId": "26410174",
            "name": "Yizhou Shan",
            "hIndex": 7
        },
        "Sarwan Ali": {
            "authorId": "150290745",
            "name": "Sarwan Ali",
            "hIndex": 14
        },
        "Dung T. K. Ha": {
            "authorId": "8622842",
            "name": "Dung T. K. Ha",
            "hIndex": 4
        },
        "Linshu Huang": {
            "authorId": "2158202168",
            "name": "Linshu Huang",
            "hIndex": 1
        },
        "Pierre Meneton": {
            "authorId": "47236191",
            "name": "P. Meneton",
            "hIndex": 49
        },
        "Gaode Chen": {
            "authorId": "2146663414",
            "name": "Gaode Chen",
            "hIndex": 4
        },
        "Zheng Liang": {
            "authorId": "40201199",
            "name": "Zhenglun Liang",
            "hIndex": 35
        },
        "Varun Rao": {
            "authorId": "3412604",
            "name": "V. Mallampalli",
            "hIndex": 4
        },
        "Katherine Metcalf": {
            "authorId": "49977049",
            "name": "K. Metcalf",
            "hIndex": 7
        },
        "Rama Akkiraju": {
            "authorId": "1807758",
            "name": "R. Akkiraju",
            "hIndex": 29
        },
        "Jacobus W. Portegies": {
            "authorId": "9032664",
            "name": "J. Portegies",
            "hIndex": 10
        },
        "Wenjie Zhao": {
            "authorId": "9178191",
            "name": "Z. Wenjie",
            "hIndex": 5
        },
        "Muhammad Abdullah Jamal": {
            "authorId": "34902211",
            "name": "Muhammad Abdullah Jamal",
            "hIndex": 5
        },
        "Yuanzhe Zhang": {
            "authorId": "3012863",
            "name": "Yuan-zhen Zhang",
            "hIndex": 22
        },
        "Jiamou Liu": {
            "name": "Jiamou Liu",
            "hIndex": 0
        },
        "Aitor Urbieta": {
            "authorId": "144673269",
            "name": "A. Urbieta",
            "hIndex": 14
        },
        "Jann Krausse": {
            "authorId": "2292336635",
            "name": "Jann Krausse",
            "hIndex": 0
        },
        "Nikola Momchev": {
            "authorId": "1470531643",
            "name": "Nikola Momchev",
            "hIndex": 7
        },
        "Bitao Dai": {
            "authorId": "36475089",
            "name": "Bi-tao Dai",
            "hIndex": 4
        },
        "Haoze Wu": {
            "authorId": "40354734",
            "name": "Haoze Wu",
            "hIndex": 10
        },
        "Conghao Yang": {
            "authorId": "35340527",
            "name": "Conghao Yang",
            "hIndex": 4
        },
        "Ashirbad Mishra": {
            "authorId": "2267254099",
            "name": "Ashirbad Mishra",
            "hIndex": 2
        },
        "Kristina Schwamborn": {
            "authorId": "4352366",
            "name": "K. Schwamborn",
            "hIndex": 28
        },
        "Will Allred": {
            "authorId": "2319829234",
            "name": "Will Allred",
            "hIndex": 0
        },
        "Jifeng Hu": {
            "authorId": "14603102",
            "name": "Jifeng Hu",
            "hIndex": 11
        },
        "Matthew Love": {
            "authorId": "103902826",
            "name": "M. Love",
            "hIndex": 5
        },
        "Tobias Windisch": {
            "authorId": "2093966627",
            "name": "Tobias Windisch",
            "hIndex": 4
        },
        "Sithira Ambepitiya": {
            "authorId": "2282540401",
            "name": "Sithira Ambepitiya",
            "hIndex": 0
        },
        "Huan Zhang": {
            "authorId": "46232317",
            "name": "Huanhuan Zhang",
            "hIndex": 14
        },
        "S\u00e1ndor Sz\u00e9n\u00e1si": {
            "name": "S\u00e1ndor Sz\u00e9n\u00e1si",
            "hIndex": 0
        },
        "Daniel Nachmias": {
            "authorId": "2319410621",
            "name": "Daniel Nachmias",
            "hIndex": 0
        },
        "Blair Yang": {
            "authorId": "2319451620",
            "name": "Blair Yang",
            "hIndex": 0
        },
        "Shen Wang": {
            "authorId": "96001287",
            "name": "W. Shen",
            "hIndex": 17
        },
        "John Burden": {
            "authorId": "2797764",
            "name": "C. J. Burden",
            "hIndex": 20
        },
        "Trinh Le Cong": {
            "authorId": "120667044",
            "name": "C. L\u00ea",
            "hIndex": 5
        },
        "Jiashuai Liu": {
            "name": "Jiashuai Liu",
            "hIndex": 0
        },
        "Yuzhi Xu": {
            "authorId": "1993790",
            "name": "Yuzhi Xu",
            "hIndex": 20
        },
        "Craig Macdonald": {
            "authorId": "145434248",
            "name": "Craig Macdonald",
            "hIndex": 52
        },
        "Zhao Song": {
            "authorId": "50780688",
            "name": "Zhao-jun Song",
            "hIndex": 9
        },
        "Shuhao Li": {
            "authorId": "2146357820",
            "name": "Shu-Hong Li",
            "hIndex": 8
        },
        "Wei Gong": {
            "authorId": "7344963",
            "name": "W. Gong",
            "hIndex": 19
        },
        "Jongwuk Lee": {
            "authorId": "1865093",
            "name": "Jongwuk Lee",
            "hIndex": 19
        },
        "Nesreen K Ahmed": {
            "authorId": "47699955",
            "name": "Nesreen Ahmed",
            "hIndex": 31
        },
        "Corey Zhang": {
            "authorId": "2304520952",
            "name": "Corey Zhang",
            "hIndex": 0
        },
        "Isabelle Mohr": {
            "authorId": "114230081",
            "name": "I. Mohr",
            "hIndex": 7
        },
        "Saransh Kumar Gupta": {
            "name": "Saransh Kumar Gupta",
            "hIndex": 0
        },
        "Hongyao Tang": {
            "authorId": "31190626",
            "name": "Hongyao Tang",
            "hIndex": 13
        },
        "Yishuang Li": {
            "authorId": "2238148791",
            "name": "Yishuang Li",
            "hIndex": 3
        },
        "Ningyi Xu": {
            "name": "Ningyi Xu",
            "hIndex": 0
        },
        "Maximilian Baader": {
            "authorId": "1389559318",
            "name": "Maximilian Baader",
            "hIndex": 10
        },
        "Tianxiang Sun": {
            "authorId": "153345698",
            "name": "Tianxiang Sun",
            "hIndex": 18
        },
        "Melkior Ornik": {
            "authorId": "1814528",
            "name": "Melkior Ornik",
            "hIndex": 10
        },
        "Maarten De Vos": {
            "authorId": "48259863",
            "name": "M. Vos",
            "hIndex": 38
        },
        "Vajira Thambawita": {
            "authorId": "2627712",
            "name": "Vajira Lasantha Thambawita",
            "hIndex": 16
        },
        "Endian Li": {
            "name": "Endian Li",
            "hIndex": 0
        },
        "Yaoxun Xu": {
            "authorId": "2214732886",
            "name": "Yaoxun Xu",
            "hIndex": 2
        },
        "Chenyang Song": {
            "authorId": "3898797",
            "name": "Chenyang Song",
            "hIndex": 12
        },
        "Sewon Min": {
            "authorId": "48872685",
            "name": "Sewon Min",
            "hIndex": 29
        },
        "Amber Ebinama": {
            "authorId": "2211605235",
            "name": "Amber Ebinama",
            "hIndex": 1
        },
        "Dirk Beyer": {
            "authorId": "20973083",
            "name": "Dirk Beyer",
            "hIndex": 49
        },
        "Brian Guo": {
            "authorId": "97736830",
            "name": "Brian H. W. Guo",
            "hIndex": 17
        },
        "Chongjie Si": {
            "authorId": "2175648371",
            "name": "Chongjie Si",
            "hIndex": 3
        },
        "Lorenzo Bini": {
            "authorId": "114815478",
            "name": "L. B. Smaghi",
            "hIndex": 12
        },
        "Akash R. Wasil": {
            "authorId": "83024519",
            "name": "Akash R. Wasil",
            "hIndex": 20
        },
        "Jialiang Wang": {
            "authorId": "2109655729",
            "name": "Jialiang Wang",
            "hIndex": 24
        },
        "Erzhi Liu": {
            "authorId": "2140614472",
            "name": "Erzhi Liu",
            "hIndex": 1
        },
        "Thomas Braunl": {
            "authorId": "152404382",
            "name": "T. Braunl",
            "hIndex": 4
        },
        "Derian Boer": {
            "authorId": "145537989",
            "name": "Derian Boer",
            "hIndex": 2
        },
        "Shifan Zhang": {
            "authorId": "19151103",
            "name": "Shifan Zhang",
            "hIndex": 12
        },
        "Bofei Gao": {
            "authorId": "2186406832",
            "name": "Bofei Gao",
            "hIndex": 2
        },
        "Juan Ye": {
            "authorId": "40456004",
            "name": "Juan Ye",
            "hIndex": 24
        },
        "Yihao Wang": {
            "authorId": "1756831",
            "name": "Yih-Ru Wang",
            "hIndex": 13
        },
        "Minglong Xue": {
            "authorId": "144149355",
            "name": "Ming Xue",
            "hIndex": 12
        },
        "Pujin Shi": {
            "name": "Pujin Shi",
            "hIndex": 0
        },
        "Christian Lagemann": {
            "authorId": "40858217",
            "name": "C. Lagemann",
            "hIndex": 7
        },
        "Zhao Kang": {
            "authorId": "2074107660",
            "name": "K. Zhao",
            "hIndex": 6
        },
        "Jiacan Yu": {
            "authorId": "2319840422",
            "name": "Jiacan Yu",
            "hIndex": 0
        },
        "Saurabh Goyal": {
            "authorId": "108383653",
            "name": "S. Goyal",
            "hIndex": 13
        },
        "James McKevitt": {
            "authorId": "15315864",
            "name": "J. McKevitt",
            "hIndex": 3
        },
        "Anh-Vu Dinh-Duc": {
            "authorId": "94493937",
            "name": "D. A. Dinh",
            "hIndex": 21
        },
        "Hiroshi Mineno": {
            "authorId": "1714824",
            "name": "H. Mineno",
            "hIndex": 15
        },
        "Junyoung Kim": {
            "authorId": "153188204",
            "name": "Jun-young Kim",
            "hIndex": 25
        },
        "Maximilian Tschochohei": {
            "authorId": "2295664086",
            "name": "Maximilian Tschochohei",
            "hIndex": 0
        },
        "Theodore Long": {
            "authorId": "38056868",
            "name": "Theodore Long",
            "hIndex": 11
        },
        "Pedro Perez": {
            "authorId": "144260254",
            "name": "Pedro A. P\u00e9rez",
            "hIndex": 7
        },
        "Deyu Zhang": {
            "authorId": "144342661",
            "name": "D. Zhang",
            "hIndex": 8
        },
        "Haojiong Shangguan": {
            "authorId": "2284064765",
            "name": "Haojiong Shangguan",
            "hIndex": 1
        },
        "Sang Hoon Woo": {
            "authorId": "2262209",
            "name": "Sanghyun Woo",
            "hIndex": 21
        },
        "Haobo Yang": {
            "authorId": "2115537069",
            "name": "Hao Yang",
            "hIndex": 5
        },
        "Peter Eastman": {
            "authorId": "145097965",
            "name": "P. Eastman",
            "hIndex": 22
        },
        "Olivia P. Dizon-Paradis": {
            "authorId": "2154764006",
            "name": "Olivia P. Dizon-Paradis",
            "hIndex": 3
        },
        "Sthembiso Mkhwanazi": {
            "authorId": "2287024449",
            "name": "Sthembiso Mkhwanazi",
            "hIndex": 0
        },
        "Shehan Perera": {
            "authorId": "7154796",
            "name": "Amal Perera",
            "hIndex": 5
        },
        "Chengyue Wang": {
            "authorId": "8206437",
            "name": "Chengyue Wang",
            "hIndex": 13
        },
        "Gwendal Cachin-Bernard": {
            "authorId": "2319414194",
            "name": "Gwendal Cachin-Bernard",
            "hIndex": 0
        },
        "Laurence Aitchison": {
            "authorId": "2724259",
            "name": "L. Aitchison",
            "hIndex": 21
        },
        "Simon Lacoste-Julien": {
            "authorId": "1388317459",
            "name": "Simon Lacoste-Julien",
            "hIndex": 44
        },
        "Andreas Stephan": {
            "authorId": "48741418",
            "name": "Andreas Stephan",
            "hIndex": 33
        },
        "Jiaxin Guo": {
            "name": "Jiaxin Guo",
            "hIndex": 0
        },
        "Sharvaree Vadgama": {
            "authorId": "101290348",
            "name": "Sharvaree P. Vadgama",
            "hIndex": 4
        },
        "Hamidreza Hatamabadi": {
            "authorId": "6838315",
            "name": "H. Hatamabadi",
            "hIndex": 22
        },
        "Lijie Wen": {
            "authorId": "40650846",
            "name": "L. Wen",
            "hIndex": 25
        },
        "Johnson Du": {
            "authorId": "144110024",
            "name": "Lee Johnson",
            "hIndex": 2
        },
        "Alaeddin Nassani": {
            "authorId": "2363053",
            "name": "Alaeddin Nassani",
            "hIndex": 7
        },
        "Mehmet Koyut\u00fcrk": {
            "authorId": "1976219",
            "name": "Mehmet Koyut\u00fcrk",
            "hIndex": 32
        },
        "Di Kang": {
            "authorId": "32132679",
            "name": "Di Kang",
            "hIndex": 14
        },
        "Jonas Verhellen": {
            "authorId": "2003517641",
            "name": "Jonas Verhellen",
            "hIndex": 2
        },
        "Aoxiang Ning": {
            "authorId": "2308030560",
            "name": "Aoxiang Ning",
            "hIndex": 0
        },
        "Matt Barnes": {
            "authorId": "38418010",
            "name": "Matt Barnes",
            "hIndex": 15
        },
        "Zhiyong Yang": {
            "authorId": "49193652",
            "name": "Yang Zhiyong",
            "hIndex": 6
        },
        "Amelia Wissink": {
            "authorId": "2319414613",
            "name": "Amelia Wissink",
            "hIndex": 0
        },
        "Junbin Gao": {
            "authorId": "1750488",
            "name": "Junbin Gao",
            "hIndex": 41
        },
        "Yuhao Du": {
            "authorId": "152984385",
            "name": "Yu Du",
            "hIndex": 13
        },
        "Zifan Zheng": {
            "authorId": "2198054563",
            "name": "Zifan Zheng",
            "hIndex": 1
        },
        "Nguyen Ho": {
            "authorId": "1421172570",
            "name": "P. Nguyen\u2010Ho",
            "hIndex": 9
        },
        "Prabal Vashisht": {
            "authorId": "2014774061",
            "name": "Prabal Vashisht",
            "hIndex": 1
        },
        "Jinlong Zhu": {
            "authorId": "47054816",
            "name": "Jinlong Zhu",
            "hIndex": 30
        },
        "Klaus Knobloch": {
            "authorId": "143988820",
            "name": "K. Knobloch",
            "hIndex": 5
        },
        "Quoc Dinh Nguyen": {
            "authorId": "145755341",
            "name": "N. Dinh",
            "hIndex": 8
        },
        "Xiaobin Lu": {
            "authorId": "2062740",
            "name": "Xiaobin Lu",
            "hIndex": 10
        },
        "Liwei Liu": {
            "authorId": "2145592358",
            "name": "Li-wei Liu",
            "hIndex": 18
        },
        "Sumit Dalal": {
            "authorId": "100859747",
            "name": "Sumit Dalal",
            "hIndex": 3
        },
        "Ryan McDonald": {
            "authorId": "143957226",
            "name": "Ryan T. McDonald",
            "hIndex": 49
        },
        "Yunjia Xi": {
            "authorId": "2056826850",
            "name": "Yunjia Xi",
            "hIndex": 8
        },
        "Lingming Zhang": {
            "authorId": "2145398332",
            "name": "Lingming Zhang",
            "hIndex": 42
        },
        "Cristiano De Nobili": {
            "authorId": "102230808",
            "name": "C. Nobili",
            "hIndex": 3
        },
        "Banooqa Banday": {
            "authorId": "2317109659",
            "name": "Banooqa H. Banday",
            "hIndex": 0
        },
        "Niccol\u00f2 Turcato": {
            "authorId": "2180424381",
            "name": "Niccolo' Turcato",
            "hIndex": 2
        },
        "He Zhang": {
            "name": "He Zhang",
            "hIndex": 0
        },
        "Heyuan Huang": {
            "authorId": "48186073",
            "name": "Heyuan Huang",
            "hIndex": 9
        },
        "Junyu Lu": {
            "authorId": "2153516385",
            "name": "Lujun Wang",
            "hIndex": 8
        },
        "Davide Tateo": {
            "authorId": "35387097",
            "name": "Davide Tateo",
            "hIndex": 11
        },
        "Menglin Liu": {
            "authorId": "102269519",
            "name": "Meng-Chao Liu",
            "hIndex": 13
        },
        "Abhijeet Sahu": {
            "authorId": "8292357",
            "name": "A. Sahu",
            "hIndex": 11
        },
        "Patrick Paroubek": {
            "authorId": "2552801",
            "name": "P. Paroubek",
            "hIndex": 19
        },
        "Chia-Yu Hsu": {
            "authorId": "2110551108",
            "name": "Chia-Yu Hsu",
            "hIndex": 14
        },
        "Maarten de Rijke": {
            "authorId": "1696030",
            "name": "M. de Rijke",
            "hIndex": 77
        },
        "Hojae Han": {
            "authorId": "2109373545",
            "name": "H. Han",
            "hIndex": 24
        },
        "Jun Luo": {
            "authorId": "1471128634",
            "name": "Jun Luo",
            "hIndex": 11
        },
        "Shuai-Shuai Xu": {
            "authorId": "2280163",
            "name": "Shuai\u2010Xia Xu",
            "hIndex": 17
        },
        "Kai Chen": {
            "authorId": "150170737",
            "name": "Kai Chen",
            "hIndex": 35
        },
        "Joseph Khoury": {
            "authorId": "40167361",
            "name": "J. E. Khoury",
            "hIndex": 22
        },
        "Yonggang Zhang": {
            "authorId": "48378948",
            "name": "Yonggang Zhang",
            "hIndex": 17
        },
        "Zhiqiang Wang": {
            "authorId": "121471688",
            "name": "W. Zhiqiang",
            "hIndex": 15
        },
        "Nikos Tsagarakis": {
            "authorId": "152912553",
            "name": "N. Tsagarakis",
            "hIndex": 31
        },
        "Chuanhui Yang": {
            "authorId": "8100263",
            "name": "Chuanhui Yang",
            "hIndex": 5
        },
        "Wenshuai Liu": {
            "authorId": "2136399346",
            "name": "Wenshuai Liu",
            "hIndex": 9
        },
        "Zhizheng Wu": {
            "authorId": "48551256",
            "name": "Zhizheng Wu",
            "hIndex": 13
        },
        "Thomas Gebhart": {
            "authorId": "47457412",
            "name": "Thomas Gebhart",
            "hIndex": 6
        },
        "Yu Xiao": {
            "authorId": "2139766323",
            "name": "X. Yu",
            "hIndex": 15
        },
        "Yuxing Yang": {
            "authorId": "2108795565",
            "name": "Yuxing Yang",
            "hIndex": 13
        },
        "Yequan Wang": {
            "authorId": "2115737996",
            "name": "Yequan Wang",
            "hIndex": 11
        },
        "Canh V. Pham": {
            "authorId": "37486925",
            "name": "Canh V. Pham",
            "hIndex": 9
        },
        "Siyuan Li": {
            "authorId": "91914045",
            "name": "L. Siyuan",
            "hIndex": 5
        },
        "Yukang Huo": {
            "authorId": "2280390074",
            "name": "Yukang Huo",
            "hIndex": 1
        },
        "Arya Gopikrishnan": {
            "authorId": "2319611748",
            "name": "Arya Gopikrishnan",
            "hIndex": 0
        },
        "Isabel Erickson": {
            "authorId": "70026185",
            "name": "Evelio Felipe Machado Ram\u00edrez",
            "hIndex": 11
        },
        "Stefano Carpin": {
            "authorId": "2930283",
            "name": "Stefano Carpin",
            "hIndex": 31
        },
        "Luoyu Mei": {
            "authorId": "2146713762",
            "name": "Luoyu Mei",
            "hIndex": 3
        },
        "Frederic Wantiez": {
            "authorId": "46244398",
            "name": "Fr\u00e9d\u00e9ric Wantiez",
            "hIndex": 1
        },
        "Ruifeng Wang": {
            "authorId": "2061979456",
            "name": "Wang Ruifeng",
            "hIndex": 5
        },
        "Yixuan Weng": {
            "authorId": "2142839441",
            "name": "Yixuan Weng",
            "hIndex": 7
        },
        "Li Ma": {
            "authorId": "81586629",
            "name": "L. Ma",
            "hIndex": 12
        },
        "Edgar Wolf": {
            "authorId": "94185780",
            "name": "E. Wolf",
            "hIndex": 6
        },
        "Ricardo Buitrago Ruiz": {
            "authorId": "146627338",
            "name": "E. L\u00f3pez",
            "hIndex": 8
        },
        "Erin van Liemt": {
            "authorId": "2283841573",
            "name": "Erin van Liemt",
            "hIndex": 2
        },
        "Yuanchun Li": {
            "authorId": "9152548",
            "name": "Li Yuanchun",
            "hIndex": 5
        },
        "Jiasheng Ye": {
            "authorId": "29821447",
            "name": "Jia-sheng Ye",
            "hIndex": 24
        },
        "Junpeng Wang": {
            "authorId": "2110149095",
            "name": "Junpeng Wang",
            "hIndex": 21
        },
        "Shi-Xiong Zhang": {
            "authorId": "2145401806",
            "name": "Shizhong Zhang",
            "hIndex": 4
        },
        "Ibrahim Alshehri": {
            "authorId": "2126936600",
            "name": "M. Alshehri",
            "hIndex": 1
        },
        "Yuhan Wu": {
            "authorId": "51009724",
            "name": "Yuhan Wu",
            "hIndex": 30
        },
        "Ahmed Ramadan": {
            "authorId": "2055793767",
            "name": "A. Ramadan",
            "hIndex": 7
        },
        "Po-Chun Chien": {
            "authorId": "1471719782",
            "name": "Po-Chun Chien",
            "hIndex": 2
        },
        "Vignesh Prasad": {
            "authorId": "35167399",
            "name": "V. Krishnamoorthy",
            "hIndex": 8
        },
        "St\u00e9fan Darmoni": {
            "authorId": "145402363",
            "name": "S. Darmoni",
            "hIndex": 28
        },
        "Amit Dhurandhar": {
            "authorId": "2145784",
            "name": "Amit Dhurandhar",
            "hIndex": 22
        },
        "Haihua Wang": {
            "authorId": "49528624",
            "name": "Hai-Hua Wang",
            "hIndex": 11
        },
        "Masanari Kimura": {
            "authorId": "94236734",
            "name": "M. Kimura",
            "hIndex": 33
        },
        "Zhizhan Zhao": {
            "authorId": "2164869937",
            "name": "Zhizhan Zhao",
            "hIndex": 2
        },
        "Vasisht Duddu": {
            "authorId": "40895513",
            "name": "Vasisht Duddu",
            "hIndex": 8
        },
        "Ziqin Tu": {
            "authorId": "2319607434",
            "name": "Ziqin Tu",
            "hIndex": 0
        },
        "Sungchul Kim": {
            "authorId": "2109571021",
            "name": "Sungchul Kim",
            "hIndex": 21
        },
        "Liyong Guo": {
            "authorId": "12273425",
            "name": "Guozhong Shi",
            "hIndex": 10
        },
        "Zhuowei Chen": {
            "authorId": "2145392743",
            "name": "Zhuo Chen",
            "hIndex": 8
        },
        "Yimeng Fan": {
            "authorId": "46389825",
            "name": "Yimeng Fan",
            "hIndex": 8
        },
        "Romain Lelong": {
            "authorId": "2926152",
            "name": "R. Lelong",
            "hIndex": 6
        },
        "Neha Kumaru": {
            "name": "Neha Kumaru",
            "hIndex": 0
        },
        "Aydin Feyzi": {
            "authorId": "137464738",
            "name": "Feyzi Aydin",
            "hIndex": 0
        },
        "Priyanka Chudasama": {
            "authorId": "2243459594",
            "name": "Priyanka Chudasama",
            "hIndex": 0
        },
        "Enrico Macii": {
            "name": "Enrico Macii",
            "hIndex": 0
        },
        "Christian S. Jensen": {
            "authorId": "49070716",
            "name": "C. S. Jensen",
            "hIndex": 19
        },
        "Andrew Seong": {
            "authorId": "49250076",
            "name": "Andrew S. Lee",
            "hIndex": 4
        },
        "Maddie Yang": {
            "name": "Maddie Yang",
            "hIndex": 0
        },
        "Ryszard Staruch": {
            "authorId": "2319829351",
            "name": "Ryszard Staruch",
            "hIndex": 0
        },
        "Hanzi Mao": {
            "authorId": "2053590350",
            "name": "Hanzi Mao",
            "hIndex": 5
        },
        "Asheesh Kumar Singh": {
            "authorId": "47264435",
            "name": "A. Singh",
            "hIndex": 18
        },
        "Adri\u00e0 Molina": {
            "authorId": "2107165734",
            "name": "A. Hern\u00e2ndez",
            "hIndex": 6
        },
        "Olivier Bachem": {
            "authorId": "1936951",
            "name": "Olivier Bachem",
            "hIndex": 32
        },
        "Lihi Zelnik-Manor": {
            "authorId": "1398327241",
            "name": "Lihi Zelnik-Manor",
            "hIndex": 38
        },
        "Jingqiang Wang": {
            "authorId": "1519286303",
            "name": "Jingqiang Wang",
            "hIndex": 19
        },
        "Luca Benini": {
            "authorId": "1710649",
            "name": "L. Benini",
            "hIndex": 104
        },
        "Lena Filatova": {
            "authorId": "115726256",
            "name": "Lena Filatova",
            "hIndex": 2
        },
        "Neo Wu": {
            "authorId": "153128698",
            "name": "Neo Wu",
            "hIndex": 7
        },
        "Yatin Nandwani": {
            "authorId": "1392630568",
            "name": "Yatin Nandwani",
            "hIndex": 5
        },
        "Imran Razzak": {
            "authorId": "1630421985",
            "name": "Imran Razzak",
            "hIndex": 30
        },
        "Constantin Enea": {
            "authorId": "1690534",
            "name": "C. Enea",
            "hIndex": 22
        },
        "Yanbo Zheng": {
            "authorId": "51326016",
            "name": "Yanbo Zheng",
            "hIndex": 4
        },
        "Olivera Kotevska": {
            "authorId": "7731413",
            "name": "O. Kotevska",
            "hIndex": 8
        },
        "Raphael Lafargue": {
            "authorId": "2101823883",
            "name": "Raphael Lafargue",
            "hIndex": 3
        },
        "Marc Andrew Choi": {
            "authorId": "2203070846",
            "name": "Marc Andrew Choi",
            "hIndex": 1
        },
        "Kristian Georgiev": {
            "authorId": "2066142336",
            "name": "Kristian Georgiev",
            "hIndex": 6
        },
        "Johane Takeuchi": {
            "authorId": "2742316",
            "name": "Johane Takeuchi",
            "hIndex": 10
        },
        "Jizhou Huang": {
            "authorId": "2864855",
            "name": "Jizhou Huang",
            "hIndex": 17
        },
        "Yoseph Palinggi": {
            "authorId": "2032227342",
            "name": "Yoseph Palinggi",
            "hIndex": 1
        },
        "Ziyan He": {
            "authorId": "9955454",
            "name": "Ziyan He",
            "hIndex": 14
        },
        "Che Liu": {
            "authorId": "1724666",
            "name": "Hsi-Che Liu",
            "hIndex": 20
        },
        "Vaskar Nath": {
            "authorId": "2151210591",
            "name": "Vaskar Nath",
            "hIndex": 1
        },
        "Trevor S. Betz": {
            "authorId": "2134626306",
            "name": "Trevor S. Betz",
            "hIndex": 2
        },
        "Xiangrui Kong": {
            "authorId": "46602083",
            "name": "X. Kong",
            "hIndex": 5
        },
        "Zahra Ahmadi": {
            "authorId": "79400089",
            "name": "Zahra Ahmadi",
            "hIndex": 27
        },
        "Marcin Sawi\u0144ski": {
            "authorId": "2224412752",
            "name": "Marcin Sawi\u0144ski",
            "hIndex": 2
        },
        "A. Sophia Koepke": {
            "authorId": "2064172797",
            "name": "A. S. Koepke",
            "hIndex": 12
        },
        "Leonid Libkin": {
            "authorId": "1681226",
            "name": "L. Libkin",
            "hIndex": 52
        },
        "Yangtian Zhang": {
            "authorId": "2278650373",
            "name": "Yangtian Zhang",
            "hIndex": 1
        },
        "Sian Fang": {
            "authorId": "116278841",
            "name": "Fang-Sian Lin",
            "hIndex": 8
        },
        "Tom Reed": {
            "authorId": "77454379",
            "name": "Thomas V. Reed",
            "hIndex": 6
        },
        "Chenglu Wen": {
            "name": "Chenglu Wen",
            "hIndex": 0
        },
        "Kaung Myat Kyaw": {
            "authorId": "34715571",
            "name": "K. Kyaw",
            "hIndex": 3
        },
        "Narichika Nomoto": {
            "authorId": "2767902",
            "name": "Narichika Nomoto",
            "hIndex": 3
        },
        "Alberto Cazzaniga": {
            "authorId": "3327834",
            "name": "A. Cazzaniga",
            "hIndex": 10
        },
        "Joao F. Henriques": {
            "authorId": "143848064",
            "name": "Jo\u00e3o F. Henriques",
            "hIndex": 28
        },
        "F. G. C. Cabarle": {
            "name": "F. G. C. Cabarle",
            "hIndex": 0
        },
        "Zahra Atf": {
            "authorId": "97906457",
            "name": "Zahra Atf",
            "hIndex": 2
        },
        "Yuwei Yin": {
            "authorId": "30868982",
            "name": "Yuwei Yin",
            "hIndex": 8
        },
        "Syed Nohsen Naqvi": {
            "name": "Syed Nohsen Naqvi",
            "hIndex": 0
        },
        "Chuhan Wu": {
            "authorId": "2005481085",
            "name": "Chuhan Wu",
            "hIndex": 14
        },
        "Gyuri Byun": {
            "authorId": "2278641500",
            "name": "Gyuri Byun",
            "hIndex": 1
        },
        "Suyeon Lee": {
            "authorId": "3829574",
            "name": "Su-Youn Lee",
            "hIndex": 10
        },
        "Hankun Wang": {
            "authorId": "4590407",
            "name": "Hankun Wang",
            "hIndex": 26
        },
        "Minh-Tan Pham": {
            "authorId": "40565711",
            "name": "M. Pham",
            "hIndex": 23
        },
        "Tong Zhang": {
            "authorId": "2780386",
            "name": "Tong-tong Zhang",
            "hIndex": 17
        },
        "Brigitte S\u00e9roussi": {
            "authorId": "144643515",
            "name": "B. S\u00e9roussi",
            "hIndex": 17
        },
        "Xiaoyan Yu": {
            "name": "Xiaoyan Yu",
            "hIndex": 0
        },
        "Hongbin Pei": {
            "authorId": "1491477771",
            "name": "Hong Pei",
            "hIndex": 2
        },
        "Wenchao Dong": {
            "authorId": "144968879",
            "name": "Wen-Fei Dong",
            "hIndex": 6
        },
        "Takuhiro Kaneko": {
            "authorId": "50509323",
            "name": "Takuhiro Kaneko",
            "hIndex": 23
        },
        "Suresh Kondepudi": {
            "authorId": "89033070",
            "name": "Suresh Kondepudi",
            "hIndex": 1
        },
        "Yohei Iwaki": {
            "authorId": "32324090",
            "name": "Y. Iwaki",
            "hIndex": 1
        },
        "Sukyoung Ryu": {
            "authorId": "39207376",
            "name": "Sukyoung Ryu",
            "hIndex": 19
        },
        "Daniel Baier": {
            "authorId": "1684058",
            "name": "Daniel Baier",
            "hIndex": 21
        },
        "Shuchang Zhou": {
            "authorId": "35132667",
            "name": "Shuchang Zhou",
            "hIndex": 19
        },
        "Guang Ping He": {
            "authorId": "38931311",
            "name": "G. He",
            "hIndex": 15
        },
        "Keshuang Tang": {
            "authorId": "3086173",
            "name": "Keshuang Tang",
            "hIndex": 19
        },
        "Ryo Igarashi": {
            "authorId": "92674580",
            "name": "R. Igarashi",
            "hIndex": 7
        },
        "Anuradha Bhamidipaty": {
            "authorId": "1804908",
            "name": "A. Bhamidipaty",
            "hIndex": 9
        },
        "Nino Vieillard": {
            "authorId": "138497788",
            "name": "Nino Vieillard",
            "hIndex": 12
        },
        "Manshan Guo": {
            "authorId": "95404125",
            "name": "M. Guo",
            "hIndex": 2
        },
        "Daniel J. Cole": {
            "authorId": "4564714",
            "name": "D. J. Cole",
            "hIndex": 40
        },
        "Yara Rizk": {
            "authorId": "2587541",
            "name": "Yara Rizk",
            "hIndex": 14
        },
        "Ana Peleteiro Ramallo": {
            "authorId": "1403783388",
            "name": "Ana Peleteiro-Ramallo",
            "hIndex": 11
        },
        "Dayang Wang": {
            "authorId": "3376984",
            "name": "Dayang Liu",
            "hIndex": 12
        },
        "Gavin Chait": {
            "authorId": "6870539",
            "name": "G. Chait",
            "hIndex": 3
        },
        "Floris Roelofsen": {
            "authorId": "1695793",
            "name": "F. Roelofsen",
            "hIndex": 23
        },
        "Yong Man Ro": {
            "authorId": "7251290",
            "name": "Yong Man Ro",
            "hIndex": 42
        },
        "Xiangnan Bai": {
            "authorId": "2320186248",
            "name": "Xiangnan Bai",
            "hIndex": 0
        },
        "Tianshuo Xu": {
            "authorId": "2162771818",
            "name": "Tianshuo Xu",
            "hIndex": 4
        },
        "Yasheng Wang": {
            "authorId": "2136912252",
            "name": "Yasheng Wang",
            "hIndex": 13
        },
        "Hiroshi Hosobe": {
            "authorId": "1710423",
            "name": "H. Hosobe",
            "hIndex": 12
        },
        "Cihang Xie": {
            "authorId": "3011497",
            "name": "Cihang Xie",
            "hIndex": 27
        },
        "Huy Phan Thanh": {
            "authorId": "143745840",
            "name": "P. T. Huy",
            "hIndex": 3
        },
        "Danilo Ascione": {
            "authorId": "98688140",
            "name": "Danilo Ascione",
            "hIndex": 1
        },
        "Jerry Yao-Chieh Hu": {
            "authorId": "102764428",
            "name": "Jerry Yao-Chieh Hu",
            "hIndex": 8
        },
        "Jeremy Qin": {
            "authorId": "2319873250",
            "name": "Jeremy Qin",
            "hIndex": 0
        },
        "Pu Feng": {
            "authorId": "7188984",
            "name": "Pufeng Qin",
            "hIndex": 21
        },
        "Sonam Gupta": {
            "authorId": "49436719",
            "name": "S. Gupta",
            "hIndex": 6
        },
        "Zhongfang Zhuang": {
            "authorId": "1712461",
            "name": "Zhongfang Zhuang",
            "hIndex": 8
        },
        "Marion Kiechle": {
            "authorId": "145397623",
            "name": "M. Kiechle",
            "hIndex": 47
        },
        "Ashish Kumar": {
            "authorId": "67018357",
            "name": "Ashish Kumar",
            "hIndex": 45
        },
        "Luke Geeson": {
            "authorId": "2260336235",
            "name": "Luke Geeson",
            "hIndex": 1
        },
        "Eric Zurfluh": {
            "authorId": "2319830429",
            "name": "Eric Zurfluh",
            "hIndex": 0
        },
        "Jianan Yao": {
            "authorId": "93386491",
            "name": "Jian-An Yao",
            "hIndex": 12
        },
        "Fabiane Queiroz": {
            "authorId": "2638660",
            "name": "Fabiane Queiroz",
            "hIndex": 5
        },
        "Ibrahim Abdelaziz": {
            "authorId": "145749443",
            "name": "I. Abdelaziz",
            "hIndex": 21
        },
        "Pengju Wang": {
            "authorId": "2296265036",
            "name": "Pengju Wang",
            "hIndex": 7
        },
        "Yang Yong": {
            "authorId": "2146265197",
            "name": "Yong Yang",
            "hIndex": 37
        },
        "Preslav Nakov": {
            "authorId": "1683562",
            "name": "Preslav Nakov",
            "hIndex": 69
        },
        "Bino A. Varghese": {
            "authorId": "35297056",
            "name": "B. Varghese",
            "hIndex": 19
        },
        "Zhou Li": {
            "authorId": "1391137870",
            "name": "L. Zhou",
            "hIndex": 20
        },
        "Wenbo Hu": {
            "authorId": "2985342",
            "name": "Wen-Fong Hu",
            "hIndex": 6
        },
        "Yuliang Liu": {
            "authorId": "2108353682",
            "name": "Yu-liang Liu",
            "hIndex": 11
        },
        "Mudi Jiang": {
            "authorId": "2169001455",
            "name": "Mudi Jiang",
            "hIndex": 2
        },
        "Jaeyoung Lee": {
            "authorId": "120704650",
            "name": "Jae-Yeap Lee",
            "hIndex": 4
        },
        "Edith C. -H. Ngai": {
            "authorId": "1748459",
            "name": "E. Ngai",
            "hIndex": 32
        },
        "Jinchang Zhou": {
            "authorId": "9162499",
            "name": "J. Zhou",
            "hIndex": 4
        },
        "Luca Soldaini": {
            "authorId": "3328733",
            "name": "Luca Soldaini",
            "hIndex": 21
        },
        "Yuling Gu": {
            "authorId": "2116405158",
            "name": "Y. Gu",
            "hIndex": 7
        },
        "Shreyas Mongia": {
            "name": "Shreyas Mongia",
            "hIndex": 0
        },
        "Fatemeh Shojaeian": {
            "authorId": "1753051705",
            "name": "F. Shojaeian",
            "hIndex": 6
        },
        "William Zhang": {
            "authorId": "153645552",
            "name": "Will Zhang",
            "hIndex": 45
        },
        "Marshall Wu": {
            "authorId": "118358618",
            "name": "M. P. Wu",
            "hIndex": 3
        },
        "Patrick Knab": {
            "authorId": "2559026",
            "name": "P. Knab",
            "hIndex": 4
        },
        "Fateme Heydari": {
            "authorId": "144771393",
            "name": "F. Heydari",
            "hIndex": 14
        },
        "Jilong Guo": {
            "authorId": "2109984318",
            "name": "Jilong Guo",
            "hIndex": 8
        },
        "Katrina Agate": {
            "name": "Katrina Agate",
            "hIndex": 0
        },
        "Vito Walter Anelli": {
            "authorId": "2431124",
            "name": "V. W. Anelli",
            "hIndex": 21
        },
        "Yezhou Yang": {
            "authorId": "7607499",
            "name": "Yezhou Yang",
            "hIndex": 57
        },
        "Jacob Andreas": {
            "name": "Jacob Andreas",
            "hIndex": 0
        },
        "Henry Lindeman": {
            "authorId": "2319419206",
            "name": "Henry Lindeman",
            "hIndex": 0
        },
        "Nesime Tatbul": {
            "authorId": "1773620",
            "name": "Nesime Tatbul",
            "hIndex": 34
        },
        "Maxim Beketov": {
            "authorId": "2124122606",
            "name": "Maxim Beketov",
            "hIndex": 1
        },
        "Chen Huang": {
            "name": "Chen Huang",
            "hIndex": 0
        },
        "Zengwei Yao": {
            "authorId": "134909283",
            "name": "Zengwei Yao",
            "hIndex": 8
        },
        "Dilruk Perera": {
            "authorId": "27035609",
            "name": "Dilruk Perera",
            "hIndex": 4
        },
        "Haiming Xu": {
            "authorId": "46485418",
            "name": "Haiming Xu",
            "hIndex": 25
        },
        "Nawsabah Noor": {
            "authorId": "2028942482",
            "name": "Nawsabah Noor",
            "hIndex": 2
        },
        "Taro Watanabe": {
            "authorId": "2157186439",
            "name": "Takahito Watanabe",
            "hIndex": 3
        },
        "Peng Liu": {
            "authorId": "48989788",
            "name": "Pengpeng Liu",
            "hIndex": 19
        },
        "Yunqi Hao": {
            "authorId": "9229538",
            "name": "Yun-qi Hao",
            "hIndex": 10
        },
        "Zexuan Zhong": {
            "authorId": "143624054",
            "name": "Ze Yang",
            "hIndex": 19
        },
        "Mingyuan Yao": {
            "authorId": "87040022",
            "name": "M. Yao",
            "hIndex": 7
        },
        "Thomas Brunschwiler": {
            "authorId": "1788251",
            "name": "T. Brunschwiler",
            "hIndex": 28
        },
        "Marc Vesin": {
            "authorId": "145346974",
            "name": "J. Vesin",
            "hIndex": 37
        },
        "Lorenza Prospero": {
            "name": "Lorenza Prospero",
            "hIndex": 0
        },
        "Hui Xiong": {
            "authorId": "93915934",
            "name": "Huihui Xiong",
            "hIndex": 12
        },
        "Yujuan Fu": {
            "authorId": "13686291",
            "name": "Yujuan Fu",
            "hIndex": 5
        },
        "Xiaonan Nie": {
            "authorId": "2113588952",
            "name": "Xiaonan Nie",
            "hIndex": 10
        },
        "Binbin Lin": {
            "authorId": "1423275103",
            "name": "B. Zheng-Lin",
            "hIndex": 9
        },
        "Sangbum Han": {
            "authorId": "14416496",
            "name": "Sangbum Han",
            "hIndex": 7
        },
        "Valentina S\u00e1nchez": {
            "authorId": "2053692197",
            "name": "V. S\u00e1nchez",
            "hIndex": 1
        },
        "You Yao": {
            "authorId": "49483479",
            "name": "Youwei Yao",
            "hIndex": 30
        },
        "Farihin Rahman": {
            "authorId": "2146221308",
            "name": "Nur Farihin Abdul Rahman",
            "hIndex": 1
        },
        "Alexandre Lu\u00eds Magalh\u00e3es Levada": {
            "authorId": "1686944",
            "name": "A. Levada",
            "hIndex": 10
        },
        "Thomas Lampe": {
            "authorId": "46534085",
            "name": "T. Lampe",
            "hIndex": 20
        },
        "Nicola Amico": {
            "name": "Nicola Amico",
            "hIndex": 0
        },
        "Fangjun Kuang": {
            "authorId": "2491830",
            "name": "Fangjun Kuang",
            "hIndex": 13
        },
        "Dustin Schwenk": {
            "name": "Dustin Schwenk",
            "hIndex": 0
        },
        "Suhang You": {
            "authorId": "150311460",
            "name": "Suhang You",
            "hIndex": 2
        },
        "Michal Gregor": {
            "authorId": "2873487",
            "name": "M. Gregor",
            "hIndex": 9
        },
        "Wing-Yin Yu": {
            "authorId": "47421438",
            "name": "Stephanie C. Y. Yu",
            "hIndex": 12
        },
        "Sanket Biswas": {
            "authorId": "2150473007",
            "name": "Sanket Biswas",
            "hIndex": 12
        },
        "Juluan Shi": {
            "authorId": "2260275849",
            "name": "Juluan Shi",
            "hIndex": 1
        },
        "Nawshad Binta Nizam": {
            "authorId": "2293637412",
            "name": "Nawshad Binta Nizam",
            "hIndex": 0
        },
        "Rosy Tsopra": {
            "authorId": "2640359",
            "name": "R. Tsopra",
            "hIndex": 15
        },
        "Rachel K. Luu": {
            "authorId": "2182256086",
            "name": "Rachel K. Luu",
            "hIndex": 5
        },
        "Dyke Ferber": {
            "authorId": "114468220",
            "name": "D. Ferber",
            "hIndex": 6
        },
        "Nicholas C. Rubin": {
            "authorId": "34743111",
            "name": "N. Rubin",
            "hIndex": 36
        },
        "Tanya Marwah": {
            "authorId": "8268761",
            "name": "Tanya Marwah",
            "hIndex": 6
        },
        "Marlin W Ulmer": {
            "authorId": "6928240",
            "name": "M. Ulmer",
            "hIndex": 24
        },
        "Ewelina Ksi\u0119\u017cniak": {
            "authorId": "2224421615",
            "name": "Ewelina Ksi\u0119\u017cniak",
            "hIndex": 2
        },
        "Yufei Huang": {
            "authorId": "9221211",
            "name": "Yufei Huang",
            "hIndex": 46
        },
        "Angelo Salatino": {
            "authorId": "40520756",
            "name": "Angelo Salatino",
            "hIndex": 15
        },
        "Luis Mayer": {
            "authorId": "4684796",
            "name": "P. Mil\u00f3n",
            "hIndex": 16
        },
        "Daren S. Mueller": {
            "authorId": "39541641",
            "name": "D. Mueller",
            "hIndex": 32
        },
        "FNU Sidharth": {
            "authorId": "2157648716",
            "name": "Fnu Sidharth",
            "hIndex": 0
        },
        "Keiran Paster": {
            "authorId": "73775191",
            "name": "Keiran Paster",
            "hIndex": 7
        },
        "Angel Beshirov": {
            "authorId": "2161652033",
            "name": "Angel Beshirov",
            "hIndex": 0
        },
        "Sara Kangaslahti": {
            "authorId": "66873248",
            "name": "Sara Kangaslahti",
            "hIndex": 2
        },
        "Claudio Bonesana": {
            "authorId": "19183843",
            "name": "Claudio Bonesana",
            "hIndex": 4
        },
        "Dexun Li": {
            "authorId": "2002642964",
            "name": "Li Yanwen",
            "hIndex": 10
        },
        "Furong Huang": {
            "authorId": "2117424462",
            "name": "Furong Huang",
            "hIndex": 17
        },
        "Xihe Qiu": {
            "authorId": "1500386397",
            "name": "Xihe Qiu",
            "hIndex": 6
        },
        "Franck Vermet": {
            "authorId": "1744033",
            "name": "F. Vermet",
            "hIndex": 11
        },
        "Spyridon Bakas": {
            "authorId": "3199900",
            "name": "S. Bakas",
            "hIndex": 40
        },
        "Benny Kimelfeld": {
            "authorId": "1679226",
            "name": "B. Kimelfeld",
            "hIndex": 30
        },
        "Niv Nayman": {
            "authorId": "93937354",
            "name": "Niv Nayman",
            "hIndex": 4
        },
        "Qun Liu": {
            "authorId": "50771845",
            "name": "L. Qun",
            "hIndex": 16
        },
        "Rui Zhang": {
            "authorId": "48263837",
            "name": "Rui Zhang",
            "hIndex": 32
        },
        "Faya Liang": {
            "authorId": "9518211",
            "name": "F. Liang",
            "hIndex": 15
        },
        "Xiaoliang Wan": {
            "authorId": "3283868",
            "name": "X. Wan",
            "hIndex": 22
        },
        "Ankit Sakhuja": {
            "authorId": "11008348",
            "name": "A. Sakhuja",
            "hIndex": 28
        },
        "Saba Shahsavan": {
            "authorId": "2164927993",
            "name": "Saba Shahsavan",
            "hIndex": 2
        },
        "Minjeong Jeon": {
            "authorId": "1872631",
            "name": "M. Jeon",
            "hIndex": 20
        },
        "Daniel Hsu": {
            "authorId": "2750838",
            "name": "D. Hsu",
            "hIndex": 49
        },
        "Taiyuan Mei": {
            "authorId": "2298916636",
            "name": "Taiyuan Mei",
            "hIndex": 3
        },
        "Zoey Chen": {
            "authorId": "100975264",
            "name": "Zoey Chen",
            "hIndex": 12
        },
        "Jeff Schneider": {
            "name": "Jeff Schneider",
            "hIndex": 0
        },
        "Chenlong Hu": {
            "authorId": "3118007",
            "name": "Chenlong Hu",
            "hIndex": 5
        },
        "Zongxing Xie": {
            "authorId": "3073584",
            "name": "Zongxing Xie",
            "hIndex": 10
        },
        "Eirini Ntoutsi": {
            "authorId": "1804618",
            "name": "Eirini Ntoutsi",
            "hIndex": 24
        },
        "Yuxin Huang": {
            "authorId": "51913832",
            "name": "Huang Yuxin",
            "hIndex": 4
        },
        "Xinyue Ye": {
            "authorId": "50184120",
            "name": "X. Ye",
            "hIndex": 46
        },
        "Chien-Chun Wang": {
            "authorId": "12092680",
            "name": "C. Wang",
            "hIndex": 9
        },
        "William Yeoh": {
            "authorId": "1805457",
            "name": "W. Yeoh",
            "hIndex": 31
        },
        "Max Hort": {
            "name": "Max Hort",
            "hIndex": 0
        },
        "Wanxu Xia": {
            "authorId": "2296892386",
            "name": "Wanxu Xia",
            "hIndex": 0
        },
        "Yakun Zhang": {
            "authorId": "2108130260",
            "name": "Yakun Zhang",
            "hIndex": 17
        },
        "Francesca Mangili": {
            "authorId": "145505364",
            "name": "F. Mangili",
            "hIndex": 17
        },
        "Junbin Xiao": {
            "authorId": "66358686",
            "name": "Junbin Xiao",
            "hIndex": 12
        },
        "Jinsong Su": {
            "authorId": "34739384",
            "name": "Jinsong Su",
            "hIndex": 32
        },
        "Jaeyoon Jung": {
            "authorId": "6787946",
            "name": "Jaeyoon Jung",
            "hIndex": 8
        },
        "Xianbing Zhao": {
            "authorId": "2116710776",
            "name": "Xianbing Zhao",
            "hIndex": 3
        },
        "Zahra Ahani": {
            "authorId": "2265758720",
            "name": "Z. Ahani",
            "hIndex": 4
        },
        "Nicolas F. Chaves-de-Plaza": {
            "authorId": "2136383472",
            "name": "Nicolas F. Chaves-de-Plaza",
            "hIndex": 2
        },
        "Biju Issac": {
            "name": "Biju Issac",
            "hIndex": 0
        },
        "Tieyong Zeng": {
            "authorId": "40227204",
            "name": "T. Zeng",
            "hIndex": 35
        },
        "Francisco Zamora-Martinez": {
            "authorId": "1398809775",
            "name": "F. Zamora-Mart\u00ednez",
            "hIndex": 16
        },
        "Eric Hu": {
            "authorId": "144262041",
            "name": "E. Hu",
            "hIndex": 40
        },
        "\u00c8ric \u015aanchez": {
            "name": "\u00c8ric \u015aanchez",
            "hIndex": 0
        },
        "Liangyu Xiang": {
            "authorId": "2054371072",
            "name": "Liang Xiang",
            "hIndex": 3
        },
        "Simon S. Du": {
            "authorId": "145697585",
            "name": "S. Du",
            "hIndex": 44
        },
        "Ling Feng": {
            "name": "Ling Feng",
            "hIndex": 0
        },
        "Marek Jankola": {
            "authorId": "2296605873",
            "name": "Marek Jankola",
            "hIndex": 1
        },
        "Benji Peng": {
            "authorId": "2319606006",
            "name": "Benji Peng",
            "hIndex": 0
        },
        "Yihao Zhang": {
            "authorId": "2108058981",
            "name": "Yihao Zhang",
            "hIndex": 8
        },
        "George Kour": {
            "authorId": "2497491",
            "name": "George Kour",
            "hIndex": 8
        },
        "Daniela Sailer": {
            "authorId": "2082215154",
            "name": "Daniela Sailer",
            "hIndex": 1
        },
        "Muhammad Kanaan": {
            "authorId": "2139171781",
            "name": "Muhammad Hamzah Kanaan",
            "hIndex": 2
        },
        "Tim Dettmers": {
            "authorId": "3239480",
            "name": "Tim Dettmers",
            "hIndex": 15
        },
        "Minjin Choi": {
            "authorId": "2111881386",
            "name": "Minjin Choi",
            "hIndex": 8
        },
        "Yuan Shen": {
            "authorId": "39499128",
            "name": "Yuanyuan Shen",
            "hIndex": 24
        },
        "Siddhesh Thakur": {
            "authorId": "51880630",
            "name": "Siddhesh P. Thakur",
            "hIndex": 11
        },
        "Yunfei Xu": {
            "authorId": "47103940",
            "name": "Yunfei Xu",
            "hIndex": 21
        },
        "Chenyang Zhao": {
            "authorId": "50015525",
            "name": "Chenyang Zhao",
            "hIndex": 15
        },
        "Pinghui Wang": {
            "authorId": "1764969",
            "name": "P. Wang",
            "hIndex": 38
        },
        "Ali Etemad": {
            "authorId": "1379982213",
            "name": "A. Etemad",
            "hIndex": 25
        },
        "Olga Zaghen": {
            "authorId": "2247612493",
            "name": "Olga Zaghen",
            "hIndex": 3
        },
        "Duyu Tang": {
            "authorId": "39483833",
            "name": "Duyu Tang",
            "hIndex": 42
        },
        "Charles Yuan": {
            "authorId": "2107147922",
            "name": "Zhiyuan Cheng",
            "hIndex": 9
        },
        "Debing Zhang": {
            "authorId": "1900504",
            "name": "Debing Zhang",
            "hIndex": 14
        },
        "Bo Du": {
            "authorId": "48155775",
            "name": "B. Du",
            "hIndex": 16
        },
        "Jens Kober": {
            "authorId": "145739642",
            "name": "Jens Kober",
            "hIndex": 28
        },
        "Tianyun Zhong": {
            "authorId": "2221320341",
            "name": "Tianyun Zhong",
            "hIndex": 2
        },
        "Li Song": {
            "authorId": "2118192752",
            "name": "Song Li",
            "hIndex": 6
        },
        "Emanuele Vivoli": {
            "authorId": "2182519264",
            "name": "Emanuele Vivoli",
            "hIndex": 2
        },
        "Fang Cai": {
            "authorId": "5899920",
            "name": "C. Cai",
            "hIndex": 41
        },
        "Yuhang Zhang": {
            "authorId": "46867043",
            "name": "Yuhang Zhang",
            "hIndex": 10
        },
        "Qiang Liu": {
            "authorId": "48873863",
            "name": "Qiang Liu",
            "hIndex": 39
        },
        "Jin B. Hong": {
            "authorId": "1931925",
            "name": "Jin B. Hong",
            "hIndex": 21
        },
        "Seyi Olojo": {
            "authorId": "2161662309",
            "name": "Seyi Olojo",
            "hIndex": 2
        },
        "Ye Yuan": {
            "authorId": "7873427",
            "name": "Yuan-yuan Ye",
            "hIndex": 21
        },
        "Mauricio Tec": {
            "authorId": "89437600",
            "name": "M. Tec",
            "hIndex": 8
        },
        "Unggi Lee": {
            "authorId": "2183084163",
            "name": "Unggi Lee",
            "hIndex": 4
        },
        "Bonan Min": {
            "authorId": "1875233",
            "name": "Bonan Min",
            "hIndex": 16
        },
        "Tian-Yi Qian": {
            "authorId": "2467668",
            "name": "T. Qian",
            "hIndex": 18
        },
        "Nishchal Bhandari": {
            "authorId": "32016698",
            "name": "Nishchal Bhandari",
            "hIndex": 7
        },
        "Johannes Hoster": {
            "authorId": "123763127",
            "name": "John L. Hoster",
            "hIndex": 1
        },
        "Henrike Stephani": {
            "authorId": "3356163",
            "name": "Henrike Stephani",
            "hIndex": 6
        },
        "Alexey Kurennoy": {
            "authorId": "2365789",
            "name": "A. Kurennoy",
            "hIndex": 7
        },
        "Tao Huang": {
            "authorId": "32687433",
            "name": "Tao Huang",
            "hIndex": 27
        },
        "Sumit Kumar Jha": {
            "authorId": "144245085",
            "name": "Sumit Kumar Jha",
            "hIndex": 19
        },
        "Ryan Xu": {
            "authorId": "70582966",
            "name": "Qing X. Ryan",
            "hIndex": 6
        },
        "Mohammad Farhadi": {
            "authorId": "144835681",
            "name": "M. Farhadi",
            "hIndex": 31
        },
        "JingSong Yang": {
            "authorId": "2121268750",
            "name": "Jingsong Yang",
            "hIndex": 25
        },
        "Philipp Wendler": {
            "authorId": "35046679",
            "name": "Philipp Wendler",
            "hIndex": 15
        },
        "Burak Yildirim": {
            "authorId": "21361102",
            "name": "B. Yildirim",
            "hIndex": 10
        },
        "Yuan Cheng": {
            "authorId": "47585153",
            "name": "Yuan Cheng",
            "hIndex": 20
        },
        "Michael Beukman": {
            "authorId": "2151088783",
            "name": "Michael Beukman",
            "hIndex": 3
        },
        "Shuyin Xia": {
            "authorId": "2644835",
            "name": "Shuyin Xia",
            "hIndex": 18
        },
        "Jeong Hun Yeo": {
            "authorId": "2161432446",
            "name": "Jeong Hun Yeo",
            "hIndex": 4
        },
        "Yanbing Mao": {
            "authorId": "1937682",
            "name": "Y. Mao",
            "hIndex": 10
        },
        "Nirmalie Wiratunga": {
            "authorId": "1784639",
            "name": "N. Wiratunga",
            "hIndex": 25
        },
        "Guole Liu": {
            "authorId": "98232012",
            "name": "Guole Liu",
            "hIndex": 15
        },
        "Ike Ebubechukwu": {
            "authorId": "2319420455",
            "name": "Ike Ebubechukwu",
            "hIndex": 0
        },
        "Jiwoo Son": {
            "authorId": "2193478191",
            "name": "Jiwoo Son",
            "hIndex": 4
        },
        "Wenlong Huang": {
            "authorId": "50526400",
            "name": "Wenlong Huang",
            "hIndex": 16
        },
        "Yushi Bai": {
            "authorId": "37335634",
            "name": "Yushi Bai",
            "hIndex": 17
        },
        "Jiawei Feng": {
            "authorId": "2196596824",
            "name": "Jia-wei Feng",
            "hIndex": 12
        },
        "Albert Gu": {
            "authorId": "39499001",
            "name": "Albert Gu",
            "hIndex": 22
        },
        "Nooshin Goudarzi": {
            "authorId": "2319605420",
            "name": "Nooshin Goudarzi",
            "hIndex": 0
        },
        "Marcus Wieder": {
            "authorId": "36038209",
            "name": "M. Wieder",
            "hIndex": 14
        },
        "Jack Y. Araz": {
            "authorId": "102957377",
            "name": "Jack Y. Araz",
            "hIndex": 12
        },
        "Mukunda N S": {
            "authorId": "2098928197",
            "name": "N. S. Mukunda",
            "hIndex": 1
        },
        "Federico Berto": {
            "authorId": "2133376851",
            "name": "Federico Berto",
            "hIndex": 5
        },
        "Hugh Zhang": {
            "authorId": "2317031638",
            "name": "Hugh Zhang",
            "hIndex": 1
        },
        "Arman Cohan": {
            "authorId": "2527954",
            "name": "Arman Cohan",
            "hIndex": 34
        },
        "Hyeongseop Rha": {
            "authorId": "95671806",
            "name": "H. Rha",
            "hIndex": 1
        },
        "Bin Cui": {
            "authorId": "48599680",
            "name": "Binbin Cui",
            "hIndex": 20
        },
        "Martin Paul L\u00fccke": {
            "name": "Martin Paul L\u00fccke",
            "hIndex": 0
        },
        "Yuchen Shi": {
            "authorId": "46571755",
            "name": "Yucheng Shi",
            "hIndex": 12
        },
        "Hwee Tou Ng": {
            "authorId": "34789794",
            "name": "H. Ng",
            "hIndex": 61
        },
        "Yusuke Sakai": {
            "authorId": "51306960",
            "name": "Y. Sakai",
            "hIndex": 18
        },
        "Ru Zhang": {
            "authorId": "2144465998",
            "name": "R. Zhang",
            "hIndex": 4
        },
        "Heyan Huang": {
            "authorId": "4590286",
            "name": "Heyan Huang",
            "hIndex": 27
        },
        "Tobias Dreyer": {
            "authorId": "46669343",
            "name": "T. Dreyer",
            "hIndex": 10
        },
        "Ajian Liu": {
            "authorId": "144701473",
            "name": "Ajian Liu",
            "hIndex": 12
        },
        "Ryan Babbush": {
            "authorId": "47066334",
            "name": "R. Babbush",
            "hIndex": 61
        },
        "Maxwell Crouse": {
            "authorId": "41036307",
            "name": "M. Crouse",
            "hIndex": 8
        },
        "Fabian Caba Heilbron": {
            "authorId": "3175258",
            "name": "Fabian Caba Heilbron",
            "hIndex": 20
        },
        "Mladen Nikolic": {
            "authorId": "145888108",
            "name": "Mladen Nikolic",
            "hIndex": 17
        },
        "Tianyu Li": {
            "authorId": "93509767",
            "name": "Liu Tianyu",
            "hIndex": 7
        },
        "Chang Yu": {
            "authorId": "2242310",
            "name": "W. Chang",
            "hIndex": 38
        },
        "Kamalika Chaudhuri": {
            "authorId": "38120884",
            "name": "Kamalika Chaudhuri",
            "hIndex": 42
        },
        "Dandan Song": {
            "authorId": "4605460",
            "name": "Da-Hyun Song",
            "hIndex": 8
        },
        "Hinrich Sch\u00fctze": {
            "authorId": "144418438",
            "name": "Hinrich Sch\u00fctze",
            "hIndex": 65
        },
        "Junge Zhang": {
            "authorId": "2136182354",
            "name": "Junge Zhang",
            "hIndex": 5
        },
        "Matthew P. Harrigan": {
            "authorId": "8236814",
            "name": "M. Harrigan",
            "hIndex": 26
        },
        "Yihang Zheng": {
            "authorId": "2158583982",
            "name": "Yi Zheng",
            "hIndex": 7
        },
        "Lemeng Zhao": {
            "authorId": "2157748358",
            "name": "Lexin Zhao",
            "hIndex": 2
        },
        "Yun Lin": {
            "authorId": "8652811",
            "name": "Y. Lin",
            "hIndex": 14
        },
        "Hong Wang": {
            "authorId": "49527846",
            "name": "Hong Wang",
            "hIndex": 31
        },
        "Haolong Chen": {
            "authorId": "2149050591",
            "name": "Haolong Chen",
            "hIndex": 9
        },
        "Zhuqing Jiang": {
            "authorId": "3334296",
            "name": "Zhuqing Jiang",
            "hIndex": 15
        },
        "Jakob Fehle": {
            "authorId": "1396084286",
            "name": "Jakob Fehle",
            "hIndex": 3
        },
        "Yi Tian": {
            "authorId": "1485444727",
            "name": "Yiyuan Tian",
            "hIndex": 12
        },
        "Nian-Ze Lee": {
            "authorId": "2300412",
            "name": "Nian-Ze Lee",
            "hIndex": 8
        },
        "Steven A. Hicks": {
            "authorId": "50994731",
            "name": "Steven Hicks",
            "hIndex": 20
        },
        "Guanhao Xu": {
            "authorId": "144184665",
            "name": "Guanhao Xu",
            "hIndex": 4
        },
        "Haijun Lv": {
            "authorId": "2064712629",
            "name": "H. Lv",
            "hIndex": 4
        },
        "Yejin Choi": {
            "authorId": "2257385142",
            "name": "Yejin Choi",
            "hIndex": 8
        },
        "Ronald Wilson": {
            "authorId": "1422392456",
            "name": "R. Werner-Wilson",
            "hIndex": 19
        },
        "Shin Ishii": {
            "authorId": "145516720",
            "name": "S. Ishii",
            "hIndex": 27
        },
        "Wei Wang": {
            "authorId": "71074804",
            "name": "Wei Wang",
            "hIndex": 21
        },
        "Pavan Kapanipathi": {
            "authorId": "2223082",
            "name": "Pavan Kapanipathi",
            "hIndex": 22
        },
        "Youshuang Ding": {
            "authorId": "31120676",
            "name": "Y. Ding",
            "hIndex": 2
        },
        "Hardik B. Sailor": {
            "authorId": "1916219",
            "name": "Hardik B. Sailor",
            "hIndex": 12
        },
        "Junshang Chen": {
            "authorId": "2320190236",
            "name": "Junshang Chen",
            "hIndex": 0
        },
        "Halley Fritze": {
            "name": "Halley Fritze",
            "hIndex": 0
        },
        "Haoran Yang": {
            "authorId": "2109226392",
            "name": "Haoran Wang",
            "hIndex": 4
        },
        "Zhi Chen": {
            "authorId": "2117100302",
            "name": "Z. Chen",
            "hIndex": 13
        },
        "Rihan Hai": {
            "authorId": "4734334",
            "name": "H. Rihan",
            "hIndex": 16
        },
        "Cheng Wang": {
            "name": "Cheng Wang",
            "hIndex": 0
        },
        "Tegan Emerson": {
            "authorId": "47341214",
            "name": "T. Emerson",
            "hIndex": 8
        },
        "Thomas B. Sch\u00f6n": {
            "authorId": "1802623",
            "name": "Thomas Bo Sch\u00f6n",
            "hIndex": 49
        },
        "Dirk Groeneveld": {
            "authorId": "3458736",
            "name": "Dirk Groeneveld",
            "hIndex": 10
        },
        "Zhiyu Li": {
            "authorId": "2109822858",
            "name": "Zhiyu Li",
            "hIndex": 15
        },
        "Jayaraman J. Thiagarajan": {
            "authorId": "1744175",
            "name": "Jayaraman J. Thiagarajan",
            "hIndex": 26
        },
        "Suhong Moon": {
            "authorId": "3540867",
            "name": "Su-Young Moon",
            "hIndex": 21
        },
        "Ping Luo": {
            "authorId": "3132402",
            "name": "P. Luo",
            "hIndex": 31
        },
        "Yiling Lou": {
            "authorId": "2539239",
            "name": "Yiling Lou",
            "hIndex": 20
        },
        "Burhan Pektas": {
            "authorId": "2419053",
            "name": "Burhan Pekta\u015f",
            "hIndex": 6
        },
        "Beng Jin": {
            "name": "Beng Jin",
            "hIndex": 0
        },
        "Hukai Huang": {
            "authorId": "2220619976",
            "name": "Hukai Huang",
            "hIndex": 1
        },
        "Shunfan Zhou": {
            "authorId": "2213777",
            "name": "Shunfan Zhou",
            "hIndex": 5
        },
        "Shijie Luo": {
            "authorId": "101738459",
            "name": "Lu Shijie",
            "hIndex": 5
        },
        "Suhang Zheng": {
            "authorId": "2376896",
            "name": "Suhang Zheng",
            "hIndex": 2
        },
        "Lianyu Hu": {
            "authorId": "1994488088",
            "name": "Lianyu Hu",
            "hIndex": 7
        },
        "Abdullatif K\u00f6ksal": {
            "authorId": "1999179692",
            "name": "Abdullatif K\u00f6ksal",
            "hIndex": 10
        },
        "Shane Arora": {
            "authorId": "2259924223",
            "name": "Shane Arora",
            "hIndex": 3
        },
        "Victor Lecomte": {
            "authorId": "32774745",
            "name": "Victor Lecomte",
            "hIndex": 3
        },
        "Zhaozhuo Xu": {
            "authorId": "9274729",
            "name": "Zhaozhuo Xu",
            "hIndex": 13
        },
        "Theerawit Wilaiprasitporn": {
            "authorId": "2236848",
            "name": "Theerawit Wilaiprasitporn",
            "hIndex": 18
        },
        "Minhao Zou": {
            "authorId": "2153784342",
            "name": "Mi Zou",
            "hIndex": 3
        },
        "Francesco Cremonesi": {
            "authorId": "153719951",
            "name": "Francesco Cremonesi",
            "hIndex": 7
        },
        "Michael Pradel": {
            "authorId": "1884064",
            "name": "Michael Pradel",
            "hIndex": 39
        },
        "Linzheng ChaI": {
            "authorId": "2165382882",
            "name": "Linzheng Chai",
            "hIndex": 4
        },
        "Farid Alimardani": {
            "authorId": "2319605012",
            "name": "Farid Alimardani",
            "hIndex": 0
        },
        "Adrian Kieback": {
            "authorId": "2319408843",
            "name": "Adrian Kieback",
            "hIndex": 0
        },
        "Carmine Ventre": {
            "authorId": "1798664",
            "name": "Carmine Ventre",
            "hIndex": 17
        },
        "Mohamed Chahine Ghanem": {
            "authorId": "2096651",
            "name": "M. Ghanem",
            "hIndex": 5
        },
        "Hang Yin": {
            "authorId": "145039745",
            "name": "H. Yin",
            "hIndex": 22
        },
        "Hidedaki Takahashi": {
            "name": "Hidedaki Takahashi",
            "hIndex": 0
        },
        "R\u00e9my Cazabet": {
            "authorId": "3420435",
            "name": "R\u00e9my Cazabet",
            "hIndex": 13
        },
        "Taesung Kim": {
            "authorId": "40592467",
            "name": "Taesung Kim",
            "hIndex": 40
        },
        "Chao Peng": {
            "authorId": "143913828",
            "name": "C. Peng",
            "hIndex": 13
        },
        "Peng Han": {
            "authorId": "3046485",
            "name": "Pengpeng Han",
            "hIndex": 9
        },
        "Sean I. Young": {
            "authorId": "2113772975",
            "name": "Sean I. Young",
            "hIndex": 8
        },
        "Jishnu Ray Chowdhury": {
            "authorId": "123467107",
            "name": "Jishnu Ray Chowdhury",
            "hIndex": 6
        },
        "Yisi Liu": {
            "authorId": "7895076",
            "name": "Yisi Liu",
            "hIndex": 23
        },
        "Le Tian": {
            "authorId": "50615667",
            "name": "Tianle Zhang",
            "hIndex": 17
        },
        "Mohamed Chetouani": {
            "authorId": "1680828",
            "name": "M. Chetouani",
            "hIndex": 41
        },
        "Yaojie Lu": {
            "authorId": "1831434",
            "name": "Yaojie Lu",
            "hIndex": 17
        },
        "Daniele Jahier Pagliari": {
            "authorId": "36204425",
            "name": "D. J. Pagliari",
            "hIndex": 13
        },
        "Xidong Wang": {
            "authorId": "2026232419",
            "name": "Xidong Wang",
            "hIndex": 37
        },
        "Cheng Wan": {
            "authorId": "7259197",
            "name": "Wan-Ju Cheng",
            "hIndex": 15
        },
        "Qianlong Xiang": {
            "authorId": "2319825956",
            "name": "Qianlong Xiang",
            "hIndex": 0
        },
        "Truc Nguyen": {
            "authorId": "47523647",
            "name": "Truc B Nguyen",
            "hIndex": 9
        },
        "Antoine Louis": {
            "authorId": "88483021",
            "name": "L. Antoine",
            "hIndex": 6
        },
        "Luca Santagata": {
            "authorId": "2319607519",
            "name": "Luca Santagata",
            "hIndex": 0
        },
        "Sally Junsong Wang": {
            "authorId": "2108751982",
            "name": "S. Wang",
            "hIndex": 2
        },
        "Mohammadmahdi Honarmand": {
            "authorId": "2037462864",
            "name": "Mohammadmahdi Honarmand",
            "hIndex": 3
        },
        "Yu Zheng": {
            "name": "Yu Zheng",
            "hIndex": 0
        },
        "Yujie Fan": {
            "authorId": "2002644519",
            "name": "Fanxu Yujie",
            "hIndex": 4
        },
        "Chuanbo Hua": {
            "authorId": "2176834016",
            "name": "Chuanbo Hua",
            "hIndex": 4
        },
        "Lei Sheng": {
            "authorId": "9931853",
            "name": "S. Lei",
            "hIndex": 37
        },
        "Marcelo Pereyra": {
            "authorId": "145766647",
            "name": "M. Pereyra",
            "hIndex": 24
        },
        "Junying Wang": {
            "authorId": "2110125730",
            "name": "Jun-ying Wang",
            "hIndex": 8
        },
        "Benjamin Cohen-Wang": {
            "authorId": "2289609036",
            "name": "Benjamin Cohen-Wang",
            "hIndex": 1
        },
        "Alex Gorodetsky": {
            "authorId": "91992392",
            "name": "A. Gorodetsky",
            "hIndex": 16
        },
        "Zhihan Li": {
            "authorId": "46947542",
            "name": "Zhihan Li",
            "hIndex": 9
        },
        "Ameneh Salehi": {
            "authorId": "123678370",
            "name": "Salehi-Rad Ameneh",
            "hIndex": 1
        },
        "Quangui Zhang": {
            "authorId": "1855309",
            "name": "Quangui Zhang",
            "hIndex": 3
        },
        "M. Boutaounte": {
            "authorId": "2708579",
            "name": "M. Boutaounte",
            "hIndex": 6
        },
        "Yunfang Wu": {
            "authorId": "2477658",
            "name": "Yunfang Wu",
            "hIndex": 18
        },
        "Asma Amalas": {
            "authorId": "2319416463",
            "name": "Asma Amalas",
            "hIndex": 0
        },
        "Mubarak Shah": {
            "authorId": "145103012",
            "name": "M. Shah",
            "hIndex": 116
        },
        "Ge Shi": {
            "authorId": "88034212",
            "name": "S. Ge",
            "hIndex": 5
        },
        "Dongha Lee": {
            "authorId": "2115474991",
            "name": "Dongha Lee",
            "hIndex": 7
        },
        "Kai Zhang": {
            "authorId": "47969273",
            "name": "Kaikai Zhang",
            "hIndex": 16
        },
        "Nan Yang": {
            "authorId": "152350515",
            "name": "N. Yang",
            "hIndex": 18
        },
        "Hsin-Min Wang": {
            "authorId": "1710199",
            "name": "H. Wang",
            "hIndex": 34
        },
        "Lin Ning": {
            "authorId": "2026253341",
            "name": "N. Lin",
            "hIndex": 10
        },
        "Josep Llad\u00f3s": {
            "authorId": "143826881",
            "name": "J. Llad\u00f3s",
            "hIndex": 35
        },
        "Mig\u00fcel Jett\u00e9": {
            "authorId": "2080121098",
            "name": "Miguel Jette",
            "hIndex": 2
        },
        "Andrii Dzhoha": {
            "authorId": "2129765226",
            "name": "A. Dzhoha",
            "hIndex": 1
        },
        "Lin Xie": {
            "authorId": "2012395749",
            "name": "L. Xie",
            "hIndex": 19
        },
        "Qing Gu": {
            "authorId": "1471713315",
            "name": "Q. Gu",
            "hIndex": 13
        },
        "John Cartlidge": {
            "authorId": "2275939",
            "name": "J. Cartlidge",
            "hIndex": 14
        },
        "Pingzhi Li": {
            "authorId": "2158237783",
            "name": "Pingzhi Li",
            "hIndex": 7
        },
        "George Dasoulas": {
            "authorId": "150109816",
            "name": "George Dasoulas",
            "hIndex": 9
        },
        "Hossein Afshar": {
            "authorId": "37083707",
            "name": "H. Afshar",
            "hIndex": 7
        },
        "Dmitry Korzun": {
            "authorId": "1783879",
            "name": "D. Korzun",
            "hIndex": 21
        },
        "Don Byrd": {
            "authorId": "145985085",
            "name": "Donald Byrd",
            "hIndex": 15
        },
        "Yi Cui": {
            "authorId": "11726938",
            "name": "Cuiping Yi",
            "hIndex": 14
        },
        "Siqi Cai": {
            "authorId": "2047222861",
            "name": "Siqi Cai",
            "hIndex": 5
        },
        "SayedHassan Khatoonabadi": {
            "authorId": "70044512",
            "name": "S. Khatoonabadi",
            "hIndex": 3
        },
        "Hasan Tercan": {
            "authorId": "2014353",
            "name": "Hasan Tercan",
            "hIndex": 10
        },
        "Sebastian Zach": {
            "authorId": "48944300",
            "name": "S. Zach",
            "hIndex": 4
        },
        "Anh Tuan Luu": {
            "authorId": "26336902",
            "name": "Anh Tuan Luu",
            "hIndex": 28
        },
        "Peiyi Wang": {
            "authorId": "3657304",
            "name": "Peiyi Wang",
            "hIndex": 20
        },
        "Rongze Gui": {
            "authorId": "2139960620",
            "name": "Rongze Gui",
            "hIndex": 1
        },
        "Carlo Luschi": {
            "authorId": "49147045",
            "name": "C. Luschi",
            "hIndex": 11
        },
        "Jared Culbertson": {
            "authorId": "37987195",
            "name": "Jared Culbertson",
            "hIndex": 6
        },
        "Bradley Green": {
            "authorId": "46470251",
            "name": "B. Green",
            "hIndex": 25
        },
        "Ming-Yen Lin": {
            "authorId": "47840445",
            "name": "Ming-Yen Lin",
            "hIndex": 14
        },
        "Miquel Ferriol-Galm\u00e9s": {
            "authorId": "73878471",
            "name": "Miquel Ferriol Galm\u00e9s",
            "hIndex": 7
        },
        "Weiwei Gu": {
            "authorId": "50098704",
            "name": "W. Gu",
            "hIndex": 8
        },
        "Chengyun Song": {
            "authorId": "3105244",
            "name": "Chengyun Song",
            "hIndex": 6
        },
        "Tanuj Khattar": {
            "authorId": "82403343",
            "name": "T. Khattar",
            "hIndex": 12
        },
        "Jungsoo Won": {
            "authorId": "2319372269",
            "name": "Jungsoo Won",
            "hIndex": 0
        },
        "Barrett W Thomas": {
            "authorId": "1680779",
            "name": "Barrett W. Thomas",
            "hIndex": 34
        },
        "Tim Doster": {
            "authorId": "2886830",
            "name": "T. Doster",
            "hIndex": 10
        },
        "Yang Liu": {
            "authorId": "49421824",
            "name": "Yang Liu",
            "hIndex": 55
        },
        "Drake Lin": {
            "authorId": "24596591",
            "name": "Jessica L. Drake",
            "hIndex": 4
        },
        "D. R. Ferreira": {
            "authorId": "145698611",
            "name": "D. R. Ferreira",
            "hIndex": 17
        },
        "Virginia Aglietti": {
            "authorId": "46254985",
            "name": "Virginia Aglietti",
            "hIndex": 7
        },
        "Alex Zito": {
            "authorId": "66832516",
            "name": "A. Zito",
            "hIndex": 2
        },
        "Ignacio Alvarez": {
            "authorId": "144267948",
            "name": "I. \u00c1lvarez",
            "hIndex": 8
        },
        "Zixu Wang": {
            "authorId": "2016909247",
            "name": "Zixu Wang",
            "hIndex": 7
        },
        "Haochen Wu": {
            "authorId": "46477368",
            "name": "Hao Wu",
            "hIndex": 12
        },
        "Rachid Oulad Haj Thami": {
            "authorId": "9382655",
            "name": "R. Thami",
            "hIndex": 14
        },
        "Christian Schilling": {
            "authorId": "144349954",
            "name": "C. Schilling",
            "hIndex": 21
        },
        "Yichu Zhang": {
            "authorId": "2188981626",
            "name": "Yichu Zhang",
            "hIndex": 8
        },
        "Marian Lingsch-Rosenfeld": {
            "authorId": "2123805176",
            "name": "M. Rosenfeld",
            "hIndex": 2
        },
        "Vukosi Marivate": {
            "authorId": "1875175",
            "name": "Vukosi Marivate",
            "hIndex": 14
        },
        "Andrei Irimia": {
            "authorId": "3124139",
            "name": "A. Irimia",
            "hIndex": 31
        },
        "Kevin Zhu": {
            "authorId": "144523665",
            "name": "Kevin Zhu",
            "hIndex": 29
        },
        "Robert McCormack": {
            "authorId": "1901615",
            "name": "R. McCormack",
            "hIndex": 31
        },
        "Zhongyin Zhao": {
            "authorId": "92424005",
            "name": "Zhong-Yin Zhao",
            "hIndex": 12
        },
        "Christian Jilek": {
            "authorId": "1959096",
            "name": "Christian Jilek",
            "hIndex": 8
        },
        "Salvish Goomanee": {
            "authorId": "2099186062",
            "name": "Salvish Goomanee",
            "hIndex": 1
        },
        "Charlesquin Kemajou Mbakam": {
            "authorId": "2290180151",
            "name": "Charlesquin Kemajou Mbakam",
            "hIndex": 0
        },
        "Yudara Kularathne": {
            "authorId": "2185979427",
            "name": "Yudara Kularathne",
            "hIndex": 1
        },
        "Mengling Feng": {
            "authorId": "2067622344",
            "name": "Mengling Feng",
            "hIndex": 17
        },
        "Ke Zhuang": {
            "authorId": "40601194",
            "name": "K. Zhuang",
            "hIndex": 19
        },
        "Jonathan Hoyin Chan": {
            "authorId": "2113350674",
            "name": "Jonathan H. Chan",
            "hIndex": 5
        },
        "Yifan Xie": {
            "authorId": "49291112",
            "name": "Yifan Xie",
            "hIndex": 13
        },
        "Jacqueline Lammert": {
            "authorId": "3370839",
            "name": "J. Lammert",
            "hIndex": 4
        },
        "Xiangyu Zhao": {
            "authorId": "50061699",
            "name": "Xiangyu Zhao",
            "hIndex": 14
        },
        "Yuntian Deng": {
            "authorId": "2505751",
            "name": "Yuntian Deng",
            "hIndex": 21
        },
        "Jie Liu": {
            "authorId": "2146651301",
            "name": "Jie Liu",
            "hIndex": 49
        },
        "Weiwei Chu": {
            "authorId": "48980334",
            "name": "W. Chu",
            "hIndex": 10
        },
        "Xuefeng Gao": {
            "authorId": "49779798",
            "name": "Xuefeng Gao",
            "hIndex": 4
        },
        "Changsong Liu": {
            "authorId": "2124912317",
            "name": "Changsong Liu",
            "hIndex": 6
        },
        "Ying Du": {
            "authorId": "10682317",
            "name": "Yingge Du",
            "hIndex": 37
        },
        "Prayag Tiwari": {
            "authorId": "2086549",
            "name": "P. Tiwari",
            "hIndex": 33
        },
        "Jinming Wang": {
            "authorId": "2110205942",
            "name": "Jinmin Wang",
            "hIndex": 10
        },
        "Wei Wen": {
            "authorId": "49261220",
            "name": "Weiwei Wen",
            "hIndex": 6
        },
        "Joseph P. Near": {
            "authorId": "35107766",
            "name": "Joseph P. Near",
            "hIndex": 17
        },
        "Anbang Xu": {
            "authorId": "1720067",
            "name": "Anbang Xu",
            "hIndex": 19
        },
        "Wentao Shi": {
            "authorId": "49833075",
            "name": "W. Shi",
            "hIndex": 9
        },
        "Haizhou Li": {
            "authorId": "39040879",
            "name": "Hai-zhou Li",
            "hIndex": 12
        },
        "Weijia Shi": {
            "authorId": "3040379",
            "name": "Weijia Shi",
            "hIndex": 18
        },
        "Niklas Muennighoff": {
            "authorId": "2037383772",
            "name": "Niklas Muennighoff",
            "hIndex": 24
        },
        "Danilo Dordevic": {
            "authorId": "2267734848",
            "name": "Danilo Dordevic",
            "hIndex": 1
        },
        "Damiano Spina": {
            "authorId": "48702898",
            "name": "Damiano Spina",
            "hIndex": 20
        },
        "Nian Yan": {
            "authorId": "2921454",
            "name": "Yongnian Yan",
            "hIndex": 37
        },
        "Wangchun Sun": {
            "authorId": "2297333542",
            "name": "Wangchun Sun",
            "hIndex": 0
        },
        "Sorin Grigorescu": {
            "authorId": "2150397",
            "name": "S. Grigorescu",
            "hIndex": 12
        },
        "Saurabh Vaichal": {
            "authorId": "1491319666",
            "name": "Saurabh Vaichal",
            "hIndex": 2
        },
        "Chuhao Wu": {
            "authorId": "1387840893",
            "name": "Chuhao Wu",
            "hIndex": 5
        },
        "Hai Zhao": {
            "authorId": "120020753",
            "name": "Hai-biao Zhao",
            "hIndex": 11
        },
        "Da Zheng": {
            "authorId": "30796989",
            "name": "Huanda Zheng",
            "hIndex": 15
        },
        "Shilong Bao": {
            "authorId": "3172846",
            "name": "Shilong Bao",
            "hIndex": 7
        },
        "Jimmy Ba": {
            "authorId": "2503659",
            "name": "Jimmy Ba",
            "hIndex": 43
        },
        "Phurich Saengthong": {
            "authorId": "2142252787",
            "name": "Phurich Saengthong",
            "hIndex": 1
        },
        "Bing Luo": {
            "authorId": "7486930",
            "name": "B. Luo",
            "hIndex": 5
        },
        "Filip Grali\u0144ski": {
            "authorId": "2195272",
            "name": "F. Gralinski",
            "hIndex": 6
        },
        "Jizhi Zhang": {
            "authorId": "2570260",
            "name": "Jizhi Zhang",
            "hIndex": 27
        },
        "Ada-Astrid Balauca": {
            "authorId": "2319414197",
            "name": "Ada-Astrid Balauca",
            "hIndex": 0
        },
        "Hoang-Thang Ta": {
            "authorId": "2183770615",
            "name": "Hoang Thang Ta",
            "hIndex": 5
        },
        "Kartik Audhkhasi": {
            "authorId": "3104038",
            "name": "Kartik Audhkhasi",
            "hIndex": 28
        },
        "Jeff Huang": {
            "authorId": "153099697",
            "name": "Jeffrey J Huang",
            "hIndex": 11
        },
        "Shahbaz Rezaei": {
            "authorId": "30728474",
            "name": "Shahbaz Rezaei",
            "hIndex": 10
        },
        "Xiaoyu Yang": {
            "authorId": "2109446123",
            "name": "Xiaoyu Yang",
            "hIndex": 11
        },
        "Yu Tsao": {
            "authorId": "47782156",
            "name": "Yuchi Tsao",
            "hIndex": 19
        },
        "Zhoujun Li": {
            "authorId": "2144279674",
            "name": "Zhoujun Li",
            "hIndex": 11
        },
        "Tanisha Singh": {
            "authorId": "38097282",
            "name": "Tanisha Singh",
            "hIndex": 10
        },
        "Dean Light": {
            "authorId": "46355238",
            "name": "D. Light",
            "hIndex": 3
        },
        "Yunus Erzurumlu": {
            "authorId": "2319814274",
            "name": "Yunus Erzurumlu",
            "hIndex": 0
        },
        "Sara Colantonio": {
            "authorId": "2522681",
            "name": "S. Colantonio",
            "hIndex": 16
        },
        "Matteo Corno": {
            "authorId": "1730317",
            "name": "M. Corno",
            "hIndex": 30
        },
        "Zengyou He": {
            "authorId": "10231415",
            "name": "Zengyou He",
            "hIndex": 31
        },
        "Dominik Filipiak": {
            "authorId": "2853993",
            "name": "D. Filipiak",
            "hIndex": 7
        },
        "Quinten McNamara": {
            "authorId": "8493000",
            "name": "Quinten McNamara",
            "hIndex": 7
        },
        "Zhiwei Jiang": {
            "authorId": "50483475",
            "name": "Jiang Zhiwei",
            "hIndex": 5
        },
        "Dan Zeng": {
            "authorId": "48680535",
            "name": "D. Zeng",
            "hIndex": 8
        },
        "Philip S. Yu": {
            "authorId": "152297693",
            "name": "Philip S. Yu",
            "hIndex": 33
        },
        "Ruohan Zhang": {
            "authorId": "2657185",
            "name": "Ruohan Zhang",
            "hIndex": 14
        },
        "Ran Zhang": {
            "authorId": "48264257",
            "name": "Ran-ran Zhang",
            "hIndex": 12
        },
        "Qing Wu": {
            "authorId": "50528785",
            "name": "Qing-Qing Wu",
            "hIndex": 27
        },
        "Jiale Zha": {
            "authorId": "2092336219",
            "name": "Jiale Zha",
            "hIndex": 1
        },
        "Kavish Chawla": {
            "name": "Kavish Chawla",
            "hIndex": 0
        },
        "Jun-Ting Wu": {
            "authorId": "6395813",
            "name": "Tingting Wu",
            "hIndex": 50
        },
        "Qingbin Tian": {
            "authorId": "2190608896",
            "name": "Qingbin Tian",
            "hIndex": 2
        },
        "Ruiyi Zhang": {
            "authorId": "2110008602",
            "name": "Ruiyi Zhang",
            "hIndex": 5
        },
        "Lin Qiu": {
            "authorId": "12767300",
            "name": "Linlin Qiu",
            "hIndex": 6
        },
        "Sushant Prakash": {
            "authorId": "28612243",
            "name": "Sushant Prakash",
            "hIndex": 6
        },
        "Neda Haj-Hosseini": {
            "name": "Neda Haj-Hosseini",
            "hIndex": 0
        },
        "Deqing Yang": {
            "authorId": "2089312",
            "name": "Deqing Yang",
            "hIndex": 15
        },
        "Tyler Kendall": {
            "authorId": "145803952",
            "name": "Tyler Kendall",
            "hIndex": 16
        },
        "Shunsi Zhang": {
            "authorId": "2278846444",
            "name": "Shunsi Zhang",
            "hIndex": 1
        },
        "Enhong Chen": {
            "authorId": "2227868312",
            "name": "Enhong Chen",
            "hIndex": 71
        },
        "Yuntao Ma": {
            "authorId": "153000378",
            "name": "Yun-yun Ma",
            "hIndex": 5
        },
        "Yu Bi": {
            "authorId": "92226185",
            "name": "Bi-tao Yu",
            "hIndex": 8
        },
        "Matimba Shingange": {
            "authorId": "1596210725",
            "name": "M. Shingange",
            "hIndex": 1
        },
        "Zhen Zhang": {
            "authorId": "2109262665",
            "name": "Zhen-zhen Zhang",
            "hIndex": 14
        },
        "Dawei Zhu": {
            "authorId": "47770157",
            "name": "Dawei Zhu",
            "hIndex": 9
        },
        "Feibin Zhang": {
            "authorId": "32664270",
            "name": "Feibin Zhang",
            "hIndex": 9
        },
        "Natalie Oliven": {
            "authorId": "2319614800",
            "name": "Natalie Oliven",
            "hIndex": 0
        },
        "Xiaochun Cao": {
            "authorId": "1719250",
            "name": "Xiaochun Cao",
            "hIndex": 62
        },
        "Haifeng Wang": {
            "authorId": "40465023",
            "name": "Hai-feng Wang",
            "hIndex": 15
        },
        "Duy-Quy Thai": {
            "authorId": "150323486",
            "name": "Thai Duy Quy",
            "hIndex": 1
        },
        "Jiachen Zhu": {
            "authorId": "32650864",
            "name": "Jia-chen Zhu",
            "hIndex": 5
        },
        "Hoang Khanh Lam": {
            "authorId": "144336218",
            "name": "V. Nguyen",
            "hIndex": 3
        },
        "Meiling Tao": {
            "authorId": "4860985",
            "name": "Meiling Tao",
            "hIndex": 6
        },
        "Yuta Suzuki": {
            "authorId": "49849816",
            "name": "Y. Suzuki",
            "hIndex": 12
        },
        "Tao Zhou": {
            "authorId": "114240369",
            "name": "Zhoushan Tao",
            "hIndex": 18
        },
        "Nicole Pfarr": {
            "authorId": "4238413",
            "name": "N. Pfarr",
            "hIndex": 35
        },
        "Andrei Kucharavy": {
            "authorId": "6564918",
            "name": "Andrei Kucharavy",
            "hIndex": 5
        },
        "Pierre Dognin": {
            "name": "Pierre Dognin",
            "hIndex": 0
        },
        "Sirui Huang": {
            "authorId": "1409942825",
            "name": "Sirui Huang",
            "hIndex": 10
        },
        "Runzhong Wang": {
            "authorId": "89674661",
            "name": "Runzhong Wang",
            "hIndex": 12
        },
        "Ziwen Liu": {
            "authorId": "49293393",
            "name": "Ziwen Liu",
            "hIndex": 10
        },
        "Taesung Kwon": {
            "authorId": "8196953",
            "name": "T. Kwon",
            "hIndex": 15
        },
        "Harshita Diddee": {
            "authorId": "2029651808",
            "name": "Harshita Diddee",
            "hIndex": 6
        },
        "Fabrizio Gilardi": {
            "name": "Fabrizio Gilardi",
            "hIndex": 0
        },
        "Vin\u00edicius Gandra M. Santos": {
            "name": "Vin\u00edicius Gandra M. Santos",
            "hIndex": 0
        },
        "Hongseok Oh": {
            "authorId": "7202988",
            "name": "H. Oh",
            "hIndex": 15
        },
        "Junzhao Zhang": {
            "authorId": "2122112798",
            "name": "Jun-Zhao Zhang",
            "hIndex": 5
        },
        "Houfeng Wang": {
            "authorId": "1781885",
            "name": "Houfeng Wang",
            "hIndex": 35
        },
        "Douglas L. L. Moura": {
            "authorId": "144787502",
            "name": "D\u00e9bora Oliveira",
            "hIndex": 25
        },
        "Segev Shlomov": {
            "authorId": "20513655",
            "name": "Segev Shlomov",
            "hIndex": 6
        },
        "Amirali Soheili": {
            "authorId": "3301138",
            "name": "A. Soheili",
            "hIndex": 7
        },
        "Jie Wu": {
            "name": "Jie Wu",
            "hIndex": 0
        },
        "Niklas Funk": {
            "authorId": "34897453",
            "name": "Niklas Funk",
            "hIndex": 8
        },
        "Yuancheng Wang": {
            "authorId": "7707541",
            "name": "Yuan-Cheng Wang",
            "hIndex": 18
        },
        "Aakarsh Malhotra": {
            "authorId": "3383517",
            "name": "Aakarsh Malhotra",
            "hIndex": 9
        },
        "Wenqi Fan": {
            "authorId": "2113533853",
            "name": "Wenqiang Fan",
            "hIndex": 3
        },
        "Zelin Dong": {
            "authorId": "9931078",
            "name": "Z. Dong",
            "hIndex": 12
        },
        "Xinyu Zhang": {
            "authorId": "2141829485",
            "name": "Xinyu Zhang",
            "hIndex": 22
        },
        "Andrew Bai": {
            "authorId": "40908840",
            "name": "A. Bai",
            "hIndex": 3
        },
        "Xinchen Wang": {
            "authorId": "50141687",
            "name": "Xin-Chen Wang",
            "hIndex": 7
        },
        "Minqiang Xu": {
            "authorId": "2641797",
            "name": "Minqiang Xu",
            "hIndex": 30
        },
        "Sriram Ganapathy": {
            "authorId": "1726355",
            "name": "Sriram Ganapathy",
            "hIndex": 26
        },
        "Sandy Huang": {
            "authorId": "2064588",
            "name": "Sandy H. Huang",
            "hIndex": 13
        },
        "Mengyi Shan": {
            "authorId": "31289209",
            "name": "Meng Li",
            "hIndex": 75
        },
        "Bruce W. Lee": {
            "authorId": "14445971",
            "name": "Bruce W. Lee",
            "hIndex": 5
        },
        "Mario Roos-Hoefgeest": {
            "authorId": "2237865895",
            "name": "Mario Roos-Hoefgeest",
            "hIndex": 1
        },
        "Sergey Zakharov": {
            "authorId": "144506584",
            "name": "S. Zakharov",
            "hIndex": 26
        },
        "Shijun Zhang": {
            "authorId": "51246268",
            "name": "Shijun Zhang",
            "hIndex": 21
        },
        "S. Supraja": {
            "authorId": "11381950",
            "name": "S. Chinthamreddy",
            "hIndex": 9
        },
        "Youngseog Chung": {
            "authorId": "1387920173",
            "name": "Youngseog Chung",
            "hIndex": 7
        },
        "Francisco Caetano": {
            "authorId": "1576747816",
            "name": "Francisco A. Marques",
            "hIndex": 3
        },
        "Yiqun Yao": {
            "authorId": "3468577",
            "name": "Yiqun Yao",
            "hIndex": 9
        },
        "Fredrik Heintz": {
            "authorId": "1711918",
            "name": "F. Heintz",
            "hIndex": 24
        },
        "Yuanzhi Li": {
            "authorId": "2110486989",
            "name": "Yuanzhi Li",
            "hIndex": 4
        },
        "Lee Smith": {
            "authorId": "1904107",
            "name": "Lee B. Smith",
            "hIndex": 36
        },
        "Homanga Bharadhwaj": {
            "authorId": "51113848",
            "name": "Homanga Bharadhwaj",
            "hIndex": 17
        },
        "Maartje ter Hoeve": {
            "authorId": "40377863",
            "name": "Maarten Bosma",
            "hIndex": 11
        },
        "Yuichiro Nomura": {
            "authorId": "2057629560",
            "name": "Y. Nomura",
            "hIndex": 7
        },
        "Ruben Tolosana": {
            "authorId": "1708502",
            "name": "Rub\u00e9n Tolosana",
            "hIndex": 25
        },
        "Rui C. Gon\u00e7alves": {
            "authorId": "145783755",
            "name": "Rui C. Gon\u00e7alves",
            "hIndex": 6
        },
        "Yi Chang": {
            "authorId": "7312359",
            "name": "Yi-Chun Chang",
            "hIndex": 12
        },
        "Kaan Icer": {
            "authorId": "2319606011",
            "name": "Kaan Icer",
            "hIndex": 0
        },
        "Chen-Chi Chang": {
            "authorId": "47184083",
            "name": "Chi\u2010Chen Chang",
            "hIndex": 27
        },
        "Xiaoliang Tan": {
            "authorId": "3862379",
            "name": "X. Tan",
            "hIndex": 10
        },
        "Radoslaw M. Cichy": {
            "authorId": "2492727",
            "name": "Radoslaw Martin Cichy",
            "hIndex": 29
        },
        "Yiwei Wang": {
            "authorId": "2108841639",
            "name": "Yiwei Wang",
            "hIndex": 14
        },
        "Inmo Jang": {
            "authorId": "18685505",
            "name": "Inmo Jang",
            "hIndex": 9
        },
        "Xuying Meng": {
            "authorId": "3134123",
            "name": "Xuying Meng",
            "hIndex": 10
        },
        "Ryo Ishii": {
            "authorId": "48366430",
            "name": "R. Ishii",
            "hIndex": 17
        },
        "Haohao Qu": {
            "authorId": "2178206079",
            "name": "Haohao Qu",
            "hIndex": 3
        },
        "Zihao Wang": {
            "authorId": "51495548",
            "name": "Zihao W. Wang",
            "hIndex": 8
        },
        "Le Sun": {
            "authorId": "1918367",
            "name": "Le-ping Sun",
            "hIndex": 16
        },
        "Junwei Liu": {
            "authorId": "9352138",
            "name": "L. Junwei",
            "hIndex": 7
        },
        "Liang Pang": {
            "authorId": "3052910",
            "name": "Liang-Teck Pang",
            "hIndex": 11
        },
        "David Cornett III": {
            "name": "David Cornett III",
            "hIndex": 0
        },
        "Michael N. Grussing": {
            "authorId": "69537873",
            "name": "M. Grussing",
            "hIndex": 10
        },
        "Jianwei Zhu": {
            "authorId": "2019708421",
            "name": "Jian-Wei Zhu",
            "hIndex": 6
        },
        "Chengkai Liu": {
            "authorId": "150060162",
            "name": "Chengkai Liu",
            "hIndex": 11
        },
        "Qingxiang Zhao": {
            "authorId": "145332461",
            "name": "Q. Zhao",
            "hIndex": 26
        },
        "Solim LeGris": {
            "authorId": "2302319105",
            "name": "Solim LeGris",
            "hIndex": 0
        },
        "Xiaobin Hu": {
            "authorId": "12184793",
            "name": "H. Xiaobin",
            "hIndex": 5
        },
        "Catherine Wu": {
            "name": "Catherine Wu",
            "hIndex": 0
        },
        "Babak Pahlavan": {
            "authorId": "2290842119",
            "name": "Babak Pahlavan",
            "hIndex": 0
        },
        "Douwe Kiela": {
            "authorId": "1743722",
            "name": "Douwe Kiela",
            "hIndex": 52
        },
        "Rajhans Samdani": {
            "authorId": "2276075",
            "name": "Rajhans Samdani",
            "hIndex": 9
        },
        "Zhanxin Hao": {
            "authorId": "2266090555",
            "name": "Zhanxin Hao",
            "hIndex": 1
        },
        "Ryan Tipton": {
            "authorId": "2221570481",
            "name": "Ryan Tipton",
            "hIndex": 2
        },
        "Li Fei-Fei": {
            "authorId": "2146325214",
            "name": "Feifei Li",
            "hIndex": 8
        },
        "Mohammadreza Ghaffarzadeh-Esfahani": {
            "authorId": "2319605261",
            "name": "Mohammadreza Ghaffarzadeh-Esfahani",
            "hIndex": 0
        },
        "Amaan Valiuddin": {
            "authorId": "119957983",
            "name": "M. Valiuddin",
            "hIndex": 3
        },
        "Adir Rahamim": {
            "authorId": "2159206959",
            "name": "Adir Rahamim",
            "hIndex": 2
        },
        "Jin Chen": {
            "authorId": "2108458820",
            "name": "Jin Y. Chen",
            "hIndex": 12
        },
        "Cosimo Della Santina": {
            "authorId": "35178897",
            "name": "C. D. Santina",
            "hIndex": 19
        },
        "Bowen Yu": {
            "authorId": "48613402",
            "name": "Yu Bowen",
            "hIndex": 21
        },
        "Md Rubel Ahmed": {
            "authorId": "2115209767",
            "name": "Md Rubel Ahmed",
            "hIndex": 3
        },
        "Albert Xiao": {
            "authorId": "116620717",
            "name": "A. Xiao",
            "hIndex": 6
        },
        "Volkan Cevher": {
            "authorId": "1678641",
            "name": "V. Cevher",
            "hIndex": 57
        },
        "Yucong Zhang": {
            "authorId": "2108472356",
            "name": "Yucong Zhang",
            "hIndex": 12
        },
        "Meliha Yetisgen": {
            "authorId": "1398215463",
            "name": "Meliha Yetisgen-Yildiz",
            "hIndex": 19
        },
        "Yiyan Qi": {
            "authorId": "29788396",
            "name": "Yiyan Qi",
            "hIndex": 6
        },
        "Austin Lee": {
            "authorId": "153143399",
            "name": "Austin Lee",
            "hIndex": 31
        },
        "Yuyan Ni": {
            "authorId": "2835047",
            "name": "Yuyan Ni",
            "hIndex": 3
        },
        "Jiayin Wang": {
            "authorId": "2110271722",
            "name": "Jiayin Wang",
            "hIndex": 22
        },
        "Shuai Zhu": {
            "authorId": "16003421",
            "name": "Shuai-Ru Zhu",
            "hIndex": 16
        },
        "Tyler Sorensen": {
            "authorId": "30718552",
            "name": "Tyler Sorensen",
            "hIndex": 10
        },
        "Wouter M. Kouw": {
            "authorId": "3191461",
            "name": "Wouter M. Kouw",
            "hIndex": 8
        },
        "Lemar Abdi": {
            "authorId": "2319832716",
            "name": "Lemar Abdi",
            "hIndex": 0
        },
        "Hai Wang": {
            "authorId": "2113221316",
            "name": "Hai Wang",
            "hIndex": 33
        },
        "Dwain Allan": {
            "authorId": "2207593963",
            "name": "Dwain D. Allan",
            "hIndex": 3
        },
        "Jiangyong Huang": {
            "authorId": "15062776",
            "name": "Hu Jiangyong",
            "hIndex": 5
        },
        "Evgeny Labzin": {
            "authorId": "2319609077",
            "name": "Evgeny Labzin",
            "hIndex": 0
        },
        "Wenjing Li": {
            "authorId": "2108955566",
            "name": "Wenjing Li",
            "hIndex": 6
        },
        "Xinyue Liu": {
            "authorId": "2110788056",
            "name": "Xinyu Liu",
            "hIndex": 9
        },
        "Hung-yi Lee": {
            "authorId": "1706104",
            "name": "Hung-yi Lee",
            "hIndex": 47
        },
        "Noureldin Yosri": {
            "authorId": "2307518361",
            "name": "Noureldin Yosri",
            "hIndex": 2
        },
        "Tim O. F. Conrad": {
            "authorId": "3024170",
            "name": "T. Conrad",
            "hIndex": 10
        },
        "Ryan Singh": {
            "authorId": "1409884279",
            "name": "Ryan R. Singh",
            "hIndex": 4
        },
        "Tianzhao Wu": {
            "authorId": "2151842326",
            "name": "Tianzhao Wu",
            "hIndex": 5
        },
        "Nafis Tanveer Islam": {
            "authorId": "2215168083",
            "name": "Nafis Tanveer Islam",
            "hIndex": 3
        },
        "Yongqiang He": {
            "authorId": "9383625",
            "name": "He Yongqiang",
            "hIndex": 5
        },
        "Soumya K. Ghosh": {
            "authorId": "49870581",
            "name": "S. Ghosh",
            "hIndex": 36
        },
        "Daniel Povey": {
            "authorId": "1792214",
            "name": "Daniel Povey",
            "hIndex": 61
        },
        "Zebang Cheng": {
            "authorId": "2261885793",
            "name": "Zebang Cheng",
            "hIndex": 3
        },
        "Maksim Belyaev": {
            "authorId": "145992378",
            "name": "M. Belyaev",
            "hIndex": 10
        },
        "Tanay Dixit": {
            "authorId": "2126503480",
            "name": "Tanay Dixit",
            "hIndex": 6
        },
        "Keisuke Nagato": {
            "authorId": "13211926",
            "name": "K. Nagato",
            "hIndex": 15
        },
        "Nidula Elgiriyewithana": {
            "authorId": "2292441186",
            "name": "Nidula Elgiriyewithana",
            "hIndex": 0
        },
        "Yujun Cai": {
            "authorId": "2362523",
            "name": "Yu-Jun Cai",
            "hIndex": 20
        },
        "M. Badouch": {
            "authorId": "2215549878",
            "name": "M. Badouch",
            "hIndex": 1
        },
        "Dhruv Malik": {
            "authorId": "38816085",
            "name": "Dhruv Malik",
            "hIndex": 6
        },
        "Jen-tse Huang": {
            "authorId": "5890360",
            "name": "Joseph Jen\u2010Tse Huang",
            "hIndex": 11
        },
        "Yizhou Liu": {
            "authorId": "2144411321",
            "name": "Yizhou Liu",
            "hIndex": 7
        },
        "Beryl Santa": {
            "authorId": "2319820544",
            "name": "Beryl Santa",
            "hIndex": 0
        },
        "Mattias Tiger": {
            "authorId": "31879645",
            "name": "Mattias Tiger",
            "hIndex": 6
        },
        "M Shifat Hossain": {
            "authorId": "48923691",
            "name": "Mir Sohrab Hossain",
            "hIndex": 4
        },
        "Noah A. Smith": {
            "authorId": "144365875",
            "name": "Noah A. Smith",
            "hIndex": 107
        },
        "Alaqsa Akbar": {
            "authorId": "2320151821",
            "name": "Alaqsa Akbar",
            "hIndex": 0
        },
        "Ziyin Zhang": {
            "authorId": "2116460228",
            "name": "Ziyin Zhang",
            "hIndex": 14
        },
        "Qinpei Zhao": {
            "authorId": "1729695",
            "name": "Qinpei Zhao",
            "hIndex": 13
        },
        "Carsten Stoll": {
            "authorId": "2217934",
            "name": "Carsten Stoll",
            "hIndex": 23
        },
        "Yan Liu": {
            "authorId": "2157169592",
            "name": "Y. Liu",
            "hIndex": 9
        },
        "Claudius Steinhardt": {
            "authorId": "34660482",
            "name": "Claudius Steinhardt",
            "hIndex": 15
        },
        "Victoria Tiki": {
            "authorId": "1992693883",
            "name": "Victoria Tiki",
            "hIndex": 1
        },
        "Luis Gravano": {
            "authorId": "1684012",
            "name": "L. Gravano",
            "hIndex": 51
        },
        "Manuel G\u00fcnther": {
            "authorId": "38636666",
            "name": "Manuel G\u00fcnther",
            "hIndex": 27
        },
        "Namyoung Kim": {
            "authorId": "30900549",
            "name": "Nam-Young Kim",
            "hIndex": 2
        },
        "Zhitao Gao": {
            "authorId": "46517352",
            "name": "Zhitao Gao",
            "hIndex": 8
        },
        "Peter Lundberg": {
            "authorId": "145633045",
            "name": "P. Lundberg",
            "hIndex": 35
        },
        "Joseph Saurine": {
            "authorId": "7663871",
            "name": "Joseph Saurine",
            "hIndex": 4
        },
        "Hongpei Li": {
            "authorId": "2139528443",
            "name": "Hongpei Li",
            "hIndex": 3
        },
        "Guanglin Niu": {
            "authorId": "2052804171",
            "name": "Guanglin Niu",
            "hIndex": 14
        },
        "Chih-Yuan Li": {
            "authorId": "32811782",
            "name": "Ke Chen",
            "hIndex": 80
        },
        "Yisi Zhan": {
            "authorId": "2319820907",
            "name": "Yisi Zhan",
            "hIndex": 0
        },
        "Andrei Velichko": {
            "authorId": "6917922",
            "name": "A. Velichko",
            "hIndex": 31
        },
        "Elias Bou-Harb": {
            "authorId": "1398710611",
            "name": "E. Bou-Harb",
            "hIndex": 27
        },
        "Nasim Montazeri Ghahjaverestan": {
            "authorId": "1491965660",
            "name": "N. Montazeri Ghahjaverestan",
            "hIndex": 6
        },
        "Lisi Mo": {
            "authorId": "2219977096",
            "name": "Lisi Mo",
            "hIndex": 0
        },
        "Yixin Cao": {
            "authorId": "30074160",
            "name": "Yi-Xiong Cao",
            "hIndex": 6
        },
        "Yiyuan Yang": {
            "authorId": "2108760259",
            "name": "Yiyuan Yang",
            "hIndex": 8
        },
        "Suryansh Kumar": {
            "authorId": "2409281",
            "name": "Suryansh Kumar",
            "hIndex": 17
        },
        "Zhe Yang": {
            "authorId": "144466318",
            "name": "Ruizhe Yang",
            "hIndex": 14
        },
        "Florent Guiotte": {
            "authorId": "146711698",
            "name": "F. Guiotte",
            "hIndex": 5
        },
        "Yunzhu Li": {
            "authorId": "2110483781",
            "name": "Yunzhu Li",
            "hIndex": 7
        },
        "Dravyansh Sharma": {
            "authorId": "2993069",
            "name": "Dravyansh Sharma",
            "hIndex": 7
        },
        "Gregory Szumel": {
            "name": "Gregory Szumel",
            "hIndex": 0
        },
        "Wei Jing": {
            "authorId": "1397590845",
            "name": "Wei Jing",
            "hIndex": 13
        },
        "Xinying Liu": {
            "authorId": "2110789612",
            "name": "Xinying Liu",
            "hIndex": 6
        },
        "Ayush Ghosh": {
            "authorId": "98739078",
            "name": "Ayush Ghosh",
            "hIndex": 1
        },
        "Nan Zhang": {
            "authorId": "144679572",
            "name": "N. Zhang",
            "hIndex": 25
        },
        "Lei Sha": {
            "authorId": "49647620",
            "name": "L. Sha",
            "hIndex": 19
        },
        "Yuzhang Shang": {
            "authorId": "2125633957",
            "name": "Yuzhang Shang",
            "hIndex": 9
        },
        "Zhe Zhang": {
            "authorId": "2117994980",
            "name": "Z. Zhang",
            "hIndex": 16
        },
        "Guanzhou Chen": {
            "authorId": "8771500",
            "name": "Guanzhou Chen",
            "hIndex": 12
        },
        "Xiaojian Ma": {
            "authorId": "2108529844",
            "name": "Xiaojian Ma",
            "hIndex": 4
        },
        "Xiangyu Zhang": {
            "authorId": "2190288763",
            "name": "Xiangyu Zhang",
            "hIndex": 10
        },
        "Hudson McBride": {
            "authorId": "2319413732",
            "name": "Hudson McBride",
            "hIndex": 0
        },
        "Siwei Zhang": {
            "authorId": "2092939059",
            "name": "Zhang Siwei",
            "hIndex": 6
        },
        "Gijs van Dijck": {
            "authorId": "30445438",
            "name": "G. van Dijck",
            "hIndex": 8
        },
        "Catherine Arnett": {
            "authorId": "2257347764",
            "name": "Catherine Arnett",
            "hIndex": 2
        },
        "Raviraj Joshi": {
            "authorId": "51890352",
            "name": "Raviraj Joshi",
            "hIndex": 15
        },
        "George Andriopoulos": {
            "authorId": "102569587",
            "name": "G. Andriopoulos",
            "hIndex": 3
        },
        "Marco Hutter": {
            "authorId": "14349870",
            "name": "Marco Hutter",
            "hIndex": 61
        },
        "Xuhong Zhang": {
            "authorId": "8444079",
            "name": "Xu-hong Zhang",
            "hIndex": 11
        },
        "Binbin Li": {
            "authorId": "48519025",
            "name": "Liu Binbin",
            "hIndex": 9
        },
        "Artem Sokolov": {
            "authorId": "145120273",
            "name": "A. Sokolov",
            "hIndex": 5
        },
        "Xiaosong Yang": {
            "authorId": "48520740",
            "name": "Xiaosong Yang",
            "hIndex": 30
        },
        "Alexander Bakumenko": {
            "authorId": "146016681",
            "name": "A. Bakumenko",
            "hIndex": 1
        },
        "Stephen Tian": {
            "authorId": "71692259",
            "name": "Stephen Tian",
            "hIndex": 9
        },
        "Eliu Huerta": {
            "authorId": "1387828671",
            "name": "E. Huerta",
            "hIndex": 78
        },
        "Yassir Lairgi": {
            "authorId": "2004639529",
            "name": "Yassir Lairgi",
            "hIndex": 1
        },
        "Maria Liliana Hern\u00e1ndez": {
            "authorId": "2107691351",
            "name": "L. Garcia",
            "hIndex": 8
        },
        "Xiurui Pan": {
            "name": "Xiurui Pan",
            "hIndex": 0
        },
        "Farshid Sepehrband": {
            "authorId": "2058214",
            "name": "F. Sepehrband",
            "hIndex": 22
        },
        "Martial Hebert": {
            "authorId": "145670946",
            "name": "M. Hebert",
            "hIndex": 105
        },
        "Zhenghao Lin": {
            "authorId": "31113759",
            "name": "Zheng-Wen Lin",
            "hIndex": 6
        },
        "Weiran Xu": {
            "authorId": "1753096",
            "name": "Weiran Xu",
            "hIndex": 23
        },
        "Keigo Sakurai": {
            "authorId": "118063648",
            "name": "Keigo Sakurai",
            "hIndex": 4
        },
        "Aditya Balu": {
            "name": "Aditya Balu",
            "hIndex": 0
        },
        "Xiao Liu": {
            "authorId": "2111310156",
            "name": "Xiao Liu",
            "hIndex": 11
        },
        "Ting Liu": {
            "authorId": "46999467",
            "name": "Ting\u2010ting Liu",
            "hIndex": 19
        },
        "Mengdi zhang": {
            "authorId": "1726023941",
            "name": "Meng-di Zhang",
            "hIndex": 6
        },
        "Gustavo Adolpho Lucas De Carvalho": {
            "authorId": "2319605868",
            "name": "Gustavo Adolpho Lucas De Carvalho",
            "hIndex": 0
        },
        "Jong-Seok Lee": {
            "authorId": "2108613459",
            "name": "J. Lee",
            "hIndex": 26
        },
        "Claudio Battiloro": {
            "authorId": "115032226",
            "name": "Claudio Battiloro",
            "hIndex": 9
        },
        "Michael S. Chen": {
            "authorId": "2107951061",
            "name": "Michael S. Chen",
            "hIndex": 9
        },
        "Shuirong Cao": {
            "authorId": "2214664271",
            "name": "Shuirong Cao",
            "hIndex": 1
        },
        "Hyunji Lee": {
            "authorId": "121406268",
            "name": "Hyunji Lee",
            "hIndex": 8
        },
        "Muhammad Hadir Khan": {
            "authorId": "2152496914",
            "name": "Muhammad Hadir Khan",
            "hIndex": 1
        },
        "Fengyuan Xu": {
            "authorId": "1741521",
            "name": "Fengyuan Xu",
            "hIndex": 24
        },
        "Cyrus Omar": {
            "authorId": "92190708",
            "name": "Cyrus O. Guss",
            "hIndex": 9
        },
        "Jun Yu": {
            "authorId": "6252125",
            "name": "Junrong Yu",
            "hIndex": 30
        },
        "Yin Chen": {
            "authorId": "50579493",
            "name": "Yinyin Chen",
            "hIndex": 6
        },
        "Alper Yilmaz": {
            "authorId": "38547813",
            "name": "A. Yilmaz",
            "hIndex": 13
        },
        "Ralf Otte": {
            "authorId": "47414633",
            "name": "R. Otte",
            "hIndex": 3
        },
        "Junsuk Choe": {
            "authorId": "3338475",
            "name": "Junsuk Choe",
            "hIndex": 13
        },
        "Ludovic Moncla": {
            "name": "Ludovic Moncla",
            "hIndex": 0
        },
        "Saurav Pahuja": {
            "authorId": "2216441264",
            "name": "Saurav Pahuja",
            "hIndex": 2
        },
        "Gorka Abad": {
            "authorId": "2144883147",
            "name": "Gorka Abad",
            "hIndex": 4
        },
        "E. V. Aretos": {
            "name": "E. V. Aretos",
            "hIndex": 0
        },
        "Zhuolin Li": {
            "authorId": "9248143",
            "name": "Zhuoling Li",
            "hIndex": 5
        },
        "Doyle McKey": {
            "authorId": "5674269",
            "name": "D. McKey",
            "hIndex": 56
        },
        "Lenhart K. Schubert": {
            "name": "Lenhart K. Schubert",
            "hIndex": 0
        },
        "Jun Zhao": {
            "authorId": "46508973",
            "name": "Jun Zhao",
            "hIndex": 40
        },
        "Xinyue Li": {
            "authorId": "2144455166",
            "name": "Xinyue Li",
            "hIndex": 10
        },
        "Ruoxi Cheng": {
            "authorId": "2172418925",
            "name": "Ruoxi Cheng",
            "hIndex": 2
        },
        "Roman Klinger": {
            "authorId": "66339110",
            "name": "Roman Klinger",
            "hIndex": 29
        },
        "Boshko Koloski": {
            "authorId": "2003646352",
            "name": "Boshko Koloski",
            "hIndex": 7
        },
        "Daniel Truhn": {
            "authorId": "3228050",
            "name": "D. Truhn",
            "hIndex": 26
        },
        "Xiangyuan Xue": {
            "authorId": "1665416083",
            "name": "Xue Xiangyuan",
            "hIndex": 2
        },
        "Junkai Wu": {
            "authorId": "7162169",
            "name": "Junkai Wu",
            "hIndex": 6
        },
        "Laurin Luttmann": {
            "authorId": "2047491141",
            "name": "Laurin Luttmann",
            "hIndex": 2
        },
        "Zhaoheng Ni": {
            "authorId": "3877669",
            "name": "Zhaoheng Ni",
            "hIndex": 11
        },
        "Veljko Kova\u010d": {
            "name": "Veljko Kova\u010d",
            "hIndex": 0
        },
        "Ningjing Sang": {
            "authorId": "2306382888",
            "name": "Ningjing Sang",
            "hIndex": 0
        },
        "Jingzehua Xu": {
            "authorId": "2218963838",
            "name": "Jingzehua Xu",
            "hIndex": 2
        },
        "Jun Ling": {
            "authorId": "48675730",
            "name": "Junling Song",
            "hIndex": 23
        },
        "Zhixing Wang": {
            "authorId": "9443642",
            "name": "W. Zhixing",
            "hIndex": 6
        },
        "Xiaofeng Yang": {
            "authorId": "39949095",
            "name": "Xiaofeng Yang",
            "hIndex": 43
        },
        "Kiran Kate": {
            "authorId": "2691347",
            "name": "K. Kate",
            "hIndex": 8
        },
        "Zhiheng Peng": {
            "authorId": "32125043",
            "name": "Zhi\u2010Peng Ye",
            "hIndex": 12
        },
        "Michel Ferreira Cardia Haddad": {
            "authorId": "118972334",
            "name": "M. Haddad",
            "hIndex": 5
        },
        "Ankur Bapna": {
            "authorId": "12295226",
            "name": "Ankur Bapna",
            "hIndex": 31
        },
        "Ziyang Zhang": {
            "authorId": "2116460345",
            "name": "Ziyang Zhang",
            "hIndex": 5
        },
        "Yibo Sun": {
            "authorId": "2109027083",
            "name": "Yibo Sun",
            "hIndex": 8
        },
        "Ljiljana Dolamic": {
            "authorId": "2094810509",
            "name": "Ljiljana Dolamic",
            "hIndex": 13
        },
        "Daniel Varab": {
            "authorId": "51117789",
            "name": "Daniel Varab",
            "hIndex": 6
        },
        "Juntao Tan": {
            "authorId": "2110449137",
            "name": "Juntao Tan",
            "hIndex": 15
        },
        "Manel Gil-Sorribes": {
            "authorId": "2313419523",
            "name": "Manel Gil Sorribes",
            "hIndex": 0
        },
        "Michael A. Riegler": {
            "authorId": "10395256",
            "name": "M. Riegler",
            "hIndex": 42
        },
        "Chuanyu Qin": {
            "authorId": "18540166",
            "name": "Chuanyu Qin",
            "hIndex": 13
        },
        "Alejandro Pe\u00f1a": {
            "authorId": "145170728",
            "name": "A. Pena",
            "hIndex": 11
        },
        "Wenjia Ma": {
            "authorId": "2111663782",
            "name": "Wenjia Ma",
            "hIndex": 4
        },
        "Bang An": {
            "authorId": "2230013399",
            "name": "An-Bang Wang",
            "hIndex": 20
        },
        "Daniela Noemi Rim": {
            "authorId": "103061199",
            "name": "Daniela N. Rim",
            "hIndex": 3
        },
        "Mustafa Hajij": {
            "authorId": "3451420",
            "name": "Mustafa Hajij",
            "hIndex": 16
        },
        "Weinan Dai": {
            "authorId": "31089271",
            "name": "Weina Dai",
            "hIndex": 5
        },
        "Qi Chai": {
            "authorId": "6366893",
            "name": "Qixiang Chai",
            "hIndex": 7
        },
        "Guorui Zhou": {
            "authorId": "35066946",
            "name": "Guorui Zhou",
            "hIndex": 16
        },
        "Hendrik F. Hamann": {
            "authorId": "30968235",
            "name": "H. Hamann",
            "hIndex": 35
        },
        "Sebastian Schelter": {
            "authorId": "2180399",
            "name": "Sebastian Schelter",
            "hIndex": 26
        },
        "Zhifang Sui": {
            "authorId": "3335836",
            "name": "Zhifang Sui",
            "hIndex": 34
        },
        "Abhishek Gupta": {
            "authorId": "2135765838",
            "name": "Abhishek K. Gupta",
            "hIndex": 25
        },
        "Eun Hyun Seo": {
            "authorId": "5124794",
            "name": "E. Seo",
            "hIndex": 23
        },
        "Alexandros Karakasidis": {
            "authorId": "3312338",
            "name": "Alexandros Karakasidis",
            "hIndex": 11
        },
        "Pierre-Henri Paris": {
            "authorId": "36560957",
            "name": "Pierre-Henri Paris",
            "hIndex": 4
        },
        "Danyu Sun": {
            "authorId": "4756525",
            "name": "Danyu Sun",
            "hIndex": 9
        },
        "Linmei Hu": {
            "authorId": "1771202",
            "name": "Linmei Hu",
            "hIndex": 18
        },
        "Zijun Gao": {
            "authorId": "2006197309",
            "name": "Zijun Gao",
            "hIndex": 2
        },
        "Shirui Pan": {
            "authorId": "2585415",
            "name": "Shirui Pan",
            "hIndex": 53
        },
        "Yunze Man": {
            "authorId": "48087997",
            "name": "Yunze Man",
            "hIndex": 7
        },
        "Haddad Philip": {
            "authorId": "34236931",
            "name": "P. Haddad",
            "hIndex": 4
        },
        "Junyoung Lee": {
            "authorId": "2108522265",
            "name": "Jun-Young Lee",
            "hIndex": 23
        },
        "Ruixuan Zhang": {
            "authorId": "2118403639",
            "name": "Rui Zhang",
            "hIndex": 7
        },
        "Jack T. Beerman": {
            "authorId": "2188172012",
            "name": "Jack T. Beerman",
            "hIndex": 1
        },
        "Qiang Sheng": {
            "authorId": "50443310",
            "name": "Shengqiang Yu",
            "hIndex": 16
        },
        "Kilian Q. Weinberger": {
            "authorId": "7446832",
            "name": "Kilian Q. Weinberger",
            "hIndex": 75
        },
        "Yanni Xue": {
            "authorId": "2149672691",
            "name": "Yanni Xue",
            "hIndex": 6
        },
        "Hajime Ebisawa": {
            "authorId": "2320153130",
            "name": "Hajime Ebisawa",
            "hIndex": 0
        },
        "Li Guo": {
            "authorId": "2027575345",
            "name": "Li Guo",
            "hIndex": 11
        },
        "Chaoqun Wan": {
            "authorId": "46560442",
            "name": "W. Chaoqun",
            "hIndex": 4
        },
        "Ziqian Bi": {
            "authorId": "2236932462",
            "name": "Ziqian Bi",
            "hIndex": 1
        },
        "Nidhi Goel": {
            "authorId": "47867288",
            "name": "N. Goel",
            "hIndex": 17
        },
        "Wachara Fungwacharakorn": {
            "authorId": "9116018",
            "name": "Wachara Fungwacharakorn",
            "hIndex": 4
        },
        "Lisa Singh": {
            "authorId": "145976303",
            "name": "L. Singh",
            "hIndex": 19
        },
        "Yanbing Bai": {
            "authorId": "3143308",
            "name": "Yanbing Bai",
            "hIndex": 13
        },
        "Kostiantyn Kucher": {
            "authorId": "34523583",
            "name": "Kostiantyn Kucher",
            "hIndex": 15
        },
        "Mojtaba Safari": {
            "authorId": "90385042",
            "name": "Mojtaba Safari",
            "hIndex": 7
        },
        "Amir Gholami": {
            "authorId": "10419477",
            "name": "A. Gholami",
            "hIndex": 37
        },
        "Simon Hoermann": {
            "authorId": "2046514",
            "name": "S. Hoermann",
            "hIndex": 17
        },
        "Ziwen Han": {
            "authorId": "9199197",
            "name": "Ziwen Han",
            "hIndex": 5
        },
        "Victoria Lin": {
            "authorId": "143724481",
            "name": "Xi Victoria Lin",
            "hIndex": 24
        },
        "Alvaro Velasquez": {
            "authorId": "145720050",
            "name": "Alvaro Velasquez",
            "hIndex": 11
        },
        "Aarti Singh": {
            "authorId": "2109423866",
            "name": "Aarti Singh",
            "hIndex": 39
        },
        "Yuanqing Wang": {
            "authorId": "7707759",
            "name": "Yuanqing Wang",
            "hIndex": 29
        },
        "Zhiqiang Xie": {
            "authorId": "151466221",
            "name": "Zhi-Xu Xie",
            "hIndex": 4
        },
        "Yutai Hou": {
            "authorId": "8471176",
            "name": "Yutai Hou",
            "hIndex": 12
        },
        "Zinuo Cai": {
            "authorId": "2152133015",
            "name": "Zinuo Cai",
            "hIndex": 2
        },
        "Piotr Sankowski": {
            "authorId": "1734541",
            "name": "P. Sankowski",
            "hIndex": 29
        },
        "Silvia Garc\u00eda-M\u00e9ndez": {
            "authorId": "1405165681",
            "name": "Silvia Garc\u00eda-M\u00e9ndez",
            "hIndex": 9
        },
        "Shuran Song": {
            "authorId": "114642789",
            "name": "Shuran Song",
            "hIndex": 5
        },
        "Michael J. Black": {
            "authorId": "2105795",
            "name": "Michael J. Black",
            "hIndex": 126
        },
        "Jinshan Yang": {
            "authorId": "2109612687",
            "name": "Jin-Suk Yang",
            "hIndex": 8
        },
        "Thai Hoang": {
            "name": "Thai Hoang",
            "hIndex": 0
        },
        "Takahiro Miki": {
            "authorId": "47884157",
            "name": "Takahiro Miki",
            "hIndex": 26
        },
        "Theodore Papamarkou": {
            "authorId": "2913894",
            "name": "T. Papamarkou",
            "hIndex": 17
        },
        "Yong Xu": {
            "authorId": "121983683",
            "name": "Yong Xu",
            "hIndex": 19
        },
        "Yihuang Kang": {
            "authorId": "3426053",
            "name": "Yihuang Kang",
            "hIndex": 6
        },
        "Stylianos Loukas Vasileiou": {
            "authorId": "38385833",
            "name": "S. Vasileiou",
            "hIndex": 6
        },
        "Zhixu Li": {
            "authorId": "115419489",
            "name": "Zhixu Li",
            "hIndex": 26
        },
        "Prince Aboagye": {
            "authorId": "35398596",
            "name": "P. Anokye",
            "hIndex": 7
        },
        "Xinyu Chen": {
            "authorId": "2157470551",
            "name": "Xinyu Chen",
            "hIndex": 6
        },
        "Nicolas Pielawski": {
            "name": "Nicolas Pielawski",
            "hIndex": 0
        },
        "Houqiang Li": {
            "authorId": "2108508299",
            "name": "Houqiang Li",
            "hIndex": 4
        },
        "Chao Qu": {
            "authorId": "1614580980",
            "name": "Chao Qu",
            "hIndex": 17
        },
        "Beidi Chen": {
            "authorId": "4319427",
            "name": "Beidi Chen",
            "hIndex": 17
        },
        "Xie Chen": {
            "authorId": "5176227",
            "name": "Chenchen Xie",
            "hIndex": 13
        },
        "Naomi Saphra": {
            "authorId": "2362960",
            "name": "Naomi Saphra",
            "hIndex": 13
        },
        "Sumaiya Ohab": {
            "authorId": "2319408135",
            "name": "Sumaiya Ohab",
            "hIndex": 0
        },
        "Xiaohan Cheng": {
            "authorId": "52560471",
            "name": "Chen Xiaohan",
            "hIndex": 4
        },
        "Cheng Long": {
            "authorId": "3696173",
            "name": "Xianlong Cheng",
            "hIndex": 27
        },
        "Xinzhi Wang": {
            "authorId": "2108149198",
            "name": "Xinzhi Wang",
            "hIndex": 31
        },
        "Daniel Justus": {
            "authorId": "39145648",
            "name": "Daniel Justus",
            "hIndex": 9
        },
        "Erik Rodner": {
            "authorId": "1679449",
            "name": "E. Rodner",
            "hIndex": 31
        },
        "Alberto Cattaneo": {
            "authorId": "2350245",
            "name": "A. Cattaneo",
            "hIndex": 28
        },
        "Cong Feng": {
            "authorId": "49356609",
            "name": "F. Cong",
            "hIndex": 16
        },
        "Federica Baccini": {
            "authorId": "2007371736",
            "name": "Federica Baccini",
            "hIndex": 4
        },
        "Wenhao Guan": {
            "authorId": "40527356",
            "name": "W. Guan",
            "hIndex": 9
        },
        "Vinayak Thapliyal": {
            "authorId": "2319418777",
            "name": "Vinayak Thapliyal",
            "hIndex": 0
        },
        "Zirui Xu": {
            "authorId": "8800524",
            "name": "Zirui Xu",
            "hIndex": 10
        },
        "Chaojun Xiao": {
            "authorId": "51131083",
            "name": "Chaojun Xiao",
            "hIndex": 16
        },
        "Huan Yee Koh": {
            "authorId": "2134585717",
            "name": "Huan Yee Koh",
            "hIndex": 8
        },
        "Haoning Xi": {
            "name": "Haoning Xi",
            "hIndex": 0
        },
        "Abdellah Chehri": {
            "authorId": "2782896",
            "name": "Abdellah Chehri",
            "hIndex": 25
        },
        "Hao Yi": {
            "authorId": "46875823",
            "name": "Y. Hao",
            "hIndex": 22
        },
        "Xing Fan": {
            "authorId": "145502043",
            "name": "Xinghe Fan",
            "hIndex": 33
        },
        "Arun Balaji Buduru": {
            "authorId": "3170352",
            "name": "Arun Balaji Buduru",
            "hIndex": 10
        },
        "Filip Ilievski": {
            "authorId": "2512264",
            "name": "Filip Ilievski",
            "hIndex": 14
        },
        "Boyan Zhang": {
            "authorId": "48335627",
            "name": "Boyan Zhang",
            "hIndex": 4
        },
        "Alexandra Rogova": {
            "authorId": "2183790417",
            "name": "Alexandra Rogova",
            "hIndex": 3
        },
        "Jinkyoo Park": {
            "authorId": "2294562330",
            "name": "Jinkyoo Park",
            "hIndex": 24
        },
        "Hao Feng": {
            "authorId": "143719945",
            "name": "H. Feng",
            "hIndex": 26
        },
        "Bahar Bateni": {
            "authorId": "2214067226",
            "name": "Bahar Bateni",
            "hIndex": 1
        },
        "Qiming Bao": {
            "authorId": "1491516930",
            "name": "Qiming Bao",
            "hIndex": 5
        },
        "Rohitash Chandra": {
            "authorId": "38743363",
            "name": "Rohitash Chandra",
            "hIndex": 23
        },
        "Tiansheng Huang": {
            "authorId": "145281931",
            "name": "Tian Huang",
            "hIndex": 39
        },
        "Binyuan Hui": {
            "authorId": "151471590",
            "name": "Binyuan Hui",
            "hIndex": 19
        },
        "Qiang Wu": {
            "authorId": "1690519",
            "name": "Qiang Wu",
            "hIndex": 38
        },
        "Jorg Bornschein": {
            "authorId": "144908539",
            "name": "J. Bornschein",
            "hIndex": 21
        },
        "Dominykas Seputis": {
            "authorId": "2112922202",
            "name": "Dominykas Seputis",
            "hIndex": 1
        },
        "Chuyi Chen": {
            "authorId": "15148735",
            "name": "Chuyi Chen",
            "hIndex": 24
        },
        "Bohan Yu": {
            "authorId": "7029804",
            "name": "Bohan Yu",
            "hIndex": 5
        },
        "Stjepan Picek": {
            "authorId": "1686538",
            "name": "S. Picek",
            "hIndex": 35
        },
        "Zhiqi Shao": {
            "authorId": "2187297431",
            "name": "Zhiqi Shao",
            "hIndex": 4
        },
        "Moussa Kassem Sbeyti": {
            "authorId": "2220095025",
            "name": "Moussa Kassem Sbeyti",
            "hIndex": 1
        },
        "Che-Ping Tsai": {
            "authorId": "41018842",
            "name": "Che-Ping Tsai",
            "hIndex": 7
        },
        "Lang Xu": {
            "authorId": "47775828",
            "name": "Lang Xu",
            "hIndex": 27
        },
        "Massimo Poncino": {
            "authorId": "1695243",
            "name": "M. Poncino",
            "hIndex": 38
        },
        "Weixiong Rao": {
            "authorId": "144084147",
            "name": "Weixiong Rao",
            "hIndex": 15
        },
        "Gemb Kaljavesi": {
            "authorId": "2225346400",
            "name": "Gemb Kaljavesi",
            "hIndex": 2
        },
        "Henrique Da Silva Gameiro": {
            "authorId": "143615304",
            "name": "J. Silva",
            "hIndex": 13
        },
        "Xiping Lin": {
            "authorId": "2115499476",
            "name": "Xi-Po Lin",
            "hIndex": 1
        },
        "Shawn O'Banion": {
            "authorId": "1405642942",
            "name": "S. O\u2019Banion",
            "hIndex": 7
        },
        "Julian McAuley": {
            "authorId": "35660011",
            "name": "Julian McAuley",
            "hIndex": 61
        },
        "Shunichi Akatsuka": {
            "authorId": "49233621",
            "name": "S. Akatsuka",
            "hIndex": 48
        },
        "Wenhan Yang": {
            "authorId": "49230339",
            "name": "Wen-Hao Yang",
            "hIndex": 16
        },
        "Chae Won Kim": {
            "authorId": "12349501",
            "name": "Chaewon Kim",
            "hIndex": 5
        },
        "Shamsuddeen Hassan Muhammad": {
            "authorId": "7744881",
            "name": "Shamsuddeen Hassan Muhammad",
            "hIndex": 12
        },
        "Ilad Alavi Darazam": {
            "authorId": "8180528",
            "name": "I. Darazam",
            "hIndex": 12
        },
        "Cho-Ying Wu": {
            "authorId": "153248704",
            "name": "Cho-Ying Wu",
            "hIndex": 8
        },
        "Alan Said": {
            "authorId": "40404161",
            "name": "A. Said",
            "hIndex": 22
        },
        "Seibu Mary Jacob": {
            "authorId": "32201532",
            "name": "S. M. Jacob",
            "hIndex": 11
        },
        "Xin Liu": {
            "authorId": "46522312",
            "name": "Xin-Xin Liu",
            "hIndex": 11
        },
        "Diego Doimo": {
            "authorId": "104350259",
            "name": "Diego Doimo",
            "hIndex": 5
        },
        "Sebastian Me\u017enar": {
            "authorId": "1947503241",
            "name": "Sebastian Me\u017enar",
            "hIndex": 3
        },
        "Melih Catal": {
            "authorId": "2319610325",
            "name": "Melih Catal",
            "hIndex": 0
        },
        "Hamza Farooq": {
            "authorId": "4754567",
            "name": "H. Farooq",
            "hIndex": 21
        },
        "Christian Hardmeier": {
            "authorId": "2579449",
            "name": "Christian Hardmeier",
            "hIndex": 22
        },
        "Cheryl Lee": {
            "authorId": "153897484",
            "name": "Cheryl T. Lee",
            "hIndex": 44
        },
        "Feifan Song": {
            "authorId": "2059836020",
            "name": "Fei Song",
            "hIndex": 10
        },
        "Elizaveta Korotkova": {
            "authorId": "145120786",
            "name": "Elizaveta Korotkova",
            "hIndex": 3
        },
        "Rafael C. Gonz\u00e1lez": {
            "authorId": "1401872218",
            "name": "R. Gonz\u00e1lez-Cano",
            "hIndex": 17
        },
        "Liang Xie": {
            "authorId": "91922089",
            "name": "Liangfu Xie",
            "hIndex": 5
        },
        "Chen Liu": {
            "authorId": "86947735",
            "name": "Liuyi Chen",
            "hIndex": 5
        },
        "Kamesh Madduri": {
            "authorId": "2421239",
            "name": "Kamesh Madduri",
            "hIndex": 31
        },
        "Han Xue": {
            "authorId": "8479908",
            "name": "Xueyao Han",
            "hIndex": 26
        },
        "Huy Phan": {
            "authorId": "8092167",
            "name": "Huy P Phan",
            "hIndex": 29
        },
        "Jinfeng Xu": {
            "authorId": "46372934",
            "name": "Jinfeng Xu",
            "hIndex": 14
        },
        "Juan C. Caicedo": {
            "authorId": "145507543",
            "name": "Juan C. Caicedo",
            "hIndex": 29
        },
        "Joscif Raigne": {
            "authorId": "2157188831",
            "name": "Joscif Raigne",
            "hIndex": 1
        },
        "Ignacio Serna": {
            "authorId": "98849914",
            "name": "Ignacio Serna",
            "hIndex": 9
        },
        "Hangyu Qin": {
            "authorId": "2041760027",
            "name": "Hang-Yu Qin",
            "hIndex": 2
        },
        "Xuechen Liang": {
            "authorId": "11522029",
            "name": "Xuechen Liang",
            "hIndex": 13
        },
        "Ian Reid": {
            "authorId": "102767772",
            "name": "I. Reid",
            "hIndex": 25
        },
        "Wenhui Zhu": {
            "authorId": "48506902",
            "name": "Wenhui Zhu",
            "hIndex": 9
        },
        "James Wooten": {
            "authorId": "98881003",
            "name": "Jadrian Wooten",
            "hIndex": 10
        },
        "Gokhan Dogru": {
            "authorId": "2283220490",
            "name": "Gokhan Dogru",
            "hIndex": 1
        },
        "Scott Mahan": {
            "authorId": "4565697",
            "name": "C. Mahan",
            "hIndex": 20
        },
        "Minjin Kim": {
            "authorId": "2109022079",
            "name": "Minjin Kim",
            "hIndex": 24
        },
        "Youliang Yuan": {
            "authorId": "49890547",
            "name": "You Zhang",
            "hIndex": 30
        },
        "Ekaterina Verbitskaia": {
            "authorId": "49733741",
            "name": "Ekaterina Verbitskaia",
            "hIndex": 3
        },
        "Rafael Mateus": {
            "authorId": "1403634021",
            "name": "R. H. Mateus-Vargas",
            "hIndex": 4
        },
        "Washington Garcia": {
            "authorId": "47238599",
            "name": "Washington Garcia",
            "hIndex": 6
        },
        "Junfeng Tian": {
            "authorId": "1815902149",
            "name": "J. Tian",
            "hIndex": 5
        },
        "Sizhe Wang": {
            "authorId": "3424952",
            "name": "Sizhe Wang",
            "hIndex": 28
        },
        "Xiang Huang": {
            "authorId": "46423235",
            "name": "Xiang Huang",
            "hIndex": 16
        },
        "\u00d6zlem Uzuner": {
            "authorId": "1723337",
            "name": "\u00d6zlem Uzuner",
            "hIndex": 39
        },
        "Xueyuan Han": {
            "authorId": "5751383",
            "name": "Xueying Han",
            "hIndex": 19
        },
        "Jianyi Liu": {
            "authorId": "2108351520",
            "name": "Jian-yi Liu",
            "hIndex": 29
        },
        "Tsuyoshi Kuroda": {
            "authorId": "50256713",
            "name": "T. Kuroda",
            "hIndex": 9
        },
        "Leon Moonen": {
            "authorId": "1762006",
            "name": "L. Moonen",
            "hIndex": 35
        },
        "Qiyang Wan": {
            "authorId": "2055596493",
            "name": "Qiyang Wan",
            "hIndex": 2
        },
        "Austin Garret": {
            "authorId": "121881730",
            "name": "A. Gross",
            "hIndex": 1
        },
        "Rajat Rawat": {
            "authorId": "1572229847",
            "name": "Rajat Rawat",
            "hIndex": 0
        },
        "Chengyu Huang": {
            "authorId": "2150607920",
            "name": "Cheng-Yu Huang",
            "hIndex": 7
        },
        "Julian Fierrez": {
            "authorId": "1701431",
            "name": "Julian Fierrez",
            "hIndex": 64
        },
        "Phuc Phan Van": {
            "authorId": "108581007",
            "name": "V. Phan",
            "hIndex": 2
        },
        "Abhinav Moudgil": {
            "authorId": "32587693",
            "name": "A. Moudgil",
            "hIndex": 5
        },
        "Haoming Li": {
            "authorId": "31475815",
            "name": "Liu Haoming",
            "hIndex": 6
        },
        "Noura Al Moubayed": {
            "authorId": "1711819",
            "name": "N. A. Moubayed",
            "hIndex": 15
        },
        "Malik Hassanaly": {
            "authorId": "102988910",
            "name": "M. Hassanaly",
            "hIndex": 14
        },
        "Andreas Christmann": {
            "authorId": "2534678",
            "name": "A. Christmann",
            "hIndex": 23
        },
        "Pallavi Kandanur": {
            "authorId": "2319606495",
            "name": "Pallavi Kandanur",
            "hIndex": 0
        },
        "Manuel Cebrian": {
            "authorId": "145512647",
            "name": "Manuel Cebrian",
            "hIndex": 35
        },
        "Sam Gijsen": {
            "authorId": "1754286792",
            "name": "S. Gijsen",
            "hIndex": 3
        },
        "Weiqing Liu": {
            "authorId": "9696354",
            "name": "Weiqing Liu",
            "hIndex": 35
        },
        "Imanol Solano": {
            "authorId": "2226519560",
            "name": "Imanol Solano",
            "hIndex": 1
        },
        "Sarah Maria Elisabeth Bechtle": {
            "authorId": "33931249",
            "name": "Sarah Bechtle",
            "hIndex": 6
        },
        "Yuben Wu": {
            "authorId": "15566081",
            "name": "Yu-Bi Wu",
            "hIndex": 4
        },
        "Deepak Gulati": {
            "authorId": "47263021",
            "name": "D. Gulati",
            "hIndex": 11
        },
        "Frederic Font": {
            "authorId": "2486931",
            "name": "F. Font",
            "hIndex": 14
        },
        "Miguel del Rio": {
            "authorId": "67299073",
            "name": "L. D. Del R\u00edo",
            "hIndex": 19
        },
        "Jinfeng Huang": {
            "authorId": "144079869",
            "name": "Jinfeng Huang",
            "hIndex": 12
        },
        "Ateret Anaby-Tavor": {
            "authorId": "1400347538",
            "name": "Ateret Anaby-Tavor",
            "hIndex": 11
        },
        "Zhaoliang Chen": {
            "authorId": "2144300725",
            "name": "Zhaoliang Chen",
            "hIndex": 6
        },
        "Adel Moumen": {
            "authorId": "2208673188",
            "name": "Adel Moumen",
            "hIndex": 1
        },
        "Marissa Masden": {
            "authorId": "1833643410",
            "name": "Marissa Masden",
            "hIndex": 2
        },
        "Ehsan Chitsaz": {
            "authorId": "2325555",
            "name": "E. Chitsaz",
            "hIndex": 19
        },
        "Yifeng Zhang": {
            "authorId": "2108463889",
            "name": "Yifeng Zhang",
            "hIndex": 9
        },
        "Yawen Li": {
            "name": "Yawen Li",
            "hIndex": 0
        },
        "Finale Doshi-Velez": {
            "authorId": "1388372395",
            "name": "F. Doshi-Velez",
            "hIndex": 51
        },
        "Duy-Ngoc Dinh Cao": {
            "name": "Duy-Ngoc Dinh Cao",
            "hIndex": 0
        },
        "Junhui Wang": {
            "authorId": "120465755",
            "name": "Junhui Wang",
            "hIndex": 27
        },
        "Igor Kulikov": {
            "authorId": "153505883",
            "name": "I. Kulikov",
            "hIndex": 14
        },
        "Shu Wu": {
            "authorId": "47403278",
            "name": "Shuyun Wu",
            "hIndex": 13
        },
        "Zhuotao Liu": {
            "authorId": "3147217",
            "name": "Zhuotao Liu",
            "hIndex": 13
        },
        "Joao Carvalho": {
            "authorId": "2067740108",
            "name": "Jo\u00e3o Carvalho",
            "hIndex": 3
        },
        "Akshara Prabhakar": {
            "authorId": "116606529",
            "name": "Akshara Prabhakar",
            "hIndex": 4
        },
        "Musfiqur Rahman": {
            "authorId": "145729196",
            "name": "Md. Musfiqur Rahman",
            "hIndex": 25
        },
        "Haorong Hong": {
            "authorId": "2107257542",
            "name": "H. Yu",
            "hIndex": 2
        },
        "Jianxiao Jiang": {
            "authorId": "49403702",
            "name": "Huan Li",
            "hIndex": 14
        },
        "Giridhar Kaushik Ramachandran": {
            "authorId": "2056114834",
            "name": "Giridhar Kaushik Ramachandran",
            "hIndex": 3
        },
        "Ryotaro Shimizu": {
            "authorId": "13745398",
            "name": "Ryotaro Shimizu",
            "hIndex": 5
        },
        "Jingrui Ye": {
            "authorId": "2116094578",
            "name": "J. Ye",
            "hIndex": 25
        },
        "Vu Khanh Quy": {
            "authorId": "36320464",
            "name": "V. K. Quy",
            "hIndex": 13
        },
        "Ananth Grama": {
            "authorId": "1732163",
            "name": "A. Grama",
            "hIndex": 46
        },
        "Alexander Weers": {
            "name": "Alexander Weers",
            "hIndex": 0
        },
        "Farhat Binte Azam": {
            "authorId": "2179576684",
            "name": "F. Azam",
            "hIndex": 3
        },
        "Anmol Guragain": {
            "authorId": "2319604866",
            "name": "Anmol Guragain",
            "hIndex": 0
        },
        "Stefano Martiniani": {
            "authorId": "7331158",
            "name": "Stefano Martiniani",
            "hIndex": 13
        },
        "Baiqi Wu": {
            "authorId": "2310776850",
            "name": "Baiqi Wu",
            "hIndex": 0
        },
        "Christian Valadez": {
            "authorId": "2068495768",
            "name": "R. Castillo",
            "hIndex": 3
        },
        "Yu Guan": {
            "authorId": "3211634",
            "name": "Guanglong Yu",
            "hIndex": 11
        },
        "Guangjing Wang": {
            "authorId": "2152582885",
            "name": "Guan Wang",
            "hIndex": 10
        },
        "Iacer Calixto": {
            "authorId": "3068677",
            "name": "Iacer Calixto",
            "hIndex": 17
        },
        "Matou\u0161 Elphick": {
            "authorId": "2217266768",
            "name": "Matou\u0161 Elphick",
            "hIndex": 1
        },
        "Yudong Zhang": {
            "authorId": "2108044130",
            "name": "Yudong Zhang",
            "hIndex": 12
        },
        "Nafis Irtiza Tripto": {
            "authorId": "66674465",
            "name": "Nafis Irtiza Tripto",
            "hIndex": 5
        },
        "Erik Miehling": {
            "authorId": "3123239",
            "name": "Erik Miehling",
            "hIndex": 11
        },
        "Jonathan Fritz": {
            "authorId": "1769429",
            "name": "J. Fritz",
            "hIndex": 33
        },
        "Fabian Koch": {
            "authorId": "2482351",
            "name": "Fabian Koch",
            "hIndex": 2
        },
        "Rui Shen": {
            "authorId": "2066592517",
            "name": "R. Shen",
            "hIndex": 13
        },
        "Hui Mao": {
            "authorId": "145344904",
            "name": "H. Mao",
            "hIndex": 18
        },
        "Peng Huang": {
            "authorId": "116421573",
            "name": "Pengliang Huang",
            "hIndex": 7
        },
        "Gaetan Frusque": {
            "authorId": "150110657",
            "name": "Ga\u00ebtan Frusque",
            "hIndex": 6
        },
        "Woosang Lim": {
            "authorId": "2397215",
            "name": "Woosang Lim",
            "hIndex": 4
        },
        "Tianbao Yang": {
            "authorId": "40381920",
            "name": "Tianbao Yang",
            "hIndex": 46
        },
        "Ishaan Domkundwar": {
            "authorId": "2320151441",
            "name": "Ishaan Domkundwar",
            "hIndex": 0
        },
        "Zach Eidex": {
            "authorId": "2164750978",
            "name": "Zach Eidex",
            "hIndex": 3
        },
        "Ninad Dixit": {
            "authorId": "92328684",
            "name": "Ninad Dixit",
            "hIndex": 4
        },
        "Zixuan Dong": {
            "authorId": "2000323637",
            "name": "Zixuan Dong",
            "hIndex": 7
        },
        "Tong Zhu": {
            "authorId": "2118258045",
            "name": "T. Zhu",
            "hIndex": 17
        },
        "Xuanang Chen": {
            "authorId": "49794910",
            "name": "Xuanang Chen",
            "hIndex": 5
        },
        "Jiaqi Ma": {
            "authorId": "144303757",
            "name": "Jiaqi Ma",
            "hIndex": 22
        },
        "Jianming Zhang": {
            "authorId": "2144188491",
            "name": "Jianming Zhang",
            "hIndex": 26
        },
        "Keno Kyrill Bressem": {
            "authorId": "28908056",
            "name": "K. Bressem",
            "hIndex": 19
        },
        "Jiawei Jiang": {
            "authorId": "144116769",
            "name": "Jiawei Jiang",
            "hIndex": 10
        },
        "Michiko Yoshitake": {
            "authorId": "145848424",
            "name": "M. Yoshitake",
            "hIndex": 19
        },
        "Felix Herrmann": {
            "authorId": "3191410",
            "name": "F. Herrmann",
            "hIndex": 42
        },
        "Ruiting Dai": {
            "authorId": "145745355",
            "name": "Rui Dai",
            "hIndex": 13
        },
        "Pengfei Cai": {
            "authorId": "143679245",
            "name": "Pengfei Cai",
            "hIndex": 32
        },
        "Ali Farhadi": {
            "authorId": "143787583",
            "name": "Ali Farhadi",
            "hIndex": 73
        },
        "Tao Peng": {
            "authorId": "145548193",
            "name": "P. Tao",
            "hIndex": 42
        },
        "Salah Eddine Laidoudi": {
            "authorId": "2273188680",
            "name": "Salah-eddine Laidoudi",
            "hIndex": 1
        },
        "Gean Santos": {
            "authorId": "2065100353",
            "name": "G. F. Santos",
            "hIndex": 3
        },
        "Harshay Shah": {
            "authorId": "32700648",
            "name": "Harshay Shah",
            "hIndex": 4
        },
        "Dmitry Scherbakov": {
            "authorId": "152754398",
            "name": "D. Scherbakov",
            "hIndex": 1
        },
        "Zhao Mandi": {
            "authorId": "2126966292",
            "name": "Zhao Mandi",
            "hIndex": 5
        },
        "WenJian Wang": {
            "authorId": "1857779",
            "name": "Wenjian Wang",
            "hIndex": 16
        },
        "Tu Nguyen Ngoc": {
            "authorId": "1486070346",
            "name": "Tu N. Nguyen",
            "hIndex": 24
        },
        "Ying Shan": {
            "authorId": "8361606",
            "name": "Yingshan Han",
            "hIndex": 23
        },
        "Gangyong Jia": {
            "authorId": "1803557",
            "name": "Gangyong Jia",
            "hIndex": 19
        },
        "Sean McCurdy": {
            "authorId": "2071651661",
            "name": "S. McCurdy",
            "hIndex": 5
        },
        "Guangwei Liu": {
            "authorId": "51008241",
            "name": "G. Liu",
            "hIndex": 10
        },
        "Chun Jason Xue": {
            "authorId": "2297316",
            "name": "C. Xue",
            "hIndex": 36
        },
        "Taiqiang Wu": {
            "authorId": "2137407647",
            "name": "Taiqiang Wu",
            "hIndex": 5
        },
        "Pashootan Vaezipoor": {
            "authorId": "1947192",
            "name": "Pashootan Vaezipoor",
            "hIndex": 7
        },
        "Ora Nova Fandina": {
            "authorId": "2119306400",
            "name": "Ora Nova Fandina",
            "hIndex": 2
        },
        "Mahsa Khosravi": {
            "authorId": "29456388",
            "name": "M. Khosravi",
            "hIndex": 3
        },
        "Bharatesh Chakravarthi": {
            "authorId": "2138533267",
            "name": "Bharatesh Chakravarthi",
            "hIndex": 3
        },
        "Thomas Corpetti": {
            "authorId": "2637442",
            "name": "T. Corpetti",
            "hIndex": 26
        },
        "Naghmeh Asadimanesh": {
            "authorId": "2164924623",
            "name": "Naghmeh Asadimanesh",
            "hIndex": 1
        },
        "Daniel Lee": {
            "authorId": "153619563",
            "name": "Daniel K. C. Lee",
            "hIndex": 20
        },
        "Xiang Chen": {
            "authorId": "2028468",
            "name": "Xiang 'Anthony' Chen",
            "hIndex": 22
        },
        "Changjie Fan": {
            "authorId": "3120655",
            "name": "Changjie Fan",
            "hIndex": 28
        },
        "John Samuel": {
            "authorId": "145850908",
            "name": "J. Samuel",
            "hIndex": 40
        },
        "Kahandakanaththage Maduni Pramuditha Perera": {
            "authorId": "2319610795",
            "name": "Kahandakanaththage Maduni Pramuditha Perera",
            "hIndex": 0
        },
        "Yalin Wang": {
            "authorId": "33973920",
            "name": "Yalin Wang",
            "hIndex": 38
        },
        "Yuming Tu": {
            "authorId": "153012628",
            "name": "Yuming Tu",
            "hIndex": 7
        },
        "Haibo Hu": {
            "authorId": "150122941",
            "name": "Ha-si Hu",
            "hIndex": 10
        },
        "Guandong Xu": {
            "authorId": "2149131224",
            "name": "Guandong Xu",
            "hIndex": 23
        },
        "Zengrui Jin": {
            "authorId": "2125082024",
            "name": "Zengrui Jin",
            "hIndex": 7
        },
        "Yifan Wei": {
            "authorId": "123187818",
            "name": "Yi-Ni Wei",
            "hIndex": 3
        },
        "Hidetaka Kamigaito": {
            "authorId": "2300756",
            "name": "Hidetaka Kamigaito",
            "hIndex": 12
        },
        "Chao Wang": {
            "authorId": "2051932445",
            "name": "Chao Wang",
            "hIndex": 10
        },
        "Fatih Ilhan": {
            "authorId": "34995928",
            "name": "F. Ilhan",
            "hIndex": 16
        },
        "Maeve Hutchinson": {
            "authorId": "48975946",
            "name": "Maeve E Hutchinson",
            "hIndex": 3
        },
        "Nasir Ghani": {
            "authorId": "145560250",
            "name": "N. Ghani",
            "hIndex": 28
        },
        "Hongliang He": {
            "authorId": "49372487",
            "name": "Hongliang He",
            "hIndex": 22
        },
        "Audun Myers": {
            "authorId": "95083823",
            "name": "Audun D. Myers",
            "hIndex": 5
        },
        "Yaping Ruan": {
            "authorId": "32210533",
            "name": "Ya-Ping Ruan",
            "hIndex": 5
        },
        "Milena Dobreva": {
            "authorId": "1705586",
            "name": "M. Dobreva",
            "hIndex": 11
        },
        "Jinyu Zhao": {
            "authorId": "46357741",
            "name": "Zhang Jinyu",
            "hIndex": 11
        },
        "Zilong Zheng": {
            "authorId": "34136888",
            "name": "Zilong Zheng",
            "hIndex": 29
        },
        "Kyle Lo": {
            "authorId": "46258841",
            "name": "Kyle Lo",
            "hIndex": 31
        },
        "Younghun Kwon": {
            "authorId": "2869546",
            "name": "Younghun Kwon",
            "hIndex": 12
        },
        "Ido Levy": {
            "authorId": "2067771498",
            "name": "Ido Levy",
            "hIndex": 4
        },
        "Xiang Xia": {
            "authorId": "4893825",
            "name": "X. Xia",
            "hIndex": 12
        },
        "Felix Biessmann": {
            "authorId": "2170760",
            "name": "F. Biessmann",
            "hIndex": 24
        },
        "Jayasankar K S": {
            "name": "Jayasankar K S",
            "hIndex": 0
        },
        "Qian Niu": {
            "authorId": "49355733",
            "name": "Qian Niu",
            "hIndex": 13
        },
        "Sai Gopinath": {
            "authorId": "40225327",
            "name": "M. Gopinath",
            "hIndex": 6
        },
        "Sepanta Zeighami": {
            "authorId": "3416086",
            "name": "Sepanta Zeighami",
            "hIndex": 7
        },
        "Wanli Ouyang": {
            "authorId": "3001348",
            "name": "Wanli Ouyang",
            "hIndex": 87
        },
        "Soumitri Chattopadhyay": {
            "authorId": "2146696888",
            "name": "Soumitri Chattopadhyay",
            "hIndex": 5
        },
        "Idris Abdulmumin": {
            "authorId": "1429833598",
            "name": "Idris Abdulmumin",
            "hIndex": 11
        },
        "Andreea-Maria Oncescu": {
            "authorId": "2028596941",
            "name": "Andreea-Maria Oncescu",
            "hIndex": 4
        },
        "Yeonseo Lee": {
            "authorId": "2119393455",
            "name": "Yeonseo Lee",
            "hIndex": 2
        },
        "Junyu Liu": {
            "authorId": "2104031",
            "name": "Junyu Liu",
            "hIndex": 22
        },
        "Lee Naish": {
            "authorId": "1686993",
            "name": "L. Naish",
            "hIndex": 25
        },
        "Mehwish Alam": {
            "authorId": "33973438",
            "name": "Mehwish Alam",
            "hIndex": 10
        },
        "Ryan Yank Wang": {
            "authorId": "2319423886",
            "name": "Ryan Yank Wang",
            "hIndex": 0
        },
        "Fengyuan Shi": {
            "authorId": "1719599",
            "name": "F. Shi",
            "hIndex": 22
        },
        "Krzysztof Jurkiewicz": {
            "authorId": "2184202042",
            "name": "Krzysztof Jurkiewicz",
            "hIndex": 1
        },
        "Zixu He": {
            "authorId": "13639323",
            "name": "Zixu He",
            "hIndex": 8
        },
        "Shifu Li": {
            "authorId": "2141819728",
            "name": "Shifu Li",
            "hIndex": 4
        },
        "Sham M. Kakade": {
            "authorId": "144695232",
            "name": "S. Kakade",
            "hIndex": 87
        },
        "Aviad Sela": {
            "authorId": "34751563",
            "name": "Aviad Sela",
            "hIndex": 5
        },
        "Shikhar Vashishth": {
            "authorId": "3404827",
            "name": "Shikhar Vashishth",
            "hIndex": 16
        },
        "Qingjiang Shi": {
            "authorId": "2113757445",
            "name": "Qingjiang Shi",
            "hIndex": 9
        },
        "George Karypis": {
            "authorId": "50877490",
            "name": "G. Karypis",
            "hIndex": 95
        },
        "Goirik Chakrabarty": {
            "authorId": "2214812658",
            "name": "Goirik Chakrabarty",
            "hIndex": 2
        },
        "Xiao Zhou": {
            "authorId": "2109126252",
            "name": "X. Zhou",
            "hIndex": 8
        },
        "Junyoung Park": {
            "authorId": "2109372500",
            "name": "Junyoung O. Park",
            "hIndex": 17
        },
        "Emad Shihab": {
            "authorId": "3318024",
            "name": "Emad Shihab",
            "hIndex": 43
        },
        "Jiapeng Yu": {
            "authorId": "47891327",
            "name": "Jiapeng Yu",
            "hIndex": 10
        },
        "Coleman Hooper": {
            "authorId": "2029486869",
            "name": "Coleman Hooper",
            "hIndex": 9
        },
        "Dahoon Park": {
            "authorId": "2152144537",
            "name": "Dahoon Park",
            "hIndex": 2
        },
        "Rodney Beard": {
            "authorId": "7325115",
            "name": "R. Beard",
            "hIndex": 8
        },
        "Michael Bloesch": {
            "authorId": "39793897",
            "name": "Michael Bloesch",
            "hIndex": 23
        },
        "Xiarun Chen": {
            "authorId": "2284518521",
            "name": "Xiarun Chen",
            "hIndex": 1
        },
        "Jing Li": {
            "authorId": "49299129",
            "name": "Jing Li",
            "hIndex": 9
        },
        "Nicolas Pr\u00f6llochs": {
            "authorId": "3254663",
            "name": "Nicolas Pr\u00f6llochs",
            "hIndex": 16
        },
        "Denis Gubin": {
            "authorId": "143787212",
            "name": "D. Gubin",
            "hIndex": 16
        },
        "Tian Qiu": {
            "authorId": "153082822",
            "name": "Qiutian Li",
            "hIndex": 13
        },
        "Ian P Barrett": {
            "authorId": "49744659",
            "name": "I. Barrett",
            "hIndex": 14
        },
        "Lei Bai": {
            "authorId": "50010487",
            "name": "Lei Bai",
            "hIndex": 23
        },
        "Xuezhi Fang": {
            "authorId": "49520233",
            "name": "X. Fang",
            "hIndex": 8
        },
        "Nina Hubig": {
            "authorId": "1999324",
            "name": "Nina C. Hubig",
            "hIndex": 4
        },
        "Xin-Yi Wang": {
            "authorId": "10830885",
            "name": "Yi-xin Wang",
            "hIndex": 29
        },
        "Marjan Celikik": {
            "authorId": "2940477",
            "name": "Marjan Celikik",
            "hIndex": 4
        },
        "Wei Chen": {
            "authorId": "145921144",
            "name": "Wei Chen",
            "hIndex": 62
        },
        "Kartik Kaushik": {
            "authorId": "20748210",
            "name": "Kartikeya Tiwari",
            "hIndex": 10
        },
        "Aliasgahr Khani": {
            "authorId": "2319600582",
            "name": "Aliasgahr Khani",
            "hIndex": 0
        },
        "Yezhaohui Wang": {
            "authorId": "2299332188",
            "name": "Yezhaohui Wang",
            "hIndex": 1
        },
        "Alexander Gelbukh": {
            "authorId": "1747784",
            "name": "Alexander Gelbukh",
            "hIndex": 49
        },
        "Yanyan Liu": {
            "name": "Yanyan Liu",
            "hIndex": 0
        },
        "Jaekwon Park": {
            "authorId": "2305043090",
            "name": "Jaekwon Park",
            "hIndex": 1
        },
        "Shouling Ji": {
            "name": "Shouling Ji",
            "hIndex": 0
        },
        "Nianyi Lin": {
            "authorId": "2210119346",
            "name": "Nianyi Lin",
            "hIndex": 2
        },
        "Malte Luttermann": {
            "authorId": "2203363794",
            "name": "Malte Luttermann",
            "hIndex": 2
        },
        "Samrat Mondal": {
            "name": "Samrat Mondal",
            "hIndex": 0
        },
        "Dan Tecuci": {
            "authorId": "1769641",
            "name": "Dan G. Tecuci",
            "hIndex": 10
        },
        "Keming Lu": {
            "authorId": "1515662094",
            "name": "K. Lu",
            "hIndex": 9
        },
        "Johanne R. Trippas": {
            "authorId": "2528063",
            "name": "Johanne R. Trippas",
            "hIndex": 15
        },
        "Zhaoting Li": {
            "authorId": "4040794",
            "name": "Zhaoting Li",
            "hIndex": 16
        },
        "Baohe Zhang": {
            "authorId": "24020091",
            "name": "Baohe Zhang",
            "hIndex": 8
        },
        "Lucy G. Cheke": {
            "authorId": "6643500",
            "name": "L. Cheke",
            "hIndex": 25
        },
        "Avraham Chapman": {
            "authorId": "2186405710",
            "name": "Avraham Chapman",
            "hIndex": 1
        },
        "Xiaoyu Zhang": {
            "authorId": "152290642",
            "name": "Zhang Xiaoyu",
            "hIndex": 13
        },
        "Iulian Emil Tampu": {
            "authorId": "151039496",
            "name": "I. Tampu",
            "hIndex": 2
        },
        "Joel Brogan": {
            "authorId": "6846673",
            "name": "Joel Brogan",
            "hIndex": 11
        },
        "Quanyu Zhu": {
            "authorId": "92810210",
            "name": "Quanyu Zhu",
            "hIndex": 3
        },
        "Kaustav Chakrabarti": {
            "name": "Kaustav Chakrabarti",
            "hIndex": 0
        },
        "Hang Pan": {
            "authorId": "4017532",
            "name": "Yu-hang Pan",
            "hIndex": 11
        },
        "Boris Knyazev": {
            "authorId": "36456529",
            "name": "B. Knyazev",
            "hIndex": 23
        },
        "Dan Ruta": {
            "authorId": "51123610",
            "name": "Dan Ruta",
            "hIndex": 4
        },
        "Giovanni Russello": {
            "authorId": "1741484",
            "name": "G. Russello",
            "hIndex": 23
        },
        "Hong Xie": {
            "authorId": "48327568",
            "name": "Hong Xie",
            "hIndex": 24
        },
        "Jian Wang": {
            "authorId": "49605162",
            "name": "Jian Wang",
            "hIndex": 22
        },
        "Eitan Farchi": {
            "authorId": "2338905",
            "name": "E. Farchi",
            "hIndex": 17
        },
        "Yanxi Chen": {
            "authorId": "2143409109",
            "name": "Yanxiao Chen",
            "hIndex": 8
        },
        "Gemma Roig": {
            "authorId": "144596911",
            "name": "G. Roig",
            "hIndex": 21
        },
        "Jun Zhong": {
            "authorId": "29501814",
            "name": "Zhongjun Ma",
            "hIndex": 24
        },
        "Jannik Peters": {
            "authorId": "2087516124",
            "name": "Jannik Peters",
            "hIndex": 5
        },
        "Desiree Heim": {
            "authorId": "2206537796",
            "name": "Desiree Heim",
            "hIndex": 1
        },
        "Ian Cannon": {
            "authorId": "95454293",
            "name": "I. Cannon",
            "hIndex": 3
        },
        "Mitchell DeHaven": {
            "authorId": "2046875291",
            "name": "Mitchell DeHaven",
            "hIndex": 4
        },
        "Soumik Dey": {
            "authorId": "91248413",
            "name": "Soumik Dey",
            "hIndex": 10
        },
        "Juergen Becker": {
            "authorId": "145685066",
            "name": "J. Becker",
            "hIndex": 22
        },
        "Luise Modersohn": {
            "authorId": "3434699",
            "name": "Luise Modersohn",
            "hIndex": 8
        },
        "Genet Asefa Gesese": {
            "authorId": "146797181",
            "name": "Genet Asefa Gesese",
            "hIndex": 6
        },
        "Xiaojie Xu": {
            "authorId": "31252803",
            "name": "Xu Xiaojie",
            "hIndex": 6
        },
        "Yan Song": {
            "authorId": "48481723",
            "name": "Yanyan Song",
            "hIndex": 39
        },
        "Chao Li": {
            "authorId": "7431269",
            "name": "Chaolin Li",
            "hIndex": 29
        },
        "Sadhana Kumaravel": {
            "authorId": "1666248581",
            "name": "Sadhana Kumaravel",
            "hIndex": 7
        },
        "Georgia Chalvatzaki": {
            "authorId": "1989757",
            "name": "G. Chalvatzaki",
            "hIndex": 18
        },
        "Jiachen Zheng": {
            "authorId": "6638129",
            "name": "Jiachen Zheng",
            "hIndex": 11
        },
        "Da Pan": {
            "authorId": "51227778",
            "name": "D. Pan",
            "hIndex": 14
        },
        "Justin Lovelace": {
            "authorId": "1398104959",
            "name": "Justin Lovelace",
            "hIndex": 5
        },
        "Ge Liu": {
            "authorId": "2054615493",
            "name": "L. Ge",
            "hIndex": 4
        },
        "Fazle Rahat": {
            "authorId": "68973389",
            "name": "F. Rahat",
            "hIndex": 2
        },
        "I. de Rodrigo": {
            "authorId": "84179817",
            "name": "R. Costa",
            "hIndex": 14
        },
        "Muxi Diao": {
            "name": "Muxi Diao",
            "hIndex": 0
        },
        "H. R. Tizhoosh": {
            "authorId": "9315255",
            "name": "H. Tizhoosh",
            "hIndex": 45
        },
        "Xuanhe Zhou": {
            "authorId": "50177381",
            "name": "Xuanhe Zhou",
            "hIndex": 15
        },
        "Murray Patterson": {
            "authorId": "33028569",
            "name": "M. Patterson",
            "hIndex": 19
        },
        "Kelsey Bradford": {
            "authorId": "2320151781",
            "name": "Kelsey Bradford",
            "hIndex": 0
        },
        "Thomas Martynec": {
            "authorId": "88640432",
            "name": "Thomas Martynec",
            "hIndex": 6
        },
        "Ghazal Alabtah": {
            "authorId": "2254283085",
            "name": "Ghazal Alabtah",
            "hIndex": 1
        },
        "Mahmoud Diab": {
            "authorId": "39212502",
            "name": "M. Diab",
            "hIndex": 12
        },
        "Xuegang Ban": {
            "authorId": "3415474",
            "name": "X. Ban",
            "hIndex": 36
        },
        "Deng Cai": {
            "authorId": "14332926",
            "name": "C. Deng",
            "hIndex": 21
        },
        "Kazi Hasan Ibn Arif": {
            "authorId": "2114865142",
            "name": "Kazi Hasan Ibn Arif",
            "hIndex": 0
        },
        "Jiangxia Cao": {
            "authorId": "2115871859",
            "name": "Jiangxia Cao",
            "hIndex": 9
        },
        "William Knottenbelt": {
            "authorId": "1786779",
            "name": "W. Knottenbelt",
            "hIndex": 37
        },
        "Jiangliu Wang": {
            "authorId": "46584859",
            "name": "Jiangliu Wang",
            "hIndex": 8
        },
        "Alexander Wettig": {
            "authorId": "2127066887",
            "name": "Alexander Wettig",
            "hIndex": 10
        },
        "Yuanjiang Cao": {
            "authorId": "2112825288",
            "name": "Yu Cao",
            "hIndex": 3
        },
        "Dhiyaan Nirmal": {
            "authorId": "2319419608",
            "name": "Dhiyaan Nirmal",
            "hIndex": 0
        },
        "Yunfeng Bai": {
            "authorId": "2593946",
            "name": "Yunfeng Bai",
            "hIndex": 16
        },
        "Tianyang Wang": {
            "name": "Tianyang Wang",
            "hIndex": 0
        },
        "Sajal K. Das": {
            "authorId": "2288899549",
            "name": "Sajal K. Das",
            "hIndex": 88
        },
        "Weiping Wen": {
            "authorId": "81691607",
            "name": "W. Wen",
            "hIndex": 23
        },
        "Luca Della Libera": {
            "authorId": "122566053",
            "name": "Luca Della Libera",
            "hIndex": 2
        },
        "Yang Zhang": {
            "authorId": "2145954018",
            "name": "Yang Zhang",
            "hIndex": 32
        },
        "Anastasia Ailamaki": {
            "authorId": "1728318",
            "name": "A. Ailamaki",
            "hIndex": 61
        },
        "Alastair F. Donaldson": {
            "authorId": "1734519",
            "name": "Alastair F. Donaldson",
            "hIndex": 30
        },
        "Xiaofu Li": {
            "authorId": "2108676650",
            "name": "Xiao-fang Li",
            "hIndex": 8
        },
        "Dongil Yang": {
            "authorId": "2152325970",
            "name": "Dongil Yang",
            "hIndex": 1
        },
        "Ben Hutchinson": {
            "authorId": "2044655623",
            "name": "Ben Hutchinson",
            "hIndex": 18
        },
        "Bryan Hooi": {
            "authorId": "2019961",
            "name": "Bryan Hooi",
            "hIndex": 38
        },
        "Dingshuo Chen": {
            "authorId": "2109123910",
            "name": "Dingshuo Chen",
            "hIndex": 5
        },
        "Catherine Letord": {
            "authorId": "3244363",
            "name": "C. Letord",
            "hIndex": 7
        },
        "Xiaoran Liu": {
            "authorId": "2108796226",
            "name": "Xiaoran Liu",
            "hIndex": 20
        },
        "Simon Halle": {
            "authorId": "1515718299",
            "name": "Simon Hall\u00e9",
            "hIndex": 1
        },
        "Qiongqiong Wang": {
            "authorId": "50621182",
            "name": "Qiongqiong Wang",
            "hIndex": 13
        },
        "Alessandro Serra": {
            "authorId": "145538400",
            "name": "A. Serra",
            "hIndex": 17
        },
        "Jinhong He": {
            "authorId": "2158100328",
            "name": "Jin He",
            "hIndex": 5
        },
        "John Z. H. Zhang": {
            "authorId": "50561959",
            "name": "J. H. Zhang",
            "hIndex": 62
        },
        "Bridget T. McInnes": {
            "authorId": "1967721",
            "name": "Bridget T. McInnes",
            "hIndex": 19
        },
        "Yuki Hirakawa": {
            "authorId": "2598322",
            "name": "Yuki Hirakawa",
            "hIndex": 17
        },
        "Liliya Imasheva": {
            "authorId": "102954787",
            "name": "L. Imasheva",
            "hIndex": 3
        },
        "Jun Yan": {
            "authorId": "144140297",
            "name": "Junhao Yan",
            "hIndex": 22
        },
        "Jianghao Lin": {
            "authorId": "2144908858",
            "name": "Jianghao Lin",
            "hIndex": 9
        },
        "Kaya Dorogi": {
            "authorId": "2319412857",
            "name": "Kaya Dorogi",
            "hIndex": 0
        },
        "Soham Ray": {
            "authorId": "13735944",
            "name": "S. Ray",
            "hIndex": 13
        },
        "Debashree Guha": {
            "authorId": "1739764",
            "name": "Debashree Guha",
            "hIndex": 16
        },
        "Yearim Kim": {
            "authorId": "2265651669",
            "name": "Yearim Kim",
            "hIndex": 1
        },
        "Michael R. Zhang": {
            "authorId": "50495487",
            "name": "Michael Ruogu Zhang",
            "hIndex": 29
        },
        "Shengchen Zhu": {
            "authorId": "1667017812",
            "name": "Shengchen Zhu",
            "hIndex": 2
        },
        "Xiuqi Zheng": {
            "authorId": "122896951",
            "name": "Xiuqiang Zheng",
            "hIndex": 2
        },
        "Shuai Zhang": {
            "authorId": "1566522742",
            "name": "Shuaipan Zhang",
            "hIndex": 8
        },
        "Chhavi Yadav": {
            "authorId": "83222216",
            "name": "Chhavi Yadav",
            "hIndex": 3
        },
        "Yan Huang": {
            "authorId": "46843905",
            "name": "Y. Huang",
            "hIndex": 40
        },
        "Garvit Gupta": {
            "authorId": "2273604452",
            "name": "Garvit Gupta",
            "hIndex": 3
        },
        "Juncheng Ma": {
            "authorId": "2152611658",
            "name": "Juncheng Ma",
            "hIndex": 5
        },
        "Qi Dou": {
            "authorId": "35647880",
            "name": "Q. Dou",
            "hIndex": 60
        },
        "Andreas Kerren": {
            "authorId": "2569160",
            "name": "A. Kerren",
            "hIndex": 27
        },
        "Leonid Kuligin": {
            "authorId": "2319403888",
            "name": "Leonid Kuligin",
            "hIndex": 0
        },
        "Lee Tarlin": {
            "authorId": "2105061693",
            "name": "L. Tarlin",
            "hIndex": 1
        },
        "Xiaojun Xiao": {
            "authorId": "92596024",
            "name": "Xiao-Bing Xiao",
            "hIndex": 7
        },
        "Aimin Zhou": {
            "authorId": "2150273525",
            "name": "Aimin Zhou",
            "hIndex": 21
        },
        "Ziqi Jin": {
            "authorId": "50327258",
            "name": "Ziqi Jin",
            "hIndex": 5
        },
        "Larisa Bakutova": {
            "authorId": "40365064",
            "name": "L. Bakutova",
            "hIndex": 7
        },
        "Jack Hessel": {
            "authorId": "2689239",
            "name": "Jack Hessel",
            "hIndex": 28
        },
        "Bang Liu": {
            "authorId": "145780928",
            "name": "Banggui Liu",
            "hIndex": 28
        },
        "Marvin Grimm": {
            "authorId": "120648004",
            "name": "M. Grimm",
            "hIndex": 13
        },
        "Xinran Zhang": {
            "authorId": "2108057070",
            "name": "Xinran Zhang",
            "hIndex": 16
        },
        "Victor De Marez": {
            "authorId": "2319400751",
            "name": "Victor De Marez",
            "hIndex": 0
        },
        "Zachary D. Sisco": {
            "authorId": "65772542",
            "name": "Zachary D. Sisco",
            "hIndex": 2
        },
        "Shiyan Zhang": {
            "authorId": "9309668",
            "name": "Shiyan Zhang",
            "hIndex": 10
        },
        "Takahiro Shinozaki": {
            "authorId": "1732454",
            "name": "T. Shinozaki",
            "hIndex": 21
        },
        "Kevin Lybarger": {
            "authorId": "31033341",
            "name": "K. Lybarger",
            "hIndex": 8
        },
        "Xin Dai": {
            "authorId": "94230932",
            "name": "X. Dai",
            "hIndex": 9
        },
        "Jaegul Choo": {
            "authorId": "1795455",
            "name": "J. Choo",
            "hIndex": 41
        },
        "Shangyu Wu": {
            "authorId": "3049681",
            "name": "Shang-Teh Wu",
            "hIndex": 13
        },
        "Atticus Geiger": {
            "authorId": "80833908",
            "name": "Atticus Geiger",
            "hIndex": 17
        },
        "Mengdi Zhang": {
            "authorId": "1726023941",
            "name": "Meng-di Zhang",
            "hIndex": 6
        },
        "Hao Mi": {
            "authorId": "39150475",
            "name": "Hao\u2010Yang Mi",
            "hIndex": 46
        },
        "Kelly W. Zhang": {
            "authorId": "2119059281",
            "name": "Kelly W. Zhang",
            "hIndex": 8
        },
        "Yingtong Dou": {
            "authorId": "8729899",
            "name": "Yingtong Dou",
            "hIndex": 15
        },
        "Yichen Di": {
            "authorId": "2257057173",
            "name": "Yichen Di",
            "hIndex": 0
        },
        "Xiaozhi Wang": {
            "authorId": "47120537",
            "name": "Xiaozhi Wang",
            "hIndex": 18
        },
        "Min Wu": {
            "authorId": "67286371",
            "name": "Min-Kai Wu",
            "hIndex": 50
        },
        "Dan Cai": {
            "authorId": "49710770",
            "name": "D. Cai",
            "hIndex": 12
        },
        "Jack Valmadre": {
            "name": "Jack Valmadre",
            "hIndex": 0
        },
        "Vasileios Tzoumas": {
            "authorId": "1886606",
            "name": "Vasileios Tzoumas",
            "hIndex": 19
        },
        "Andrew C. Myers": {
            "authorId": "144687080",
            "name": "A. Myers",
            "hIndex": 49
        },
        "Ian Leong": {
            "name": "Ian Leong",
            "hIndex": 0
        },
        "Tianhao Wang": {
            "authorId": "150325655",
            "name": "Tianhao Wang",
            "hIndex": 9
        },
        "Anye Zhou": {
            "authorId": "51124913",
            "name": "Anye Zhou",
            "hIndex": 10
        },
        "Aman Chadha": {
            "authorId": "40016108",
            "name": "Aman Chadha",
            "hIndex": 10
        },
        "Michail Zervas": {
            "authorId": "153928569",
            "name": "M. Zervas",
            "hIndex": 3
        },
        "Min Li": {
            "authorId": "2145626718",
            "name": "M. Li",
            "hIndex": 4
        },
        "Faramarz Fekri": {
            "authorId": "1730720",
            "name": "F. Fekri",
            "hIndex": 36
        },
        "Robert Fuder": {
            "authorId": "2224261856",
            "name": "Robert Fuder",
            "hIndex": 0
        },
        "William S. Moses": {
            "authorId": "16521420",
            "name": "William S. Moses",
            "hIndex": 10
        },
        "Carlos Medel-Ram\u00edrez": {
            "authorId": "113645778",
            "name": "Carlos Medel-Ram\u00edrez",
            "hIndex": 2
        },
        "Jianhui Yu": {
            "authorId": "113270064",
            "name": "Jianhui Yu",
            "hIndex": 14
        },
        "Xinfeng Liao": {
            "authorId": "2320188012",
            "name": "Xinfeng Liao",
            "hIndex": 0
        },
        "Pohsun Feng": {
            "authorId": "2319607771",
            "name": "Pohsun Feng",
            "hIndex": 0
        },
        "Carles Ventura": {
            "authorId": "2065494187",
            "name": "Carles Ventura",
            "hIndex": 4
        },
        "Marcus Mohr": {
            "authorId": "4260051",
            "name": "M. Mohr",
            "hIndex": 14
        },
        "Longzhi Yang": {
            "authorId": "1706028",
            "name": "Longzhi Yang",
            "hIndex": 28
        },
        "Chen Wang": {
            "authorId": "2136710296",
            "name": "Chenchen Wang",
            "hIndex": 17
        },
        "Arian Abbasi": {
            "authorId": "48123038",
            "name": "Asad Karim",
            "hIndex": 14
        },
        "Xiangtian Li": {
            "authorId": "152496559",
            "name": "X. Xiao",
            "hIndex": 24
        },
        "Sikai Chen": {
            "authorId": "51143271",
            "name": "Sikai Chen",
            "hIndex": 18
        },
        "Tanvi Ranade": {
            "name": "Tanvi Ranade",
            "hIndex": 0
        },
        "Fabian Wolz": {
            "authorId": "2319608098",
            "name": "Fabian Wolz",
            "hIndex": 0
        },
        "Fu Lee Wang": {
            "authorId": "52140967",
            "name": "F. Wang",
            "hIndex": 17
        },
        "Han Diao": {
            "authorId": "2047869444",
            "name": "Jinghan Diao",
            "hIndex": 4
        },
        "Yangyang Li": {
            "authorId": "11993695",
            "name": "Liu Yangyang",
            "hIndex": 10
        },
        "Amir Khasahmadi": {
            "authorId": "2005585581",
            "name": "A. Khasahmadi",
            "hIndex": 4
        },
        "Michael G. H. Bell": {
            "authorId": "34139480",
            "name": "M. Bell",
            "hIndex": 55
        },
        "Kairui Liu": {
            "authorId": "49600017",
            "name": "Kairui Liu",
            "hIndex": 12
        },
        "Peifang Xu": {
            "authorId": "14385922",
            "name": "Peifang Xu",
            "hIndex": 11
        },
        "Thomas Bonald": {
            "authorId": "1774132",
            "name": "T. Bonald",
            "hIndex": 35
        },
        "Prathamesh Dinesh Joshi": {
            "name": "Prathamesh Dinesh Joshi",
            "hIndex": 0
        },
        "Resit Sendag": {
            "authorId": "2190784",
            "name": "Resit Sendag",
            "hIndex": 10
        },
        "Damon L. Woodard": {
            "authorId": "2171076",
            "name": "D. Woodard",
            "hIndex": 32
        },
        "Ming-Yu Liu": {
            "authorId": "2108104088",
            "name": "Yu-Ming Liu",
            "hIndex": 14
        },
        "Liane Galanti": {
            "authorId": "2158815727",
            "name": "Liane Galanti",
            "hIndex": 3
        },
        "Thammathip Piumsomboon": {
            "authorId": "2297177",
            "name": "Thammathip Piumsomboon",
            "hIndex": 22
        },
        "Adnan Alshehri": {
            "authorId": "2320151661",
            "name": "Adnan Alshehri",
            "hIndex": 0
        },
        "Siyuan Huang": {
            "authorId": "47156966",
            "name": "Si-yuan Huang",
            "hIndex": 6
        },
        "Yexuan Shi": {
            "authorId": "114438759",
            "name": "Yexuan Shi",
            "hIndex": 9
        },
        "Wei Luo": {
            "authorId": "6217611",
            "name": "Wei-Wei Luo",
            "hIndex": 16
        },
        "Heyang Xu": {
            "authorId": "46485196",
            "name": "He-yang Xu",
            "hIndex": 12
        },
        "Yanyu Huang": {
            "authorId": "7524507",
            "name": "Yanyu Huang",
            "hIndex": 10
        },
        "Hung-Shin Lee": {
            "authorId": "2765566",
            "name": "Hung-Shin Lee",
            "hIndex": 9
        },
        "Yang Cheng": {
            "authorId": "2252110",
            "name": "Cheng-Fu Yang",
            "hIndex": 24
        },
        "Andrija Djurisic": {
            "authorId": "2185501454",
            "name": "Andrija Djurisic",
            "hIndex": 1
        },
        "Han Zheng": {
            "authorId": "144227469",
            "name": "Z. Han",
            "hIndex": 29
        },
        "Moozhan Maleki": {
            "authorId": "2319612005",
            "name": "Moozhan Maleki",
            "hIndex": 0
        },
        "Yu-Hsiang Wang": {
            "authorId": "2659283",
            "name": "Hsiang-Yu Wang",
            "hIndex": 19
        },
        "Ke Chang": {
            "authorId": "145433485",
            "name": "Ke-Vin Chang",
            "hIndex": 37
        },
        "Michael Banf": {
            "authorId": "2772660",
            "name": "M. Banf",
            "hIndex": 10
        },
        "Jiaqi Liu": {
            "authorId": "49722299",
            "name": "Jia-qi Liu",
            "hIndex": 42
        },
        "Bugra Onal": {
            "authorId": "2003036796",
            "name": "Bugra Onal",
            "hIndex": 1
        },
        "Amir Hossein Raffiee": {
            "authorId": "32150102",
            "name": "Amir Hossein Raffiee",
            "hIndex": 10
        },
        "Dhruv Alamuri": {
            "authorId": "2319417874",
            "name": "Dhruv Alamuri",
            "hIndex": 0
        },
        "Tangina Sultana": {
            "authorId": "50299803",
            "name": "Tangina Sultana",
            "hIndex": 6
        },
        "Alessandro Ronca": {
            "authorId": "3253281",
            "name": "Alessandro Ronca",
            "hIndex": 7
        },
        "Zhiyang Xu": {
            "authorId": "48559808",
            "name": "Zhiyan Xu",
            "hIndex": 7
        },
        "Yuexin Ma": {
            "authorId": "1997242309",
            "name": "Yue Ma",
            "hIndex": 22
        },
        "Jan Peters": {
            "name": "Jan Peters",
            "hIndex": 0
        },
        "Herprit Mahal": {
            "authorId": "2319412676",
            "name": "Herprit Mahal",
            "hIndex": 0
        },
        "Daniel Holden": {
            "authorId": "145273745",
            "name": "D. Holden",
            "hIndex": 19
        },
        "Qingqing Ye": {
            "authorId": "50540226",
            "name": "Qing Ye",
            "hIndex": 14
        },
        "Rinchin Iakovlev": {
            "authorId": "2320152261",
            "name": "Rinchin Iakovlev",
            "hIndex": 0
        },
        "Yizhen Zheng": {
            "authorId": "26956796",
            "name": "Yizhen Zheng",
            "hIndex": 12
        },
        "Enrico Motta": {
            "authorId": "1721851",
            "name": "E. Motta",
            "hIndex": 64
        },
        "Tianlong Chen": {
            "authorId": "46618483",
            "name": "Tianlong Chen",
            "hIndex": 5
        },
        "Juri Ezzaini": {
            "authorId": "2319820116",
            "name": "Juri Ezzaini",
            "hIndex": 0
        },
        "Bo Xu": {
            "authorId": "8928554",
            "name": "Bo-Qing Xu",
            "hIndex": 57
        },
        "Jean-Baptiste Lamy": {
            "authorId": "37241519",
            "name": "J. Lamy",
            "hIndex": 29
        },
        "Mark Adams": {
            "authorId": "11040332",
            "name": "M. Adams",
            "hIndex": 12
        },
        "Li Liu": {
            "authorId": "3059957",
            "name": "Liu Li",
            "hIndex": 8
        },
        "Maheep Chaudhary": {
            "authorId": "2310329680",
            "name": "Maheep Chaudhary",
            "hIndex": 1
        },
        "Hechang Chen": {
            "authorId": "2721051",
            "name": "Hechang Chen",
            "hIndex": 13
        },
        "Jinyoung Yeo": {
            "authorId": "1898428",
            "name": "Jinyoung Yeo",
            "hIndex": 9
        },
        "Dongyoung Jeong": {
            "authorId": "2056897318",
            "name": "D. Jeong",
            "hIndex": 2
        },
        "Yuanjing Liu": {
            "authorId": "2143862091",
            "name": "Yuan Liu",
            "hIndex": 5
        },
        "Hanqun Cao": {
            "authorId": "2238916715",
            "name": "Hanqun Cao",
            "hIndex": 3
        },
        "Zhe Lin": {
            "authorId": "3608968",
            "name": "Zhong\u2010Zhe Lin",
            "hIndex": 26
        },
        "Yaozong Gan": {
            "authorId": "2142824223",
            "name": "Yaozong Gan",
            "hIndex": 1
        },
        "Yizhi Song": {
            "authorId": "1705408",
            "name": "Yi-Zhe Song",
            "hIndex": 42
        },
        "Mathias L\u00f6we": {
            "authorId": "31530556",
            "name": "Mathias L\u00f6we",
            "hIndex": 4
        },
        "Nalin Asanka Gamagedara Arachchilage": {
            "authorId": "2991616",
            "name": "N. Arachchilage",
            "hIndex": 19
        },
        "Chengxi Pan": {
            "authorId": "2084641990",
            "name": "Cheng Pan",
            "hIndex": 3
        },
        "Faegheh Hasibi": {
            "authorId": "1951737",
            "name": "Faegheh Hasibi",
            "hIndex": 14
        },
        "Yair Stolero": {
            "authorId": "2223592696",
            "name": "Yair Stolero",
            "hIndex": 1
        },
        "Nada Boudegzdame": {
            "authorId": "2210748952",
            "name": "Nada Boudegzdame",
            "hIndex": 1
        },
        "Mohammadreza Fani Sani": {
            "authorId": "18151603",
            "name": "M. Sani",
            "hIndex": 12
        },
        "Martina G. Vilas": {
            "authorId": "51249476",
            "name": "M. Vilas",
            "hIndex": 9
        },
        "Ayla Rigouts Terryn": {
            "authorId": "41123782",
            "name": "Ayla Rigouts Terryn",
            "hIndex": 5
        },
        "G\u00e9raud Le Falher": {
            "authorId": "3256331",
            "name": "G. L. Falher",
            "hIndex": 5
        },
        "Xiangke Zeng": {
            "authorId": "2152781361",
            "name": "Xianghui Zeng",
            "hIndex": 8
        },
        "Keith Ross": {
            "authorId": "1829862",
            "name": "K. Ross",
            "hIndex": 68
        },
        "Weilin Zhao": {
            "authorId": "2452732",
            "name": "Weiling Zhao",
            "hIndex": 36
        },
        "Juntong Fan": {
            "authorId": "2108736739",
            "name": "J. Fan",
            "hIndex": 1
        },
        "Roy Abitbol": {
            "authorId": "2121354399",
            "name": "Roy Abitbol",
            "hIndex": 1
        },
        "Georgios Ioannides": {
            "authorId": "1581557148",
            "name": "Georgios Ioannides",
            "hIndex": 3
        },
        "Haohui Lu": {
            "authorId": "2149891854",
            "name": "Haohui Lu",
            "hIndex": 8
        },
        "Yong Yu": {
            "authorId": "19247787",
            "name": "Yong-liang Yu",
            "hIndex": 20
        },
        "Hannah An": {
            "authorId": "21410010",
            "name": "S. Hannah",
            "hIndex": 6
        },
        "Sen Zhang": {
            "authorId": "2107968157",
            "name": "Sen Zhang",
            "hIndex": 59
        },
        "Jin Song": {
            "authorId": "145330783",
            "name": "Jin H. Song",
            "hIndex": 25
        },
        "Lu Liu": {
            "authorId": "2118466955",
            "name": "Lulu Liu",
            "hIndex": 8
        },
        "Alessandro Antonucci": {
            "authorId": "34662081",
            "name": "Alessandro Antonucci",
            "hIndex": 18
        },
        "Kun Sun": {
            "authorId": "144124747",
            "name": "Kun Sun",
            "hIndex": 29
        },
        "Min Qin": {
            "authorId": "29287564",
            "name": "M. Qin",
            "hIndex": 18
        },
        "Kangjin Wang": {
            "authorId": "1435353869",
            "name": "Kangjin Wang",
            "hIndex": 2
        },
        "Ni Wayan Switrayni": {
            "authorId": "2091035095",
            "name": "N. W. Switrayni",
            "hIndex": 7
        },
        "Xin Peng": {
            "authorId": "152469179",
            "name": "X. Peng",
            "hIndex": 33
        },
        "Raju Halder": {
            "name": "Raju Halder",
            "hIndex": 0
        },
        "Keisuke Maeda": {
            "authorId": "2914117",
            "name": "K. Maeda",
            "hIndex": 30
        },
        "Florian Sikora": {
            "authorId": "1784109",
            "name": "F. Sikora",
            "hIndex": 15
        },
        "Mischa de Ridder": {
            "authorId": "37786186",
            "name": "M. de Ridder",
            "hIndex": 8
        },
        "Qianchi Zhang": {
            "authorId": "2143904373",
            "name": "Qian-cai Zhang",
            "hIndex": 1
        },
        "Eric Anderson": {
            "authorId": "50601431",
            "name": "E. Anderson",
            "hIndex": 11
        },
        "Yuan Yang": {
            "authorId": "1892968",
            "name": "Yuan-Sen Yang",
            "hIndex": 12
        },
        "Hongyu Zhu": {
            "authorId": "51393863",
            "name": "Hongyu Zhu",
            "hIndex": 14
        },
        "Zheng Li": {
            "authorId": "2146247550",
            "name": "Zheng Li",
            "hIndex": 21
        },
        "Lorenzo Cavallaro": {
            "authorId": "2189170",
            "name": "L. Cavallaro",
            "hIndex": 32
        },
        "Fulong Ma": {
            "name": "Fulong Ma",
            "hIndex": 0
        },
        "Chongming Gao": {
            "authorId": "31446099",
            "name": "Chongming Gao",
            "hIndex": 12
        },
        "Yuqian Wu": {
            "authorId": "2115529171",
            "name": "Yuqian Wu",
            "hIndex": 5
        },
        "Fan Yang": {
            "authorId": "5132728",
            "name": "Yangfan Yang",
            "hIndex": 16
        },
        "Rong Xie": {
            "authorId": "2143721468",
            "name": "Rong\u2010Jun Xie",
            "hIndex": 12
        },
        "Rajkumar Buyya": {
            "authorId": "1709598",
            "name": "R. Buyya",
            "hIndex": 138
        },
        "Tianyu Du": {
            "authorId": "2056897899",
            "name": "Tianyu Du",
            "hIndex": 6
        },
        "Liat Peterfreund": {
            "authorId": "3139922",
            "name": "L. Peterfreund",
            "hIndex": 10
        },
        "Nidhi Kowtal": {
            "authorId": "2273469257",
            "name": "Nidhi Kowtal",
            "hIndex": 1
        },
        "Andrea Borgarelli": {
            "authorId": "2007389365",
            "name": "Andrea Borgarelli",
            "hIndex": 2
        },
        "Partha Pratim Das": {
            "name": "Partha Pratim Das",
            "hIndex": 0
        },
        "Xu Han": {
            "authorId": "51070201",
            "name": "Xu Han",
            "hIndex": 46
        },
        "Ruoyu Wang": {
            "authorId": "48397132",
            "name": "Ruoyu Wang",
            "hIndex": 30
        },
        "Jie Su": {
            "authorId": "2087680178",
            "name": "J. Su",
            "hIndex": 12
        },
        "Xu Shen": {
            "authorId": "50457500",
            "name": "Xuan Shen",
            "hIndex": 16
        },
        "Rupak Majumdar": {
            "authorId": "144029582",
            "name": "R. Majumdar",
            "hIndex": 59
        },
        "Xiaodong Feng": {
            "authorId": "150106250",
            "name": "F. Xiaodong",
            "hIndex": 4
        },
        "Xuefeng Li": {
            "authorId": "9272116",
            "name": "Liu Xuefeng",
            "hIndex": 12
        },
        "Daniela Schuster": {
            "authorId": "143772531",
            "name": "D. Schuster",
            "hIndex": 47
        },
        "Claire Cardie": {
            "authorId": "1748501",
            "name": "Claire Cardie",
            "hIndex": 75
        },
        "Saeid Asgari Taghanaki": {
            "authorId": "17803311",
            "name": "Saeid Asgari Taghanaki",
            "hIndex": 12
        },
        "Takashi Wada": {
            "authorId": "2066975121",
            "name": "T. Wada",
            "hIndex": 8
        },
        "Jacopo Banfi": {
            "authorId": "2283746",
            "name": "Jacopo Banfi",
            "hIndex": 13
        },
        "Ruiping Wang": {
            "authorId": "2208753934",
            "name": "Ruiping Wang",
            "hIndex": 16
        },
        "Javier San Agustin": {
            "authorId": "2020625",
            "name": "Javier San Agustin",
            "hIndex": 16
        },
        "Xinqi Wang": {
            "authorId": "2108125060",
            "name": "Xinqing Wang",
            "hIndex": 8
        },
        "Michael Stonebraker": {
            "authorId": "145345023",
            "name": "M. Stonebraker",
            "hIndex": 96
        },
        "Phuong N. H. Pham": {
            "authorId": "144016941",
            "name": "P. H. Pham",
            "hIndex": 2
        },
        "Weipeng Chen": {
            "authorId": "2109593611",
            "name": "Weipeng Chen",
            "hIndex": 10
        },
        "Runming Yang": {
            "authorId": "2206433621",
            "name": "Run Yang",
            "hIndex": 2
        },
        "Christian Heumann": {
            "authorId": "2316681",
            "name": "C. Heumann",
            "hIndex": 23
        },
        "Sari Sadiya": {
            "authorId": "1411038811",
            "name": "S. Saba-Sadiya",
            "hIndex": 9
        },
        "Mattis Hartwig": {
            "authorId": "40112877",
            "name": "Mattis Hartwig",
            "hIndex": 3
        },
        "Hongpeng Cao": {
            "authorId": "1774928",
            "name": "H. Cao",
            "hIndex": 30
        },
        "Takanori Fujiwara": {
            "authorId": "2392540",
            "name": "T. Fujiwara",
            "hIndex": 14
        },
        "Long Lin": {
            "authorId": "92000115",
            "name": "Long-su Lin",
            "hIndex": 11
        },
        "Takahiro Ogawa": {
            "authorId": "2113645842",
            "name": "T. Ogawa",
            "hIndex": 16
        },
        "Martin Boeker": {
            "authorId": "1772992",
            "name": "M. Boeker",
            "hIndex": 29
        },
        "Keqiu Li": {
            "authorId": "143668329",
            "name": "Keqiu Li",
            "hIndex": 40
        },
        "Chenxi Qiu": {
            "authorId": "2546746",
            "name": "Chenxi Qiu",
            "hIndex": 16
        },
        "Darui Lu": {
            "authorId": "2280378416",
            "name": "Darui Lu",
            "hIndex": 1
        },
        "Bobby Azad": {
            "authorId": "144537960",
            "name": "Babak Azad",
            "hIndex": 9
        },
        "Sunjun Kim": {
            "authorId": "2072144",
            "name": "Sunjun Kim",
            "hIndex": 14
        },
        "Amy Xiao": {
            "authorId": "145411398",
            "name": "Xiao Hu",
            "hIndex": 17
        },
        "Jiaxuan You": {
            "authorId": "145829303",
            "name": "Jiaxuan You",
            "hIndex": 19
        },
        "Paul Groth": {
            "authorId": "1727784",
            "name": "Paul T. Groth",
            "hIndex": 36
        },
        "Gaojie Lin": {
            "authorId": "2004689620",
            "name": "Gaojie Lin",
            "hIndex": 3
        },
        "Huihong Shi": {
            "authorId": "2112524483",
            "name": "Hui-juan Shi",
            "hIndex": 7
        },
        "Tim Lawson": {
            "authorId": "11669300",
            "name": "T. Lawson",
            "hIndex": 20
        },
        "Manthan Chelenahalli Satish": {
            "authorId": "2319387664",
            "name": "Manthan Chelenahalli Satish",
            "hIndex": 0
        },
        "Baoxiong Jia": {
            "name": "Baoxiong Jia",
            "hIndex": 0
        },
        "Jiayu Lin": {
            "authorId": "4478825",
            "name": "Jiayuh Lin",
            "hIndex": 58
        },
        "Weiran Yao": {
            "authorId": "19671609",
            "name": "Weiran Yao",
            "hIndex": 13
        },
        "Zhiyuan Li": {
            "authorId": "5309808",
            "name": "L. Zhiyuan",
            "hIndex": 8
        },
        "Mingxiu Sui": {
            "authorId": "2171801875",
            "name": "Mingxiu Sui",
            "hIndex": 3
        },
        "Jiaxin Yu": {
            "authorId": "2109437136",
            "name": "Jiaxin Yu",
            "hIndex": 7
        },
        "Winston K. G. Seah": {
            "authorId": "2054506",
            "name": "Winston K.G. Seah",
            "hIndex": 37
        },
        "Ming Shan Hee": {
            "authorId": "35154708",
            "name": "Simon S. Woo",
            "hIndex": 33
        },
        "Muchammad Daniyal Kautsar": {
            "authorId": "2282778553",
            "name": "Muchammad Daniyal Kautsar",
            "hIndex": 1
        },
        "Bob Zhang": {
            "authorId": "11809511",
            "name": "Bobo Zhang",
            "hIndex": 11
        },
        "Kuang Yu": {
            "authorId": "33225111",
            "name": "Yudi Kuang",
            "hIndex": 48
        },
        "Ke Lin": {
            "authorId": "2148833334",
            "name": "Ke Lin",
            "hIndex": 24
        },
        "Yingcong Chen": {
            "authorId": "2109289860",
            "name": "Ying-Cong Chen",
            "hIndex": 14
        },
        "Kaiwen Zheng": {
            "authorId": "2052687835",
            "name": "K. Zheng",
            "hIndex": 10
        },
        "Daisaku Yokoyama": {
            "authorId": "3271117",
            "name": "Daisaku Yokoyama",
            "hIndex": 7
        },
        "Tejas Deshpande": {
            "authorId": "2023398836",
            "name": "T. Deshpande",
            "hIndex": 6
        },
        "Rui Huang": {
            "authorId": "48241528",
            "name": "Rui Huang",
            "hIndex": 60
        },
        "Zhengzhuo Xu": {
            "authorId": "1390847590",
            "name": "Zhengzhuo Xu",
            "hIndex": 9
        },
        "Sushant Gautam": {
            "authorId": "72173969",
            "name": "Sushant Gautam",
            "hIndex": 4
        },
        "Sascha Marton": {
            "authorId": "2035508911",
            "name": "Sascha Marton",
            "hIndex": 2
        },
        "Mukhammadsaid Mamasaidov": {
            "authorId": "2051460909",
            "name": "Mukhammadsaid Mamasaidov",
            "hIndex": 1
        },
        "Ralf M\u00f6ller": {
            "authorId": "152893505",
            "name": "R. M\u00f6ller",
            "hIndex": 25
        },
        "Chaoren Wei": {
            "authorId": "2281911124",
            "name": "Chaoren Wei",
            "hIndex": 1
        },
        "\u00c7a\u011fatay Demiralp": {
            "authorId": "1786137",
            "name": "\u00c7a\u011fatay Demiralp",
            "hIndex": 23
        },
        "Hinal Jajal": {
            "authorId": "2319411754",
            "name": "Hinal Jajal",
            "hIndex": 0
        },
        "Yutong Gou": {
            "authorId": "2319610672",
            "name": "Yutong Gou",
            "hIndex": 0
        },
        "Di Shang": {
            "authorId": "38581830",
            "name": "Baodi Shang",
            "hIndex": 6
        },
        "Xu Huang": {
            "authorId": "13075450",
            "name": "Xu-xu Huang",
            "hIndex": 7
        },
        "Yuanchun Wang": {
            "authorId": "2146016979",
            "name": "Yuanchun Wang",
            "hIndex": 5
        },
        "Geoffrey I. Webb": {
            "authorId": "1726660",
            "name": "Geoffrey I. Webb",
            "hIndex": 76
        },
        "Karima Sedki": {
            "authorId": "1749336",
            "name": "Karima Sedki",
            "hIndex": 12
        },
        "Feng Yao": {
            "authorId": "2691583",
            "name": "F. Yao",
            "hIndex": 10
        },
        "Laura Davies": {
            "authorId": "153431203",
            "name": "L. Davies",
            "hIndex": 10
        },
        "Abror Shopulatov": {
            "authorId": "2320150328",
            "name": "Abror Shopulatov",
            "hIndex": 0
        },
        "Yihao Chen": {
            "authorId": "2116613438",
            "name": "Yihao Chen",
            "hIndex": 20
        },
        "Tan D. Tran": {
            "authorId": "37584034",
            "name": "T. Tan",
            "hIndex": 13
        },
        "Gonzalo Bohorquez": {
            "authorId": "2319412818",
            "name": "Gonzalo Bohorquez",
            "hIndex": 0
        },
        "George Church": {
            "name": "George Church",
            "hIndex": 0
        },
        "Michael Schlichtkrull": {
            "authorId": "8804828",
            "name": "M. Schlichtkrull",
            "hIndex": 14
        },
        "Siqi Shen": {
            "authorId": "2959135",
            "name": "S. Shen",
            "hIndex": 14
        },
        "Andre L. L. Aquino": {
            "authorId": "1754971",
            "name": "Andre L. L. Aquino",
            "hIndex": 17
        },
        "Mythra V. Balakuntala": {
            "authorId": "144415258",
            "name": "Mythra V. Balakuntala",
            "hIndex": 6
        },
        "Alia Shamikh": {
            "authorId": "9969524",
            "name": "A. Shamikh",
            "hIndex": 8
        },
        "Joshua A. Rackers": {
            "authorId": "3216470",
            "name": "Joshua A. Rackers",
            "hIndex": 9
        },
        "Michael Haman": {
            "authorId": "1483740597",
            "name": "M. Haman",
            "hIndex": 6
        },
        "Nell Barber": {
            "authorId": "2186185649",
            "name": "Nell Barber",
            "hIndex": 2
        },
        "Hao Wu": {
            "authorId": "47987341",
            "name": "Hao-liang Wu",
            "hIndex": 22
        },
        "Zedong Xing": {
            "authorId": "2319418761",
            "name": "Zedong Xing",
            "hIndex": 0
        },
        "Yuxuan Wang": {
            "authorId": "2115828848",
            "name": "Yuxuan Wang",
            "hIndex": 41
        },
        "Khai Hao Moo": {
            "authorId": "2284863107",
            "name": "Moo Khai Hao",
            "hIndex": 1
        },
        "Kowshik Thopalli": {
            "authorId": "51149615",
            "name": "Kowshik Thopalli",
            "hIndex": 5
        },
        "Manli Li": {
            "authorId": "67135845",
            "name": "Liu Manli",
            "hIndex": 5
        },
        "Aaron Courville": {
            "authorId": "1760871",
            "name": "Aaron C. Courville",
            "hIndex": 88
        },
        "Siddharth Jha": {
            "authorId": "11581571",
            "name": "S. Jha",
            "hIndex": 5
        },
        "Ben L. Titzer": {
            "authorId": "2982459",
            "name": "Ben L. Titzer",
            "hIndex": 12
        },
        "Hassan El Alami": {
            "name": "Hassan El Alami",
            "hIndex": 0
        },
        "Zhengyan Zhang": {
            "authorId": "2148904307",
            "name": "Zhengyang Zhang",
            "hIndex": 8
        },
        "Jamila Smith-Loud": {
            "authorId": "1452730703",
            "name": "Jamila Smith-Loud",
            "hIndex": 4
        },
        "Neeladri Bhuiya": {
            "name": "Neeladri Bhuiya",
            "hIndex": 0
        },
        "Mathilde Papillon": {
            "authorId": "2179109497",
            "name": "Mathilde Papillon",
            "hIndex": 3
        },
        "Jinhua Zhao": {
            "authorId": "65983550",
            "name": "Jinhuan Zhao",
            "hIndex": 45
        },
        "Hanzhi Chen": {
            "authorId": "2118024258",
            "name": "Hanzhi Chen",
            "hIndex": 5
        },
        "Xiangjun Gao": {
            "authorId": "11732267",
            "name": "Xiangjun Gao",
            "hIndex": 14
        },
        "Victoria Sara Weso\u0142owska": {
            "name": "Victoria Sara Weso\u0142owska",
            "hIndex": 0
        },
        "Ibrahim Said Ahmad": {
            "authorId": "9382925",
            "name": "I. Said",
            "hIndex": 14
        },
        "Zuchao Li": {
            "authorId": "30658665",
            "name": "Z. Li",
            "hIndex": 20
        },
        "Alok Singh": {
            "authorId": "50286013",
            "name": "Alok K. Singh",
            "hIndex": 19
        },
        "Jack William Miller": {
            "authorId": "2229023715",
            "name": "Jack William Miller",
            "hIndex": 2
        },
        "Xin Wang": {
            "authorId": "2108022135",
            "name": "Xin-xin Wang",
            "hIndex": 14
        },
        "Wilco Dijkstra": {
            "authorId": "145778872",
            "name": "J. Dijkstra",
            "hIndex": 18
        },
        "Guochao Jiang": {
            "authorId": "2048702034",
            "name": "Guochao Jiang",
            "hIndex": 3
        },
        "Samer Francy": {
            "name": "Samer Francy",
            "hIndex": 0
        },
        "David S. Yu": {
            "authorId": "39820222",
            "name": "David S. Yu",
            "hIndex": 24
        },
        "Meeyoung Cha": {
            "authorId": "1775511",
            "name": "M. Cha",
            "hIndex": 43
        },
        "Chun Yuan": {
            "authorId": "145946240",
            "name": "Changzhou Yuan",
            "hIndex": 24
        },
        "Kurt Keutzer": {
            "authorId": "1732330",
            "name": "K. Keutzer",
            "hIndex": 91
        },
        "Jiayan Lin": {
            "authorId": "120803579",
            "name": "Jia-Ling Lin",
            "hIndex": 5
        },
        "Kiet Van Nguyen": {
            "name": "Kiet Van Nguyen",
            "hIndex": 0
        },
        "Narges Gazmeh": {
            "authorId": "2319610282",
            "name": "Narges Gazmeh",
            "hIndex": 0
        },
        "Xing Zi": {
            "authorId": "40211659",
            "name": "Zipeng Xing",
            "hIndex": 51
        },
        "Abdullah Arafat Miah": {
            "authorId": "119735022",
            "name": "Abdullah Arafat Miah",
            "hIndex": 1
        },
        "Ana Milanova": {
            "authorId": "2572691",
            "name": "Ana L. Milanova",
            "hIndex": 21
        },
        "Genan Dai": {
            "authorId": "9694932",
            "name": "Genan Dai",
            "hIndex": 4
        },
        "Rakib Hossen": {
            "authorId": "1520016820",
            "name": "Rakib Hossen",
            "hIndex": 4
        },
        "Sanyukta Adap": {
            "authorId": "2319604750",
            "name": "Sanyukta Adap",
            "hIndex": 0
        },
        "Hyunsoo Kim": {
            "authorId": "13385234",
            "name": "Hyun-Surk Kim",
            "hIndex": 9
        },
        "Kaihui Chen": {
            "authorId": "2218630796",
            "name": "Kai-Hui Chen",
            "hIndex": 11
        },
        "Yanlin Wu": {
            "authorId": "1390683855",
            "name": "Yanlin Wu",
            "hIndex": 11
        },
        "Chen Zhang": {
            "authorId": "15281425",
            "name": "Zhang-he Chen",
            "hIndex": 19
        },
        "Mounir Ghogho": {
            "authorId": "1696513",
            "name": "M. Ghogho",
            "hIndex": 38
        },
        "Sergen Cansiz": {
            "authorId": "74413792",
            "name": "S. Cansiz",
            "hIndex": 2
        },
        "Jonathan Zhang": {
            "authorId": "1932145881",
            "name": "Zhiwen Zhang",
            "hIndex": 17
        },
        "Jingyu Zhang": {
            "authorId": "2108123027",
            "name": "Jingyu Zhang",
            "hIndex": 9
        },
        "Jinman Zhao": {
            "authorId": "26128283",
            "name": "Jinman Zhao",
            "hIndex": 7
        },
        "Poppy Collis": {
            "authorId": "2265579887",
            "name": "Poppy Collis",
            "hIndex": 0
        },
        "Oline Ranum": {
            "authorId": "2302560889",
            "name": "Oline Ranum",
            "hIndex": 1
        },
        "Alban Puech": {
            "authorId": "2311114043",
            "name": "Alban Puech",
            "hIndex": 1
        },
        "Yunxin Liu": {
            "authorId": "3180228",
            "name": "Yunxin Liu",
            "hIndex": 37
        },
        "Jeiran Choupan": {
            "authorId": "3057500",
            "name": "Jeiran Choupan",
            "hIndex": 11
        },
        "Mahlatse S. Mbooi": {
            "authorId": "2182487931",
            "name": "Mahlatse S Mbooi",
            "hIndex": 1
        },
        "Jiechao Gao": {
            "authorId": "1505808706",
            "name": "Jiechao Gao",
            "hIndex": 15
        },
        "Quentin Anthony": {
            "authorId": "4190989",
            "name": "Q. Fogg",
            "hIndex": 14
        },
        "Hossein Toreyhi": {
            "authorId": "1675984575",
            "name": "Hossein Toreyhi",
            "hIndex": 6
        },
        "Giacomo Medda": {
            "authorId": "2040090509",
            "name": "Giacomo Medda",
            "hIndex": 4
        },
        "Bhavik Chandna": {
            "authorId": "2319399027",
            "name": "Bhavik Chandna",
            "hIndex": 0
        },
        "Kentaro Hirahara": {
            "authorId": "2320152678",
            "name": "Kentaro Hirahara",
            "hIndex": 0
        },
        "Yongxin Chen": {
            "authorId": "2369515",
            "name": "Yongxin Chen",
            "hIndex": 33
        },
        "Tanveer Hussain": {
            "authorId": "50328194",
            "name": "T. Hussain",
            "hIndex": 28
        },
        "Chunyuan Yuan": {
            "authorId": "2218685519",
            "name": "Chun-yuan Yu",
            "hIndex": 12
        },
        "Yong Lin": {
            "authorId": "2108138362",
            "name": "Yong Lin",
            "hIndex": 12
        },
        "Harishchandra Kumar": {
            "authorId": "4472188",
            "name": "R. Harishchandra",
            "hIndex": 11
        },
        "Nicholas Konz": {
            "authorId": "2092037701",
            "name": "N. Konz",
            "hIndex": 6
        },
        "Xin Sun": {
            "authorId": "4441159",
            "name": "Xinjun Sun",
            "hIndex": 21
        },
        "He Yu": {
            "authorId": "46968053",
            "name": "Yu He",
            "hIndex": 7
        },
        "Davide Vega": {
            "authorId": "39108101",
            "name": "Davide Vega",
            "hIndex": 11
        },
        "Gabriel Y. Arteaga": {
            "authorId": "146136064",
            "name": "A. Morales",
            "hIndex": 6
        },
        "Erasmo Purificato": {
            "authorId": "40845845",
            "name": "Erasmo Purificato",
            "hIndex": 6
        },
        "Da-Hui Wang": {
            "authorId": "2111191132",
            "name": "Da-hui Wang",
            "hIndex": 21
        },
        "Christian Bitter": {
            "authorId": "3287218",
            "name": "Christian Bitter",
            "hIndex": 5
        },
        "Yunhan Li": {
            "authorId": "15463121",
            "name": "Yunhan Li",
            "hIndex": 5
        },
        "Conghui He": {
            "authorId": "3486481",
            "name": "Conghui He",
            "hIndex": 21
        },
        "Zhimeng Yin": {
            "authorId": "3255550",
            "name": "Zhimeng Yin",
            "hIndex": 18
        },
        "Gilles Gesquiere": {
            "authorId": "2051501",
            "name": "G. Gesqui\u00e8re",
            "hIndex": 14
        },
        "Mohan Shi": {
            "authorId": "2113796710",
            "name": "Mohan Shi",
            "hIndex": 4
        },
        "Elzbieta Lewa\u0144ska": {
            "authorId": "1885809",
            "name": "Elzbieta Lewanska",
            "hIndex": 2
        },
        "Shintaro Ozaki": {
            "authorId": "117623927",
            "name": "Shin-Ya Ozaki",
            "hIndex": 1
        },
        "Jianrong Wang": {
            "name": "Jianrong Wang",
            "hIndex": 0
        },
        "Andreas Dengel": {
            "authorId": "145279674",
            "name": "A. Dengel",
            "hIndex": 55
        },
        "Guibin Zhang": {
            "authorId": "144756392",
            "name": "Guibin Zhang",
            "hIndex": 11
        },
        "Nicholas Galioto": {
            "authorId": "35563761",
            "name": "Nicholas J Galioto",
            "hIndex": 4
        },
        "Chenpeng Du": {
            "authorId": "1658323286",
            "name": "Chenpeng Du",
            "hIndex": 12
        },
        "Cise Midoglu": {
            "authorId": "3365939",
            "name": "Cise Midoglu",
            "hIndex": 10
        },
        "Shiwan Zhao": {
            "authorId": "2516425",
            "name": "Shiwan Zhao",
            "hIndex": 18
        },
        "Emir Demirovi\u0107": {
            "authorId": "2198370",
            "name": "Emir Demirovic",
            "hIndex": 13
        },
        "Henry Kvinge": {
            "authorId": "51042250",
            "name": "Henry Kvinge",
            "hIndex": 6
        },
        "Tao Xue": {
            "authorId": "153552533",
            "name": "Taotao Xue",
            "hIndex": 7
        },
        "Feiyu Xiong": {
            "name": "Feiyu Xiong",
            "hIndex": 0
        },
        "Wei Chu": {
            "authorId": "7467219",
            "name": "W. Chu",
            "hIndex": 10
        },
        "Uddip Acharjee Shuvo": {
            "authorId": "2307473810",
            "name": "Uddip Acharjee Shuvo",
            "hIndex": 1
        },
        "Jie Zhang": {
            "authorId": "36829932",
            "name": "Jie Zhang",
            "hIndex": 36
        },
        "Shuhong Zheng": {
            "name": "Shuhong Zheng",
            "hIndex": 0
        },
        "Shams Nafisa Ali": {
            "authorId": "1822124045",
            "name": "Shams Nafisa Ali",
            "hIndex": 6
        },
        "Fabian Wenz": {
            "authorId": "152800149",
            "name": "Fabian Wenz",
            "hIndex": 3
        },
        "Ruggero Carli": {
            "authorId": "1687569",
            "name": "R. Carli",
            "hIndex": 33
        },
        "Yuxiang Wang": {
            "authorId": "2115828793",
            "name": "Yuxiang S. Wang",
            "hIndex": 24
        },
        "Tianyu Qi": {
            "authorId": "2053720078",
            "name": "Tianyu F Qi",
            "hIndex": 6
        },
        "Edem Wornyo": {
            "name": "Edem Wornyo",
            "hIndex": 0
        },
        "Inkit Padhi": {
            "authorId": "8350409",
            "name": "Inkit Padhi",
            "hIndex": 16
        },
        "Robert G. Belleman": {
            "authorId": "2405330",
            "name": "R. Belleman",
            "hIndex": 15
        },
        "Iyad Rahwan": {
            "authorId": "1705156",
            "name": "Iyad Rahwan",
            "hIndex": 50
        },
        "Hongwei Jin": {
            "authorId": "1694993",
            "name": "Hongwei Jin",
            "hIndex": 31
        },
        "Romain L\u00e9guillon": {
            "authorId": "32131915",
            "name": "R. L\u0117guillon",
            "hIndex": 10
        },
        "Weijing Xie": {
            "authorId": "2113722227",
            "name": "W. Xie",
            "hIndex": 9
        },
        "Hongfei Lin": {
            "authorId": "2116456164",
            "name": "Hongfei Lin",
            "hIndex": 12
        },
        "Jianchao Bi": {
            "authorId": "2138373",
            "name": "Jianchao Bi",
            "hIndex": 3
        },
        "Sheng Zhong": {
            "authorId": "12686478",
            "name": "Zhongyi Sheng",
            "hIndex": 20
        },
        "Teng Wang": {
            "authorId": "1492077292",
            "name": "Tengqi Wang",
            "hIndex": 8
        },
        "Xiao Yan": {
            "authorId": "145608644",
            "name": "Xiao-Jie Yan",
            "hIndex": 28
        },
        "Aziz Amezian El Khalfioui": {
            "authorId": "2000567267",
            "name": "Aziz Amezian El Khalfioui",
            "hIndex": 2
        },
        "Yunpeng Liu": {
            "authorId": "2145507324",
            "name": "Yunpeng Liu",
            "hIndex": 18
        },
        "Nasim Sheikh-Bahaei": {
            "authorId": "1401551648",
            "name": "N. Sheikh-Bahaei",
            "hIndex": 12
        },
        "John M. Carroll": {
            "authorId": "145066246",
            "name": "John Millar Carroll",
            "hIndex": 82
        },
        "Benjamin Sowell": {
            "authorId": "1925528",
            "name": "B. Sowell",
            "hIndex": 9
        },
        "Chen Lin": {
            "authorId": "2118536665",
            "name": "L. Chen",
            "hIndex": 47
        },
        "Grigori Sidorov": {
            "authorId": "144140335",
            "name": "G. Sidorov",
            "hIndex": 30
        },
        "Yuji Yasui": {
            "authorId": "72770747",
            "name": "Y. Yasui",
            "hIndex": 8
        },
        "Kanae Tsushima": {
            "authorId": "34375314",
            "name": "Kanae Tsushima",
            "hIndex": 5
        },
        "Tural Gurbanov": {
            "authorId": "3442257",
            "name": "Tural Gurbanov",
            "hIndex": 5
        },
        "A. Ram\u00edrez-de-Arellano": {
            "authorId": "40514832",
            "name": "A. Ram\u00edrez De Arellano",
            "hIndex": 5
        },
        "Wei Cheng": {
            "authorId": "2004161058",
            "name": "W. Cheng",
            "hIndex": 26
        },
        "Yihao Cai": {
            "authorId": "2111026293",
            "name": "Yi-Yu Cai",
            "hIndex": 1
        },
        "Xinpeng Xie": {
            "authorId": "13046253",
            "name": "Xinpeng Xie",
            "hIndex": 12
        },
        "Jackson Petty": {
            "authorId": "104576891",
            "name": "A. Petty",
            "hIndex": 6
        },
        "Faiz Hasan": {
            "authorId": "113421096",
            "name": "F. Hasan",
            "hIndex": 7
        },
        "Lizhen Cui": {
            "authorId": "101457473",
            "name": "Li-zhen Cui",
            "hIndex": 24
        },
        "Reshma Lal Jagadheesh": {
            "authorId": "2319830658",
            "name": "Reshma Lal Jagadheesh",
            "hIndex": 0
        },
        "Hong Qiao": {
            "authorId": "40217516",
            "name": "Qiaohong Zhou",
            "hIndex": 27
        },
        "Fionn D. Malone": {
            "authorId": "145839285",
            "name": "F. Malone",
            "hIndex": 20
        },
        "Kyle Sargent": {
            "authorId": "2121366367",
            "name": "Kyle Sargent",
            "hIndex": 6
        },
        "Muhammad Umair": {
            "authorId": "2257111445",
            "name": "M. Umair",
            "hIndex": 8
        },
        "Keyu Chen": {
            "authorId": "2152955593",
            "name": "Ke Chen",
            "hIndex": 5
        },
        "Zicheng Liu": {
            "authorId": "51110012",
            "name": "Zicheng Liu",
            "hIndex": 18
        },
        "Wenchao Jiang": {
            "authorId": "49408720",
            "name": "Wen Jiang",
            "hIndex": 11
        },
        "Qing Li": {
            "authorId": "1629838595",
            "name": "Lili Qing",
            "hIndex": 7
        },
        "Chaoyi Tan": {
            "authorId": "2225616377",
            "name": "Chaoyi Tan",
            "hIndex": 1
        },
        "Ryan Shivers": {
            "authorId": "121738700",
            "name": "Ryan Shivers",
            "hIndex": 2
        },
        "Arun Ahuja": {
            "authorId": "37968006",
            "name": "Arun Ahuja",
            "hIndex": 20
        },
        "Tahir Azim": {
            "authorId": "34948637",
            "name": "T. Azim",
            "hIndex": 10
        },
        "Guanting Dong": {
            "authorId": "51490462",
            "name": "Guanting Dong",
            "hIndex": 10
        },
        "Zhaolin Chen": {
            "authorId": "2150006",
            "name": "C. Zhaolin",
            "hIndex": 7
        },
        "Vivek Shetty": {
            "authorId": "144644109",
            "name": "V. Shetty",
            "hIndex": 28
        },
        "Raviteja Anantha": {
            "authorId": "1666320488",
            "name": "R. Anantha",
            "hIndex": 5
        },
        "Uday Singh Saini": {
            "authorId": "3407898",
            "name": "Uday Singh Saini",
            "hIndex": 2
        },
        "Kexin Pei": {
            "authorId": "40428350",
            "name": "Kexin Pei",
            "hIndex": 18
        },
        "Byung Wook Kim": {
            "authorId": "2152315602",
            "name": "B. Kim",
            "hIndex": 10
        },
        "Tieying Zhang": {
            "authorId": "2124495231",
            "name": "Tieying Zhang",
            "hIndex": 4
        },
        "Yanyun Wang": {
            "authorId": "91963118",
            "name": "Yanyung Wang",
            "hIndex": 11
        },
        "Duo Lu": {
            "authorId": "2231832817",
            "name": "Shilong Lu",
            "hIndex": 9
        },
        "Jey Puget Gil": {
            "authorId": "2262264656",
            "name": "Jey Puget Gil",
            "hIndex": 0
        },
        "Ruifeng Xu": {
            "authorId": "48974077",
            "name": "R. Xu",
            "hIndex": 10
        },
        "Jie Ma": {
            "authorId": "79321547",
            "name": "Jieying Ma",
            "hIndex": 8
        },
        "Peter B. Lerner": {
            "authorId": "47202139",
            "name": "P. Lerner",
            "hIndex": 11
        },
        "Fragkiskos D. Malliaros": {
            "authorId": "2817467",
            "name": "Fragkiskos D. Malliaros",
            "hIndex": 17
        },
        "Changmiao Wang": {
            "authorId": "2005020",
            "name": "Changmiao Wang",
            "hIndex": 8
        },
        "Julius Ott": {
            "authorId": "4468133",
            "name": "Fred J Ott",
            "hIndex": 5
        },
        "Xin Jiang": {
            "authorId": "4157465",
            "name": "Xin-guo Jiang",
            "hIndex": 69
        },
        "Radu Jianu": {
            "authorId": "2819751",
            "name": "R. Jianu",
            "hIndex": 16
        },
        "Akshita Bhagia": {
            "authorId": "2166136235",
            "name": "Akshita Bhagia",
            "hIndex": 8
        },
        "Jing Cui": {
            "authorId": "2315414",
            "name": "Jing-jing Cui",
            "hIndex": 8
        },
        "Yudi Dai": {
            "authorId": "153883481",
            "name": "Yudi Dai",
            "hIndex": 8
        },
        "Jugal Kalita": {
            "authorId": "34694214",
            "name": "J. Kalita",
            "hIndex": 42
        },
        "Zihao Sheng": {
            "authorId": "2046871285",
            "name": "Zihao Sheng",
            "hIndex": 4
        },
        "Yicheng Zou": {
            "authorId": "51192034",
            "name": "Yicheng Zou",
            "hIndex": 11
        },
        "Zezhong Wang": {
            "authorId": "2108725066",
            "name": "Zezhong Wang",
            "hIndex": 8
        },
        "Ziyu Li": {
            "authorId": "9266069",
            "name": "Liu Ziyu",
            "hIndex": 6
        },
        "Abdul Rehman": {
            "authorId": "40942790",
            "name": "A. Rehman",
            "hIndex": 18
        },
        "Ruiquan Ge": {
            "authorId": "3362288",
            "name": "Ruiquan Ge",
            "hIndex": 9
        },
        "Xin Lv": {
            "authorId": "7829546",
            "name": "Xin-jun Lv",
            "hIndex": 8
        },
        "Ke Xu": {
            "authorId": "49343381",
            "name": "Ke-qiang Xu",
            "hIndex": 24
        },
        "Lingyun Song": {
            "authorId": "67130509",
            "name": "S. Ling-yun",
            "hIndex": 2
        },
        "Deniz Aykac": {
            "authorId": "2405391",
            "name": "D. Aykac",
            "hIndex": 8
        },
        "Jaeha Kung": {
            "authorId": "2484938",
            "name": "J. Kung",
            "hIndex": 14
        },
        "Palak Handa": {
            "authorId": "2080055924",
            "name": "Palak Handa",
            "hIndex": 5
        },
        "Keld Lundgaard": {
            "name": "Keld Lundgaard",
            "hIndex": 0
        },
        "Lixiao Huang": {
            "authorId": "40394791",
            "name": "Lixiao Huang",
            "hIndex": 7
        },
        "Zeyu Cui": {
            "authorId": "72723327",
            "name": "Zeyu Cui",
            "hIndex": 11
        },
        "Sihao Hu": {
            "authorId": "1576223504",
            "name": "Sihao Hu",
            "hIndex": 6
        },
        "Bozhidar Stevanoski": {
            "authorId": "84464088",
            "name": "Bozhidar Stevanoski",
            "hIndex": 2
        },
        "Wenhu Chen": {
            "authorId": "48993749",
            "name": "Wenhu Chen",
            "hIndex": 10
        },
        "Runxin Xu": {
            "authorId": "1748844142",
            "name": "Runxin Xu",
            "hIndex": 10
        },
        "Hisashi Kawai": {
            "authorId": "1712237302",
            "name": "H. Kawai",
            "hIndex": 30
        },
        "Ji Wu": {
            "authorId": "2128514626",
            "name": "J. Wu",
            "hIndex": 10
        },
        "S\u00e9bastien Lef\u00e8vre": {
            "authorId": "49256436",
            "name": "S. Lef\u00e8vre",
            "hIndex": 7
        },
        "Victor Guallar": {
            "authorId": "3320031",
            "name": "V. Guallar",
            "hIndex": 46
        },
        "Ruhui Ma": {
            "authorId": "1840514",
            "name": "Ruhui Ma",
            "hIndex": 19
        },
        "Yi Zhang": {
            "authorId": "46868234",
            "name": "Yi Zhang",
            "hIndex": 12
        },
        "J\u00e9r\u00f4me Lang": {
            "authorId": "145417235",
            "name": "J. Lang",
            "hIndex": 55
        },
        "Tao Wang": {
            "authorId": "2156017772",
            "name": "Tao Wang",
            "hIndex": 23
        },
        "Jiajie Zhang": {
            "authorId": "40430889",
            "name": "Jiajie Zhang",
            "hIndex": 14
        },
        "Yuxuan Sun": {
            "authorId": "2117104351",
            "name": "Yu Sun",
            "hIndex": 7
        },
        "Alessandro Salatiello": {
            "authorId": "1672626904",
            "name": "Alessandro Salatiello",
            "hIndex": 3
        },
        "Tatyana Tserne": {
            "authorId": "11054148",
            "name": "Tatyana A. Tserne",
            "hIndex": 6
        },
        "Eyal Cohen": {
            "authorId": "145087159",
            "name": "E. Cohen",
            "hIndex": 43
        },
        "Jialong Wang": {
            "authorId": "51049624",
            "name": "Jia\u2010Lian Wang",
            "hIndex": 8
        },
        "Nawras Alnaasan": {
            "authorId": "2130317834",
            "name": "Nawras Alnaasan",
            "hIndex": 2
        },
        "Pengfei Bai": {
            "authorId": "38065218",
            "name": "P. Bai",
            "hIndex": 11
        },
        "Chak Tou Leong": {
            "authorId": "2257035605",
            "name": "Chak Tou Leong",
            "hIndex": 4
        },
        "Jingwei Zhang": {
            "authorId": "50153121",
            "name": "Z. Jingwei",
            "hIndex": 7
        },
        "Shuvendu Roy": {
            "authorId": "50138049",
            "name": "Shuvendu Roy",
            "hIndex": 9
        },
        "Miehleketo Mathebula": {
            "authorId": "2319402407",
            "name": "Miehleketo Mathebula",
            "hIndex": 0
        },
        "Yifan Jiang": {
            "authorId": "66723523",
            "name": "Jia Yifan",
            "hIndex": 4
        },
        "Yifeng Jiang": {
            "authorId": "152771744",
            "name": "Yi-feng Jiang",
            "hIndex": 22
        },
        "Sonu Kumar": {
            "authorId": "120595290",
            "name": "S. Mahawer",
            "hIndex": 7
        },
        "Francesco Carlucci": {
            "authorId": "39096554",
            "name": "F. Carlucci",
            "hIndex": 13
        },
        "Haimin Zhang": {
            "authorId": "2315252690",
            "name": "Haimin Zhang",
            "hIndex": 40
        },
        "Eugene Belilovsky": {
            "authorId": "1829344",
            "name": "Eugene Belilovsky",
            "hIndex": 17
        },
        "Jeremy Straub": {
            "authorId": "144295874",
            "name": "J. Straub",
            "hIndex": 24
        },
        "Adam White": {
            "authorId": "50259372",
            "name": "Adam K. White",
            "hIndex": 9
        },
        "Amanpreet Singh": {
            "authorId": "2053102384",
            "name": "A. Gill",
            "hIndex": 2
        },
        "Tanja Schultz": {
            "authorId": "2113276815",
            "name": "T. Schultz",
            "hIndex": 11
        },
        "Samiul Based Shuvo": {
            "authorId": "1933183901",
            "name": "Samiul Based Shuvo",
            "hIndex": 5
        },
        "Ye Tian": {
            "authorId": "145639074",
            "name": "Tiangui Ye",
            "hIndex": 35
        },
        "Kristian Hildebrand": {
            "authorId": "37560378",
            "name": "Kristian Hildebrand",
            "hIndex": 18
        },
        "Martin Vechev": {
            "authorId": "1736447",
            "name": "Martin T. Vechev",
            "hIndex": 55
        },
        "Pragya Gupta": {
            "authorId": "17264695",
            "name": "Pragya Gupta",
            "hIndex": 14
        },
        "Leslie A. Lenert": {
            "authorId": "1764483",
            "name": "L. Lenert",
            "hIndex": 41
        },
        "Yuchen Yan": {
            "authorId": "2109341751",
            "name": "Yuchen Yan",
            "hIndex": 10
        },
        "Yuhan Ma": {
            "authorId": "47009649",
            "name": "Yu-heng Ma",
            "hIndex": 6
        },
        "Yuanqing Yu": {
            "authorId": "37505929",
            "name": "Yuanqing Xia",
            "hIndex": 81
        },
        "Hossein Majlesi": {
            "authorId": "14391131",
            "name": "H. Majlesi",
            "hIndex": 2
        },
        "Jingbo Zhang": {
            "authorId": "47539072",
            "name": "Jing-Bo Zhang",
            "hIndex": 23
        },
        "Erick Mas": {
            "authorId": "6316045",
            "name": "E. Mas",
            "hIndex": 28
        },
        "Muru Zhang": {
            "authorId": "2116096584",
            "name": "Muru Zhang",
            "hIndex": 4
        },
        "Yifan Tang": {
            "authorId": "2933264",
            "name": "Yifan Tang",
            "hIndex": 15
        },
        "Arianna Muti": {
            "authorId": "2038102772",
            "name": "Arianna Muti",
            "hIndex": 5
        },
        "Maria Leon": {
            "authorId": "145946649",
            "name": "M. Leon",
            "hIndex": 23
        },
        "Yunxiao Shi": {
            "authorId": "28941108",
            "name": "Y. Shi",
            "hIndex": 6
        },
        "Jiahui Hou": {
            "authorId": "7287408",
            "name": "Jiahui Hou",
            "hIndex": 10
        },
        "Marco Caccamo": {
            "authorId": "1749138",
            "name": "M. Caccamo",
            "hIndex": 45
        },
        "Ruben D. Fonnegra": {
            "authorId": "35966697",
            "name": "Rub\u00e9n D. Fonnegra",
            "hIndex": 4
        },
        "Xugang Lu": {
            "name": "Xugang Lu",
            "hIndex": 0
        },
        "John Collomosse": {
            "authorId": "1680236",
            "name": "J. Collomosse",
            "hIndex": 37
        },
        "Sicheng Wang": {
            "authorId": "47672850",
            "name": "Sicheng Wang",
            "hIndex": 8
        },
        "Jean-Francois Giovannelli": {
            "name": "Jean-Francois Giovannelli",
            "hIndex": 0
        },
        "Steffen Eger": {
            "authorId": "2620186",
            "name": "Steffen Eger",
            "hIndex": 25
        },
        "Sakuna Harinda Jayasundara": {
            "authorId": "2139661492",
            "name": "Sakuna Jayasundara",
            "hIndex": 1
        },
        "Shivesh Prakash": {
            "authorId": "5807435",
            "name": "S. Prakash",
            "hIndex": 18
        },
        "P\u00e5l Halvorsen": {
            "authorId": "143733939",
            "name": "P. Halvorsen",
            "hIndex": 44
        },
        "Shoyad Ibn Sabur Khan Nuhash": {
            "authorId": "2187097615",
            "name": "Shoyad Ibn Sabur Khan Nuhash",
            "hIndex": 2
        },
        "Shijia Peng": {
            "authorId": "38654394",
            "name": "Shenmin Zhang",
            "hIndex": 68
        },
        "Zhuoyi Yang": {
            "authorId": "2109506541",
            "name": "Zhuoyi Yang",
            "hIndex": 8
        },
        "Guosheng Dong": {
            "authorId": "9283785",
            "name": "G. Dong",
            "hIndex": 2
        },
        "Suzanne Dikker": {
            "authorId": "4909531",
            "name": "Suzanne Dikker",
            "hIndex": 18
        },
        "Durga Toshniwal": {
            "authorId": "2404552",
            "name": "Durga Toshniwal",
            "hIndex": 31
        },
        "Yangguang Chen": {
            "authorId": "47559112",
            "name": "Yanguang Chen",
            "hIndex": 13
        },
        "Pourya Adibfar": {
            "authorId": "2319417307",
            "name": "Pourya Adibfar",
            "hIndex": 0
        },
        "Yasunari Suzuki": {
            "authorId": "2111380351",
            "name": "Yasunari Suzuki",
            "hIndex": 13
        },
        "Xulin Fan": {
            "authorId": "2258711678",
            "name": "Xulin Fan",
            "hIndex": 1
        },
        "Bohou Li": {
            "authorId": "143771569",
            "name": "Bo Li",
            "hIndex": 53
        },
        "Danilo Fernandes": {
            "authorId": "6522019",
            "name": "Danilo B. Fernandes",
            "hIndex": 7
        },
        "Ana-Maria Cretu": {
            "authorId": "1680861",
            "name": "A. Cr\u00e9tu",
            "hIndex": 19
        },
        "Matthias Kettl": {
            "authorId": "2160992830",
            "name": "Matthias Kettl",
            "hIndex": 1
        },
        "Dimitar Dimitrov": {
            "name": "Dimitar Dimitrov",
            "hIndex": 0
        },
        "Pu Li": {
            "authorId": "30481546",
            "name": "L. Pu",
            "hIndex": 22
        },
        "Jiaqi Shao": {
            "authorId": "1691384401",
            "name": "Jiaqi Shao",
            "hIndex": 7
        },
        "Guangyong Chen": {
            "authorId": "2149509170",
            "name": "Guang-yong Chen",
            "hIndex": 12
        },
        "Summer Yue": {
            "authorId": "2107032220",
            "name": "Summer Yue",
            "hIndex": 3
        },
        "Hainan Zhang": {
            "authorId": "2785903",
            "name": "Hai-nan Zhang",
            "hIndex": 17
        },
        "Stephen Bonner": {
            "authorId": "1867107",
            "name": "S. Bonner",
            "hIndex": 16
        },
        "Richard L. J. Qiu": {
            "name": "Richard L. J. Qiu",
            "hIndex": 0
        },
        "Hongqiu Wu": {
            "authorId": "2120430544",
            "name": "Hongqiu Wu",
            "hIndex": 8
        },
        "Bolong Yang": {
            "authorId": "2157853631",
            "name": "Bolong Yang",
            "hIndex": 8
        },
        "Huan Yang": {
            "authorId": "2155585576",
            "name": "Hua Yang",
            "hIndex": 23
        },
        "Arian Salahi-Niri": {
            "authorId": "2319610278",
            "name": "Arian Salahi-Niri",
            "hIndex": 0
        },
        "Qipeng Guo": {
            "authorId": "16318205",
            "name": "Q. Guo",
            "hIndex": 46
        },
        "Emma Hoes": {
            "authorId": "2132123040",
            "name": "E. Hoes",
            "hIndex": 3
        },
        "Yeonji Jung": {
            "authorId": "41157643",
            "name": "Yeonji Jung",
            "hIndex": 6
        },
        "Hector Falcoff": {
            "authorId": "2582019",
            "name": "H. Falcoff",
            "hIndex": 13
        },
        "Kyunghyun Cho": {
            "authorId": "2111049203",
            "name": "Kyunghyun Cho",
            "hIndex": 15
        },
        "Doehyun Baek": {
            "authorId": "2313391086",
            "name": "Doehyun Baek",
            "hIndex": 0
        },
        "Zhou Shen": {
            "authorId": "8090553",
            "name": "Shenzhou Lu",
            "hIndex": 28
        },
        "Ming Zhong": {
            "authorId": "14698842",
            "name": "Mingqiang Zhong",
            "hIndex": 33
        },
        "Yanchao Zhao": {
            "authorId": "31006543",
            "name": "Zhao Yanchao",
            "hIndex": 4
        },
        "Man Luo": {
            "authorId": "2063303110",
            "name": "Man (Melody) Luo",
            "hIndex": 5
        },
        "Wenlin Zhou": {
            "authorId": "4720167",
            "name": "Wenlin Zhou",
            "hIndex": 7
        },
        "Christian Simon": {
            "authorId": "152541366",
            "name": "C. Simon",
            "hIndex": 26
        },
        "Lars Nieradzik": {
            "name": "Lars Nieradzik",
            "hIndex": 0
        },
        "Ruihong Wang": {
            "authorId": "2108755147",
            "name": "Rui\u2010Hua Wang",
            "hIndex": 7
        },
        "Yuanyuan Zhu": {
            "authorId": "46759089",
            "name": "Yuan Zhu",
            "hIndex": 33
        },
        "Ian Soboroff": {
            "authorId": "144526707",
            "name": "I. Soboroff",
            "hIndex": 43
        },
        "Anusha Chhabra": {
            "authorId": "41096824",
            "name": "A. Chhabra",
            "hIndex": 5
        },
        "Daniel Jakobsson": {
            "authorId": "77624474",
            "name": "Daniel Jakobsson",
            "hIndex": 1
        },
        "Gomer Otterspeer": {
            "authorId": "2302558570",
            "name": "Gom\u00e8r Otterspeer",
            "hIndex": 1
        },
        "Xin Lu": {
            "authorId": "2179873",
            "name": "Xin-Jiang Lu",
            "hIndex": 23
        },
        "Wenjie Wang": {
            "authorId": "51235215",
            "name": "Wang Wenjie",
            "hIndex": 9
        },
        "Levin Brinkmann": {
            "authorId": "13566762",
            "name": "Levin Brinkmann",
            "hIndex": 4
        },
        "Imke van Heerden": {
            "authorId": "84734726",
            "name": "Imke van Heerden",
            "hIndex": 2
        },
        "Yu Liang": {
            "authorId": "40170111",
            "name": "Yu-Chih Liang",
            "hIndex": 43
        },
        "Zhaojie Fang": {
            "authorId": "152216958",
            "name": "Z. Fang",
            "hIndex": 3
        },
        "Sili Huang": {
            "authorId": "2124427348",
            "name": "Sili Huang",
            "hIndex": 3
        },
        "Selim Furkan Tekin": {
            "authorId": "2066601176",
            "name": "S. Tekin",
            "hIndex": 4
        },
        "Minji Kang": {
            "authorId": "152922456",
            "name": "Mi-Hyung Kang",
            "hIndex": 5
        },
        "Xiangmo Zhao": {
            "authorId": "2031134",
            "name": "Xiangmo Zhao",
            "hIndex": 28
        },
        "Stijn Vansummeren": {
            "authorId": "1709642",
            "name": "Stijn Vansummeren",
            "hIndex": 25
        },
        "Jacob Hilton": {
            "authorId": "2052366271",
            "name": "Jacob Hilton",
            "hIndex": 8
        },
        "Girish Nadkarni": {
            "authorId": "3159260",
            "name": "G. Nadkarni",
            "hIndex": 60
        },
        "Jan Hofmann": {
            "authorId": "40172097",
            "name": "J. Hofmann",
            "hIndex": 40
        },
        "Yuancheng Xu": {
            "authorId": "1731740",
            "name": "Yuan Xu",
            "hIndex": 25
        },
        "James Brotherston": {
            "authorId": "37529449",
            "name": "J. Brotherston",
            "hIndex": 22
        },
        "Dat Nguyen Minh": {
            "authorId": "93900796",
            "name": "N. M. Dat",
            "hIndex": 18
        },
        "Jian Guo": {
            "authorId": "15563411",
            "name": "Jian-guo Guo",
            "hIndex": 7
        },
        "Geymerson S. Ramos": {
            "authorId": "123143460",
            "name": "Geymerson S. Ramos",
            "hIndex": 3
        },
        "Oriol Ramos Terrades": {
            "name": "Oriol Ramos Terrades",
            "hIndex": 0
        },
        "Joselyn Rodriguez": {
            "authorId": "50388497",
            "name": "Jos\u00e9 Ra\u00fal Rodr\u00edguez Rodr\u00edguez",
            "hIndex": 34
        },
        "Lipeng Ma": {
            "authorId": "50709644",
            "name": "Lipeng Ma",
            "hIndex": 4
        },
        "Ruifeng She": {
            "authorId": "1409388489",
            "name": "Ruifeng She",
            "hIndex": 4
        },
        "Likun Zhang": {
            "authorId": "47058851",
            "name": "Likun Zhang",
            "hIndex": 7
        },
        "Jediah Katz": {
            "authorId": "2309715860",
            "name": "Jediah Katz",
            "hIndex": 0
        },
        "Bidyut Saha": {
            "authorId": "39908191",
            "name": "B. Saha",
            "hIndex": 69
        },
        "Peng Shen": {
            "authorId": "48652366",
            "name": "P. Shen",
            "hIndex": 16
        },
        "Yan Han": {
            "authorId": "2115629860",
            "name": "Yan Han",
            "hIndex": 8
        },
        "Sean Ransom": {
            "authorId": "11795566",
            "name": "S. Ransom",
            "hIndex": 12
        },
        "Jacob Morrison": {
            "authorId": "2146964035",
            "name": "Jacob Daniel Morrison",
            "hIndex": 5
        },
        "Weijian Shang": {
            "authorId": "2022283",
            "name": "Weijian Shang",
            "hIndex": 13
        },
        "Chunyan An": {
            "authorId": "7348797",
            "name": "Chunyan An",
            "hIndex": 10
        },
        "Zhengjie Huang": {
            "authorId": "3468568",
            "name": "Zhengjie Huang",
            "hIndex": 19
        },
        "Ahmed Elazab": {
            "authorId": "6127333",
            "name": "A. Elazab",
            "hIndex": 18
        },
        "Patryk Rygiel": {
            "authorId": "2193191294",
            "name": "Patryk Rygiel",
            "hIndex": 1
        },
        "Xiangtao Kong": {
            "authorId": "34204559",
            "name": "Xiang-tao Kong",
            "hIndex": 11
        },
        "Dazhi Jiang": {
            "authorId": "49731236",
            "name": "D. Jiang",
            "hIndex": 27
        },
        "Maxim Ushakov": {
            "authorId": "144213438",
            "name": "M. Ushakov",
            "hIndex": 3
        },
        "Zefan Cai": {
            "authorId": "2117632647",
            "name": "Zefan Cai",
            "hIndex": 6
        },
        "Asim Munawar": {
            "name": "Asim Munawar",
            "hIndex": 0
        },
        "Ishtiaque Ahmed Khan": {
            "authorId": "1637279688",
            "name": "Ishtiaque Ahmed Khan",
            "hIndex": 4
        },
        "Dinesh Raghu": {
            "authorId": "47563862",
            "name": "D. Raghu",
            "hIndex": 12
        },
        "Zanxin Chen": {
            "authorId": "2307343429",
            "name": "Zanxin Chen",
            "hIndex": 0
        },
        "Tao Feng": {
            "authorId": "145184500",
            "name": "F. Tao",
            "hIndex": 26
        },
        "Yu Su": {
            "authorId": "2171101636",
            "name": "Y. Su",
            "hIndex": 18
        },
        "Sean Hendryx": {
            "authorId": "21178826",
            "name": "S. Hendryx",
            "hIndex": 6
        },
        "Teresita D\u00edaz de St\u00e5hl": {
            "authorId": "1929217791",
            "name": "T. de St\u00e5hl",
            "hIndex": 16
        },
        "Benjamin L. Badger": {
            "authorId": "40654398",
            "name": "Benjamin L. Badger",
            "hIndex": 4
        },
        "Richard Voyles": {
            "name": "Richard Voyles",
            "hIndex": 0
        },
        "Qi Wang": {
            "authorId": "145346762",
            "name": "Qi Wang",
            "hIndex": 53
        },
        "Peiyue Li": {
            "authorId": "3476003",
            "name": "Peiyue Li",
            "hIndex": 64
        },
        "Arnav Nagle": {
            "authorId": "2273561965",
            "name": "Arnav M Nagle",
            "hIndex": 1
        },
        "Mark Klibanov": {
            "authorId": "2319414572",
            "name": "Mark Klibanov",
            "hIndex": 0
        },
        "Anders Jonsson": {
            "authorId": "46668849",
            "name": "A. Jonsson",
            "hIndex": 36
        },
        "Marius Staring": {
            "authorId": "2602873",
            "name": "M. Staring",
            "hIndex": 36
        },
        "Rithesh Murthy": {
            "authorId": "2223748790",
            "name": "Rithesh Murthy",
            "hIndex": 4
        },
        "Ida Blystad": {
            "authorId": "3545746",
            "name": "I. Blystad",
            "hIndex": 7
        },
        "Jianguo Sun": {
            "authorId": "2294528504",
            "name": "Jianguo Sun",
            "hIndex": 35
        },
        "Alan Aqrawi": {
            "authorId": "2319832000",
            "name": "Alan Aqrawi",
            "hIndex": 0
        },
        "Sheila Castilho": {
            "authorId": "3041243",
            "name": "Sheila Castilho",
            "hIndex": 15
        },
        "Xiyan Su": {
            "authorId": "2106353317",
            "name": "Xi Su",
            "hIndex": 2
        },
        "Hsien-Te Kao": {
            "authorId": "39922117",
            "name": "Hsien-Te Kao",
            "hIndex": 6
        },
        "Shenheng Xu": {
            "authorId": "143879525",
            "name": "Shenheng Xu",
            "hIndex": 39
        },
        "Siyu Xia": {
            "authorId": "121689779",
            "name": "Si-Yu Xia",
            "hIndex": 6
        },
        "Yu-Xiong Wang": {
            "authorId": "2302062",
            "name": "Yu-Xiong Wang",
            "hIndex": 27
        },
        "Ludovico Boratto": {
            "authorId": "1824224",
            "name": "Ludovico Boratto",
            "hIndex": 26
        },
        "Todd M. Gureckis": {
            "authorId": "3013567",
            "name": "T. Gureckis",
            "hIndex": 30
        },
        "Leena G Pillai": {
            "authorId": "115528615",
            "name": "Leena G. Pillai",
            "hIndex": 2
        },
        "Erez Bilgory": {
            "name": "Erez Bilgory",
            "hIndex": 0
        },
        "Feroz Ahmad": {
            "authorId": "144688292",
            "name": "Feroz Ahmad",
            "hIndex": 14
        },
        "Kaijing Ma": {
            "authorId": "2281290823",
            "name": "Kaijing Ma",
            "hIndex": 2
        },
        "Ke Zhou": {
            "authorId": "2075359586",
            "name": "Ke Zhou",
            "hIndex": 11
        },
        "Pete Walsh": {
            "authorId": "29585800",
            "name": "P. Walsh",
            "hIndex": 14
        },
        "Mihai Zaha": {
            "authorId": "2101203442",
            "name": "Mihai V. Zaha",
            "hIndex": 3
        },
        "Botao Yu": {
            "authorId": "2110706166",
            "name": "Botao Yu",
            "hIndex": 8
        },
        "Zhi Qu": {
            "authorId": "93636392",
            "name": "Zhirong Qu",
            "hIndex": 25
        },
        "Benyou Wang": {
            "authorId": "2894465",
            "name": "Benyou Wang",
            "hIndex": 25
        },
        "Fenglei Fan": {
            "authorId": "145483466",
            "name": "Fenglei Fan",
            "hIndex": 16
        },
        "Wenhan Yao": {
            "authorId": "2114975894",
            "name": "Wen Yao",
            "hIndex": 3
        },
        "Kosuke Nakanishi": {
            "authorId": "51312690",
            "name": "K. Nakanishi",
            "hIndex": 22
        },
        "Kerstin Ritter": {
            "authorId": "34315219",
            "name": "K. Ritter",
            "hIndex": 12
        },
        "Luyang Liu": {
            "authorId": "33548806",
            "name": "Luyang Liu",
            "hIndex": 12
        },
        "Arnon Turetzky": {
            "name": "Arnon Turetzky",
            "hIndex": 0
        },
        "Zeyuan Chen": {
            "authorId": "5478513",
            "name": "Zeyuan Chen",
            "hIndex": 12
        },
        "Jianguo Zhang": {
            "authorId": "2108311745",
            "name": "Jianguo Zhang",
            "hIndex": 27
        },
        "Yanming Wan": {
            "authorId": "52639679",
            "name": "Wang Yanming",
            "hIndex": 4
        },
        "Shun Fu": {
            "authorId": "47411356",
            "name": "Shunfu Xu",
            "hIndex": 7
        },
        "Chan Hsu": {
            "authorId": "39355584",
            "name": "S. Hsu",
            "hIndex": 21
        },
        "Mirko Marras": {
            "authorId": "28922901",
            "name": "M. Marras",
            "hIndex": 18
        },
        "Alvaro Martinez": {
            "authorId": "2201823300",
            "name": "Alvaro A. Martinez",
            "hIndex": 62
        },
        "Lunkai Lin": {
            "authorId": "2319589609",
            "name": "Lunkai Lin",
            "hIndex": 0
        },
        "Shikai Fang": {
            "authorId": "1816748885",
            "name": "Shikai Fang",
            "hIndex": 6
        },
        "Martin Riedmiller": {
            "authorId": "3137672",
            "name": "Martin A. Riedmiller",
            "hIndex": 53
        },
        "Xiangdong Wang": {
            "authorId": "2144798030",
            "name": "Xiangdong Wang",
            "hIndex": 6
        },
        "Wei Lu": {
            "authorId": "72537283",
            "name": "W. Lu",
            "hIndex": 55
        },
        "Ding Wang": {
            "authorId": "2029334830",
            "name": "Ding-sheng Wang",
            "hIndex": 30
        },
        "Tom Bekor": {
            "name": "Tom Bekor",
            "hIndex": 0
        },
        "Albert Cohen": {
            "authorId": "145509578",
            "name": "A. Cohen",
            "hIndex": 37
        },
        "Aleksandr Vladimirovich Petrov": {
            "authorId": "2175024140",
            "name": "A. Petrov",
            "hIndex": 5
        },
        "Yao Liu": {
            "authorId": "4332428",
            "name": "Yao-Wu Liu",
            "hIndex": 21
        },
        "Colin Zhang": {
            "authorId": "2116343846",
            "name": "Colin Zhang",
            "hIndex": 6
        },
        "Yudong Zhao": {
            "authorId": "49339157",
            "name": "Yu-Dong Zhao",
            "hIndex": 6
        },
        "Naitong Yu": {
            "authorId": "4766906",
            "name": "N. Yu",
            "hIndex": 8
        },
        "Shengwen Liang": {
            "authorId": "5648971",
            "name": "S. Liang",
            "hIndex": 17
        },
        "Yuwen Pu": {
            "authorId": "46217276",
            "name": "Yuwen Pu",
            "hIndex": 6
        },
        "Yoann Le Bars": {
            "authorId": "103196816",
            "name": "Y. L. Bars",
            "hIndex": 2
        },
        "Zhongsheng Wang": {
            "authorId": "151256790",
            "name": "Wang Zhongsheng",
            "hIndex": 4
        },
        "Xiaying Wang": {
            "authorId": "2108010719",
            "name": "Xiaying Wang",
            "hIndex": 10
        },
        "Ali Soroush": {
            "authorId": "4657400",
            "name": "A. Soroush",
            "hIndex": 20
        },
        "Ao Zhang": {
            "authorId": "151494403",
            "name": "Ao Zhang",
            "hIndex": 13
        },
        "Tsegaye Misikir Tashu": {
            "authorId": "41066556",
            "name": "Tsegaye Misikir Tashu",
            "hIndex": 7
        },
        "Yajing Zhan": {
            "authorId": "4356192",
            "name": "Yajing Zhan",
            "hIndex": 5
        },
        "Hirokazu Kameoka": {
            "authorId": "1787190",
            "name": "H. Kameoka",
            "hIndex": 41
        },
        "Liqiang Nie": {
            "authorId": "143982887",
            "name": "Liqiang Nie",
            "hIndex": 66
        },
        "Min Xu": {
            "authorId": "47723361",
            "name": "Minmin Xu",
            "hIndex": 15
        },
        "Hongwei Feng": {
            "authorId": "27155736",
            "name": "Hongwei Feng",
            "hIndex": 9
        },
        "Zhen Lei": {
            "authorId": "145754451",
            "name": "Z. Lei",
            "hIndex": 15
        },
        "Zhaojie Liu": {
            "authorId": "2125551",
            "name": "Zhao-Jie Liu",
            "hIndex": 6
        },
        "Qing Wang": {
            "authorId": "151044924",
            "name": "Qing Wang",
            "hIndex": 25
        },
        "Stefan Winkler": {
            "authorId": "145231261",
            "name": "S. Winkler",
            "hIndex": 36
        },
        "Xiaojiang Peng": {
            "name": "Xiaojiang Peng",
            "hIndex": 0
        },
        "Zhongjiang He": {
            "authorId": "2218319722",
            "name": "Zhongjiang He",
            "hIndex": 3
        },
        "Jason Street": {
            "authorId": "14559663",
            "name": "Jason Street",
            "hIndex": 19
        },
        "Guoyin Wang": {
            "name": "Guoyin Wang",
            "hIndex": 0
        },
        "Tingyu Wang": {
            "authorId": "2155390290",
            "name": "Ting Wang",
            "hIndex": 12
        },
        "Miao Xu": {
            "authorId": "2152794923",
            "name": "Miao-Qing Xu",
            "hIndex": 8
        },
        "Tao He": {
            "authorId": "2055131160",
            "name": "T. He",
            "hIndex": 12
        },
        "Benjamin Rozonoyer": {
            "authorId": "2266751217",
            "name": "Benjamin Rozonoyer",
            "hIndex": 2
        },
        "Sangwoo Jung": {
            "authorId": "2611176",
            "name": "Sang-Hack Jung",
            "hIndex": 13
        },
        "Yukai Liu": {
            "authorId": "119923840",
            "name": "Yukai Liu",
            "hIndex": 15
        },
        "Huanhuan Ma": {
            "authorId": "7186726",
            "name": "Huanhuan Ma",
            "hIndex": 10
        },
        "Guillaume Baudart": {
            "authorId": "2920563",
            "name": "Guillaume Baudart",
            "hIndex": 10
        },
        "Qinghua Zhou": {
            "authorId": "49146161",
            "name": "Qinghua Zhou",
            "hIndex": 19
        },
        "Jie Cao": {
            "authorId": "145293452",
            "name": "Jie Cao",
            "hIndex": 26
        },
        "Mohamad Amin Pourhoseingholi": {
            "authorId": "6138247",
            "name": "M. Pourhoseingholi",
            "hIndex": 33
        },
        "Ayush Thakur": {
            "authorId": "114142117",
            "name": "Ayushi Thakur",
            "hIndex": 3
        },
        "Barry-John Theobald": {
            "authorId": "2785748",
            "name": "B. Theobald",
            "hIndex": 21
        },
        "Jiaqi Yang": {
            "authorId": "2109749941",
            "name": "Jiaqi Yang",
            "hIndex": 19
        },
        "Dan Hao": {
            "authorId": "1477737796",
            "name": "D. Hao",
            "hIndex": 8
        },
        "Mevan Ekanayake": {
            "name": "Mevan Ekanayake",
            "hIndex": 0
        },
        "Sachin Pathiyan Cherumanal": {
            "authorId": "2119713346",
            "name": "Sachin Pathiyan Cherumanal",
            "hIndex": 4
        },
        "Paul Christiano": {
            "authorId": "145791315",
            "name": "P. Christiano",
            "hIndex": 15
        },
        "Zac Wellmer": {
            "authorId": "3438192",
            "name": "Zac Wellmer",
            "hIndex": 2
        },
        "Zhenya Yan": {
            "authorId": "3121233",
            "name": "Zhenya Yan",
            "hIndex": 48
        },
        "Yeonjun In": {
            "authorId": "2161339914",
            "name": "Yeonjun In",
            "hIndex": 3
        },
        "Dazhong Rong": {
            "authorId": "2161339519",
            "name": "Dazhong Rong",
            "hIndex": 3
        },
        "Wanjun Gu": {
            "authorId": "10856816",
            "name": "W. Gu",
            "hIndex": 34
        },
        "Bowen Zhang": {
            "authorId": "51453810",
            "name": "Bowen Zhang",
            "hIndex": 21
        },
        "Deepak Raina": {
            "authorId": "144378331",
            "name": "D. Raina",
            "hIndex": 28
        },
        "Ben Zhu": {
            "authorId": "120251432",
            "name": "Benchao Zhu",
            "hIndex": 8
        },
        "Eren Dogan": {
            "authorId": "40457590",
            "name": "M. Eren",
            "hIndex": 13
        },
        "Yves-Alexandre de Montjoye": {
            "authorId": "7400876",
            "name": "Yves-Alexandre de Montjoye",
            "hIndex": 15
        },
        "Gary Egan": {
            "authorId": "2064565749",
            "name": "Gary F. Egan",
            "hIndex": 13
        },
        "Samir Otmane": {
            "authorId": "2880868",
            "name": "S. Otmane",
            "hIndex": 16
        },
        "Daniel J. Tan": {
            "authorId": "145050877",
            "name": "Daniel J Tan",
            "hIndex": 10
        },
        "Sahil Pocker": {
            "name": "Sahil Pocker",
            "hIndex": 0
        },
        "Shenlong Wang": {
            "authorId": "1892247",
            "name": "Shenlong Wang",
            "hIndex": 32
        },
        "Zilin Bian": {
            "authorId": "152385441",
            "name": "Zilin Bian",
            "hIndex": 8
        },
        "S. Hemati": {
            "authorId": "3673655",
            "name": "S. Hemati",
            "hIndex": 13
        },
        "Makoto Koike": {
            "authorId": "10214348",
            "name": "M. Koike",
            "hIndex": 45
        },
        "Patrick Metzger": {
            "authorId": "145597251",
            "name": "R. Metzger",
            "hIndex": 25
        },
        "Michael Gerovitch": {
            "authorId": "103352265",
            "name": "Michael Gerovitch",
            "hIndex": 2
        },
        "Yanbin Wang": {
            "authorId": "49418459",
            "name": "Yanbin Wang",
            "hIndex": 55
        },
        "Xinv Zhu": {
            "authorId": "31231577",
            "name": "Xinv Zhu",
            "hIndex": 1
        },
        "Afia Zahin": {
            "authorId": "2212346387",
            "name": "Afia Zahin Nita Hossain",
            "hIndex": 1
        },
        "David Eckel": {
            "authorId": "107836733",
            "name": "M. D. Eckel",
            "hIndex": 7
        },
        "Benzi John": {
            "authorId": "90254944",
            "name": "B. John",
            "hIndex": 10
        },
        "Benjamin Streiff": {
            "authorId": "2319814662",
            "name": "Benjamin Streiff",
            "hIndex": 0
        },
        "Tian Lan": {
            "authorId": "144412348",
            "name": "T. Lan",
            "hIndex": 42
        },
        "Michael G\u00fcnther": {
            "authorId": "49946987",
            "name": "M. G\u00fcnther",
            "hIndex": 24
        },
        "Tao Tang": {
            "authorId": "145011373",
            "name": "X. Tao",
            "hIndex": 57
        },
        "Jaros\u0142aw Kwapie\u0144": {
            "name": "Jaros\u0142aw Kwapie\u0144",
            "hIndex": 0
        },
        "Jamil S Samaan": {
            "authorId": "49272315",
            "name": "Jamil S. Samaan",
            "hIndex": 10
        },
        "Luhao Zhang": {
            "authorId": "2156145866",
            "name": "Luhao Zhang",
            "hIndex": 7
        },
        "Qiang Zhang": {
            "authorId": "8326265",
            "name": "Qiangqiang Zhang",
            "hIndex": 16
        },
        "Wenxuan Wang": {
            "name": "Wenxuan Wang",
            "hIndex": 0
        },
        "Ramya Hebbalaguppe": {
            "authorId": "3177394",
            "name": "R. Hebbalaguppe",
            "hIndex": 13
        },
        "Xilin Jiang": {
            "authorId": "2249713137",
            "name": "Xilin Jiang",
            "hIndex": 2
        },
        "Zhifeng Chen": {
            "authorId": "49865068",
            "name": "Zhi-feng Chen",
            "hIndex": 8
        },
        "Yiwen Ding": {
            "authorId": "12944678",
            "name": "Yiwen Ding",
            "hIndex": 6
        },
        "Constantin Waubert de Puiseau": {
            "authorId": "66316655",
            "name": "C. W. D. Puiseau",
            "hIndex": 5
        },
        "Ben Niu": {
            "authorId": "1887661",
            "name": "B. Niu",
            "hIndex": 28
        },
        "Kai Zhao": {
            "authorId": "27005659",
            "name": "Kaile Zhao",
            "hIndex": 14
        },
        "Xiaodong Zhang": {
            "authorId": "49724338",
            "name": "Xiaodong Zhang",
            "hIndex": 43
        },
        "Yejie Wang": {
            "authorId": "2220306008",
            "name": "Yejie Wang",
            "hIndex": 3
        },
        "Shengbang Tong": {
            "authorId": "2143202419",
            "name": "Shengbang Tong",
            "hIndex": 10
        },
        "Tianchi Liu": {
            "authorId": "1776911",
            "name": "Tianchi Liu",
            "hIndex": 14
        },
        "Dipankar Srirag": {
            "authorId": "2046813903",
            "name": "Dipankar Srirag",
            "hIndex": 1
        },
        "Hao Zhu": {
            "authorId": "2115313468",
            "name": "H. Zhu",
            "hIndex": 15
        },
        "Anushka Swarup": {
            "authorId": "80805581",
            "name": "A. Swarup",
            "hIndex": 2
        },
        "Michelle Karg": {
            "authorId": "2326840",
            "name": "M. Karg",
            "hIndex": 15
        },
        "Xiaoyu Li": {
            "authorId": "50291589",
            "name": "L. Xiaoyu",
            "hIndex": 12
        },
        "Ming Zhu": {
            "authorId": "37097461",
            "name": "M. Zhu",
            "hIndex": 15
        },
        "Xiulong Liu": {
            "authorId": "1500522513",
            "name": "Xiulong Liu",
            "hIndex": 24
        },
        "Zhifei Zhang": {
            "authorId": "2109019615",
            "name": "Zhifei Zhang",
            "hIndex": 12
        },
        "Mikhail Borisenkov": {
            "authorId": "3940190",
            "name": "M. Borisenkov",
            "hIndex": 17
        },
        "Yiwei Guo": {
            "name": "Yiwei Guo",
            "hIndex": 0
        },
        "Dayuan Fu": {
            "authorId": "2187926603",
            "name": "Dayuan Fu",
            "hIndex": 4
        },
        "Anoop R Katti": {
            "authorId": "1719068",
            "name": "Anoop R. Katti",
            "hIndex": 3
        },
        "Michael Spannowsky": {
            "authorId": "15740682",
            "name": "M. Spannowsky",
            "hIndex": 57
        },
        "Qingan Li": {
            "authorId": "2108224963",
            "name": "Qing\u2019an Li",
            "hIndex": 20
        },
        "Haoyu Lan": {
            "authorId": "115128202",
            "name": "Haoyu Lan",
            "hIndex": 6
        },
        "Zhexu Wang": {
            "authorId": "2049865064",
            "name": "Zhexuan Wang",
            "hIndex": 4
        },
        "Iris Li": {
            "authorId": "8600699",
            "name": "I. Li",
            "hIndex": 18
        },
        "P. Curvo": {
            "authorId": "3564770",
            "name": "R. Curvo",
            "hIndex": 5
        },
        "Phairot Autthasan": {
            "authorId": "51241491",
            "name": "Phairot Autthasan",
            "hIndex": 7
        },
        "Yan Zhang": {
            "authorId": "2152819217",
            "name": "Yan Zhang",
            "hIndex": 14
        },
        "Sajjad Hossein Zadeh": {
            "authorId": "2319603723",
            "name": "Sajjad Hossein Zadeh",
            "hIndex": 0
        },
        "Zihao Liu": {
            "authorId": "14631359",
            "name": "Zi-Zhang Liu",
            "hIndex": 21
        },
        "Zi Liang": {
            "authorId": "9370610",
            "name": "Zi-ze Liang",
            "hIndex": 22
        },
        "Stanley Uwakwe": {
            "authorId": "121277140",
            "name": "S. Uwakwe",
            "hIndex": 1
        },
        "Graham Johnson": {
            "authorId": "2108603218",
            "name": "G. Johnson",
            "hIndex": 13
        },
        "Yanfeng Zhou": {
            "authorId": "2145925543",
            "name": "Yan-Feng Zhou",
            "hIndex": 10
        },
        "Zhenpeng Chen": {
            "authorId": "115023982",
            "name": "Zhenpeng Chen",
            "hIndex": 18
        },
        "Miao Zhang": {
            "authorId": "4465233",
            "name": "Miaoxin Zhang",
            "hIndex": 14
        },
        "Chengjin Xu": {
            "authorId": "103750594",
            "name": "Chengjin Xu",
            "hIndex": 11
        },
        "Lingqi Jiang": {
            "authorId": "2268069286",
            "name": "Lingqi Jiang",
            "hIndex": 1
        },
        "Xiyuan Zhao": {
            "authorId": "2157217235",
            "name": "Xiyuan Zhao",
            "hIndex": 3
        },
        "Sangyu Han": {
            "authorId": "2283002198",
            "name": "Sangyu Han",
            "hIndex": 1
        },
        "Milan \u0160koln\u00edk": {
            "authorId": "1582068956",
            "name": "M. \u0160koln\u00edk",
            "hIndex": 6
        },
        "Dinesh Kumar Vishwakarma": {
            "authorId": "47731526",
            "name": "D. Vishwakarma",
            "hIndex": 31
        },
        "Arthur W Toga": {
            "authorId": "1699926",
            "name": "A. Toga",
            "hIndex": 172
        },
        "Junjie Hu": {
            "authorId": "145919381",
            "name": "Junjie Hu",
            "hIndex": 25
        },
        "Chen Gong": {
            "authorId": "46879367",
            "name": "Gong-meng Chen",
            "hIndex": 27
        },
        "Ramesh Jain": {
            "authorId": "1621760222",
            "name": "Ramesh C. Jain",
            "hIndex": 7
        },
        "Liqun Yang": {
            "authorId": "49576006",
            "name": "Liqun Yang",
            "hIndex": 23
        },
        "Yibo Miao": {
            "name": "Yibo Miao",
            "hIndex": 0
        },
        "Baobao Chang": {
            "authorId": "102457636",
            "name": "Baobao Chang",
            "hIndex": 28
        },
        "R. Jorge": {
            "authorId": "6487613",
            "name": "R. Jorge",
            "hIndex": 49
        },
        "Linsey Pang": {
            "authorId": "2373834",
            "name": "L. Pang",
            "hIndex": 5
        },
        "Ziyuan Zhuang": {
            "authorId": "2091446947",
            "name": "Ziyuan Zhuang",
            "hIndex": 2
        },
        "Matt Piekenbrock": {
            "authorId": "7428049",
            "name": "Matt Piekenbrock",
            "hIndex": 4
        },
        "Yoji Tomita": {
            "authorId": "104776067",
            "name": "Y. Tomita",
            "hIndex": 2
        },
        "Maria-Florina Balcan": {
            "authorId": "1745410",
            "name": "Maria-Florina Balcan",
            "hIndex": 51
        },
        "Manuel Lecha": {
            "authorId": "150230943",
            "name": "Miguel S\u00e1nchez Caja",
            "hIndex": 34
        },
        "Xiang-Yang Li": {
            "authorId": "48569567",
            "name": "Xiangyang Li",
            "hIndex": 12
        },
        "Chao Gu": {
            "authorId": "30886670",
            "name": "Chaojun Gu",
            "hIndex": 10
        },
        "Michael Bowling": {
            "authorId": "3224955",
            "name": "J. Bowling",
            "hIndex": 23
        },
        "Ce Zhou": {
            "authorId": "7670364",
            "name": "Wence Zhou",
            "hIndex": 21
        },
        "Gaurav Trivedi": {
            "authorId": "145199032",
            "name": "G. Trivedi",
            "hIndex": 11
        },
        "Christian Wolff": {
            "authorId": "46992660",
            "name": "Christian M. Wolff",
            "hIndex": 32
        },
        "Giannis Karamanolakis": {
            "authorId": "8458211",
            "name": "Giannis Karamanolakis",
            "hIndex": 9
        },
        "Veera G. Gude": {
            "authorId": "71591355",
            "name": "V. Gude",
            "hIndex": 45
        },
        "Zenan Zhou": {
            "authorId": "2112445790",
            "name": "Zenan Zhou",
            "hIndex": 4
        },
        "Christopher L Buckley": {
            "authorId": "40555190",
            "name": "C. Buckley",
            "hIndex": 23
        },
        "Gloria M. D\u00edaz": {
            "authorId": "144883900",
            "name": "Gloria M. D\u00edaz",
            "hIndex": 6
        },
        "Dahua Lin": {
            "authorId": "1807606",
            "name": "Dahua Lin",
            "hIndex": 74
        },
        "Gopala Anumanchipalli": {
            "authorId": "1692246",
            "name": "G. Anumanchipalli",
            "hIndex": 17
        },
        "Raghubir Singh": {
            "authorId": "153915813",
            "name": "Raghubir Singh",
            "hIndex": 15
        },
        "Federico Cassano": {
            "authorId": "2197063831",
            "name": "Federico Cassano",
            "hIndex": 6
        },
        "Jiacheng Guo": {
            "authorId": "50115499",
            "name": "JIA-LING Guo",
            "hIndex": 3
        },
        "Dildar Ali": {
            "authorId": "6730635",
            "name": "Dildar Ali",
            "hIndex": 7
        },
        "Yichun Li": {
            "authorId": "2128130710",
            "name": "Yichun Li",
            "hIndex": 7
        },
        "Fons van der Sommen": {
            "authorId": "15065097",
            "name": "F. van der Sommen",
            "hIndex": 13
        },
        "Yang Dang": {
            "authorId": "144485151",
            "name": "Dan Yang",
            "hIndex": 36
        },
        "Qianqian Xu": {
            "authorId": "34679664",
            "name": "Qianqian Xu",
            "hIndex": 21
        },
        "Stefan Kramer": {
            "authorId": "145471896",
            "name": "Stefan Kramer",
            "hIndex": 36
        },
        "Zhenning Li": {
            "authorId": "51194213",
            "name": "Zhenning Li",
            "hIndex": 15
        },
        "Lianxi Wang": {
            "authorId": "2144696566",
            "name": "Lian-xi Wang",
            "hIndex": 5
        },
        "Wen Ye": {
            "authorId": "145888463",
            "name": "Wen Ye",
            "hIndex": 20
        },
        "Adam M. Smith": {
            "authorId": "2109352620",
            "name": "Adam M. Smith",
            "hIndex": 23
        },
        "Wenting Zhao": {
            "authorId": "2954369",
            "name": "Wenting Zhao",
            "hIndex": 8
        },
        "Wenwen Li": {
            "authorId": "50135801",
            "name": "Wenwen Li",
            "hIndex": 8
        },
        "Kun Ho Lee": {
            "authorId": "2151415",
            "name": "K. Lee",
            "hIndex": 23
        },
        "Estefania Serral": {
            "authorId": "1795430",
            "name": "Estefan\u00eda Serral",
            "hIndex": 19
        },
        "Qi Li": {
            "authorId": "8194278",
            "name": "Qi Li",
            "hIndex": 13
        },
        "Mohit Sharma": {
            "authorId": "46320145",
            "name": "Mohit K. Sharma",
            "hIndex": 26
        },
        "Ji Zhao": {
            "authorId": "51262460",
            "name": "Z. Ji",
            "hIndex": 8
        },
        "Jiasheng Shi": {
            "authorId": "24453635",
            "name": "Jiasheng Shi",
            "hIndex": 5
        },
        "Hanspeter Pfister": {
            "authorId": "143758236",
            "name": "H. Pfister",
            "hIndex": 76
        },
        "Rumais Blatrix": {
            "authorId": "2319418272",
            "name": "Rumais Blatrix",
            "hIndex": 0
        },
        "Guanming Huang": {
            "authorId": "2880357",
            "name": "Guan-Ying Huang",
            "hIndex": 14
        },
        "Mohammad Hassan Bagheri": {
            "authorId": "101702463",
            "name": "H. Bagheri",
            "hIndex": 8
        },
        "Wenqian Zhang": {
            "authorId": "2052110383",
            "name": "Zhang Wenqian",
            "hIndex": 4
        },
        "Zhuoma Gongque": {
            "authorId": "2256989532",
            "name": "Zhuoma Gongque",
            "hIndex": 3
        },
        "Lucy Farnik": {
            "authorId": "2247589113",
            "name": "Lucy Farnik",
            "hIndex": 1
        },
        "Sehoon Kim": {
            "authorId": "1455966171",
            "name": "Sehoon Kim",
            "hIndex": 6
        },
        "Bin Fu": {
            "authorId": "34862055",
            "name": "B. Fu",
            "hIndex": 20
        },
        "Baki Uzun": {
            "authorId": "2296043022",
            "name": "Baki Uzun",
            "hIndex": 1
        },
        "Davide Abbattista": {
            "authorId": "2320151706",
            "name": "Davide Abbattista",
            "hIndex": 0
        },
        "Tzu-Quan Lin": {
            "name": "Tzu-Quan Lin",
            "hIndex": 0
        },
        "David S. Bolme": {
            "authorId": "1708802",
            "name": "D. Bolme",
            "hIndex": 24
        },
        "Zhen Chen": {
            "authorId": "1410161734",
            "name": "Z. Chen",
            "hIndex": 27
        },
        "Shreshtha Jha": {
            "authorId": "71641178",
            "name": "Shreshtha Jha",
            "hIndex": 1
        },
        "Sjoerd van Steenkiste": {
            "authorId": "3440930",
            "name": "Sjoerd van Steenkiste",
            "hIndex": 17
        },
        "Jose Hernandez-Orallo": {
            "authorId": "1398777358",
            "name": "J. Hern\u00e1ndez-Orallo",
            "hIndex": 39
        },
        "Weinan Gan": {
            "authorId": "1577238357",
            "name": "Weinan Gan",
            "hIndex": 1
        },
        "Weidong Yang": {
            "authorId": "2109430583",
            "name": "Weidong. Yang",
            "hIndex": 8
        },
        "Rui Miao Li": {
            "authorId": "2150925715",
            "name": "Rui Li",
            "hIndex": 6
        },
        "Zezhong Ding": {
            "authorId": "2291987151",
            "name": "Zezhong Ding",
            "hIndex": 1
        },
        "Shuo Miao": {
            "authorId": "39412421",
            "name": "Shuo Miao",
            "hIndex": 10
        },
        "Yuqi Liu": {
            "authorId": "2155395493",
            "name": "Yuqi Liu",
            "hIndex": 10
        },
        "Zuquan Peng": {
            "authorId": "1515723274",
            "name": "Zuquan Peng",
            "hIndex": 1
        },
        "Haolin Chen": {
            "name": "Haolin Chen",
            "hIndex": 0
        },
        "Chaohao Yang": {
            "authorId": "2110142629",
            "name": "Chao Lu",
            "hIndex": 3
        },
        "Ruofeng Liu": {
            "authorId": "27002736",
            "name": "Ruofeng Liu",
            "hIndex": 9
        },
        "Qianyi Sun": {
            "authorId": "2138109180",
            "name": "Qianying Sun",
            "hIndex": 3
        },
        "Sean O'Brien": {
            "authorId": "2245816023",
            "name": "Sean M. O\u2019Brien",
            "hIndex": 13
        },
        "Ruixin Ni": {
            "authorId": "2319826006",
            "name": "Ruixin Ni",
            "hIndex": 0
        },
        "Tian Gou": {
            "authorId": "103053271",
            "name": "Tian-Xiang Gou",
            "hIndex": 7
        },
        "Nakul Gopalan": {
            "authorId": "2646714",
            "name": "N. Gopalan",
            "hIndex": 15
        },
        "Haowen Xu": {
            "authorId": "2108835763",
            "name": "Hao Xu",
            "hIndex": 8
        },
        "Sirin Botan": {
            "authorId": "46194313",
            "name": "Sirin Botan",
            "hIndex": 5
        },
        "Hilario Medel-L\u00f3pez": {
            "authorId": "1413153248",
            "name": "Hilario Medel-L\u00f3pez",
            "hIndex": 2
        },
        "Yujun Chen": {
            "authorId": "2383039",
            "name": "Yu-Jun Chen",
            "hIndex": 7
        },
        "Marco Antonio M. Carvalho": {
            "authorId": "1402489680",
            "name": "M. A. Carvalho-Filho",
            "hIndex": 11
        },
        "Bhavin Choksi": {
            "authorId": "93165011",
            "name": "Bhavin Choksi",
            "hIndex": 7
        },
        "Wu Ning": {
            "authorId": "49176403",
            "name": "Ningning Wu",
            "hIndex": 14
        },
        "Hideaki Takeda": {
            "authorId": "46966356",
            "name": "Hideaki Takeda",
            "hIndex": 27
        },
        "Aidan Slingsby": {
            "authorId": "1747005",
            "name": "A. Slingsby",
            "hIndex": 22
        },
        "Kaifeng Han": {
            "authorId": "35789570",
            "name": "Kaifeng Han",
            "hIndex": 13
        },
        "Francisco de Arriba-P\u00e9rez": {
            "authorId": "2034282614",
            "name": "Francisco de Arriba-P\u00e9rez",
            "hIndex": 5
        },
        "Zhiping Xiao": {
            "authorId": "1560295029",
            "name": "Zhiping Xiao",
            "hIndex": 13
        },
        "Michael Burnham": {
            "authorId": "34410628",
            "name": "M. Burnham",
            "hIndex": 6
        },
        "Koen Kraaijveld": {
            "authorId": "2215452363",
            "name": "Koen Kraaijveld",
            "hIndex": 0
        },
        "Zitian Gao": {
            "name": "Zitian Gao",
            "hIndex": 0
        },
        "Bertran Miquel-Oliver": {
            "authorId": "1644129650",
            "name": "Bertran Miquel Oliver",
            "hIndex": 0
        },
        "Ethan Chen": {
            "authorId": "2120420318",
            "name": "Y. Li",
            "hIndex": 10
        },
        "Ye Li": {
            "authorId": "49291780",
            "name": "L. Ye",
            "hIndex": 22
        },
        "Dongyang Geng": {
            "authorId": "97841868",
            "name": "D. Geng",
            "hIndex": 2
        },
        "Haoxuan Liu": {
            "authorId": "2143855973",
            "name": "Haoxuan Liu",
            "hIndex": 4
        },
        "Marco Sorbi": {
            "authorId": "2319834514",
            "name": "M. Sorbi",
            "hIndex": 0
        },
        "Ruoding Wang": {
            "authorId": "28198803",
            "name": "Ruoding Wang",
            "hIndex": 14
        },
        "Abdallah Saffidine": {
            "authorId": "1772287",
            "name": "Abdallah Saffidine",
            "hIndex": 14
        },
        "Lichao Sun": {
            "authorId": "49755259",
            "name": "Lichao Sun",
            "hIndex": 28
        },
        "Raymond Lee": {
            "authorId": "144250186",
            "name": "Raymond M. Lee",
            "hIndex": 23
        },
        "Taekyung Ahn": {
            "authorId": "38344320",
            "name": "Tae-Joon Ahn",
            "hIndex": 4
        },
        "Shuai Yu": {
            "authorId": "15941052",
            "name": "Shuaiqin Yu",
            "hIndex": 10
        },
        "Shuo Han": {
            "authorId": "1390542522",
            "name": "Shuo Han",
            "hIndex": 31
        },
        "Christoforos Kachris": {
            "authorId": "145437794",
            "name": "C. Kachris",
            "hIndex": 18
        },
        "Zhaoye Qin": {
            "authorId": "98641345",
            "name": "Zhao-ye Qin",
            "hIndex": 41
        },
        "Atsushi Igarashi": {
            "authorId": "32732801",
            "name": "Atsushi Igarashi",
            "hIndex": 21
        },
        "Seyed Amir Ahmad Safavi-Naini": {
            "authorId": "2115010786",
            "name": "Seyed Amir Ahmad Safavi-Naini",
            "hIndex": 5
        },
        "Tianxing Chen": {
            "authorId": "153489821",
            "name": "Tian\u2010jie Chen",
            "hIndex": 3
        },
        "Bo Tang": {
            "authorId": "145617398",
            "name": "B. Tang",
            "hIndex": 23
        },
        "Anton Ehrmanntraut": {
            "authorId": "2136599373",
            "name": "Anton Ehrmanntraut",
            "hIndex": 2
        },
        "Jiaxiang Geng": {
            "authorId": "2185865153",
            "name": "Jiaxiang Geng",
            "hIndex": 1
        },
        "Jun Zhu": {
            "authorId": "2065024",
            "name": "Jun\u2010Jie Zhu",
            "hIndex": 106
        },
        "Qiao Li": {
            "authorId": "117220403",
            "name": "Qiaoqiao Li",
            "hIndex": 17
        },
        "Peiying Yu": {
            "authorId": "2114104498",
            "name": "Peiying Yu",
            "hIndex": 9
        },
        "Yizhou Zhang": {
            "authorId": "2145055359",
            "name": "Yizhou Zhang",
            "hIndex": 13
        },
        "Maya Srikanth": {
            "authorId": "2060692103",
            "name": "Maya Srikanth",
            "hIndex": 7
        },
        "Lucia Innocenti": {
            "authorId": "145856945",
            "name": "L. Innocenti",
            "hIndex": 3
        },
        "Andrej Risteski": {
            "authorId": "3181040",
            "name": "Andrej Risteski",
            "hIndex": 23
        },
        "Liang Wang": {
            "authorId": "2144690248",
            "name": "L. Wang",
            "hIndex": 6
        },
        "Pengrui Han": {
            "authorId": "2125166410",
            "name": "P. Han",
            "hIndex": 1
        },
        "Jari I. Andersen": {
            "authorId": "2302562380",
            "name": "Jari I. Andersen",
            "hIndex": 1
        },
        "Barrett Martin Lattimer": {
            "authorId": "2186115342",
            "name": "B. Lattimer",
            "hIndex": 1
        },
        "Janis Keuper": {
            "authorId": "3299100",
            "name": "J. Keuper",
            "hIndex": 15
        },
        "Mohammad Sadegh Talebi": {
            "authorId": "47181013",
            "name": "M. S. Talebi",
            "hIndex": 14
        },
        "Sobhan Hemati": {
            "authorId": "5164922",
            "name": "S. Hemati",
            "hIndex": 7
        },
        "Heeyoul Choi": {
            "authorId": "40663606",
            "name": "Heeyoul Choi",
            "hIndex": 16
        },
        "Damianos Karakos": {
            "authorId": "2841922",
            "name": "Damianos G. Karakos",
            "hIndex": 22
        },
        "Esraa Elelimy": {
            "authorId": "2086720736",
            "name": "Esraa Elelimy",
            "hIndex": 2
        },
        "Alessio Burrello": {
            "authorId": "51091982",
            "name": "Alessio Burrello",
            "hIndex": 16
        },
        "Julian Suk": {
            "authorId": "66475114",
            "name": "Julian Suk",
            "hIndex": 5
        },
        "Jinghui Yuan": {
            "authorId": "35464957",
            "name": "Jinghui Yuan",
            "hIndex": 21
        },
        "Wentao Liu": {
            "authorId": "2109178378",
            "name": "Wentao Liu",
            "hIndex": 14
        },
        "Changhyun Kwon": {
            "authorId": "33342770",
            "name": "C. Kwon",
            "hIndex": 25
        },
        "Qingyang Hong": {
            "authorId": "2395909",
            "name": "Q. Hong",
            "hIndex": 12
        },
        "Ali Alfatemi": {
            "authorId": "2311501848",
            "name": "Ali Alfatemi",
            "hIndex": 1
        },
        "Xiang Qu": {
            "authorId": "13847700",
            "name": "Xiang-Long Qu",
            "hIndex": 16
        },
        "Annika Heuser": {
            "authorId": "2264206866",
            "name": "Annika Lea Heuser",
            "hIndex": 1
        },
        "Kavya Manohar": {
            "authorId": "2064971953",
            "name": "K. Manohar",
            "hIndex": 2
        },
        "Zach Johnson": {
            "authorId": "5278544",
            "name": "Zachary V. Johnson",
            "hIndex": 14
        },
        "Kinjal Basu": {
            "authorId": "38428283",
            "name": "Kinjal Basu",
            "hIndex": 13
        },
        "Walid G. Aref": {
            "authorId": "1709661",
            "name": "W. Aref",
            "hIndex": 57
        },
        "Zhuoyan Luo": {
            "name": "Zhuoyan Luo",
            "hIndex": 0
        },
        "Guillaume Desjardins": {
            "authorId": "2755582",
            "name": "Guillaume Desjardins",
            "hIndex": 24
        },
        "A. J. Lopez-Lopez": {
            "authorId": "2216325782",
            "name": "J. Lopez",
            "hIndex": 51
        },
        "Daniel Lehmann": {
            "authorId": "1727595",
            "name": "D. Lehmann",
            "hIndex": 36
        },
        "Hewei Wang": {
            "authorId": "10401086",
            "name": "Hewei Wang",
            "hIndex": 12
        },
        "Katherine Liu": {
            "authorId": "31173412",
            "name": "Katherine J. M. Liu",
            "hIndex": 8
        },
        "Marie-Christine Jakobs": {
            "authorId": "33592809",
            "name": "Marie-Christine Jakobs",
            "hIndex": 7
        },
        "Ivana Be\u0148ov\u00e1": {
            "name": "Ivana Be\u0148ov\u00e1",
            "hIndex": 0
        },
        "Yujiu Yang": {
            "authorId": "3001727",
            "name": "Yujiu Yang",
            "hIndex": 26
        },
        "Kai Liang Tan": {
            "authorId": "2029175318",
            "name": "K. Tan",
            "hIndex": 7
        },
        "Madjid Maidi": {
            "name": "Madjid Maidi",
            "hIndex": 0
        },
        "Tianyu Liu": {
            "authorId": "2115346267",
            "name": "Tianyu Liu",
            "hIndex": 10
        },
        "Zhipeng Bao": {
            "authorId": "103803288",
            "name": "Zhipeng Bao",
            "hIndex": 7
        },
        "Hanqing Guo": {
            "authorId": "7014630",
            "name": "Hanqing Guo",
            "hIndex": 11
        },
        "Li Sun": {
            "authorId": "2110653266",
            "name": "Li-li Sun",
            "hIndex": 9
        },
        "Maria Sofia Bucarelli": {
            "authorId": "2131012031",
            "name": "Maria Sofia Bucarelli",
            "hIndex": 4
        },
        "Gunho Lee": {
            "authorId": "2110794270",
            "name": "Gunho Lee",
            "hIndex": 5
        },
        "Jianfei Cai": {
            "authorId": "2152629962",
            "name": "Jianfei Cai",
            "hIndex": 13
        },
        "Francesco Papaleo": {
            "authorId": "2725598",
            "name": "F. Papaleo",
            "hIndex": 34
        },
        "Sinan Du": {
            "authorId": "2330663",
            "name": "Sinan Du",
            "hIndex": 9
        },
        "Lorenzo Pacchiardi": {
            "authorId": "104481131",
            "name": "Lorenzo Pacchiardi",
            "hIndex": 7
        },
        "Liang He": {
            "authorId": "144002876",
            "name": "Liang He",
            "hIndex": 46
        },
        "Yujia Tian": {
            "authorId": "8193658",
            "name": "Yuji Tian",
            "hIndex": 8
        },
        "Pranay Jiljith T": {
            "name": "Pranay Jiljith T",
            "hIndex": 0
        },
        "Wenxiao Zhang": {
            "authorId": "2108144395",
            "name": "Wenxia Zhang",
            "hIndex": 9
        },
        "Yicheng Fu": {
            "authorId": "46956473",
            "name": "Yicheng Fu",
            "hIndex": 12
        },
        "Rajarshi Ghosh": {
            "authorId": "33130854",
            "name": "R. Ghosh",
            "hIndex": 34
        },
        "Buzhou Tang": {
            "authorId": "2987453",
            "name": "Buzhou Tang",
            "hIndex": 35
        },
        "Huawei Sun": {
            "authorId": "8961302",
            "name": "Huawei Sun",
            "hIndex": 5
        },
        "Lifeng Shang": {
            "authorId": "50812138",
            "name": "Lifeng Shang",
            "hIndex": 31
        },
        "Zhuohan Wang": {
            "authorId": "1390877172",
            "name": "Zhuohan Wang",
            "hIndex": 4
        },
        "Cheng Peng": {
            "authorId": "49471528",
            "name": "Peng-Fei Cheng",
            "hIndex": 11
        },
        "Mengyuan Chen": {
            "authorId": "50133500",
            "name": "Mengyuan Chen",
            "hIndex": 6
        },
        "Jianbing Ni": {
            "authorId": "39684105",
            "name": "Jianbing Ni",
            "hIndex": 36
        },
        "Yanfeng Lv": {
            "authorId": "95947897",
            "name": "Yan-Ran Lv",
            "hIndex": 14
        },
        "Jiayin Zhao": {
            "authorId": "49530936",
            "name": "Jia-ying Zhao",
            "hIndex": 6
        },
        "Peter de With": {
            "authorId": "8582219",
            "name": "P. D. With",
            "hIndex": 35
        },
        "Ken Satoh": {
            "authorId": "152861079",
            "name": "K. Satoh",
            "hIndex": 10
        },
        "Naama Zwerdling": {
            "authorId": "2652616",
            "name": "Naama Zwerdling",
            "hIndex": 13
        },
        "Roberto Cipollone": {
            "authorId": "34857579",
            "name": "R. Cipollone",
            "hIndex": 26
        },
        "HanQin Cai": {
            "authorId": "30104645",
            "name": "HanQin Cai",
            "hIndex": 10
        },
        "Jingyi Zhao": {
            "authorId": "2145802221",
            "name": "Jingyi Zhao",
            "hIndex": 5
        },
        "Mohammad Amin Khalafi": {
            "authorId": "116878230",
            "name": "M. Khalafi",
            "hIndex": 2
        },
        "Shikhar Bharadwaj": {
            "authorId": "102937984",
            "name": "S. Bharadwaj",
            "hIndex": 4
        },
        "Adrian Cardenas": {
            "authorId": "84733677",
            "name": "A. Bonilla",
            "hIndex": 14
        },
        "Bla\u017e \u0160krlj": {
            "authorId": "7746943",
            "name": "Bla\u017e \u0160krlj",
            "hIndex": 17
        },
        "Cornelia Caragea": {
            "authorId": "1690656",
            "name": "Cornelia Caragea",
            "hIndex": 37
        },
        "Woody Zhidong Zhang": {
            "name": "Woody Zhidong Zhang",
            "hIndex": 0
        },
        "Dan Sun": {
            "authorId": "49200671",
            "name": "Dan Sun",
            "hIndex": 37
        },
        "Christian Rupprecht": {
            "authorId": "49359942",
            "name": "C. Rupprecht",
            "hIndex": 32
        },
        "Inacio Vieira": {
            "authorId": "115206922",
            "name": "A. D. Silva",
            "hIndex": 7
        },
        "Maosong Sun": {
            "authorId": "1753344",
            "name": "Maosong Sun",
            "hIndex": 87
        },
        "Subhamoy Mandal": {
            "authorId": "2724058",
            "name": "S. Mandal",
            "hIndex": 13
        },
        "Dashanka De Silva": {
            "name": "Dashanka De Silva",
            "hIndex": 0
        },
        "Micka\u00ebl Coustaty": {
            "authorId": "1732746",
            "name": "Micka\u00ebl Coustaty",
            "hIndex": 19
        },
        "Bahador Saket": {
            "authorId": "3288410",
            "name": "B. Saket",
            "hIndex": 15
        },
        "Xuan Yan": {
            "authorId": "12595898",
            "name": "Yanxuan Wen",
            "hIndex": 19
        },
        "Fan Wu": {
            "authorId": "10099390",
            "name": "Fann Wu",
            "hIndex": 23
        },
        "Pengfei Gao": {
            "authorId": "144579862",
            "name": "Peng Gao",
            "hIndex": 17
        },
        "Jia Zheng": {
            "authorId": "145655636",
            "name": "J. Zheng",
            "hIndex": 8
        },
        "Julian Dolby": {
            "authorId": "145989430",
            "name": "Julian T Dolby",
            "hIndex": 36
        },
        "Kai-Wei Chang": {
            "authorId": "2782886",
            "name": "Kai-Wei Chang",
            "hIndex": 57
        },
        "Sreedevi Indu": {
            "authorId": "48891880",
            "name": "I. Sreedevi",
            "hIndex": 10
        },
        "Hui-Kuo Shu": {
            "authorId": "144736862",
            "name": "H. Shu",
            "hIndex": 37
        },
        "Shuaishuai Huang": {
            "authorId": "4174616",
            "name": "Shuaishuai Huang",
            "hIndex": 15
        },
        "Yang Wang": {
            "authorId": "2146021066",
            "name": "Yangyang Wang",
            "hIndex": 15
        },
        "Renqiu Xia": {
            "authorId": "2239108883",
            "name": "Renqiu Xia",
            "hIndex": 3
        },
        "Baochao Chen": {
            "authorId": "144010725",
            "name": "B. Hu",
            "hIndex": 50
        },
        "Elena Ortega-Beltr\u00e1n": {
            "name": "Elena Ortega-Beltr\u00e1n",
            "hIndex": 0
        },
        "Jeffrey Xu Yu": {
            "authorId": "144666776",
            "name": "J. Yu",
            "hIndex": 77
        },
        "Yaru Fu": {
            "authorId": "40497879",
            "name": "Yaru Fu",
            "hIndex": 20
        },
        "Neo Putini": {
            "authorId": "2216798369",
            "name": "Neo Putini",
            "hIndex": 1
        },
        "Zoran Majkic": {
            "authorId": "1700417",
            "name": "Z. Majkic",
            "hIndex": 11
        },
        "Saurabh Sonawani": {
            "authorId": "2319601679",
            "name": "Saurabh Sonawani",
            "hIndex": 0
        },
        "Luis Lastras": {
            "authorId": "1398849383",
            "name": "L. A. Lastras-Monta\u00f1o",
            "hIndex": 14
        },
        "Tianxu Liu": {
            "authorId": "2115346776",
            "name": "Tianxu Liu",
            "hIndex": 8
        },
        "Guangxu Zhu": {
            "name": "Guangxu Zhu",
            "hIndex": 0
        },
        "Zhenzhen Ma": {
            "authorId": "8763608",
            "name": "Zhenzhen Ma",
            "hIndex": 10
        },
        "Markus Wulfmeier": {
            "authorId": "3331786",
            "name": "Markus Wulfmeier",
            "hIndex": 21
        },
        "Yichen Zhao": {
            "authorId": "2034348413",
            "name": "Yicheng Zhao",
            "hIndex": 42
        },
        "Zhongyi Xia": {
            "authorId": "92775078",
            "name": "Z. Xia",
            "hIndex": 3
        },
        "Ruihong Zeng": {
            "authorId": "40109623",
            "name": "Ruihong Zeng",
            "hIndex": 11
        },
        "Jinhee Kim": {
            "name": "Jinhee Kim",
            "hIndex": 0
        },
        "Hu Huang": {
            "authorId": "7690057",
            "name": "Zhihuang Hu",
            "hIndex": 25
        },
        "Hansen Lillemark": {
            "name": "Hansen Lillemark",
            "hIndex": 0
        },
        "Wei Xu": {
            "authorId": "145738412",
            "name": "W. Xu",
            "hIndex": 68
        },
        "Liang Sun": {
            "authorId": "2000287940",
            "name": "Liangliang Sun",
            "hIndex": 13
        },
        "Shuo Yang": {
            "authorId": "2131876427",
            "name": "Shuo Yang",
            "hIndex": 13
        },
        "Rui Zeng": {
            "authorId": "145602046",
            "name": "R. Zeng",
            "hIndex": 26
        },
        "Aixin Sun": {
            "authorId": "1735962",
            "name": "Aixin Sun",
            "hIndex": 58
        },
        "Bhakti Baheti": {
            "authorId": "9273624",
            "name": "B. Baheti",
            "hIndex": 12
        },
        "Shuang Yang": {
            "authorId": "117785406",
            "name": "Shuang\u2010Ning Yang",
            "hIndex": 6
        },
        "Peng Di": {
            "authorId": "38802271",
            "name": "D. Peng",
            "hIndex": 23
        },
        "Sunkyung Lee": {
            "authorId": "2108302382",
            "name": "Sunkyung Lee",
            "hIndex": 8
        },
        "Renshuai Tao": {
            "authorId": "1380391852",
            "name": "Renshuai Tao",
            "hIndex": 9
        },
        "Miao Fan": {
            "authorId": "34843668",
            "name": "M. Fan",
            "hIndex": 18
        },
        "Taufiq Hasan": {
            "authorId": "144782474",
            "name": "T. Hasan",
            "hIndex": 20
        },
        "Tobias Meisen": {
            "authorId": "2661383",
            "name": "Tobias Meisen",
            "hIndex": 20
        },
        "Zhenguang Liu": {
            "authorId": "47780862",
            "name": "Zhenguang Liu",
            "hIndex": 25
        },
        "Lin Li": {
            "authorId": "93052925",
            "name": "Li Lin",
            "hIndex": 9
        },
        "Mark E. Tuckerman": {
            "authorId": "2616915",
            "name": "M. Tuckerman",
            "hIndex": 70
        },
        "Matthew R. Guthaus": {
            "name": "Matthew R. Guthaus",
            "hIndex": 0
        },
        "Clark Barrett": {
            "authorId": "1680661",
            "name": "Clark W. Barrett",
            "hIndex": 44
        },
        "Dayiheng Liu": {
            "authorId": "7415571",
            "name": "Dayiheng Liu",
            "hIndex": 16
        },
        "Kazuya Matsuo": {
            "authorId": "71238349",
            "name": "K. Matsuo",
            "hIndex": 21
        },
        "Michael Backes": {
            "authorId": "144588806",
            "name": "M. Backes",
            "hIndex": 67
        },
        "Yishi Xu": {
            "authorId": "2110598147",
            "name": "Yishi Xu",
            "hIndex": 9
        },
        "Yifan Yang": {
            "authorId": "2108989077",
            "name": "Yifan Yang",
            "hIndex": 14
        },
        "Hang Yan": {
            "authorId": "50214869",
            "name": "Y. Hang",
            "hIndex": 22
        },
        "Rahul Yumlembam": {
            "authorId": "115348240",
            "name": "Yumlembam Rahul",
            "hIndex": 2
        },
        "Qingxiu Dong": {
            "authorId": "2047143813",
            "name": "Qingxiu Dong",
            "hIndex": 12
        },
        "Fei Xia": {
            "authorId": "145205810",
            "name": "Yun-Fei Xia",
            "hIndex": 29
        },
        "Michael Gienger": {
            "authorId": "1713430",
            "name": "M. Gienger",
            "hIndex": 37
        },
        "Mario Haim": {
            "authorId": "46693800",
            "name": "Mario Haim",
            "hIndex": 13
        },
        "Zhongxia Yan": {
            "authorId": "1390567306",
            "name": "Zhongxia Yan",
            "hIndex": 4
        },
        "Ricardo Knauer": {
            "authorId": "2159001563",
            "name": "Ricardo Knauer",
            "hIndex": 1
        },
        "Jianhai Chen": {
            "authorId": "1481023980",
            "name": "Jian-hai Chen",
            "hIndex": 15
        },
        "Jiayin Lin": {
            "authorId": "17700613",
            "name": "Jiayin Lin",
            "hIndex": 7
        },
        "Wen Xiao": {
            "authorId": "3323875",
            "name": "Wenmin Xiao",
            "hIndex": 11
        },
        "Minho Lee": {
            "authorId": "2109503276",
            "name": "Minho Lee",
            "hIndex": 6
        },
        "Zhiming Zheng": {
            "authorId": "15175192",
            "name": "Zhimin Zheng",
            "hIndex": 14
        },
        "Kai Yu": {
            "authorId": "8599360",
            "name": "Kai-fun Yu",
            "hIndex": 26
        },
        "Qinsheng Zhang": {
            "authorId": "94615164",
            "name": "Qinsheng Zhang",
            "hIndex": 11
        },
        "Mari Ostendorf": {
            "authorId": "144339506",
            "name": "Mari Ostendorf",
            "hIndex": 61
        },
        "Yiming Chen": {
            "authorId": "50636895",
            "name": "C. Yiming",
            "hIndex": 6
        },
        "Jo\u00e3o Sedoc": {
            "authorId": "2662374",
            "name": "Jo\u00e3o Sedoc",
            "hIndex": 19
        },
        "Kyuree Ahn": {
            "authorId": "1560394962",
            "name": "Kyuree Ahn",
            "hIndex": 4
        },
        "Anningzhe Gao": {
            "authorId": "2266840612",
            "name": "Anningzhe Gao",
            "hIndex": 4
        },
        "Xinhao Deng": {
            "authorId": "152508462",
            "name": "Xinhao Deng",
            "hIndex": 5
        },
        "Martin Carrasco": {
            "authorId": "1403914388",
            "name": "F. Mart\u00edn-Carrasco",
            "hIndex": 13
        },
        "Zhengying Liu": {
            "authorId": "2239065052",
            "name": "Zhengying Liu",
            "hIndex": 8
        },
        "Junjie Zhao": {
            "authorId": "91156924",
            "name": "Junjie Zhao",
            "hIndex": 25
        },
        "Omid Yazdani": {
            "name": "Omid Yazdani",
            "hIndex": 0
        },
        "Cooper Stansbury": {
            "authorId": "66507630",
            "name": "Cooper M. Stansbury",
            "hIndex": 4
        },
        "Sebastian Lange": {
            "authorId": "2052367203",
            "name": "S. Lange",
            "hIndex": 15
        },
        "N. D. Kodikara": {
            "authorId": "144436434",
            "name": "N. Kodikara",
            "hIndex": 8
        },
        "Khanh Quoc Tran": {
            "authorId": "36745781",
            "name": "T. Khanh",
            "hIndex": 12
        },
        "Jiwon Chang": {
            "authorId": "2153054832",
            "name": "J. Chang",
            "hIndex": 3
        },
        "Shirley Kokane": {
            "authorId": "2201216882",
            "name": "Shirley Kokane",
            "hIndex": 1
        },
        "Yang Xu": {
            "authorId": "3324943",
            "name": "Xuhai Yang",
            "hIndex": 14
        },
        "Ying Li": {
            "authorId": "2111233314",
            "name": "Yingying Li",
            "hIndex": 14
        },
        "Ling Liu": {
            "authorId": "2146021651",
            "name": "Lingyi Liu",
            "hIndex": 8
        },
        "Tal Linzen": {
            "authorId": "2467508",
            "name": "Tal Linzen",
            "hIndex": 37
        },
        "Xiaodong Cun": {
            "authorId": "30176430",
            "name": "Xiaodong Cun",
            "hIndex": 24
        },
        "Yongna Guo": {
            "authorId": "2115274423",
            "name": "Yong Guo",
            "hIndex": 3
        },
        "Junchi Yan": {
            "authorId": "8247516",
            "name": "Jun Yan",
            "hIndex": 11
        },
        "Prathosh AP": {
            "authorId": "2988091",
            "name": "A. Prathosh",
            "hIndex": 11
        },
        "Yunhui Liu": {
            "authorId": "145594296",
            "name": "Yun-hui Liu",
            "hIndex": 24
        },
        "Pang Wei Koh": {
            "authorId": "2572525",
            "name": "Pang Wei Koh",
            "hIndex": 28
        },
        "Xiaoran Chen": {
            "authorId": "1425073732",
            "name": "Xiao-Ran Chen",
            "hIndex": 3
        },
        "Prakash Chourasia": {
            "authorId": "2296408573",
            "name": "Devdutt Singh",
            "hIndex": 5
        },
        "Nikoletta Koilia": {
            "authorId": "2319824936",
            "name": "Nikoletta Koilia",
            "hIndex": 0
        },
        "Harrison Lee": {
            "authorId": "2368539",
            "name": "L. Harrison",
            "hIndex": 89
        },
        "Daniel J\u00f6nsson": {
            "authorId": "48000274",
            "name": "D. J\u00f6nsson",
            "hIndex": 17
        },
        "Eleftheria Astrenidou": {
            "authorId": "2319538175",
            "name": "Eleftheria Astrenidou",
            "hIndex": 0
        },
        "Tatsuya Kawahara": {
            "authorId": "1717105",
            "name": "Tatsuya Kawahara",
            "hIndex": 41
        },
        "Lindsey Roberts": {
            "authorId": "144478793",
            "name": "L. Roberts",
            "hIndex": 7
        },
        "Kaichen Ni": {
            "authorId": "2319821678",
            "name": "Kaichen Ni",
            "hIndex": 0
        },
        "Lizhen Qu": {
            "authorId": "14564042",
            "name": "Lizhen Qu",
            "hIndex": 23
        },
        "Amit Ben Artzy": {
            "authorId": "2319833371",
            "name": "Amit Ben Artzy",
            "hIndex": 0
        },
        "Jie Fu": {
            "authorId": "145016605",
            "name": "Jie Fu",
            "hIndex": 43
        },
        "Ulrich Neumann": {
            "authorId": "143840663",
            "name": "U. Neumann",
            "hIndex": 62
        },
        "Natalia Zhang": {
            "authorId": "147723262",
            "name": "N. Zhang",
            "hIndex": 0
        },
        "Kay Choong See": {
            "authorId": "4822484",
            "name": "K. See",
            "hIndex": 25
        },
        "Ruofeng Wei": {
            "authorId": "2051285958",
            "name": "Ruofeng Wei",
            "hIndex": 5
        },
        "Aythami Morales": {
            "authorId": "144083995",
            "name": "A. Morales",
            "hIndex": 35
        },
        "Rodrigo P\u00e9rez-Dattari": {
            "authorId": "1405220630",
            "name": "Rodrigo P\u00e9rez-Dattari",
            "hIndex": 5
        },
        "Alexis Molina": {
            "authorId": "1441402754",
            "name": "D. Molina",
            "hIndex": 4
        },
        "Lingrui Li": {
            "authorId": "2145674104",
            "name": "Lingrui Li",
            "hIndex": 10
        },
        "Jie Zhou": {
            "authorId": "89272710",
            "name": "Jie Zhou",
            "hIndex": 17
        },
        "Chanjun Park": {
            "authorId": "1399879763",
            "name": "Chan Eon Park",
            "hIndex": 48
        },
        "Sicheng Zhu": {
            "authorId": "2110018762",
            "name": "Sicheng Zhu",
            "hIndex": 7
        },
        "Muhammad Arbab Arshad": {
            "authorId": "1625511888",
            "name": "Muhammad Arbab Arshad",
            "hIndex": 3
        },
        "Liza Van der Laan": {
            "authorId": "2124507972",
            "name": "Liza Van der Laan",
            "hIndex": 2
        },
        "Diyi Yang": {
            "authorId": "2022168",
            "name": "Diyi Yang",
            "hIndex": 38
        },
        "Zichao Li": {
            "authorId": "2118272706",
            "name": "Zichao Li",
            "hIndex": 36
        },
        "Yukun Cao": {
            "authorId": "2112823208",
            "name": "Yukun Cao",
            "hIndex": 5
        },
        "Josep Cabacas-Maso": {
            "authorId": "2311699609",
            "name": "Josep Cabacas-Maso",
            "hIndex": 0
        },
        "W\u0142odzimierz Lewoniewski": {
            "authorId": "2513991",
            "name": "W\u0142odzimierz Lewoniewski",
            "hIndex": 10
        },
        "Jos\u00e9 Hern\u00e1ndez-Orallo": {
            "authorId": "1398777358",
            "name": "J. Hern\u00e1ndez-Orallo",
            "hIndex": 39
        },
        "Rajat Dandekar": {
            "authorId": "93400894",
            "name": "Raj Dandekar",
            "hIndex": 6
        },
        "Alireza Soheilipour": {
            "authorId": "2265570945",
            "name": "Alireza Soheilipour",
            "hIndex": 0
        },
        "Kaixin Wang": {
            "authorId": "2726825",
            "name": "Kaixin Wang",
            "hIndex": 13
        },
        "Zeyu Zhang": {
            "authorId": "50317109",
            "name": "Z. Zhang",
            "hIndex": 5
        },
        "Andy Way": {
            "authorId": "144315616",
            "name": "Andy Way",
            "hIndex": 46
        },
        "Defu Cao": {
            "authorId": "120783624",
            "name": "Defu Cao",
            "hIndex": 9
        },
        "Steven L. Brunton": {
            "authorId": "3083169",
            "name": "S. Brunton",
            "hIndex": 65
        },
        "Thomas Winters": {
            "authorId": "40664628",
            "name": "T. Winters",
            "hIndex": 18
        },
        "Tanzima Z. Islam": {
            "authorId": "2219526",
            "name": "T. Islam",
            "hIndex": 10
        },
        "Jiaheng Liu": {
            "authorId": "2182423032",
            "name": "Jiaheng Liu",
            "hIndex": 6
        },
        "Marcel Zalmanovici": {
            "authorId": "3109799",
            "name": "Marcel Zalmanovici",
            "hIndex": 7
        },
        "Roy Schwartz": {
            "authorId": "4671928",
            "name": "Roy Schwartz",
            "hIndex": 28
        },
        "Zuxin Liu": {
            "authorId": "76923918",
            "name": "Zuxin Liu",
            "hIndex": 15
        },
        "Lefei Zhang": {
            "authorId": "2107901992",
            "name": "Lefei Zhang",
            "hIndex": 46
        },
        "Yun Young Choi": {
            "authorId": "103278324",
            "name": "Yun-Young Choi",
            "hIndex": 24
        },
        "Danqing Liu": {
            "authorId": "66953953",
            "name": "Dan-Qing Liu",
            "hIndex": 18
        },
        "Hanlin Wang": {
            "authorId": "1737107140",
            "name": "Hanlin L. Wang",
            "hIndex": 44
        },
        "Xavier Lizarraga-Seijas": {
            "authorId": "2216573655",
            "name": "Xavier Lizarraga-Seijas",
            "hIndex": 2
        },
        "Erdi Gao": {
            "authorId": "2306199604",
            "name": "Erdi Gao",
            "hIndex": 3
        },
        "S. M. Sakeef Sani": {
            "authorId": "2066967276",
            "name": "S. Sani",
            "hIndex": 3
        },
        "Xingshan Zeng": {
            "authorId": "46180553",
            "name": "Xingshan Zeng",
            "hIndex": 11
        },
        "Jamshaid Ul Rahman": {
            "authorId": "98293086",
            "name": "J. Rahman",
            "hIndex": 8
        },
        "Vikash Kumar": {
            "authorId": "74610855",
            "name": "K. Vikash",
            "hIndex": 5
        },
        "Nusrat Binta Nizam": {
            "authorId": "2042628656",
            "name": "N. Nizam",
            "hIndex": 3
        },
        "Tong Wang": {
            "authorId": "1919402049",
            "name": "Tong Wang",
            "hIndex": 31
        },
        "Huafeng Qin": {
            "authorId": "2869289",
            "name": "Huafeng Qin",
            "hIndex": 15
        },
        "Longfei Liu": {
            "authorId": "2116179332",
            "name": "Long-fei Liu",
            "hIndex": 6
        },
        "Yude Zou": {
            "authorId": "2320101926",
            "name": "Yude Zou",
            "hIndex": 0
        },
        "Milena Str\u00f3\u017cyna": {
            "authorId": "3420296",
            "name": "Milena Str\u00f3\u017cyna",
            "hIndex": 6
        },
        "Mohsen Shirali": {
            "authorId": "66325175",
            "name": "Mohsen Shirali",
            "hIndex": 4
        },
        "Diletta Goglia": {
            "authorId": "2236945857",
            "name": "Diletta Goglia",
            "hIndex": 1
        },
        "Shiyu Jin": {
            "authorId": "11595465",
            "name": "Shiyu Jin",
            "hIndex": 11
        },
        "Zhi-Qi Cheng": {
            "authorId": "3493929",
            "name": "Zhi-Qi Cheng",
            "hIndex": 20
        },
        "Kenichiro Takaba": {
            "name": "Kenichiro Takaba",
            "hIndex": 0
        },
        "Silviu Pitis": {
            "authorId": "32305445",
            "name": "Silviu Pitis",
            "hIndex": 11
        },
        "Roman Smirnov": {
            "name": "Roman Smirnov",
            "hIndex": 0
        },
        "Ian Harris": {
            "authorId": "144134176",
            "name": "I. Harris",
            "hIndex": 21
        },
        "Soumik Sarkar": {
            "authorId": "144016196",
            "name": "S. Sarkar",
            "hIndex": 39
        },
        "Joann Qiongna Chen": {
            "authorId": "84681047",
            "name": "Joann Qiongna Chen",
            "hIndex": 2
        },
        "Suyuchen Wang": {
            "authorId": "1643451565",
            "name": "Suyuchen Wang",
            "hIndex": 3
        },
        "Jin Wang": {
            "authorId": "2145296021",
            "name": "Jinjin Wang",
            "hIndex": 7
        },
        "Rafael M. Martins": {
            "authorId": "35054228",
            "name": "R. M. Martins",
            "hIndex": 19
        },
        "German Magai": {
            "authorId": "2162837414",
            "name": "German Magai",
            "hIndex": 3
        },
        "Jue Wang": {
            "authorId": "2144536506",
            "name": "Jue Wang",
            "hIndex": 12
        },
        "Ali Lotfi": {
            "authorId": "145877336",
            "name": "A. Lotfi",
            "hIndex": 13
        },
        "Aleksandra Wojewoda": {
            "authorId": "1421600755",
            "name": "A. Grela-Wojewoda",
            "hIndex": 8
        },
        "Ahmad Aiashy": {
            "authorId": "2319417201",
            "name": "Ahmad Aiashy",
            "hIndex": 0
        },
        "Ahana Deb": {
            "authorId": "2219552121",
            "name": "Ahana Deb",
            "hIndex": 1
        },
        "Yunyao Li": {
            "authorId": "1718694",
            "name": "Yunyao Li",
            "hIndex": 27
        },
        "Wenjie Li": {
            "authorId": "2005458334",
            "name": "Liu Wenjie",
            "hIndex": 6
        },
        "Chunbin Gu": {
            "authorId": "51039718",
            "name": "Chunbin Gu",
            "hIndex": 3
        },
        "Thibaud Levrard": {
            "authorId": "3824347",
            "name": "Thibaud Levrard",
            "hIndex": 3
        },
        "Mark Hasegawa-Johnson": {
            "authorId": "1399115926",
            "name": "M. Hasegawa-Johnson",
            "hIndex": 41
        },
        "Zhixun Li": {
            "authorId": "1866307607",
            "name": "Zhixun Li",
            "hIndex": 5
        },
        "Manman Wang": {
            "authorId": "66517707",
            "name": "Wang Manman",
            "hIndex": 6
        },
        "Tariq M Khan": {
            "name": "Tariq M Khan",
            "hIndex": 0
        },
        "Robin Jia": {
            "authorId": "3422908",
            "name": "Robin Jia",
            "hIndex": 24
        },
        "Jingyi Yu": {
            "authorId": "2152356",
            "name": "Jingyi Yu",
            "hIndex": 45
        },
        "Jianbin Jiao": {
            "authorId": "24350293",
            "name": "Jianbin Jiao",
            "hIndex": 35
        },
        "Henrik Wachowitz": {
            "authorId": "2223134087",
            "name": "Henrik Wachowitz",
            "hIndex": 1
        },
        "Xianghua Fu": {
            "authorId": "7780595",
            "name": "Xianghua Fu",
            "hIndex": 19
        },
        "Mohanna Hoveyda": {
            "authorId": "2315624030",
            "name": "Mohanna Hoveyda",
            "hIndex": 0
        },
        "Yanfeng Lu": {
            "authorId": "47006406",
            "name": "Yanfeng Lu",
            "hIndex": 16
        },
        "Tulika Awalgaonkar": {
            "authorId": "2047548223",
            "name": "Tulika Awalgaonkar",
            "hIndex": 2
        },
        "Liehuang Zhu": {
            "authorId": "1692039",
            "name": "Liehuang Zhu",
            "hIndex": 45
        },
        "Yusung Sim": {
            "authorId": "2150711782",
            "name": "Yusung Sim",
            "hIndex": 1
        },
        "Jiaxin Ma": {
            "authorId": "94532742",
            "name": "Jia-ping Ma",
            "hIndex": 7
        },
        "DongNyeong Heo": {
            "authorId": "1941953949",
            "name": "DongNyeong Heo",
            "hIndex": 6
        },
        "Nidhi Bhatt": {
            "authorId": "49332196",
            "name": "Nidhi Bhatt",
            "hIndex": 7
        },
        "Giorgia Adorni": {
            "authorId": "2105604423",
            "name": "Giorgia Adorni",
            "hIndex": 2
        },
        "Yifei Chen": {
            "authorId": "2109142024",
            "name": "Yifei Chen",
            "hIndex": 5
        },
        "Gang Li": {
            "authorId": "46439393",
            "name": "Gang-feng Li",
            "hIndex": 7
        },
        "Itzik Klein": {
            "authorId": "38292217",
            "name": "I. Klein",
            "hIndex": 17
        },
        "Pierrick Leroy": {
            "name": "Pierrick Leroy",
            "hIndex": 0
        },
        "Jakob Nikolas Kather": {
            "authorId": "3955759",
            "name": "Jakob Nikolas Kather",
            "hIndex": 43
        },
        "Zhihu Wang": {
            "authorId": "47196768",
            "name": "Zhihu Wang",
            "hIndex": 6
        },
        "Phuc-Tinh Pham Do": {
            "authorId": "2282523337",
            "name": "Tinh Pham Phuc Do",
            "hIndex": 1
        },
        "Qingming Huang": {
            "authorId": "2111526569",
            "name": "Qin Huang",
            "hIndex": 6
        },
        "Zhe Cao": {
            "authorId": "47060593",
            "name": "Z. Cao",
            "hIndex": 11
        },
        "Xavier Serra": {
            "authorId": "2004265380",
            "name": "X. Serra\u2010Maluquer",
            "hIndex": 10
        },
        "Amirali Mohsenzadeh-Kermani": {
            "authorId": "2319605431",
            "name": "Amirali Mohsenzadeh-Kermani",
            "hIndex": 0
        },
        "Wenjun Chen": {
            "authorId": "2190906053",
            "name": "Wen-Jia Chen",
            "hIndex": 8
        },
        "Ingo Ziegler": {
            "authorId": "2188733054",
            "name": "Ingo Ziegler",
            "hIndex": 0
        },
        "Tomohiki Yokoyama": {
            "authorId": "2319347757",
            "name": "Tomohiki Yokoyama",
            "hIndex": 0
        },
        "Olga Fink": {
            "authorId": "2319401168",
            "name": "Olga Fink",
            "hIndex": 8
        },
        "Yuan He": {
            "authorId": "3083525",
            "name": "Yuan-jun He",
            "hIndex": 7
        },
        "Xuesong Niu": {
            "authorId": "35649732",
            "name": "Xuesong Niu",
            "hIndex": 13
        },
        "Xuechun Wang": {
            "authorId": "1391276933",
            "name": "Wang Xuechun",
            "hIndex": 8
        },
        "Junfeng Yang": {
            "authorId": "47988283",
            "name": "Junfeng Yang",
            "hIndex": 6
        },
        "Jun Xie": {
            "authorId": "1713418",
            "name": "Junfeng Xie",
            "hIndex": 11
        },
        "Kwangyoun Kim": {
            "authorId": "7826646",
            "name": "Kwangyoun Kim",
            "hIndex": 11
        },
        "Huimin Chen": {
            "authorId": "2155551419",
            "name": "Hui-Min Chen",
            "hIndex": 9
        },
        "Yuanda Wang": {
            "authorId": "7708611",
            "name": "Yuan-da Wang",
            "hIndex": 12
        },
        "Hai Ye": {
            "authorId": "11424584",
            "name": "Haimu Ye",
            "hIndex": 25
        },
        "Sungho Won": {
            "authorId": "2911201",
            "name": "Sungho Won",
            "hIndex": 32
        },
        "Yuxian Wang": {
            "authorId": "47903719",
            "name": "Yuxian Wang",
            "hIndex": 36
        },
        "Zhuoming Chen": {
            "authorId": "153267755",
            "name": "Zhuoming Chen",
            "hIndex": 11
        },
        "Shuaishuai Zhou": {
            "authorId": "94425029",
            "name": "Shuaishuai Zhou",
            "hIndex": 13
        },
        "Am\u00e9lie Gheerbrant": {
            "authorId": "2979582",
            "name": "Am\u00e9lie Gheerbrant",
            "hIndex": 10
        },
        "Mayank Mishra": {
            "authorId": "2055279298",
            "name": "M. Mishra",
            "hIndex": 18
        },
        "Junhui He": {
            "authorId": "6847127",
            "name": "Junhui He",
            "hIndex": 57
        },
        "Mingjie Zhou": {
            "authorId": "2314282",
            "name": "Mingjie Zhou",
            "hIndex": 17
        },
        "Kejun Chen": {
            "authorId": "48543346",
            "name": "Kejun Chen",
            "hIndex": 4
        },
        "Long Quan": {
            "authorId": "2108646338",
            "name": "Quanlong Li",
            "hIndex": 17
        },
        "Arthur Hemmer": {
            "authorId": "2221011761",
            "name": "Arthur Hemmer",
            "hIndex": 1
        },
        "Peter Fonagy": {
            "authorId": "3177814",
            "name": "P. Fonagy",
            "hIndex": 126
        },
        "John Lacalamita": {
            "authorId": "117302834",
            "name": "John Lacalamita",
            "hIndex": 1
        },
        "Quanqing Xu": {
            "authorId": "2237081",
            "name": "Quanqing Xu",
            "hIndex": 21
        },
        "Zeyu Lu": {
            "authorId": "46951079",
            "name": "Zeyu Lu",
            "hIndex": 36
        },
        "David Wadden": {
            "authorId": "30051202",
            "name": "David Wadden",
            "hIndex": 15
        },
        "Omid Mohareri": {
            "authorId": "2146343",
            "name": "O. Mohareri",
            "hIndex": 14
        },
        "Xusheng Dai": {
            "authorId": "30772532",
            "name": "Xusheng Dai",
            "hIndex": 4
        },
        "Yihao Feng": {
            "authorId": "22758695",
            "name": "Yihao Feng",
            "hIndex": 16
        },
        "Wenqi Ren": {
            "authorId": "2260700948",
            "name": "Wenqi Ren",
            "hIndex": 3
        },
        "Zhewei Huang": {
            "authorId": "47272086",
            "name": "Zhewei Huang",
            "hIndex": 10
        },
        "Cornelia Sindermann": {
            "authorId": "5408820",
            "name": "C. Sindermann",
            "hIndex": 27
        },
        "Romina Esbati": {
            "authorId": "2123360098",
            "name": "Romina Esbati",
            "hIndex": 5
        },
        "Guillaume Lajoie": {
            "authorId": "49921594",
            "name": "Guillaume Lajoie",
            "hIndex": 17
        },
        "Di Huang": {
            "authorId": "47817823",
            "name": "D. Huang",
            "hIndex": 10
        },
        "Atharva Vaidya": {
            "authorId": "2128278515",
            "name": "Atharva Vaidya",
            "hIndex": 1
        },
        "Aleksander Madry": {
            "authorId": "143826246",
            "name": "A. Madry",
            "hIndex": 51
        },
        "Erfan Taherifard": {
            "authorId": "1394536303",
            "name": "Erfan Taherifard",
            "hIndex": 7
        },
        "Yan Rong": {
            "authorId": "23132530",
            "name": "Rong-rong Yan",
            "hIndex": 7
        },
        "Shubham Singh": {
            "authorId": "2118408336",
            "name": "S. Singh",
            "hIndex": 6
        },
        "Qiben Yan": {
            "authorId": "2480351",
            "name": "Qiben Yan",
            "hIndex": 24
        },
        "Shashank Tripathi": {
            "authorId": "153759551",
            "name": "S. Tripathi",
            "hIndex": 21
        },
        "Xiaolong Luo": {
            "authorId": "67001617",
            "name": "Lu Xiaolong",
            "hIndex": 6
        },
        "Haowei Yang": {
            "authorId": "2115537643",
            "name": "Haowei Yang",
            "hIndex": 5
        },
        "Saghir Alfasly": {
            "authorId": "117737459",
            "name": "Saghir Alfasly",
            "hIndex": 7
        },
        "Xiang Ren": {
            "authorId": "5820406",
            "name": "Xiangliang Ren",
            "hIndex": 11
        },
        "Xilin Chen": {
            "authorId": "48283638",
            "name": "Xilin Chen",
            "hIndex": 35
        },
        "Oleksandr Zinenko": {
            "authorId": "144895391",
            "name": "O. Zinenko",
            "hIndex": 14
        },
        "Sihan Ren": {
            "authorId": "2107932889",
            "name": "S. Lin",
            "hIndex": 1
        },
        "Prerak Mody": {
            "authorId": "70070271",
            "name": "P. Mody",
            "hIndex": 4
        },
        "SangHak Yi": {
            "authorId": "35817703",
            "name": "Sang\u2010Hoon Yi",
            "hIndex": 15
        },
        "Shelby Heinecke": {
            "authorId": "71926704",
            "name": "Shelby Heinecke",
            "hIndex": 10
        },
        "Ching-Yuan Chen": {
            "authorId": "5916599",
            "name": "Chen-Ching Yuan",
            "hIndex": 14
        },
        "Indika Rajapakse": {
            "authorId": "3134436",
            "name": "I. Rajapakse",
            "hIndex": 22
        },
        "Stephanie Elena Crowe": {
            "authorId": "2283068392",
            "name": "Stephanie Elena Crowe",
            "hIndex": 1
        },
        "Xiang Wan": {
            "authorId": "144830136",
            "name": "X. Wan",
            "hIndex": 32
        },
        "Mukul Lokhande": {
            "authorId": "2314693841",
            "name": "Mukul Lokhande",
            "hIndex": 0
        },
        "Yiqi Zhong": {
            "authorId": "148419708",
            "name": "Yiqi Zhong",
            "hIndex": 10
        },
        "Jianliang Xu": {
            "authorId": "2144296960",
            "name": "Jianliang Xu",
            "hIndex": 5
        },
        "Aiwei Liu": {
            "authorId": "10017193",
            "name": "Aiwei Liu",
            "hIndex": 9
        },
        "Ming Fan": {
            "authorId": "2054875771",
            "name": "Mingming Fan",
            "hIndex": 6
        },
        "Parisa Kordjamshidi": {
            "authorId": "2190934",
            "name": "Parisa Kordjamshidi",
            "hIndex": 18
        },
        "Rohit Kundu": {
            "authorId": "1380458205",
            "name": "Rohit Kundu",
            "hIndex": 13
        },
        "Jahyun Koo": {
            "authorId": "46500875",
            "name": "J. Koo",
            "hIndex": 6
        },
        "Yuxiang Wei": {
            "authorId": "2156252416",
            "name": "Yuxiang Wei",
            "hIndex": 8
        },
        "Mohim Tash": {
            "authorId": "119458504",
            "name": "Mohim Tash",
            "hIndex": 2
        },
        "Bocheng Chen": {
            "authorId": "2001273",
            "name": "Bocheng Chen",
            "hIndex": 6
        },
        "Seunghee Han": {
            "authorId": "2115753068",
            "name": "Seunghee Han",
            "hIndex": 11
        },
        "Tommaso Caselli": {
            "authorId": "1864635",
            "name": "Tommaso Caselli",
            "hIndex": 22
        },
        "Hanna Martin": {
            "authorId": "143997470",
            "name": "H. Martin",
            "hIndex": 7
        },
        "Jesse Wright": {
            "authorId": "145230056",
            "name": "Jesse H. Wright",
            "hIndex": 20
        },
        "Fan Zhang": {
            "authorId": "12223570",
            "name": "Fanhua Zhang",
            "hIndex": 11
        },
        "Klaus Hildebrandt": {
            "authorId": "145059537",
            "name": "K. Hildebrandt",
            "hIndex": 22
        },
        "Juan Andrade-Cetto": {
            "authorId": "1400165226",
            "name": "J. Andrade-Cetto",
            "hIndex": 33
        },
        "Stefan Feuerriegel": {
            "authorId": "3207649",
            "name": "S. Feuerriegel",
            "hIndex": 37
        },
        "Hao Li": {
            "authorId": "144813683",
            "name": "L. Hao",
            "hIndex": 15
        },
        "Haoling Qiu": {
            "authorId": "1817035",
            "name": "Haoling Qiu",
            "hIndex": 5
        },
        "Chikahito Nakane": {
            "authorId": "2320151852",
            "name": "Chikahito Nakane",
            "hIndex": 0
        },
        "Qin Chen": {
            "authorId": "47260981",
            "name": "Qinqin Chen",
            "hIndex": 27
        },
        "Michel Steuwer": {
            "authorId": "1795890",
            "name": "Michel Steuwer",
            "hIndex": 17
        },
        "Chamath Palihawadana": {
            "authorId": "116301958",
            "name": "Chamath Palihawadana",
            "hIndex": 5
        },
        "Jinkun Chen": {
            "authorId": "2108407544",
            "name": "Jinkun Chen",
            "hIndex": 15
        },
        "Ian Smith": {
            "authorId": "89343054",
            "name": "I. Smith",
            "hIndex": 19
        },
        "Zhouhong Gu": {
            "authorId": "2160631240",
            "name": "Zhouhong Gu",
            "hIndex": 5
        },
        "Liang Zhangd": {
            "authorId": "2320152901",
            "name": "Liang Zhangd",
            "hIndex": 0
        },
        "Evan Wang": {
            "authorId": "23749516",
            "name": "Evan W. Wang",
            "hIndex": 8
        },
        "Siheng Xiong": {
            "authorId": "1739023723",
            "name": "Siheng Xiong",
            "hIndex": 6
        },
        "Yamuna Prasad": {
            "authorId": "153423142",
            "name": "Y. Singh",
            "hIndex": 8
        },
        "Jingtao Ding": {
            "authorId": "2470644",
            "name": "Jingtao Ding",
            "hIndex": 17
        },
        "Yong Zhang": {
            "authorId": "2144287980",
            "name": "Yong Zhang",
            "hIndex": 64
        },
        "Will Song": {
            "authorId": "1410015319",
            "name": "W. Song",
            "hIndex": 2
        },
        "Nicholas Lee": {
            "authorId": "50705885",
            "name": "N. Lee",
            "hIndex": 28
        },
        "Lu He": {
            "authorId": "11311138",
            "name": "Hejun Lu",
            "hIndex": 7
        },
        "Zhongfeng Wang": {
            "authorId": "47196693",
            "name": "Zhongfeng Wang",
            "hIndex": 35
        },
        "Ivan P. Yamshchikov": {
            "authorId": "3380587",
            "name": "Ivan P. Yamshchikov",
            "hIndex": 9
        },
        "Runlong Zhou": {
            "authorId": "2080124819",
            "name": "Runlong Zhou",
            "hIndex": 3
        },
        "Emmanouil Panagiotou": {
            "authorId": "1807479492",
            "name": "E. Panagiotou",
            "hIndex": 4
        },
        "Yinghui Xu": {
            "authorId": "50125801",
            "name": "Yinghui Xu",
            "hIndex": 19
        },
        "Mingze Gao": {
            "authorId": "46573210",
            "name": "M. Gao",
            "hIndex": 7
        },
        "Kristina Toutanova": {
            "authorId": "3259253",
            "name": "Kristina Toutanova",
            "hIndex": 46
        },
        "Zijian Zhao": {
            "authorId": "2117927545",
            "name": "Zijian Zhao",
            "hIndex": 11
        },
        "Yi Liu": {
            "authorId": "3286022",
            "name": "Yi Liu",
            "hIndex": 24
        },
        "Yunfeng Diao": {
            "authorId": "32404738",
            "name": "Yun-feng Diao",
            "hIndex": 7
        },
        "Michael Matthews": {
            "authorId": "40500273",
            "name": "M. Matthews",
            "hIndex": 22
        },
        "Jinglei Tao": {
            "authorId": "145903450",
            "name": "Jing Wu",
            "hIndex": 52
        },
        "Shunichi Koshimura": {
            "authorId": "48696510",
            "name": "S. Koshimura",
            "hIndex": 39
        },
        "Qianjun Pan": {
            "name": "Qianjun Pan",
            "hIndex": 0
        },
        "Aditya Joshi": {
            "authorId": "18257401",
            "name": "A. Joshi",
            "hIndex": 26
        },
        "Jian Shao": {
            "authorId": "39076407",
            "name": "J. Shao",
            "hIndex": 23
        },
        "Co\u015fku Acay": {
            "authorId": "9550400",
            "name": "Cosku Acay",
            "hIndex": 2
        },
        "Guoyao Yu": {
            "authorId": "14938931",
            "name": "Guoyao Yu",
            "hIndex": 19
        },
        "Wenjie Jacky Mo": {
            "authorId": "2266978162",
            "name": "W. Mo",
            "hIndex": 2
        },
        "Kai Shu": {
            "name": "Kai Shu",
            "hIndex": 0
        },
        "Keqing He": {
            "authorId": "2793678",
            "name": "K. He",
            "hIndex": 18
        },
        "Aditya Chandrasekar": {
            "authorId": "32173131",
            "name": "A. Chandrasekar",
            "hIndex": 4
        },
        "Zixuan Guo": {
            "authorId": "153757327",
            "name": "Zi-Rao Guo",
            "hIndex": 8
        },
        "Lijun Deng": {
            "authorId": "48633021",
            "name": "L. Deng",
            "hIndex": 21
        },
        "Jing Tao": {
            "authorId": "48572843",
            "name": "J. Tao",
            "hIndex": 18
        },
        "Jiansheng Chen": {
            "authorId": "1752427",
            "name": "Jiansheng Chen",
            "hIndex": 16
        },
        "Xiongkun Linghu": {
            "authorId": "2170539011",
            "name": "Xiongkun Linghu",
            "hIndex": 1
        },
        "Kazuki Hayashi": {
            "authorId": "2087025540",
            "name": "K. Hayashi",
            "hIndex": 5
        },
        "Liang Geng": {
            "authorId": "12995737",
            "name": "Lianggeng Gong",
            "hIndex": 7
        },
        "Ahmad Farooq": {
            "authorId": "2088138377",
            "name": "H. F. Ahmad",
            "hIndex": 18
        },
        "Yiwen Peng": {
            "name": "Yiwen Peng",
            "hIndex": 0
        },
        "Yulan Hu": {
            "authorId": "1884372",
            "name": "Yu\u2010Lan Hu",
            "hIndex": 18
        },
        "Giordan Escalona": {
            "authorId": "2313642965",
            "name": "Giordan Escalona",
            "hIndex": 0
        },
        "Zhiyuan Hu": {
            "authorId": "145473299",
            "name": "Zhiyuan Hu",
            "hIndex": 44
        },
        "Jaeyeon Kim": {
            "authorId": "2109223569",
            "name": "Jaeyeon Kim",
            "hIndex": 2
        },
        "Danial Samiei Nasr": {
            "authorId": "2088631356",
            "name": "Danial Samiei Nasr",
            "hIndex": 1
        },
        "Komal Gupta": {
            "authorId": "2117809853",
            "name": "K. P. Gupta",
            "hIndex": 8
        },
        "Florian Delerue": {
            "authorId": "6281158",
            "name": "Florian Delerue",
            "hIndex": 10
        },
        "Matt Welsh": {
            "authorId": "47673206",
            "name": "Matthew Welsh",
            "hIndex": 9
        },
        "Wu Guo": {
            "authorId": "2118063651",
            "name": "Guo-zhong Wu",
            "hIndex": 13
        },
        "Andrea Cavallo": {
            "authorId": "37783905",
            "name": "A. Cavallo",
            "hIndex": 26
        },
        "Julen Urain": {
            "authorId": "51233860",
            "name": "Julen Urain",
            "hIndex": 9
        },
        "Falk Scholer": {
            "authorId": "1732541",
            "name": "Falk Scholer",
            "hIndex": 32
        },
        "Fei Gao": {
            "authorId": "3461557",
            "name": "F. Gao",
            "hIndex": 23
        },
        "Wei Kang": {
            "authorId": "152986072",
            "name": "W. Kang",
            "hIndex": 11
        },
        "Byunggun Kim": {
            "authorId": "2151899594",
            "name": "Byunggun Kim",
            "hIndex": 2
        },
        "Yuan Gao": {
            "authorId": "2145975478",
            "name": "Yuanning Gao",
            "hIndex": 20
        },
        "Yijian Lu": {
            "authorId": "2143442343",
            "name": "Yijian Lu",
            "hIndex": 3
        },
        "Yan Yan": {
            "authorId": "40353534",
            "name": "Yan Yan",
            "hIndex": 19
        },
        "Mike Zhang": {
            "authorId": "9385577",
            "name": "M. Zhang",
            "hIndex": 9
        },
        "Per Nyman": {
            "authorId": "143639116",
            "name": "P. Nyman",
            "hIndex": 29
        },
        "Blake Wulfe": {
            "authorId": "9414028",
            "name": "Blake Wulfe",
            "hIndex": 9
        },
        "Moein Shahiki Tash": {
            "authorId": "2192603925",
            "name": "M. Tash",
            "hIndex": 4
        },
        "Anthony Tomasic": {
            "authorId": "1693125",
            "name": "A. Tomasic",
            "hIndex": 30
        },
        "Tuong Vy Nguyen": {
            "authorId": "1576151271",
            "name": "Tuong-Vy Nguyen",
            "hIndex": 3
        },
        "Xiang Li": {
            "authorId": "123698321",
            "name": "Li-xiang Li",
            "hIndex": 11
        },
        "Hemanth Kandula": {
            "authorId": "52225379",
            "name": "H. Kandula",
            "hIndex": 11
        },
        "Yun Cheng": {
            "authorId": "2128663737",
            "name": "Chengyun Li",
            "hIndex": 10
        },
        "Amirreza Allahgholipour": {
            "authorId": "2123704118",
            "name": "Amirreza Allahgholipour",
            "hIndex": 2
        },
        "Debjani Chakraborty": {
            "authorId": "1693883",
            "name": "Debjani Chakraborty",
            "hIndex": 23
        },
        "Etai Littwin": {
            "authorId": "1762320",
            "name": "Etai Littwin",
            "hIndex": 10
        },
        "Zheng Shi": {
            "authorId": "5262882",
            "name": "Zheng-zheng Shi",
            "hIndex": 24
        },
        "Liangbo Ning": {
            "authorId": "2168924373",
            "name": "Liangbo Ning",
            "hIndex": 3
        },
        "Sheng Zha": {
            "authorId": "9779709",
            "name": "C. Zha",
            "hIndex": 38
        },
        "Lauren T. May": {
            "authorId": "39815383",
            "name": "L. May",
            "hIndex": 36
        },
        "Vlado Menkovski": {
            "authorId": "2266428",
            "name": "Vlado Menkovski",
            "hIndex": 19
        },
        "Tong Jia": {
            "authorId": "120260782",
            "name": "Tongtong Jia",
            "hIndex": 10
        },
        "Weinan Zhang": {
            "authorId": "2108309290",
            "name": "Weina Zhang",
            "hIndex": 13
        },
        "Jeongsoo Kim": {
            "authorId": "5818946",
            "name": "Jeongsoon Kim",
            "hIndex": 11
        },
        "Mingyang Li": {
            "authorId": "2108932454",
            "name": "Ming-yang Li",
            "hIndex": 7
        },
        "Sara Roos-Hoefgeest": {
            "authorId": "2139917973",
            "name": "Sara Roos-Hoefgeest Toribio",
            "hIndex": 2
        },
        "Albert Gatt": {
            "authorId": "1700894",
            "name": "Albert Gatt",
            "hIndex": 29
        },
        "Guanyu Zhou": {
            "authorId": "47260914",
            "name": "Guanyu Zhou",
            "hIndex": 16
        },
        "Yixuan Chen": {
            "authorId": "92709226",
            "name": "Yixuan Chen",
            "hIndex": 11
        },
        "Chin-Chia Michael Yeh": {
            "authorId": "3056465",
            "name": "Chin-Chia Michael Yeh",
            "hIndex": 22
        },
        "Limin Wang": {
            "authorId": "2141353051",
            "name": "Li-Min Wang",
            "hIndex": 13
        },
        "Beilong Tang": {
            "authorId": "2284594956",
            "name": "Beilong Tang",
            "hIndex": 1
        },
        "Ali Moradi": {
            "authorId": "4917457",
            "name": "A. Moradi",
            "hIndex": 26
        },
        "Xiping Hu": {
            "authorId": "1718919",
            "name": "Xiping Hu",
            "hIndex": 38
        },
        "Jai Bardhan": {
            "authorId": "2193550968",
            "name": "Jai Bardhan",
            "hIndex": 2
        },
        "Chang Xu": {
            "authorId": "46748110",
            "name": "Changjie Xu",
            "hIndex": 28
        },
        "Xiangyang Liu": {
            "authorId": "2144225907",
            "name": "Xiangyang Liu",
            "hIndex": 9
        },
        "Zhangsihao Yang": {
            "authorId": "51126028",
            "name": "Zhangsihao Yang",
            "hIndex": 4
        },
        "Yubo Wang": {
            "authorId": "2115721458",
            "name": "Yubo Wang",
            "hIndex": 11
        },
        "Paul F Kinghorn": {
            "authorId": "2101910547",
            "name": "Paul F Kinghorn",
            "hIndex": 3
        },
        "Junran Peng": {
            "authorId": "34735086",
            "name": "Junran Peng",
            "hIndex": 11
        },
        "Haijun Wang": {
            "authorId": "47273397",
            "name": "Wang Haijun",
            "hIndex": 10
        },
        "Alexandru Vasilache": {
            "authorId": "97804729",
            "name": "C. A. Vasilache",
            "hIndex": 1
        },
        "Brenden M. Lake": {
            "authorId": "2373318",
            "name": "B. Lake",
            "hIndex": 27
        },
        "Ray Umphrey": {
            "authorId": "2319408254",
            "name": "Ray Umphrey",
            "hIndex": 0
        },
        "Zehao Xiao": {
            "authorId": "152287370",
            "name": "Zehao Xiao",
            "hIndex": 13
        },
        "Leonard Lausen": {
            "authorId": "8789103",
            "name": "Leonard Lausen",
            "hIndex": 7
        },
        "Yiu-ming Cheung": {
            "authorId": "143840540",
            "name": "Yiu-ming Cheung",
            "hIndex": 42
        },
        "Ping Li": {
            "authorId": "2109069105",
            "name": "Ping Li",
            "hIndex": 10
        },
        "Philippe J. Giabbanelli": {
            "authorId": "3291438",
            "name": "P. Giabbanelli",
            "hIndex": 22
        },
        "Chao Liang": {
            "authorId": "49057208",
            "name": "C. Liang",
            "hIndex": 8
        },
        "Prateek Gupta": {
            "authorId": "2130387807",
            "name": "Prateek K. Gupta",
            "hIndex": 25
        },
        "Peter Barnett": {
            "authorId": "40251764",
            "name": "P. Barnett",
            "hIndex": 19
        },
        "Mingming Sun": {
            "authorId": "49632879",
            "name": "Ming-ming Sun",
            "hIndex": 9
        },
        "Keer Lu": {
            "authorId": "2191071973",
            "name": "Keer Lu",
            "hIndex": 1
        },
        "Prathapa Janitha": {
            "authorId": "2282541073",
            "name": "Prathapa Janitha",
            "hIndex": 0
        },
        "Weiwen Liu": {
            "authorId": "120639936",
            "name": "Weiwen Liu",
            "hIndex": 7
        },
        "Suman Banerjee": {
            "authorId": "2111264334",
            "name": "Suman Banerjee",
            "hIndex": 57
        },
        "Ansh Sharma": {
            "authorId": "2109671557",
            "name": "Anshujit Sharma",
            "hIndex": 4
        },
        "Sachindra Joshi": {
            "name": "Sachindra Joshi",
            "hIndex": 0
        },
        "Jieping Ye": {
            "authorId": "2268645316",
            "name": "Jieping Ye",
            "hIndex": 3
        },
        "Bo Wang": {
            "authorId": "49292465",
            "name": "Bosheng Wang",
            "hIndex": 13
        },
        "David Chu": {
            "authorId": "49768807",
            "name": "D. Chu",
            "hIndex": 10
        },
        "Rushil Gupta": {
            "authorId": "2119758034",
            "name": "Rushil Gupta",
            "hIndex": 3
        },
        "Yoshitaka Ushiku": {
            "authorId": "3250559",
            "name": "Y. Ushiku",
            "hIndex": 23
        },
        "Yi Luo": {
            "authorId": "46491610",
            "name": "Yi Luo",
            "hIndex": 22
        },
        "Zeyu Gao": {
            "authorId": "152982249",
            "name": "J. Gao",
            "hIndex": 10
        },
        "Katsuhiko Hayashi": {
            "authorId": "145245267",
            "name": "K. Hayashi",
            "hIndex": 39
        },
        "Hongyuan Su": {
            "authorId": "40319595",
            "name": "H. Ling",
            "hIndex": 8
        },
        "Soham Chatterjee": {
            "authorId": "50342965",
            "name": "S. Chatterjee",
            "hIndex": 8
        },
        "Libo Qin": {
            "authorId": "47657628",
            "name": "Libo Qin",
            "hIndex": 8
        },
        "Yanxu Chen": {
            "authorId": "117744870",
            "name": "Yanxu Chen",
            "hIndex": 11
        },
        "Haoyue Zhan": {
            "authorId": "2134590693",
            "name": "Haoyue Zhan",
            "hIndex": 2
        },
        "Sonja Mathes": {
            "authorId": "2141643832",
            "name": "S. Mathes",
            "hIndex": 4
        },
        "Zuhair Zafar": {
            "authorId": "32436099",
            "name": "Zuhair Zafar",
            "hIndex": 4
        },
        "Inbal Nahum-Shani": {
            "authorId": "1397927581",
            "name": "I. Nahum-Shani",
            "hIndex": 33
        },
        "Shankar Sastry": {
            "authorId": "2256610869",
            "name": "S. Sastry",
            "hIndex": 7
        },
        "Defu Lian": {
            "authorId": "1862782",
            "name": "Defu Lian",
            "hIndex": 37
        },
        "S\u00e9amus Lankford": {
            "authorId": "2040096928",
            "name": "S\u00e9amus Lankford",
            "hIndex": 5
        },
        "Manxi Wu": {
            "authorId": "2220759",
            "name": "Manxi Wu",
            "hIndex": 9
        },
        "Hao Shi": {
            "authorId": "80639781",
            "name": "Haohao Shi",
            "hIndex": 14
        },
        "Zhejian Yang": {
            "authorId": "121937496",
            "name": "Zhejian Yang",
            "hIndex": 1
        },
        "Zhiyong Cui": {
            "authorId": "145106761",
            "name": "Zhiyong Cui",
            "hIndex": 20
        },
        "Jakob Getz": {
            "authorId": "2313444956",
            "name": "Jakob Getz",
            "hIndex": 0
        },
        "Bo-Ru Lu": {
            "authorId": "39872583",
            "name": "M. Wang",
            "hIndex": 89
        },
        "Chenghao Qian": {
            "authorId": "2082474028",
            "name": "Chenghao Qian",
            "hIndex": 3
        },
        "Julia Roeb": {
            "authorId": "15620276",
            "name": "J. Roeb",
            "hIndex": 4
        },
        "Miki Haseyama": {
            "authorId": "144029207",
            "name": "M. Haseyama",
            "hIndex": 23
        },
        "Zara Wudiri": {
            "authorId": "79849252",
            "name": "Zara Wudiri",
            "hIndex": 3
        },
        "Skyler Seto": {
            "authorId": "31855650",
            "name": "Skyler Seto",
            "hIndex": 5
        },
        "Zhao Zhang": {
            "authorId": "92536772",
            "name": "Zhaoen Zhang",
            "hIndex": 11
        },
        "Xinyi Shen": {
            "authorId": "48946923",
            "name": "Xinyi Shen",
            "hIndex": 22
        },
        "Shucheng Li": {
            "authorId": "2109103948",
            "name": "Shucheng Li",
            "hIndex": 5
        },
        "Jianwei Yu": {
            "authorId": "49402813",
            "name": "J. Yu",
            "hIndex": 16
        },
        "Jinpeng Hu": {
            "authorId": "2118516991",
            "name": "Jinpeng Hu",
            "hIndex": 9
        },
        "Qihang Lin": {
            "name": "Qihang Lin",
            "hIndex": 0
        },
        "Sonja Schmer-Galunder": {
            "authorId": "1403006514",
            "name": "S. Schmer-Galunder",
            "hIndex": 6
        },
        "George Em Karniadakis": {
            "authorId": "1720124",
            "name": "G. Karniadakis",
            "hIndex": 128
        },
        "Sriram Veturi": {
            "authorId": "2319823357",
            "name": "Sriram Veturi",
            "hIndex": 0
        },
        "Srinidhi Nagendra": {
            "authorId": "145369036",
            "name": "S. Nagendra",
            "hIndex": 1
        },
        "Sen Shen": {
            "authorId": "7333309",
            "name": "Zhisen Shen",
            "hIndex": 22
        },
        "Dohee Kim": {
            "authorId": "122204581",
            "name": "Do-Hyun Kim",
            "hIndex": 8
        },
        "Wenwu Guo": {
            "authorId": "2552988",
            "name": "Wenwu Guo",
            "hIndex": 35
        },
        "Yuzhuo Jia": {
            "authorId": "98579201",
            "name": "Y. Jia",
            "hIndex": 2
        },
        "Ulrich A. Schatz": {
            "authorId": "47733178",
            "name": "U. Schatz",
            "hIndex": 9
        },
        "Xipeng Qiu": {
            "authorId": "2188058565",
            "name": "Xipeng Qiu",
            "hIndex": 13
        },
        "Dan Wu": {
            "authorId": "144770006",
            "name": "Dandan Wu",
            "hIndex": 23
        },
        "Guan-Ting Lin": {
            "authorId": "5944313",
            "name": "Guangxin Lin",
            "hIndex": 8
        },
        "Rosanne Liu": {
            "authorId": "48757909",
            "name": "Rosanne Liu",
            "hIndex": 14
        },
        "Anders Ynnerman": {
            "authorId": "1697451",
            "name": "A. Ynnerman",
            "hIndex": 34
        },
        "Jun Liu": {
            "authorId": "49723000",
            "name": "Junjun Liu",
            "hIndex": 35
        },
        "Raj Abhijit Dandekar": {
            "name": "Raj Abhijit Dandekar",
            "hIndex": 0
        },
        "Cristian Gariboldi": {
            "authorId": "2319605239",
            "name": "Cristian Gariboldi",
            "hIndex": 0
        },
        "Wenchuan Yang": {
            "authorId": "2625332",
            "name": "Wenchuan Yang",
            "hIndex": 5
        },
        "Haoran Zhang": {
            "authorId": "2144613384",
            "name": "Hao Zhang",
            "hIndex": 8
        },
        "Songning Lai": {
            "authorId": "2185295161",
            "name": "Songning Lai",
            "hIndex": 5
        },
        "Muhammad Sonny Abfertiawan": {
            "name": "Muhammad Sonny Abfertiawan",
            "hIndex": 0
        },
        "Eric Zhang": {
            "authorId": "11738886",
            "name": "E. Zhang",
            "hIndex": 10
        },
        "Ingkarat Rak-amnouykit": {
            "authorId": "2028216186",
            "name": "Ingkarat Rak-amnouykit",
            "hIndex": 2
        },
        "Hongyu Lin": {
            "authorId": "6958900",
            "name": "L. Hongyu",
            "hIndex": 3
        },
        "Abdullah Hamdi": {
            "authorId": "1409309753",
            "name": "A. Al-Hamdi",
            "hIndex": 12
        },
        "Fei Liu": {
            "authorId": "2136134607",
            "name": "Fei-fei Liu",
            "hIndex": 22
        },
        "Ryan A. Rossi": {
            "authorId": "1862090",
            "name": "Ryan A. Rossi",
            "hIndex": 35
        },
        "Ian McQuillan": {
            "name": "Ian McQuillan",
            "hIndex": 0
        },
        "Md Mehrab Tanjim": {
            "authorId": "35631602",
            "name": "Md. Mehrab Tanjim",
            "hIndex": 5
        },
        "Barys Liskavets": {
            "authorId": "2319414392",
            "name": "Barys Liskavets",
            "hIndex": 0
        },
        "Luke Smith": {
            "authorId": "2010641",
            "name": "Luke D. Smith",
            "hIndex": 13
        },
        "Witold Pedrycz": {
            "authorId": "1731634",
            "name": "W. Pedrycz",
            "hIndex": 119
        },
        "Bhuvana Ramabhadran": {
            "authorId": "1720857",
            "name": "B. Ramabhadran",
            "hIndex": 49
        },
        "Daoqi Liu": {
            "authorId": "96816482",
            "name": "Liu Daoqi",
            "hIndex": 3
        },
        "D. G. Sotiropoulos": {
            "authorId": "144777218",
            "name": "D. G. Sotiropoulos",
            "hIndex": 9
        },
        "Yi Yang": {
            "authorId": "92871258",
            "name": "Yangyi Yang",
            "hIndex": 27
        },
        "Johan Mathe": {
            "authorId": "41151846",
            "name": "Johan Mathe",
            "hIndex": 7
        },
        "Mark Billinghurst": {
            "authorId": "1684805",
            "name": "M. Billinghurst",
            "hIndex": 83
        },
        "Van Ho Long": {
            "authorId": "35836332",
            "name": "L. Ho",
            "hIndex": 7
        },
        "Chengxi Zhang": {
            "authorId": "103112869",
            "name": "Cheng Zhang",
            "hIndex": 5
        }
    }
}
